{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e54053b",
   "metadata": {},
   "source": [
    "## Ê¶ÇË¶Å"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53236702",
   "metadata": {},
   "source": [
    "Open-Sora 2.0„ÅØ„ÄÅ3000‰∏áÂÜÜ„ÅßÂÆüÁèæÂèØËÉΩ„Å™ÂïÜÁî®„É¨„Éô„É´„ÅÆÂãïÁîªÁîüÊàê„É¢„Éá„É´\n",
    "\n",
    "Ë®ìÁ∑¥„Ç≥„Çπ„Éà„ÅØ„ÄÅÂêåÁ≠â„ÅÆ„É¢„Éá„É´ÔºàMovieGen„ÇÑStep-Video-T2VÔºâ„Çà„Çä„ÇÇ5~10ÂÄç‰Ωé„ÅÑ\n",
    "\n",
    "‰∫∫„ÅÆË©ï‰æ°„Å®VBench„ÅÆ„Çπ„Ç≥„Ç¢„Åß„ÅØ„ÄÅHuyyuan Video„ÇÑRunway Gen-3 Alpha„Å´ÂåπÊïµ:\n",
    "\n",
    "![](image/fig1.png)\n",
    "\n",
    "- Visual Quality: Ë¶ñË¶öÂìÅË≥™\n",
    "- Prompt Following: ÊåáÁ§∫ËøΩÂæìÊÄß\n",
    "- Motion Quality: Âãï„Åç„ÅÆÂìÅË≥™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12386bd",
   "metadata": {},
   "source": [
    "## „Éá„Éº„Çø"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1780041",
   "metadata": {},
   "source": [
    "ÁõÆÁöÑ„ÅØ„ÄÅÂ≠¶Áøí„ÅÆÈÄ≤Êçó„Å´Âêà„Çè„Åõ„Åü„Éá„Éº„Çø„Éî„É©„Éü„ÉÉ„ÉâÔºàhierarchical data pyramidÔºâ„ÅÆÊßãÁØâ\n",
    "\n",
    "Êßò„ÄÖ„Å™Á®ÆÈ°û„ÅÆ„Éá„Éº„Çø„ÇíÊ§úÂá∫ÂèØËÉΩ„Å™„Å™„Éï„Ç£„É´„Çø„ÇíÈñãÁô∫\n",
    "\n",
    "Â≠¶Áøí„ÅÆÈÄ≤Êçó„Å´Âøú„Åò„Å¶„ÄÅ„Éï„Ç£„É´„Çø„É™„É≥„Ç∞„ÅÆÂº∑Â∫¶„ÇíÈ´ò„ÇÅ„ÄÅÁ¥îÂ∫¶„Å®ÂìÅË≥™„ÅÆÈ´ò„ÅÑÂ∞è„Åï„ÅÑ„Çµ„Éñ„Çª„ÉÉ„Éà„ÅßË®ìÁ∑¥\n",
    "\n",
    "„Éá„Éº„Çø„Éï„Ç£„É´„Çø„É™„É≥„Ç∞„ÅÆÂÖ®‰ΩìÂÉè:\n",
    "\n",
    "![](image/fig2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7093517c",
   "metadata": {},
   "source": [
    "- Á¥´: Áîü„ÅÆÂãïÁîª„ÅÆÂâçÂá¶ÁêÜ\n",
    "    1. „Éï„Ç£„É´„Çø„É™„É≥„Ç∞\n",
    "        - Á†¥Êêç„Åó„Åü„Éï„Ç°„Ç§„É´„ÅÆÈô§Âéª\n",
    "        - Ê•µÁ´Ø„Å™ÂãïÁîª„ÅÆÈô§Âéª\n",
    "            - ÂÜçÁîüÊôÇÈñì„Åå2ÁßíÊú™Ê∫Ä\n",
    "            - 1ÁîªÂÉè„ÅÇ„Åü„Çä„ÅÆ„Éá„Éº„ÇøÈáèÔºàBit per pixelÔºâ„Åå0.02Êú™Ê∫Ä\n",
    "            - „Éï„É¨„Éº„É†„É¨„Éº„ÉàÔºàfpsÔºâ„Åå16Êú™Ê∫Ä\n",
    "            - „Ç¢„Çπ„Éö„ÇØ„ÉàÊØî„ÅåÁØÑÂõ≤Â§ñÔºà1/3, 3Ôºâ\n",
    "            - ÁâπÂÆö„ÅÆ‰ΩéÂìÅË≥™„Å™„Ç®„É≥„Ç≥„Éº„ÉâË®≠ÂÆöÔºàConstrained Baseline profileÔºâ\n",
    "    2. ÈÄ£Á∂ö„Åó„ÅüÊò†ÂÉè„ÇíÊ§úÂá∫„Åó„ÄÅÁü≠„ÅÑ„ÇØ„É™„ÉÉ„Éó„Å´ÂàÜÂâ≤\n",
    "        - FFmpeg„ÅÆlibavfilter„Çí‰ΩøÁî®„Åó„ÄÅ„Ç∑„Éº„É≥„Çπ„Ç≥„Ç¢Ôºà„Éï„É¨„Éº„É†Èñì„ÅÆË¶ñË¶öÂ∑ÆÂàÜÔºâ„ÇíË®àÁÆó\n",
    "    3. ÂãïÁîª„ÅÆ„Éï„Ç©„Éº„Éû„ÉÉ„Éà\n",
    "        - „Éï„É¨„Éº„É†„É¨„Éº„Éà„ÅØ30fps‰ª•‰∏ã\n",
    "        - Èï∑Ëæ∫„ÅØ1080px‰ª•‰∏ã\n",
    "        - „Ç≥„Éº„Éá„ÉÉ„ÇØÔºàÂúßÁ∏ÆÂΩ¢ÂºèÔºâ„ÅØH.264\n",
    "        - ÂãïÁîª„ÅÆÈªíÂ∏Ø„ÇíÂâäÈô§\n",
    "        - 8Áßí„ÇíË∂Ö„Åà„Çã„Ç∑„Éß„ÉÉ„Éà„ÅØ„ÄÅ8Áßí„ÅÆ„ÇØ„É™„ÉÉ„Éó„Å´ÂàÜÂâ≤„Åó„ÄÅ2ÁßíÊú™Ê∫Ä„ÅØÁ†¥Ê£Ñ\n",
    "- Èùí: „ÇØ„É™„ÉÉ„ÉóÂãïÁîª„ÅÆ„Çπ„Ç≥„Ç¢„Éï„Ç£„É´„Çø„É™„É≥„Ç∞\n",
    "    - ÁæéÁöÑ„Çπ„Ç≥„Ç¢„Åß„ÅÆ„Éï„Ç£„É´„Çø„É™„É≥„Ç∞\n",
    "        - [CLIP„Å®MLP][1]„ÅßÁæéÁöÑ„Çπ„Ç≥„Ç¢„Çí‰∫àÊ∏¨\n",
    "            - ÊúÄÂàù„Éª‰∏≠Èñì„ÉªÊúÄÁµÇ„Éï„É¨„Éº„É†„ÇíÊäΩÂá∫„Åó„ÄÅ„Çπ„Ç≥„Ç¢„ÇíË®àÁÆó„Åó„ÄÅÂπ≥Âùá\n",
    "    - ÈÆÆÊòé„Åï„ÅÆ‰Ωé„ÅÑÂãïÁîª„ÅÆ„Éï„Ç£„É´„Çø„É™„É≥„Ç∞\n",
    "        - OpenCV„ÅÆ„É©„Éó„É©„Ç∑„Ç¢„É≥ÊºîÁÆóÂ≠ê„ÅßÁîªÂÉè„ÅÆÂàÜÊï£„Åå‰Ωé„ÅÑÔºà„Åº„ÇÑ„Åë„Å¶„ÅÑ„ÇãÔºâÂãïÁîª„ÇíÈô§Âéª\n",
    "    - Êâã„Éñ„É¨„ÅÆÂ§ö„ÅÑÂãïÁîª„ÅÆ„Éï„Ç£„É´„Çø„É™„É≥„Ç∞\n",
    "        - PySceneDetect„Çí‰ΩøÁî®„Åó„Å¶„ÄÅ„Éï„É¨„Éº„É†Èñì„ÅÆÂ§âÂåñ„ÅåÂ§ß„Åç„ÅÑÂãïÁîª„ÇíÈô§Âéª\n",
    "    - ÈáçË§á„ÇØ„É™„ÉÉ„Éó„ÅÆ„Éï„Ç£„É´„Çø„É™„É≥„Ç∞\n",
    "- Á∑ë: 256px„ÅÆ‰ΩéËß£ÂÉèÂ∫¶ÂãïÁîª\n",
    "    - Â§ö„Åè„ÅÆÊñáÁ´†„ÅåÂê´„Åæ„Çå„ÇãÂãïÁîª„ÅÆ„Éï„Ç£„É´„Çø„É™„É≥„Ç∞\n",
    "        - PaddleOCR„Çí‰ΩøÁî®„Åó„Å¶„ÄÅ„ÉÜ„Ç≠„Çπ„Éà„ÅÆ„Éê„Ç¶„É≥„Éá„Ç£„É≥„Ç∞„Éú„ÉÉ„ÇØ„Çπ„ÇíÊ§úÂá∫\n",
    "        - ‰ø°È†ºÂ∫¶„Çπ„Ç≥„Ç¢„Åå0.7„ÇíË∂Ö„Åà„Çã„Éú„ÉÉ„ÇØ„Çπ„ÅÆÁ∑èÈù¢Á©ç„ÅåÂ§ö„ÅÑÂãïÁîª„ÇíÈô§Âéª\n",
    "    - „É¢„Éº„Ç∑„Éß„É≥„Çπ„Ç≥„Ç¢„Åß„ÅÆ„Éï„Ç£„É´„Çø„É™„É≥„Ç∞\n",
    "        - libavfilter„ÅÆVMAF„Çí‰ΩøÁî®„Åó„Å¶ÂãïÁîª„ÅÆÂãï„Åç„ÅÆÊøÄ„Åó„Åï„ÇíÊ∏¨ÂÆö\n",
    "        - „É¢„Éº„Ç∑„Éß„É≥„Çπ„Ç≥„Ç¢„ÅåÊ•µÁ´Ø„Å´‰Ωé„ÅÑ„ÉªÈ´ò„ÅÑÂãïÁîª„ÇíÈô§Âéª\n",
    "- ÈªÑËâ≤: 768px„ÅÆÈ´òËß£ÂÉèÂ∫¶ÂãïÁîª\n",
    "\n",
    "[1]: https://github.com/christophschuhmann/improved-aesthetic-predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa995eea",
   "metadata": {},
   "source": [
    "ÂãïÁîª„ÅÆ„Ç≠„É£„Éó„Ç∑„Éß„É≥„Çí‰ΩúÊàê„Åô„Çã„Åü„ÇÅ„Å´„ÄÅË¶ñË¶öË®ÄË™û„É¢„Éá„É´„Çí‰ΩøÁî®:\n",
    "\n",
    "- 256pxÂãïÁîª„Å´„ÅØ„ÄÅLLaVA-Video\n",
    "- 778pxÂãïÁîª„Å´„ÅØ„ÄÅQwen 2.5 MaxÔºà„Éè„É´„Ç∑„Éç„Éº„Ç∑„Éß„É≥„ÅåÂ∞ë„Å™„ÅÑ„Éó„É≠„Éó„É©„Ç§„Ç®„Çø„É™„É¢„Éá„É´Ôºâ\n",
    "\n",
    "„Ç≠„É£„Éó„Ç∑„Éß„É≥ÁîüÊàê„ÅÆ„Éó„É≠„É≥„Éó„Éà„ÅÆÊßãÊàê:\n",
    "\n",
    "- ‰∏ª„Å™Ë¢´ÂÜô‰Ωì\n",
    "- Ë¢´ÂÜô‰Ωì„ÅÆÂãï„Åç\n",
    "- ËÉåÊôØ„ÇÑÁí∞Â¢É\n",
    "- Ë®ºÊòéÊù°‰ª∂„ÇÑÈõ∞Âõ≤Ê∞ó\n",
    "- „Ç´„É°„É©„ÉØ„Éº„ÇØ\n",
    "- „É™„Ç¢„É´„Éª„Ç∑„Éç„Éû„ÉÜ„Ç£„ÉÉ„ÇØ„Éª3D„Éª„Ç¢„Éã„É°„Å™„Å©„ÅÆÂãïÁîª„ÅÆ„Çπ„Çø„Ç§„É´\n",
    "\n",
    "ÁîüÊàêÊôÇ„Å´Âãï„Åç„ÅÆÂº∑Â∫¶„ÇíË™øÊï¥ÂèØËÉΩ„Å´„Åô„Çã„Åü„ÇÅ„ÄÅ„Ç≠„É£„Éó„Ç∑„Éß„É≥„ÅÆÊúÄÂæå„Å´„É¢„Éº„Ç∑„Éß„É≥„Çπ„Ç≥„Ç¢„ÇíËøΩË®ò"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7587fc55",
   "metadata": {},
   "source": [
    "„Éï„Ç£„É´„Çø„É™„É≥„Ç∞Âæå„ÅÆ„Éá„Éº„Çø„ÅÆÁµ±Ë®à:\n",
    "\n",
    "![](image/fig3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff7e82d",
   "metadata": {},
   "source": [
    "- ÁæéÁöÑ„Çπ„Ç≥„Ç¢„ÅØ4.5~5.5„Åß‰∏≠Á®ãÂ∫¶\n",
    "- ÂãïÁîª„ÅÆÈï∑„Åï„ÅØ2~8Áßí„Åß„ÄÅÂçäÂàÜËøë„Åè„Åå6~8Áßí\n",
    "- „Ç¢„Çπ„Éö„ÇØ„ÉàÊØî„ÅØ„ÄÅÂ§ßÈÉ®ÂàÜ„Åå0.5~0.75Ôºà16:9„ÅÆÊ®™Èï∑ÂãïÁîªÔºâ\n",
    "- „Ç≠„É£„Éó„Ç∑„Éß„É≥„ÅÆ70%„ÅØ75ÂçòË™û„ÇíË∂Ö„Åà„Å¶„ÅÑ„Å¶ÊÉÖÂ†±Èáè„ÅåÂ§ö„ÅÑ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1277b7",
   "metadata": {},
   "source": [
    "„Ç≠„É£„Éó„Ç∑„Éß„É≥„ÅÆ„ÉØ„Éº„Éâ„ÇØ„É©„Ç¶„Éâ:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab60e0b",
   "metadata": {},
   "source": [
    "![](image/fig4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1d547b",
   "metadata": {},
   "source": [
    "- ËÉåÊôØ„ÇÑÁÖßÊòéÊù°‰ª∂„ÇÇÂê´„Åæ„Çå„Å¶„ÅÑ„Å¶„ÄÅË¢´ÂÜô‰Ωì„ÅØ‰∫∫Áâ©„ÅåÂ§ö„ÅÑ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c25823",
   "metadata": {},
   "source": [
    "## „Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346c05aa",
   "metadata": {},
   "source": [
    "### 3Ê¨°ÂÖÉ„Ç™„Éº„Éà„Ç®„É≥„Ç≥„Éº„ÉÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a577fd",
   "metadata": {},
   "source": [
    "[Hunyuan Video VAE][1]„ÅÆ„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„ÇíÂäπÁéáÂåñ„Åó„ÅüVideo DC-AE„ÇíÈñãÁô∫\n",
    "\n",
    "DC-AE„ÅØ„ÄÅ[Deep Compression Autoencoder][2]„ÅÆÁï•\n",
    "\n",
    "ÂúßÁ∏ÆÁéá„ÅØ„ÄÅ$4\\times 32\\times 32$ÔºàÊôÇÈñì„ÅØ $\\frac{1}{4}$„ÄÅÁ∏¶„Å®Ê®™„ÅØ$\\frac{1}{32}$„Å´ÂúßÁ∏ÆÔºâ\n",
    "\n",
    "„Ç™„Éº„Éà„Ç®„É≥„Ç≥„Éº„ÉÄ„Éº„ÅÆÂ≠¶Áøí„Éá„Éº„Çø„ÅØ32„Éï„É¨„Éº„É†„ÄÅ256px„ÅÆ„Åü„ÇÅ„ÄÅÊΩúÂú®Ë°®Áèæ„ÅØ$8\\times 8\\times 8$\n",
    "\n",
    "[1]: https://arxiv.org/abs/2412.03603\n",
    "[2]: https://arxiv.org/abs/2410.10733"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef5fb66",
   "metadata": {},
   "source": [
    "Video DC-AE„ÅÆ„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£:\n",
    "\n",
    "![](image/fig5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346d9939",
   "metadata": {},
   "source": [
    "- „Ç®„É≥„Ç≥„Éº„ÉÄ\n",
    "    - 3Â±§„ÅÆResBlock„Å®3Â±§„ÅÆEfficientViT Block„ÅßÊßãÊàê„Åï„Çå„Çã\n",
    "    - ÊúÄÂàù„ÅÆ5„Å§„ÅÆ„Éñ„É≠„ÉÉ„ÇØ„ÅØ„ÄÅ„ÉÄ„Ç¶„É≥„Çµ„É≥„Éó„É™„É≥„Ç∞Áî®\n",
    "    - Â≠¶Áøí„ÇíÂèØËÉΩ„Å´„Åô„Çã„Åü„ÇÅ„ÄÅ„ÉÄ„Ç¶„É≥„Çµ„É≥„Éó„É´„Éñ„É≠„ÉÉ„ÇØ„Å´„ÅØ„ÄÅÊÆãÂ∑ÆÊé•Á∂ö„ÅåÂ∞éÂÖ•\n",
    "    - ÊÆãÂ∑ÆÊé•Á∂ö„ÅØ„Éî„ÇØ„Çª„É´„Ç¢„É≥„Ç∑„É£„ÉÉ„Éï„É™„É≥„Ç∞„Çí‰ΩøÁî®ÔºàSpace&Time->ChannelÔºâ\n",
    "- „Éá„Ç≥„Éº„ÉÄ\n",
    "    - 3Â±§„ÅÆEfficientViT Block„Å®3Â±§„ÅÆResBlock„ÅßÊßãÊàê„Åï„Çå„Çã\n",
    "    - ÊúÄÂæå„ÅÆ5„Å§„ÅÆ„Éñ„É≠„ÉÉ„ÇØ„ÅØ„ÄÅ„Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É™„É≥„Ç∞Áî®\n",
    "    - Â≠¶Áøí„ÇíÂèØËÉΩ„Å´„Åô„Çã„Åü„ÇÅ„ÄÅ„Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É´„Éñ„É≠„ÉÉ„ÇØ„Å´„ÅØ„ÄÅÊÆãÂ∑ÆÊé•Á∂ö„ÅåÂ∞éÂÖ•\n",
    "    - ÊÆãÂ∑ÆÊé•Á∂ö„ÅØ„Éî„ÇØ„Çª„É´„Ç∑„É£„ÉÉ„Éï„É™„É≥„Ç∞„Çí‰ΩøÁî®ÔºàChannel->Space&TimeÔºâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb61f01",
   "metadata": {},
   "source": [
    "Video DC-AE„Çí„Çπ„ÇØ„É©„ÉÉ„ÉÅ„Åã„ÇâÂ≠¶Áøí„Åó„ÄÅÂÜçÊßãÊàêÂìÅË≥™„ÇíË©ï‰æ°:\n",
    "\n",
    "![](image/table1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df750de",
   "metadata": {},
   "source": [
    "- LPIPS: ‰∫∫Èñì„ÅÆÁü•Ë¶ö„Å´Ëøë„ÅÑÁîªË≥™Ë©ï‰æ°ÊåáÊ®ô"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adeda3cd",
   "metadata": {},
   "source": [
    "## DiT„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759c3421",
   "metadata": {},
   "source": [
    "Èõ¢„Çå„Åü„Éï„É¨„Éº„É†„ÇÑÁîªÁ¥†ÂêåÂ£´„ÅÆÈñ¢‰øÇ„ÇíÂäπÊûúÁöÑ„Å´Êçâ„Åà„Çã„Éï„É´„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥„ÇíÊé°Áî®\n",
    "\n",
    "ÂãïÁîª„ÅØVideo DC-AE„ÅßÂúßÁ∏ÆÂæå„ÄÅ„Éë„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫1Ôºà=„Éë„ÉÉ„ÉÅÂåñÁÑ°„ÅóÔºâ„Åß„Éï„É©„ÉÉ„ÉàÂåñ\n",
    "\n",
    "Hunyuan Video„ÅÆ„Ç™„Éº„Éà„Ç®„É≥„Ç≥„Éº„ÉÄ„Éº„Çí‰ΩøÁî®„Åô„ÇãÂ†¥Âêà„ÅØ„ÄÅ„Éë„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫2„ÅåÂøÖË¶Å"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df79314e",
   "metadata": {},
   "source": [
    "FLUX„ÅÆ[MMDiT][1]„ÇíÂèÇËÄÉ„Å´„ÄÅ„Éá„É•„Ç¢„É´„Çπ„Éà„É™„Éº„É†„Å®„Ç∑„É≥„Ç∞„É´„Çπ„Éà„É™„Éº„É†„Åã„Çâ„Å™„ÇãÊßãÈÄ†„ÇíÊé°Áî®:\n",
    "\n",
    "![](image/fig6.png)\n",
    "\n",
    "[1]: https://github.com/black-forest-labs/flux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968e2d5c",
   "metadata": {},
   "source": [
    "- „Éá„É•„Ç¢„É´„Çπ„Éà„É™„Éº„É†„Éñ„É≠„ÉÉ„ÇØ„Åß„ÄÅÂãïÁîª„Å®„ÉÜ„Ç≠„Çπ„Éà„ÅåÂà•„ÄÖ„Å´ÁâπÂæ¥ÊäΩÂá∫„Åï„Çå„Çã\n",
    "- „Ç∑„É≥„Ç∞„É´„Çπ„Éà„É™„Éº„É†„Éñ„É≠„ÉÉ„ÇØ„Åß„ÄÅÁâπÂæ¥„ÇíÁµ±Âêà„Åô„Çã"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f3e053",
   "metadata": {},
   "source": [
    "Á©∫Èñì„Å®ÊôÇÈñìÊÉÖÂ†±„ÇíÊçâ„Åà„Çã„Åü„ÇÅ„Å´„ÄÅ3D RoPE„ÇíÊé°Áî®\n",
    "\n",
    "„ÉÜ„Ç≠„Çπ„Éà„Éé„Ç®„É≥„Ç≥„Éº„Éâ„ÅØ„ÄÅ2„Å§„ÅÆ‰∫ãÂâçÂ≠¶ÁøíÊ∏à„Åø„É¢„Éá„É´„ÇíÊé°Áî®:\n",
    "\n",
    "- T5-XXL: Ë§áÈõë„Å™„ÉÜ„Ç≠„Çπ„Éà„ÅÆÊÑèÂë≥„ÇíÊçâ„Åà„Çã\n",
    "- CLIP-Large: „ÉÜ„Ç≠„Çπ„Éà„Å®Ë¶ñË¶öÊ¶ÇÂøµ„ÅÆÊï¥ÂêàÊÄß„ÇíÊçâ„Åà„ÇãÔºà=ÊåáÁ§∫ËøΩÂæìÊÄß„ÇíÈ´ò„ÇÅ„ÇãÔºâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6548ce9c",
   "metadata": {},
   "source": [
    "![](image/table2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baaba03",
   "metadata": {},
   "source": [
    "## ÂÆüË£Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf13bf8",
   "metadata": {},
   "source": [
    "- image.py: ÁîªÂÉè„ÅÆ„Åø„ÅßÂ≠¶Áøí„ÄÇ\n",
    "- stage1.py: 256pxËß£ÂÉèÂ∫¶„ÅÆÂãïÁîª„ÅßÂ≠¶Áøí„ÄÇ\n",
    "- stage2.py: 768pxËß£ÂÉèÂ∫¶„ÅÆÂãïÁîª„ÅßÂ≠¶ÁøíÔºà„Ç∑„Éº„Ç±„É≥„Çπ‰∏¶ÂàóÂåñ„Çí‰ΩøÁî®„ÄÅ„Éá„Éï„Ç©„É´„Éà„ÅØ4Ôºâ„ÄÇ\n",
    "- stage1_i2v.py: 256pxËß£ÂÉèÂ∫¶„ÅßT2VÔºà„ÉÜ„Ç≠„Çπ„Éà„Åã„ÇâÂãïÁîªÔºâ„Å®I2VÔºàÁîªÂÉè„Åã„ÇâÂãïÁîªÔºâ„ÇíÂ≠¶Áøí„ÄÇ\n",
    "- stage2_i2v.py: 768pxËß£ÂÉèÂ∫¶„ÅßT2V„Å®I2V„ÇíÂ≠¶Áøí„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e895226e",
   "metadata": {},
   "source": [
    "## Áí∞Â¢ÉÊßãÁØâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9f1318",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd /workspaces/open-sora/Open-Sora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52ec244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import platform\n",
    "import subprocess\n",
    "\n",
    "if os.path.exists(\"debug.log\"):\n",
    "    os.remove(\"debug.log\")\n",
    "\n",
    "def custom_format(record):\n",
    "    match record.levelno:\n",
    "        case logging.DEBUG:\n",
    "            level = \"üü¶\"\n",
    "        case logging.INFO:\n",
    "            level = \"üü©\"\n",
    "        case logging.WARNING:\n",
    "            level = \"üü®\"\n",
    "        case logging.ERROR:\n",
    "            level = \"üü•\"\n",
    "        case logging.CRITICAL:\n",
    "            level = \"üõë\"\n",
    "    return f\"{level} {record.getMessage()}\"\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "for handler in logger.handlers:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "formatter = logging.Formatter()\n",
    "formatter.format = custom_format\n",
    "\n",
    "file_handler = logging.FileHandler(\"debug.log\")\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "PYTHON_VERSION = platform.python_version()\n",
    "logger.info(f\"Python {PYTHON_VERSION}\")\n",
    "\n",
    "NVIDIA_SMI = subprocess.run(\"nvidia-smi\", capture_output=True, text=True).stdout\n",
    "logger.info(f\"{NVIDIA_SMI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57815940",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    %pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "    %pip install \\\n",
    "        accelerate \\\n",
    "        av \\\n",
    "        colossalai \\\n",
    "        ftfy \\\n",
    "        liger-kernel \\\n",
    "        omegaconf \\\n",
    "        mmengine \\\n",
    "        openai \\\n",
    "        pandas \\\n",
    "        pandarallel \\\n",
    "        pyarrow \\\n",
    "        tensorboard \\\n",
    "        wandb \\\n",
    "        --extra-index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "    %pip install flash-attn --no-build-isolation\n",
    "\n",
    "    %pip install -e . --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a52b84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -qU av==13.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a745e96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from einops import rearrange\n",
    "from flash_attn import flash_attn_func as flash_attn_func_v2\n",
    "from functools import partial\n",
    "from huggingface_hub import PyTorchModelHubMixin\n",
    "from inspect import signature\n",
    "from liger_kernel.ops.rms_norm import LigerRMSNormFunction\n",
    "from liger_kernel.ops.rope import LigerRopeFunction\n",
    "from omegaconf import MISSING, OmegaConf\n",
    "from torch import Tensor, nn\n",
    "from torch.nn.modules.batchnorm import _BatchNorm\n",
    "from typing import Any, Callable, Optional, Union, Tuple\n",
    "import diffusers\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "from mmengine.config import Config\n",
    "import ast\n",
    "\n",
    "try:\n",
    "    from flash_attn_interface import flash_attn_func as flash_attn_func_v3\n",
    "    SUPPORT_FA3 = True\n",
    "except:\n",
    "    SUPPORT_FA3 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320458ef",
   "metadata": {},
   "source": [
    "## „ÉÄ„Ç¶„É≥„É≠„Éº„Éâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d5b66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/hpcai-tech/Open-Sora-v2-Video-DC-AE\n",
    "# F32T4C128_AE.safetensors 919MB\n",
    "# Open_Sora_v2_Video_DC_AE.safetensors 23.8GB\n",
    "\n",
    "if False:\n",
    "    !huggingface-cli download hpcai-tech/Open-Sora-v2-Video-DC-AE --local-dir ./ckpts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b44d43",
   "metadata": {},
   "source": [
    "## CLI„Éë„Éº„Çµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29127b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(args) -> tuple[str, argparse.Namespace]:\n",
    "    \"\"\"\n",
    "    This function parses the command line arguments.\n",
    "\n",
    "    Returns:\n",
    "        tuple[str, argparse.Namespace]: The path to the configuration file and the command line arguments.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"config\", type=str, help=\"model config file path\")\n",
    "    args, unknown_args = parser.parse_known_args(args)\n",
    "    return args.config, unknown_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a7b984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config(config_path: str) -> Config:\n",
    "    \"\"\"\n",
    "    This function reads the configuration file.\n",
    "\n",
    "    Args:\n",
    "        config_path (str): The path to the configuration file.\n",
    "\n",
    "    Returns:\n",
    "        Config: The configuration object.\n",
    "    \"\"\"\n",
    "    cfg = Config.fromfile(config_path)\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328e3ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_convert(value: str) -> int | float | bool | list | dict | None:\n",
    "    \"\"\"\n",
    "    Automatically convert a string to the appropriate Python data type,\n",
    "    including int, float, bool, list, dict, etc.\n",
    "\n",
    "    Args:\n",
    "        value (str): The string to convert.\n",
    "\n",
    "    Returns:\n",
    "        int, float, bool, list |  dict: The converted value.\n",
    "    \"\"\"\n",
    "    # Handle empty string\n",
    "    if value == \"\":\n",
    "        return value\n",
    "\n",
    "    # Handle None\n",
    "    if value.lower() == \"none\":\n",
    "        return None\n",
    "\n",
    "    # Handle boolean values\n",
    "    lower_value = value.lower()\n",
    "    if lower_value == \"true\":\n",
    "        return True\n",
    "    elif lower_value == \"false\":\n",
    "        return False\n",
    "\n",
    "    # Try to convert the string to an integer or float\n",
    "    try:\n",
    "        # Try converting to an integer\n",
    "        return int(value)\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        # Try converting to a float\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    # Try to convert the string to a list, dict, tuple, etc.\n",
    "    try:\n",
    "        return ast.literal_eval(value)\n",
    "    except (ValueError, SyntaxError):\n",
    "        pass\n",
    "\n",
    "    # If all attempts fail, return the original string\n",
    "    return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb75c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_args(cfg: Config, args: argparse.Namespace) -> Config:\n",
    "    \"\"\"\n",
    "    This function merges the configuration file and command line arguments.\n",
    "\n",
    "    Args:\n",
    "        cfg (Config): The configuration object.\n",
    "        args (argparse.Namespace): The command line arguments.\n",
    "\n",
    "    Returns:\n",
    "        Config: The configuration object.\n",
    "    \"\"\"\n",
    "    for k, v in zip(args[::2], args[1::2]):\n",
    "        assert k.startswith(\"--\"), f\"Invalid argument: {k}\"\n",
    "        k = k[2:].replace(\"-\", \"_\")\n",
    "        k_split = k.split(\".\")\n",
    "        target = cfg\n",
    "        for key in k_split[:-1]:\n",
    "            assert key in cfg, f\"Key {key} not found in config\"\n",
    "            target = target[key]\n",
    "        if v.lower() == \"none\":\n",
    "            v = None\n",
    "        elif k in target:\n",
    "            v_type = type(target[k])\n",
    "            if v_type == bool:\n",
    "                v = auto_convert(v)\n",
    "            else:\n",
    "                v = type(target[k])(v)\n",
    "        else:\n",
    "            v = auto_convert(v)\n",
    "        target[k_split[-1]] = v\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6510015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_configs(args) -> Config:\n",
    "    \"\"\"\n",
    "    This function parses the configuration file and command line arguments.\n",
    "\n",
    "    Returns:\n",
    "        Config: The configuration object.\n",
    "    \"\"\"\n",
    "    config, args = parse_args(args)\n",
    "    cfg = read_config(config)\n",
    "    cfg = merge_args(cfg, args)\n",
    "    cfg.config_path = config\n",
    "\n",
    "    # hard-coded for spatial compression\n",
    "    if cfg.get(\"ae_spatial_compression\", None) is not None:\n",
    "        os.environ[\"AE_SPATIAL_COMPRESSION\"] = str(cfg.ae_spatial_compression)\n",
    "\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a28b05",
   "metadata": {},
   "source": [
    "### „Ç™„Éº„Éâ„Ç®„É≥„Ç≥„Éº„ÉÄ„ÅÆÊé®Ë´ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d338cab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchrun --nproc_per_node 1 --standalone scripts/vae/inference.py configs/vae/inference/video_dc_ae.py --save-dir samples/dcae\n",
    "\n",
    "parse_configs([\n",
    "    \"configs/vae/inference/video_dc_ae.py\",\n",
    "    \"--save-dir\",\n",
    "    \"samples/dcae\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f044d52b",
   "metadata": {},
   "source": [
    "### DC-AEÁî®„ÅÆÂãïÁîªÁîüÊàê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f42ebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/high_compression.py --prompt \"The story of a robot's life in a cyberpunk setting.\" \n",
    "\n",
    "parse_configs([\n",
    "    \"configs/diffusion/inference/high_compression.py\",\n",
    "    \"--prompt\",\n",
    "    \"The story of a robot's life in a cyberpunk setting.\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af47018",
   "metadata": {},
   "source": [
    "### hunyuanVideoAEÁî®„ÅÆÂãïÁîªÁîüÊàê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f16fd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/256px.py --prompt \"raining, sea\"\n",
    "\n",
    "parse_configs([\n",
    "    \"configs/diffusion/inference/256px.py\",\n",
    "    \"--prompt\",\n",
    "    \"raining, sea\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8a2f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchrun --nproc_per_node 8 --standalone scripts/diffusion/inference.py configs/diffusion/inference/768px.py --prompt \"raining, sea\"\n",
    "\n",
    "parse_configs([\n",
    "    \"configs/diffusion/inference/768px.py\",\n",
    "    \"--prompt\",\n",
    "    \"raining, sea\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b41885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text-to-Video 256x256\n",
    "\n",
    "# torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_256px.py --save-dir samples --prompt \"raining, sea\"\n",
    "\n",
    "parse_configs([\n",
    "    \"configs/diffusion/inference/t2i2v_256px.py\",\n",
    "    \"--save-dir\",\n",
    "    \"samples\",\n",
    "    \"--prompt\",\n",
    "    \"raining, sea\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10dde6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text-to-Video 256x256 from CSV\n",
    "\n",
    "# torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_256px.py --save-dir samples --dataset.data-path assets/texts/example.csv\n",
    "\n",
    "parse_configs([\n",
    "    \"configs/diffusion/inference/t2i2v_256px.py\",\n",
    "    \"--save-dir\",\n",
    "    \"samples\",\n",
    "    \"--dataset.data-path\",\n",
    "    \"assets/texts/example.csv\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67e71bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text-to-Video 768x768 \n",
    "\n",
    "# torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_768px.py --save-dir samples --prompt \"raining, sea\"\n",
    "\n",
    "parse_configs([\n",
    "    \"configs/diffusion/inference/t2i2v_768px.py\",\n",
    "    \"--save-dir\",\n",
    "    \"samples\",\n",
    "    \"--prompt\",\n",
    "    \"raining, sea\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaf072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text-to-Video 768x768 multi-GPU\n",
    "\n",
    "# torchrun --nproc_per_node 8 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_768px.py --save-dir samples --prompt \"raining, sea\"\n",
    "\n",
    "parse_configs([\n",
    "    \"configs/diffusion/inference/t2i2v_768px.py\",\n",
    "    \"--save-dir\",\n",
    "    \"samples\",\n",
    "    \"--prompt\",\n",
    "    \"raining, sea\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4596f327",
   "metadata": {},
   "source": [
    "### ÁîªÂÉè„Åã„ÇâÂãïÁîªÁîüÊàê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba7ede2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/256px.py --cond_type i2v_head --prompt \"A plump pig wallows in a muddy pond on a rustic farm, its pink snout poking out as it snorts contentedly. The camera captures the pig's playful splashes, sending ripples through the water under the midday sun. Wooden fences and a red barn stand in the background, framed by rolling green hills. The pig's muddy coat glistens in the sunlight, showcasing the simple pleasures of its carefree life.\" --ref assets/texts/i2v.png\n",
    "\n",
    "parse_configs([\n",
    "    \"configs/diffusion/inference/256px.py\",\n",
    "    \"--cond_type\",\n",
    "    \"i2v_head\",\n",
    "    \"--prompt\",\n",
    "    \"A plump pig wallows in a muddy pond on a rustic farm, its pink snout poking out as it snorts contentedly. The camera captures the pig's playful splashes, sending ripples through the water under the midday sun. Wooden fences and a red barn stand in the background, framed by rolling green hills. The pig's muddy coat glistens in the sunlight, showcasing the simple pleasures of its carefree life.\",\n",
    "    \"--ref\",\n",
    "    \"assets/texts/i2v.png\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb0808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 256px with csv\n",
    "\n",
    "# torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/256px.py --cond_type i2v_head --dataset.data-path assets/texts/i2v.csv\n",
    "\n",
    "parse_configs([\n",
    "    \"configs/diffusion/inference/256px.py\",\n",
    "    \"--cond_type\",\n",
    "    \"i2v_head\",\n",
    "    \"--dataset.data-path\",\n",
    "    \"assets/texts/i2v.csv\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1d1686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-GPU 768px\n",
    "\n",
    "# torchrun --nproc_per_node 8 --standalone scripts/diffusion/inference.py configs/diffusion/inference/768px.py --cond_type i2v_head --dataset.data-path assets/texts/i2v.csv\n",
    "\n",
    "parse_configs([\n",
    "    \"configs/diffusion/inference/768px.py\",\n",
    "    \"--cond_type\",\n",
    "    \"i2v_head\",\n",
    "    \"--dataset.data-path\",\n",
    "    \"assets/texts/i2v.csv\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7a9d32",
   "metadata": {},
   "source": [
    "### „É¢„Éº„Ç∑„Éß„É≥„Çπ„Ç≥„Ç¢„ÇíÊåáÂÆö„Åó„ÅüÂãïÁîªÁîüÊàê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d96279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_256px.py --save-dir samples --prompt \"raining, sea\" --motion-score 4\n",
    "\n",
    "parse_configs([\n",
    "    \"configs/diffusion/inference/t2i2v_256px.py\",\n",
    "    \"--save-dir\",\n",
    "    \"samples\",\n",
    "    \"--prompt\",\n",
    "    \"raining, sea\",\n",
    "    \"--motion-score\",\n",
    "    \"4\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb42ddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÂãïÁîª„ÅÆ„É¢„Éº„Ç∑„Éß„É≥„Çπ„Ç≥„Ç¢„ÇíË©ï‰æ°\n",
    "# OpenAI API Key„ÅåÂøÖË¶Å\n",
    "\n",
    "# torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_256px.py --save-dir samples --prompt \"raining, sea\" --motion-score dynamic\n",
    "\n",
    "parse_configs([\n",
    "    \"configs/diffusion/inference/t2i2v_256px.py\",\n",
    "    \"--save-dir\",\n",
    "    \"samples\",\n",
    "    \"--prompt\",\n",
    "    \"raining, sea\",\n",
    "    \"--motion-score\",\n",
    "    \"dynamic\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e427a0dc",
   "metadata": {},
   "source": [
    "### „Éó„É≠„É≥„Éó„Éà„ÅÆÊ¥óÁ∑¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec44de4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export OPENAI_API_KEY=sk-xxxx\n",
    "# torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_256px.py --save-dir samples --prompt \"raining, sea\" --refine-prompt True\n",
    "\n",
    "parse_configs([\n",
    "    \"configs/diffusion/inference/t2i2v_256px.py\",\n",
    "    \"--save-dir\",\n",
    "    \"samples\",\n",
    "    \"--prompt\",\n",
    "    \"raining, sea\",\n",
    "    \"--refine-prompt\",\n",
    "    \"True\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4173ec15",
   "metadata": {},
   "source": [
    "### „Ç∑„Éº„ÉâÂõ∫ÂÆö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1192a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_256px.py --save-dir samples --prompt \"raining, sea\" --sampling_option.seed 42 --seed 42\n",
    "\n",
    "parse_configs([\n",
    "    \"configs/diffusion/inference/t2i2v_256px.py\",\n",
    "    \"--save-dir\",\n",
    "    \"samples\",\n",
    "    \"--prompt\",\n",
    "    \"raining, sea\",\n",
    "    \"--sampling_option.seed\",\n",
    "    \"42\",\n",
    "    \"--seed\",\n",
    "    \"42\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf2924f",
   "metadata": {},
   "source": [
    "### Ë®ìÁ∑¥Ë®≠ÂÆö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f665b022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DC-AEË®ìÁ∑¥\n",
    "\n",
    "# https://github.com/hpcaitech/Open-Sora/blob/main/docs/hcae.md\n",
    "\n",
    "# torchrun --nproc_per_node 8 scripts/vae/train.py configs/vae/train/video_dc_ae.py\n",
    "\n",
    "parse_configs([\n",
    "    \"configs/vae/train/video_dc_ae.py\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebbccda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DC-AE-DISCË®ìÁ∑¥\n",
    "\n",
    "# torchrun --nproc_per_node 8 scripts/vae/train.py configs/vae/train/video_dc_ae_disc.py --model.from_pretrained <model_ckpt>\n",
    "\n",
    "# args_2 = [\n",
    "#     \"configs/vae/train/video_dc_ae_disc.py\",\n",
    "#     \"--model.from_pretrained\",\n",
    "#     \"<model_ckpt>\"\n",
    "# ]\n",
    "\n",
    "parse_configs([\n",
    "    \"configs/vae/train/video_dc_ae_disc.py\",\n",
    "    \"--model.from_pretrained\",\n",
    "    \"<model_ckpt>\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873ddf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DiffusionË®ìÁ∑¥\n",
    "\n",
    "# torchrun --nproc_per_node 8 scripts/diffusion/train.py configs/diffusion/train/stage1.py --dataset.data-path datasets/pexels_45k_necessary.csv\n",
    "\n",
    "parse_configs([\n",
    "    \"configs/diffusion/train/stage1.py\",\n",
    "    \"--dataset.data-path\",\n",
    "    \"datasets/pexels_45k_necessary.csv\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14a0a26",
   "metadata": {},
   "source": [
    "## DC-AE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c2122b",
   "metadata": {},
   "source": [
    "### „É¶„Éº„ÉÜ„Ç£„É™„ÉÜ„Ç£Èñ¢Êï∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54477e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ceil_to_divisible(n: int, dividend: int) -> int:\n",
    "    return math.ceil(dividend / (dividend // n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c352dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val2list(x: Union[list, tuple, Any], repeat_time=1) -> list:\n",
    "    \"\"\"\n",
    "    ÂÄ§„Çí„É™„Çπ„Éà„Å´Â§âÊèõ„Åô„Çã\n",
    "\n",
    "    Args:\n",
    "        x (Union[list, tuple, Any]): Â§âÊèõ„Åô„ÇãÂÄ§\n",
    "        repeat_time (int, optional): x„Åå„É™„Çπ„Éà„ÇÑ„Çø„Éó„É´„Åß„Å™„ÅÑÂ†¥Âêà„ÅÆÁπ∞„ÇäËøî„ÅóÂõûÊï∞\n",
    "    Returns:\n",
    "        list: Â§âÊèõÂæå„ÅÆ„É™„Çπ„Éà\n",
    "    \"\"\"\n",
    "\n",
    "    # „É™„Çπ„Éà„Åã„Çø„Éó„É´„ÅÆÂ†¥Âêà\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return list(x)\n",
    "\n",
    "    # „Åù„Çå‰ª•Â§ñ„ÅÆÂ†¥Âêà\n",
    "    return [x for _ in range(repeat_time)]\n",
    "\n",
    "# Ê§úË®º\n",
    "val2list([1,2,3]), val2list((4,5)), val2list(5, repeat_time=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ed0c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val2tuple(x: Union[list, tuple, Any], min_len: int = 1, idx_repeat: int = -1) -> tuple:\n",
    "    \"\"\"\n",
    "    ÂÄ§„Çí„Çø„Éó„É´„Å´Â§âÊèõ„Åô„Çã\n",
    "\n",
    "    Args:\n",
    "        x (Union[list, tuple, Any]): Â§âÊèõ„Åô„ÇãÂÄ§\n",
    "        min_len (int, optional): „Çø„Éó„É´„ÅÆÊúÄÂ∞èÈï∑„Åï\n",
    "        idx_repeat (int, optional): Áπ∞„ÇäËøî„ÅóÊåøÂÖ•„Åô„Çã„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ\n",
    "    Returns:\n",
    "        tuple: Â§âÊèõÂæå„ÅÆ„Çø„Éó„É´\n",
    "    \"\"\"\n",
    "\n",
    "    # ÂÄ§„Çí„É™„Çπ„Éà„Å´Â§âÊèõ\n",
    "    x = val2list(x)\n",
    "\n",
    "    # ÂøÖË¶Å„Å´Âøú„Åò„Å¶Ë¶ÅÁ¥†„ÇíÁπ∞„ÇäËøî„Åó\n",
    "    if len(x) > 0:\n",
    "        x[idx_repeat:idx_repeat] = [\n",
    "            x[idx_repeat] for _ in range(min_len - len(x))\n",
    "        ]\n",
    "\n",
    "    return tuple(x)\n",
    "\n",
    "# Ê§úË®º\n",
    "val2tuple((True, False), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fd9c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_kwargs_from_config(config: dict, target_func: Callable) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Ë®≠ÂÆö„Åã„ÇâÈñ¢Êï∞„ÅÆ„Ç≠„Éº„ÉØ„Éº„ÉâÂºïÊï∞„ÇíÊßãÁØâ„Åô„Çã\n",
    "\n",
    "    Args:\n",
    "        config (dict): Ë®≠ÂÆöËæûÊõ∏\n",
    "        target_func (Callable): ÂØæË±°„ÅÆÈñ¢Êï∞\n",
    "    Returns:\n",
    "        dict[str, Any]: Èñ¢Êï∞„Å´Ê∏°„Åô„Ç≠„Éº„ÉØ„Éº„ÉâÂºïÊï∞\n",
    "    \"\"\"\n",
    "    logger.info(f\"build_kwargs_from_config {config=} {target_func=}\")\n",
    "\n",
    "    valid_keys = list(signature(target_func).parameters)\n",
    "    kwargs = {}\n",
    "    for key in config:\n",
    "        if key in valid_keys:\n",
    "            kwargs[key] = config[key]\n",
    "    return kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8397c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_same_padding(kernel_size: Union[int, tuple[int, ...]]) -> Union[int, tuple[int, ...]]:\n",
    "    \"\"\"\n",
    "    „Ç´„Éº„Éç„É´„Çµ„Ç§„Ç∫„Å´Âü∫„Å•„ÅÑ„Å¶Âêå„Åò„Éë„Éá„Ç£„É≥„Ç∞„ÇíË®àÁÆó„Åô„Çã\n",
    "    ConvLayer, LiteMLA„Åß‰ΩøÁî®\n",
    "\n",
    "    Args:\n",
    "        kernel_size (Union[int, tuple[int, ...]]): „Ç´„Éº„Éç„É´„Çµ„Ç§„Ç∫\n",
    "    Returns:\n",
    "        Union[int, tuple[int, ...]]: Âêå„Åò„Éë„Éá„Ç£„É≥„Ç∞„Çµ„Ç§„Ç∫\n",
    "    \"\"\"\n",
    "    logger.info(f\"get_same_padding {kernel_size=}\")\n",
    "\n",
    "    # False\n",
    "    if isinstance(kernel_size, tuple):\n",
    "        result =  tuple([get_same_padding(ks) for ks in kernel_size])\n",
    "        loggger.debug(f\"get_same_padding result: {result}\")\n",
    "        return result\n",
    "\n",
    "    # True\n",
    "    else:\n",
    "        assert kernel_size % 2 > 0, \"kernel size should be odd number\"\n",
    "\n",
    "        # 5 // 2 = 2\n",
    "        result = kernel_size // 2\n",
    "        logger.debug(f\"get_same_padding result: {result}\")\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68670c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_grad_checkpoint(module, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Ëá™ÂãïÂãæÈÖç„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„ÉàÈñ¢Êï∞\n",
    "    ÂãæÈÖç„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà„ÅåÊúâÂäπ„Å™Â†¥Âêà„ÄÅ„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà„Çí‰ΩøÁî®„Åó„Å¶„É¢„Ç∏„É•„Éº„É´„ÇíÂÆüË°å„Åô„Çã\n",
    "    „Åù„ÅÜ„Åß„Å™„ÅÑÂ†¥Âêà„ÄÅÈÄöÂ∏∏„ÅÆ„Éï„Ç©„ÉØ„Éº„Éâ„Éë„Çπ„ÇíÂÆüË°å„Åô„Çã\n",
    "\n",
    "    Args:\n",
    "        module: nn.Module„Åæ„Åü„ÅØnn.Module„ÅÆIterable\n",
    "        *args: „É¢„Ç∏„É•„Éº„É´„Å∏„ÅÆÂºïÊï∞\n",
    "        **kwargs: „É¢„Ç∏„É•„Éº„É´„Å∏„ÅÆ„Ç≠„Éº„ÉØ„Éº„ÉâÂºïÊï∞\n",
    "    Returns:\n",
    "        „É¢„Ç∏„É•„Éº„É´„ÅÆÂá∫Âäõ\n",
    "    \"\"\"\n",
    "    logger.info(f\"auto_grad_checkpoint {len(args)=} {args[0].shape=} {kwargs=}\")\n",
    "\n",
    "    # False\n",
    "    if getattr(module, \"grad_checkpointing\", False):\n",
    "        if not isinstance(module, Iterable):\n",
    "            return checkpoint(module, *args, use_reentrant=True, **kwargs)\n",
    "\n",
    "        gc_step = module[0].grad_checkpointing_step\n",
    "        logger.debug(f\"gc_step: {gc_step}\")\n",
    "\n",
    "        return checkpoint_sequential(module, gc_step, *args, use_reentrant=False, **kwargs)\n",
    "\n",
    "    return module(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fcbe88",
   "metadata": {},
   "source": [
    "### „É¢„Ç∏„É•„Éº„É´„É™„Çπ„Éà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaebc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpSequential(nn.Module):\n",
    "    \"\"\"\n",
    "    Ë§áÊï∞„ÅÆÊìç‰Ωú„ÇíÈ†ÜÊ¨°ÈÅ©Áî®„Åô„Çã„É¢„Ç∏„É•„Éº„É´\n",
    "    \"\"\"\n",
    "    def __init__(self, op_list: list[Optional[nn.Module]]):\n",
    "        logger.info(f\"OpSequential init with {op_list=}\")\n",
    "        super().__init__()\n",
    "        valid_op_list = []\n",
    "        for op in op_list:\n",
    "            if op is not None:\n",
    "                valid_op_list.append(op)\n",
    "        self.op_list = nn.ModuleList(valid_op_list)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logger.info(f\"OpSequential forward {x.shape=}\")\n",
    "        for op in self.op_list:\n",
    "            x = op(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cd1bca",
   "metadata": {},
   "source": [
    "### ÊÅíÁ≠âÂ±§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f403bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    ÊÅíÁ≠âÂ±§\n",
    "    ÊÆãÂ∑ÆÊé•Á∂ö„ÅÆ„Ç∑„Éß„Éº„Éà„Ç´„ÉÉ„Éà„Éë„ÇπÁî®\n",
    "    build_block, EfficientViTBlock„Åß‰ΩøÁî®\n",
    "    \"\"\"\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logger.info(f\"IdentityLayer forward {x.shape=}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5208b6",
   "metadata": {},
   "source": [
    "### Ê≠£Ë¶èÂåñÂ±§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5b8446",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm2d(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_features: int, eps: float = 1e-5, elementwise_affine: bool = True, bias: bool = True\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        2Ê¨°ÂÖÉRMSÊ≠£Ë¶èÂåñÂ±§\n",
    "\n",
    "        Args:\n",
    "            num_features (int): Ê≠£Ë¶èÂåñ„Åô„ÇãÁâπÂæ¥Èáè„ÅÆÊï∞\n",
    "            eps (float, optional): Êï∞ÂÄ§ÂÆâÂÆöÊÄß„ÅÆ„Åü„ÇÅ„ÅÆÂ∞è„Åï„Å™ÂÄ§\n",
    "            elementwise_affine (bool, optional): Ë¶ÅÁ¥†„Åî„Å®„ÅÆ„Ç¢„Éï„Ç£„É≥Â§âÊèõ„Çí‰ΩøÁî®„Åô„Çã„Åã\n",
    "            bias (bool, optional): „Éê„Ç§„Ç¢„ÇπÈ†Ö„Çí‰ΩøÁî®„Åô„Çã„Åã\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(f\"RMSNorm2d init with {num_features=}, {eps=}, {elementwise_affine=}, {bias=}\")\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_features = num_features\n",
    "\n",
    "        self.eps = eps\n",
    "\n",
    "        self.elementwise_affine = elementwise_affine\n",
    "\n",
    "        if self.elementwise_affine:\n",
    "            self.weight = torch.nn.parameter.Parameter(\n",
    "                torch.empty(self.num_features)\n",
    "            )\n",
    "\n",
    "            if bias:\n",
    "                self.bias = torch.nn.parameter.Parameter(\n",
    "                    torch.empty(self.num_features)\n",
    "                )\n",
    "            else:\n",
    "                self.register_parameter(\"bias\", None)\n",
    "        else:\n",
    "            self.register_parameter(\"weight\", None)\n",
    "            self.register_parameter(\"bias\", None)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = (x / torch.sqrt(torch.square(x.float()).mean(dim=1, keepdim=True) + self.eps)).to(x.dtype)\n",
    "        if self.elementwise_affine:\n",
    "            x = x * self.weight.view(1, -1, 1, 1) + self.bias.view(1, -1, 1, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248a773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm3d(RMSNorm2d):\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        3Ê¨°ÂÖÉRMSÊ≠£Ë¶èÂåñÂ±§„ÅÆ„Éï„Ç©„ÉØ„Éº„Éâ„Éë„Çπ\n",
    "        Args:\n",
    "            x (torch.Tensor): ÂÖ•Âäõ„ÉÜ„É≥„ÇΩ„É´ (B, C, D, H, W)\n",
    "        Returns:\n",
    "            torch.Tensor: Ê≠£Ë¶èÂåñ„Åï„Çå„ÅüÂá∫Âäõ„ÉÜ„É≥„ÇΩ„É´\n",
    "        \"\"\"\n",
    "        logger.info(f\"RMSNorm3d forward {x.shape=}\")\n",
    "\n",
    "        x = (\n",
    "            x / torch.sqrt(torch.square(x.float()).mean(dim=1, keepdim=True) +\n",
    "            self.eps)\n",
    "        ).to(x.dtype)\n",
    "\n",
    "        if self.elementwise_affine:\n",
    "            x = x * self.weight.view(1, -1, 1, 1, 1) + self.bias.view(1, -1, 1, 1, 1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f3527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register normalization function here\n",
    "REGISTERED_NORM_DICT: dict[str, type] = {\n",
    "    # \"bn2d\": nn.BatchNorm2d,\n",
    "    # \"ln\": nn.LayerNorm,\n",
    "    # \"ln2d\": LayerNorm2d,\n",
    "    # \"rms2d\": RMSNorm2d,\n",
    "    \"rms3d\": RMSNorm3d,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4289a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_norm(name=\"bn2d\", num_features=None, **kwargs) -> Optional[nn.Module]:\n",
    "    \"\"\"\n",
    "    Ê≠£Ë¶èÂåñÂ±§„ÇíÊßãÁØâ„Åô„Çã\n",
    "    RMSNorm3d„ÅÆ„Åø‰ΩøÁî®\n",
    "\n",
    "    Args:\n",
    "        name (str, optional): Ê≠£Ë¶èÂåñÂ±§„ÅÆÂêçÂâç\n",
    "        num_features (int, optional): Ê≠£Ë¶èÂåñÂ±§„ÅÆÁâπÂæ¥Êï∞\n",
    "        **kwargs: Ê≠£Ë¶èÂåñÂ±§„ÅÆËøΩÂä†ÂºïÊï∞\n",
    "    Returns:\n",
    "        Optional[nn.Module]: ÊßãÁØâ„Åï„Çå„ÅüÊ≠£Ë¶èÂåñÂ±§„ÄÅ„Åæ„Åü„ÅØNone\n",
    "    \"\"\"\n",
    "    logger.info(f\"build_norm {name=} {num_features=} {kwargs=}\")\n",
    "\n",
    "    # False\n",
    "    if name in [\"ln\", \"ln2d\"]:\n",
    "        kwargs[\"normalized_shape\"] = num_features\n",
    "    else:\n",
    "        kwargs[\"num_features\"] = num_features\n",
    "\n",
    "    if name in REGISTERED_NORM_DICT:\n",
    "        norm_cls = REGISTERED_NORM_DICT[name]\n",
    "        args = build_kwargs_from_config(kwargs, norm_cls)\n",
    "        return norm_cls(**args)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc58644",
   "metadata": {},
   "source": [
    "### Ê¥ªÊÄßÂåñÈñ¢Êï∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e510b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGISTERED_ACT_DICT: dict[str, type] = {\n",
    "    \"relu\": nn.ReLU,\n",
    "    # \"relu6\": nn.ReLU6,\n",
    "    # \"hswish\": nn.Hardswish,\n",
    "    \"silu\": nn.SiLU,\n",
    "    # \"gelu\": partial(nn.GELU, approximate=\"tanh\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e410569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_act(name: str, **kwargs) -> Optional[nn.Module]:\n",
    "    \"\"\"\n",
    "    Ê¥ªÊÄßÂåñÈñ¢Êï∞„ÇíÊßãÁØâ„Åô„Çã\n",
    "    ReLU„ÅãSiLu„ÅÆ„Åø‰ΩøÁî®\n",
    "\n",
    "    Args:\n",
    "        name (str): Ê¥ªÊÄßÂåñÈñ¢Êï∞„ÅÆÂêçÂâç\n",
    "        **kwargs: Ê¥ªÊÄßÂåñÈñ¢Êï∞„ÅÆËøΩÂä†ÂºïÊï∞\n",
    "    Returns:\n",
    "        Optional[nn.Module]: ÊßãÁØâ„Åï„Çå„ÅüÊ¥ªÊÄßÂåñÈñ¢Êï∞„ÄÅ„Åæ„Åü„ÅØNone\n",
    "    \"\"\"\n",
    "    logger.info(f\"build_act {name=} {kwargs=}\")\n",
    "    if name in REGISTERED_ACT_DICT:\n",
    "        act_cls = REGISTERED_ACT_DICT[name]\n",
    "        args = build_kwargs_from_config(kwargs, act_cls)\n",
    "        return act_cls(**args)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7188dbd0",
   "metadata": {},
   "source": [
    "### Áï≥„ÅøËæº„ÅøÂ±§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca76b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelChunkConv3d(nn.Conv3d):\n",
    "    \"\"\"\n",
    "    „ÉÅ„É£„É≥„Éç„É´„Çí„ÉÅ„É£„É≥„ÇØ„Å´ÂàÜÂâ≤„Åó„Å¶Conv3d„ÇíÂÆüË°å„Åô„Çã„ÇØ„É©„Çπ\n",
    "    „É°„É¢„É™‰∏çË∂≥(OOM)„ÇíÂõûÈÅø„Åô„Çã„Åü„ÇÅ„Å´„ÄÅ„ÉÅ„É£„É≥„Éç„É´„ÇíÂàÜÂâ≤„Åó„Å¶Â∞ë„Åó„Åö„Å§Ë®àÁÆó„Åó„ÄÅÁµêÊûú„ÇíÁµêÂêà„Åô„Çã\n",
    "    ConvLayer„Å®LiteMLA„Åß‰ΩøÁî®\n",
    "    ÂêàË®à170Âõû‰ΩøÁî®\n",
    "    \"\"\"\n",
    "\n",
    "    # ÊºîÁÆó‰∏ä„ÅÆË¶ÅÁ¥†Êï∞Âà∂Èôê\n",
    "    CONV3D_NUMEL_LIMIT = 2**31\n",
    "\n",
    "    def _get_output_numel(self, input_shape: torch.Size) -> int:\n",
    "        \"\"\"\n",
    "        Âá∫Âäõ„ÉÜ„É≥„ÇΩ„É´„ÅÆÊºîÁÆó‰∏ä„ÅÆË¶ÅÁ¥†Êï∞„ÇíË®àÁÆó„Åô„Çã\n",
    "\n",
    "        Args:\n",
    "            input_shape (torch.Size): ÂÖ•Âäõ„ÉÜ„É≥„ÇΩ„É´„ÅÆÂΩ¢Áä∂\n",
    "        Returns:\n",
    "            int: Âá∫Âäõ„ÉÜ„É≥„ÇΩ„É´„ÅÆÊºîÁÆó‰∏ä„ÅÆË¶ÅÁ¥†Êï∞\n",
    "        \"\"\"\n",
    "        logger.info(f\"ChannelChunkConv3d._get_output_numel {input_shape=}\")\n",
    "\n",
    "        numel = self.out_channels\n",
    "        logger.debug(f\"ChannelChunkConv3d._get_output_numel initial {numel=}\")\n",
    "\n",
    "        # True\n",
    "        if len(input_shape) == 5:\n",
    "            numel *= input_shape[0]\n",
    "\n",
    "        for i, d in enumerate(input_shape[-3:]):\n",
    "            d_out = math.floor(\n",
    "                (d + 2 * self.padding[i] - self.dilation[i] * (self.kernel_size[i] - 1) - 1) / self.stride[i] + 1\n",
    "            )\n",
    "            numel *= d_out\n",
    "\n",
    "        logger.info(f\"ChannelChunkConv3d._get_output_numel final {numel=}\")\n",
    "        return numel\n",
    "\n",
    "    def _get_n_chunks(self, numel: int, n_channels: int):\n",
    "        \"\"\"\n",
    "        „ÉÅ„É£„É≥„ÇØÊï∞„ÇíË®àÁÆó„Åô„Çã\n",
    "        1„ÉÅ„É£„É≥„ÇØ„ÅÇ„Åü„Çä„ÅÆË¶ÅÁ¥†Êï∞„ÅåCONV3D_NUMEL_LIMIT‰ª•‰∏ã„Å´„Å™„Çã„Çà„ÅÜ„Å´„ÉÅ„É£„É≥„ÇØÊï∞„ÇíÊ±∫ÂÆö„Åô„Çã\n",
    "\n",
    "        Args:\n",
    "            numel (int): ÊºîÁÆó‰∏ä„ÅÆË¶ÅÁ¥†Êï∞\n",
    "            n_channels (int): „ÉÅ„É£„É≥„Éç„É´Êï∞\n",
    "        Returns:\n",
    "            int: „ÉÅ„É£„É≥„ÇØÊï∞\n",
    "        \"\"\"\n",
    "        logger.info(f\"ChannelChunkConv3d._get_n_chunks {numel=} {n_channels=}\")\n",
    "\n",
    "        # ÊºîÁÆó‰∏ä„ÅÆË¶ÅÁ¥†Êï∞ / Âà∂Èôê = „ÉÅ„É£„É≥„ÇØÊï∞\n",
    "        n_chunks = math.ceil(numel / ChannelChunkConv3d.CONV3D_NUMEL_LIMIT)\n",
    "\n",
    "        # „ÉÅ„É£„É≥„ÇØÊï∞„Åå„ÉÅ„É£„É≥„Éç„É´Êï∞„ÅßÂâ≤„ÇäÂàá„Çå„Çã„Çà„ÅÜ„Å´Ë™øÊï¥\n",
    "        n_chunks = ceil_to_divisible(n_chunks, n_channels)\n",
    "\n",
    "        logger.debug(f\"ChannelChunkConv3d._get_n_chunks {n_chunks=}\")\n",
    "\n",
    "        return n_chunks\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        ÂÖ•Âäõ„ÉÜ„É≥„ÇΩ„É´„Å´ÂØæ„Åó„Å¶„ÉÅ„É£„É≥„ÇØÂåñ„Åï„Çå„ÅüConv3d„ÇíÂÆüË°å„Åô„Çã\n",
    "        \"\"\"\n",
    "        logger.info(f\"ChannelChunkConv3d.forward {input.shape=}\")\n",
    "\n",
    "        # Ë¶ÅÁ¥†Êï∞„ÅåÂà∂Èôê‰ª•‰∏ã„ÅÆÂ†¥Âêà„ÄÅÈÄöÂ∏∏„ÅÆConv3d„ÇíÂÆüË°å\n",
    "        if input.numel() // input.size(0) < ChannelChunkConv3d.CONV3D_NUMEL_LIMIT:\n",
    "            return super().forward(input)\n",
    "\n",
    "        # ÂÖ•Âäõ„ÉÅ„É£„É≥„Éç„É´„ÅÆÂàÜÂâ≤Êï∞„ÇíË®àÁÆó\n",
    "        n_in_chunks = self._get_n_chunks(input.numel(), self.in_channels)\n",
    "\n",
    "        # Âá∫Âäõ„ÉÅ„É£„É≥„Éç„É´„ÅÆÂàÜÂâ≤Êï∞„ÇíË®àÁÆó\n",
    "        n_out_chunks = self._get_n_chunks(self._get_output_numel(input.shape), self.out_channels)\n",
    "\n",
    "        # ÂÖ•Âäõ„Å®Âá∫Âäõ„ÅÆ„ÉÅ„É£„É≥„ÇØÊï∞„Åå1„ÅÆÂ†¥Âêà„ÄÅÈÄöÂ∏∏„ÅÆConv3d„ÇíÂÆüË°å\n",
    "        if n_in_chunks == 1 and n_out_chunks == 1:\n",
    "            return super().forward(input)\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        # ÂÖ•Âäõ„Çí„ÉÅ„É£„É≥„ÇØ„Å´ÂàÜÂâ≤\n",
    "        input_shards = input.chunk(n_in_chunks, dim=1)\n",
    "\n",
    "        #  Èáç„Åø„Å®„Éê„Ç§„Ç¢„Çπ„Çí„ÉÅ„É£„É≥„ÇØ„Å´ÂàÜÂâ≤„Åó„Å¶Conv3d„ÇíÂÆüË°å\n",
    "        for weight, bias in zip(self.weight.chunk(n_out_chunks), self.bias.chunk(n_out_chunks)):\n",
    "\n",
    "            # Èáç„Åø„ÇíÊõ¥„Å´„ÉÅ„É£„É≥„ÇØ„Å´ÂàÜÂâ≤\n",
    "            weight_shards = weight.chunk(n_in_chunks, dim=1)\n",
    "\n",
    "            o = None\n",
    "\n",
    "            # ÂÖ•Âäõ„ÅÆ„Ç∑„É£„Éº„Éâ„Å®Èáç„Åø„ÅÆ„Ç∑„É£„Éº„Éâ„ÇíÁµÑ„ÅøÂêà„Çè„Åõ„Å¶Conv3d„ÇíÂÆüË°å\n",
    "            for x, w in zip(input_shards, weight_shards):\n",
    "\n",
    "                # ÂàùÂõû\n",
    "                if o is None:\n",
    "                    # „Éê„Ç§„Ç¢„Çπ„ÅÇ„Çä\n",
    "                    o = F.conv3d(x, w, bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "                # 2ÂõûÁõÆ‰ª•Èôç\n",
    "                else:\n",
    "                    # „Éê„Ç§„Ç¢„Çπ„Å™„Åó\n",
    "                    o += F.conv3d(x, w, None, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "            outputs.append(o)\n",
    "\n",
    "        # Âá∫Âäõ„ÇíÁµêÂêà„Åó„Å¶Ëøî„Åô\n",
    "        return torch.cat(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1339b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    2D„Åæ„Åü„ÅØ3DÁï≥„ÅøËæº„ÅøÂ±§„ÄÅÊ≠£Ë¶èÂåñ„ÄÅÊ¥ªÊÄßÂåñÈñ¢Êï∞„ÇíÁµÑ„ÅøÂêà„Çè„Åõ„ÅüÊ±éÁî®„É¢„Ç∏„É•„Éº„É´\n",
    "    build_downsample_block, build_encoder_project_in_block, GLUMBConv\n",
    "    InterpolateConvUpSamplerLayer, LiteMLA, ResBlock„Åß‰ΩøÁî®\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size=3, stride=1, dilation=1, groups=1, use_bias=False, dropout=0, norm=\"bn2d\", act_func=\"relu\", is_video=False, pad_mode_3d=\"constant\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): ÂÖ•Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "            out_channels (int): Âá∫Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "            kernel_size (int, optional): Áï≥„ÅøËæº„Åø„Ç´„Éº„Éç„É´„ÅÆ„Çµ„Ç§„Ç∫\n",
    "            stride (int, optional): Áï≥„ÅøËæº„Åø„ÅÆ„Çπ„Éà„É©„Ç§„Éâ\n",
    "            dilation (int, optional): Áï≥„ÅøËæº„Åø„ÅÆ„ÉÄ„Ç§„É¨„Éº„Ç∑„Éß„É≥\n",
    "            groups (int, optional): Áï≥„ÅøËæº„Åø„ÅÆ„Ç∞„É´„Éº„ÉóÊï∞\n",
    "            use_bias (bool, optional): „Éê„Ç§„Ç¢„Çπ„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "            dropout (float, optional): „Éâ„É≠„ÉÉ„Éó„Ç¢„Ç¶„ÉàÁéá\n",
    "            norm (str, optional): Ê≠£Ë¶èÂåñÂ±§„ÅÆÁ®ÆÈ°û\n",
    "            act_func (str, optional): Ê¥ªÊÄßÂåñÈñ¢Êï∞„ÅÆÁ®ÆÈ°û\n",
    "            is_video (bool, optional): 3DÁï≥„ÅøËæº„Åø„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "            pad_mode_3d (str, optional): 3DÁï≥„ÅøËæº„Åø„ÅÆ„Éë„Éá„Ç£„É≥„Ç∞„É¢„Éº„Éâ\n",
    "        \"\"\"\n",
    "        logger.info(f\"ConvLayer.__init__ {in_channels=} {out_channels=} {kernel_size=} {stride=} {dilation=} {groups=} {use_bias=} {dropout=} {norm=} {act_func=} {is_video=} {pad_mode_3d=}\")\n",
    "\n",
    "        # 1) ÂàùÊúüÂåñ\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.is_video = is_video # True\n",
    "\n",
    "        if self.is_video:\n",
    "            assert dilation == 1, \"only support dilation=1 for 3d conv\"\n",
    "            assert kernel_size % 2 == 1, \"only support odd kernel size for 3d conv\"\n",
    "\n",
    "            # 1-1) „Éë„Éá„Ç£„É≥„Ç∞„ÅÆË®≠ÂÆö\n",
    "\n",
    "            # Hanyuan„ÅÆCausalConv3d„Å´Âêà„Çè„Åõ„ÅüË®≠ÂÆö\n",
    "            # constant\n",
    "            self.pad_mode_3d = pad_mode_3d\n",
    "\n",
    "            # kernel_size=1„ÅÆ„Å®„Åç„ÄÅ0,0,0,0,0,0\n",
    "            # kernel_size=3„ÅÆ„Å®„Åç„ÄÅ1,1,1,1,1,1\n",
    "            padding = (\n",
    "                kernel_size // 2,\n",
    "                kernel_size // 2,\n",
    "                kernel_size // 2,\n",
    "                kernel_size // 2,\n",
    "                kernel_size // 2,\n",
    "                kernel_size // 2,\n",
    "            )\n",
    "\n",
    "            self.padding = padding\n",
    "\n",
    "            # 1-2) „Éâ„É≠„ÉÉ„Éó„Ç¢„Ç¶„Éà„ÅÆË®≠ÂÆö\n",
    "\n",
    "            # None\n",
    "            self.dropout = nn.Dropout3d(dropout, inplace=False) if dropout > 0 else None\n",
    "\n",
    "            assert isinstance(stride, (int, tuple)), \"stride must be an integer or 3-tuple for 3d conv\"\n",
    "\n",
    "\n",
    "            # 1-3) 3DÁï≥„ÅøËæº„ÅøÂ±§„ÅÆË®≠ÂÆö\n",
    "\n",
    "            # padding is handled by F.pad() in forward()\n",
    "            self.conv = ChannelChunkConv3d(\n",
    "                in_channels, # 3„Å™„Å©\n",
    "                out_channels, # 128„Å™„Å©\n",
    "                kernel_size=(kernel_size, kernel_size, kernel_size), # (3,3,3)„Å™„Å©\n",
    "                stride=(stride, stride, stride) if isinstance(stride, int) else stride, # (1,1,1)„Å™„Å©\n",
    "                groups=groups, # 1\n",
    "                bias=use_bias, # True„Å™„Å©\n",
    "            )\n",
    "\n",
    "        # False\n",
    "        else:\n",
    "            padding = get_same_padding(kernel_size)\n",
    "            padding *= dilation\n",
    "            self.dropout = nn.Dropout2d(dropout, inplace=False) if dropout > 0 else None\n",
    "            self.conv = nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=(kernel_size, kernel_size),\n",
    "                stride=(stride, stride),\n",
    "                padding=padding,\n",
    "                dilation=(dilation, dilation),\n",
    "                groups=groups,\n",
    "                bias=use_bias,\n",
    "            )\n",
    "\n",
    "        # 2) Ê≠£Ë¶èÂåñÂ±§„ÅÆË®≠ÂÆö\n",
    "\n",
    "        # rms3d or None\n",
    "        self.norm = build_norm(norm, num_features=out_channels)\n",
    "\n",
    "        # 3) Ê¥ªÊÄßÂåñÈñ¢Êï∞„ÅÆË®≠ÂÆö\n",
    "\n",
    "        # silu or None\n",
    "        self.act = build_act(act_func)\n",
    "\n",
    "        # 4) „Éë„Éá„Ç£„É≥„Ç∞Èñ¢Êï∞„ÅÆË®≠ÂÆö\n",
    "\n",
    "        self.pad = F.pad\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): ÂÖ•Âäõ„ÉÜ„É≥„ÇΩ„É´\n",
    "        Returns:\n",
    "            torch.Tensor: Âá∫Âäõ„ÉÜ„É≥„ÇΩ„É´\n",
    "        \"\"\"\n",
    "        logger.info(f\"ConvLayer.forward {x.shape=}\")\n",
    "\n",
    "        # 1) „Éâ„É≠„ÉÉ„Éó„Ç¢„Ç¶„Éà„ÅÆÈÅ©Áî®\n",
    "\n",
    "        # False\n",
    "        if self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        # 2) 3DÁï≥„ÅøËæº„Åø„ÅÆ„Ç´„Çπ„Çø„É†„Éë„Éá„Ç£„É≥„Ç∞„ÅÆÈÅ©Áî® \n",
    "\n",
    "        # True\n",
    "        if self.is_video:\n",
    "            # constant„ÅÆÂ†¥Âêà„ÄÅ„Éá„Éï„Ç©„É´„Éà„Åß0„Åß„Éë„Éá„Ç£„É≥„Ç∞\n",
    "            x = self.pad(x, self.padding, mode=self.pad_mode_3d)\n",
    "            logger.debug(f\"ConvLayer.forward after pad {x.shape=}\")\n",
    "\n",
    "        # 3) Áï≥„ÅøËæº„Åø„ÅÆÈÅ©Áî®\n",
    "\n",
    "        x = self.conv(x)\n",
    "        logger.debug(f\"ConvLayer.forward after conv {x.shape=}\")\n",
    "\n",
    "        # 4) Ê≠£Ë¶èÂåñ„ÅÆÈÅ©Áî®\n",
    "\n",
    "        if self.norm:\n",
    "            # rms3d\n",
    "            x = self.norm(x)\n",
    "\n",
    "        # 5) Ê¥ªÊÄßÂåñÈñ¢Êï∞„ÅÆÈÅ©Áî®\n",
    "\n",
    "        if self.act:\n",
    "            # silu\n",
    "            x = self.act(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8478f9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLUMBConv(nn.Module):\n",
    "    \"\"\"\n",
    "    GLU(Gated Liner Unit)„ÇíÁµÑ„ÅøËæº„Çì„Å†Inverted Bottleneck Convolution„É¢„Ç∏„É•„Éº„É´\n",
    "    EfficientViTBlock„ÅÆ„É≠„Éº„Ç´„É´„É¢„Ç∏„É•„Éº„É´„Å®„Åó„Å¶‰ΩøÁî®„Åó„ÄÅÂ±ÄÊâÄÁöÑ„Å™ÁâπÂæ¥„ÇíÊäΩÂá∫ËÉΩÂäõ„ÇíÈ´ò„ÇÅ„Çã\n",
    "    ÂêàË®à18Âõû‰ΩøÁî®\n",
    "\n",
    "    1. 1x1Áï≥„ÅøËæº„Åø„Åß„ÉÅ„É£„Éç„É´Êï∞„ÇíÊã°Âºµ\n",
    "    2. DepthwiseÁï≥„ÅøËæº„Åø„ÅßÁ©∫ÈñìÁöÑÁâπÂæ¥„ÇíÊäΩÂá∫\n",
    "    3. GLUÊ¥ªÊÄßÂåñÈñ¢Êï∞„ÅßÊÉÖÂ†±„Çí„Ç≤„Éº„Éà\n",
    "    4. 1x1Áï≥„ÅøËæº„Åø„Åß„ÉÅ„É£„Éç„É´Êï∞„ÇíÂúßÁ∏Æ„Åó„ÄÅÂÖÉ„ÅÆ„ÉÅ„É£„Éç„É´Êï∞„Å´Êàª„Åô\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size=3, stride=1, mid_channels=None, expand_ratio=6, use_bias=False, norm=(None, None, \"ln2d\"), act_func=(\"silu\", \"silu\", None), is_video=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): ÂÖ•Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "            out_channels (int): Âá∫Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "            kernel_size (int, optional): Áï≥„ÅøËæº„Åø„Ç´„Éº„Éç„É´„ÅÆ„Çµ„Ç§„Ç∫\n",
    "            stride (int, optional): Áï≥„ÅøËæº„Åø„ÅÆ„Çπ„Éà„É©„Ç§„Éâ\n",
    "            mid_channels (int, optional): ‰∏≠Èñì„ÉÅ„É£„Éç„É´Êï∞\n",
    "            expand_ratio (float, optional): Êã°ÂºµÊØîÁéá\n",
    "            use_bias (bool or tuple, optional): „Éê„Ç§„Ç¢„Çπ„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "            norm (str or tuple, optional): Ê≠£Ë¶èÂåñÂ±§„ÅÆÁ®ÆÈ°û\n",
    "            act_func (str or tuple, optional): Ê¥ªÊÄßÂåñÈñ¢Êï∞„ÅÆÁ®ÆÈ°û\n",
    "            is_video (bool, optional): 3DÁï≥„ÅøËæº„Åø„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "        \"\"\"\n",
    "        logger.info(f\"GLUMBConv.__init__ {in_channels=} {out_channels=} {kernel_size=} {stride=} {mid_channels=} {expand_ratio=} {use_bias=} {norm=} {act_func=} {is_video=}\")\n",
    "\n",
    "        # 1) ÂàùÊúüÂåñ\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # (True, True, False)\n",
    "        use_bias = val2tuple(use_bias, 3)\n",
    "\n",
    "        # (None, None, \"rms3d\")\n",
    "        norm = val2tuple(norm, 3)\n",
    "\n",
    "        # (\"silu\", \"silu\", None)\n",
    "        act_func = val2tuple(act_func, 3)\n",
    "\n",
    "        # 512 * 4 = 2048„Å™„Å©\n",
    "        mid_channels = round(in_channels * expand_ratio) if mid_channels is None else mid_channels\n",
    "\n",
    "        # 2) GLUÊ¥ªÊÄßÂåñÈñ¢Êï∞„ÅÆË®≠ÂÆö\n",
    "\n",
    "        self.glu_act = build_act(act_func[1], inplace=False)\n",
    "\n",
    "        # 3) „ÉÅ„É£„É≥„Éç„É´Êï∞Êã°ÂºµÔºàinvertedÔºâÁî®„ÅÆ1x1Áï≥„ÅøËæº„ÅøÂ±§„ÅÆÊßãÁØâ\n",
    "\n",
    "        self.inverted_conv = ConvLayer(\n",
    "            in_channels, # 512„Å™„Å©\n",
    "            mid_channels * 2, # 2048 * 2 = 4096„Å™„Å©\n",
    "            1,\n",
    "            use_bias=use_bias[0], # True\n",
    "            norm=norm[0], # None\n",
    "            act_func=act_func[0], # \"silu\"\n",
    "            is_video=is_video, # True\n",
    "        )\n",
    "\n",
    "        # 4) DepthwiseÁï≥„ÅøËæº„ÅøÂ±§„ÅÆÊßãÁØâ\n",
    "\n",
    "        self.depth_conv = ConvLayer(\n",
    "            mid_channels * 2, # 2048 * 2 = 4096„Å™„Å©\n",
    "            mid_channels * 2, # 2048 * 2 = 4096„Å™„Å©\n",
    "            kernel_size, # 3\n",
    "            stride=stride, # 1\n",
    "            groups=mid_channels * 2, # 2048 * 2 = 4096„Å™„Å©\n",
    "            use_bias=use_bias[1], # True\n",
    "            norm=norm[1], # None\n",
    "            act_func=None,\n",
    "            is_video=is_video, # True\n",
    "        )\n",
    "\n",
    "        # 5) „ÉÅ„É£„É≥„Éç„É´Êï∞ÂúßÁ∏ÆÁî®„ÅÆPointwiseÔºà1x1ÔºâÁï≥„ÅøËæº„ÅøÂ±§„ÅÆÊßãÁØâ\n",
    "\n",
    "        self.point_conv = ConvLayer(\n",
    "            mid_channels, # 2048„Å™„Å©\n",
    "            out_channels, # 512„Å™„Å©\n",
    "            1,\n",
    "            use_bias=use_bias[2], # False\n",
    "            norm=norm[2], # rms3d\n",
    "            act_func=act_func[2], # None\n",
    "            is_video=is_video, # True\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logger.info(f\"GLUMBConv.forward {x.shape=}\")\n",
    "\n",
    "        # 1) „ÉÅ„É£„É≥„Éç„É´Êï∞Êã°ÂºµÔºàinvertedÔºâ\n",
    "\n",
    "        x = self.inverted_conv(x)\n",
    "        logger.debug(f\"GLUMBConv.forward after inverted_conv {x.shape=}\")\n",
    "\n",
    "        # 2) DepthwiseÁï≥„ÅøËæº„Åø\n",
    "\n",
    "        x = self.depth_conv(x)\n",
    "        logger.debug(f\"GLUMBConv.forward after depth_conv {x.shape=}\")\n",
    "\n",
    "        # 3) GLUÊ¥ªÊÄßÂåñÈñ¢Êï∞„ÅÆÈÅ©Áî®\n",
    "\n",
    "        # „ÉÅ„É£„Éç„É´„Çí2„Å§„Å´ÂàÜÂâ≤\n",
    "        x, gate = torch.chunk(x, 2, dim=1)\n",
    "\n",
    "        # GLUÊ¥ªÊÄßÂåñÈñ¢Êï∞(silu)„ÇíÈÅ©Áî®„Åó„ÄÅÊÉÖÂ†±„ÅÆÈÄöÈÅéÈáè„ÅÆ„Ç≤„Éº„Éà„ÇíÁîüÊàê\n",
    "        gate = self.glu_act(gate)\n",
    "\n",
    "        # Ë¶ÅÁ¥†„Åî„Å®„Å´‰πóÁÆó„Åó„Å¶„Ç≤„Éº„Éà„ÇíÈÅ©Áî®\n",
    "        x = x * gate\n",
    "\n",
    "        logger.debug(f\"GLUMBConv.forward after GLU {x.shape=}\")\n",
    "\n",
    "        # 4) „ÉÅ„É£„É≥„Éç„É´Êï∞ÂúßÁ∏Æ\n",
    "\n",
    "        x = self.point_conv(x)\n",
    "\n",
    "        logger.debug(f\"GLUMBConv.forward after point_conv {x.shape=}\")\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aecd9ea",
   "metadata": {},
   "source": [
    "### „Éî„ÇØ„Çª„É´„Ç∑„É£„ÉÉ„Éï„É™„É≥„Ç∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e2f06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_unshuffle_3d(x, downsample_factor):\n",
    "    \"\"\"\n",
    "    3D„Éî„ÇØ„Çª„É´„Ç¢„É≥„Ç∑„É£„ÉÉ„Éï„É´\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): ÂÖ•Âäõ„ÉÜ„É≥„ÇΩ„É´ (B, C, T, H, W)\n",
    "        downsample_factor (int): „ÉÄ„Ç¶„É≥„Çµ„É≥„Éó„É™„É≥„Ç∞Âõ†Â≠ê\n",
    "    Returns:\n",
    "        torch.Tensor: Âá∫Âäõ„ÉÜ„É≥„ÇΩ„É´ (B, C * r^3, T/r, H/r, W/r)\n",
    "    \"\"\"\n",
    "    logger.info(f\"pixel_unshuffle_3d {x.shape=} {downsample_factor=}\")\n",
    "\n",
    "    B, C, T, H, W = x.shape\n",
    "\n",
    "    r = downsample_factor\n",
    "    assert T % r == 0, f\"Êó∂Èó¥Áª¥Â∫¶ÂøÖÈ°ªÊòØ‰∏ãÈááÊ†∑Âõ†Â≠êÁöÑÂÄçÊï∞, got shape {x.shape}\"\n",
    "    assert H % r == 0, f\"È´òÂ∫¶Áª¥Â∫¶ÂøÖÈ°ªÊòØ‰∏ãÈááÊ†∑Âõ†Â≠êÁöÑÂÄçÊï∞, got shape {x.shape}\"\n",
    "    assert W % r == 0, f\"ÂÆΩÂ∫¶Áª¥Â∫¶ÂøÖÈ°ªÊòØ‰∏ãÈááÊ†∑Âõ†Â≠êÁöÑÂÄçÊï∞, got shape {x.shape}\"\n",
    "\n",
    "    # ÊôÇÈñì, È´ò„Åï, ÂπÖ„ÅÆÂêÑÊ¨°ÂÖÉ„Çír„ÅßÂàÜÂâ≤\n",
    "    T_new = T // r\n",
    "    H_new = H // r\n",
    "    W_new = W // r\n",
    "\n",
    "    # „ÉÅ„É£„Éç„É´Êï∞„Çír^3ÂÄç„Å´Â¢óÂä†\n",
    "    C_new = C * (r * r * r)\n",
    "\n",
    "    # r„ÅßÂàÜÂâ≤\n",
    "    # (B, C, T, H, W) -> (B, C, T_new, r, H_new, r, W_new, r)\n",
    "    x = x.view(B, C, T_new, r, H_new, r, W_new, r)\n",
    "\n",
    "    # (B, C, T_new, r, H_new, r, W_new, r) -> (B, C, r, r, r, T_new, H_new, W_new)\n",
    "    x = x.permute(0, 1, 3, 5, 7, 2, 4, 6)\n",
    "\n",
    "    # „ÉÅ„É£„É≥„Éç„É´Ê¨°ÂÖÉ„ÇíÁµêÂêà\n",
    "    # (B, C, r, r, r, T_new, H_new, W_new) -> (B, C * r^3, T_new, H_new, W_new)\n",
    "    y = x.reshape(B, C_new, T_new, H_new, W_new)\n",
    "\n",
    "    logger.debug(f\"pixel_unshuffle_3d output {y.shape=}\")\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02842da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelUnshuffleChannelAveragingDownSampleLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    „Éî„ÇØ„Çª„É´„Ç¢„É≥„Ç∑„É£„ÉÉ„Éï„É´„Å®„ÉÅ„É£„Éç„É´Âπ≥ÂùáÂåñ„Å´„Çà„Çã„ÉÄ„Ç¶„É≥„Çµ„É≥„Éó„É™„É≥„Ç∞Â±§\n",
    "    build_downsample_block„Åß5Âõû‰ΩøÁî®„Åó„ÄÅbuild_encoder_project_out_block„Åß1Âõû‰ΩøÁî®\n",
    "    build_downsample_block„Åß„ÅØ„ÄÅÁï≥„ÅøËæº„ÅøÂ±§„ÅÆÊÆãÂ∑ÆÊé•Á∂ö„ÅÆ„Ç∑„Éß„Éº„Éà„Ç´„ÉÉ„Éà„Å®„Åó„Å¶‰ΩøÁî®\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, factor: int, temporal_downsample: bool = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): ÂÖ•Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "            out_channels (int): Âá∫Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "            factor (int): „ÉÄ„Ç¶„É≥„Çµ„É≥„Éó„É™„É≥„Ç∞„Éï„Ç°„ÇØ„Çø„Éº\n",
    "            temporal_downsample (bool, optional):\n",
    "                5DÂÖ•Âäõ„ÉÜ„É≥„ÇΩ„É´„Å´ÂØæ„Åó„Å¶ÊôÇÈñìÊñπÂêë„ÅÆ„ÉÄ„Ç¶„É≥„Çµ„É≥„Éó„É™„É≥„Ç∞„ÇíË°å„ÅÜ„Åã„Å©„ÅÜ„Åã\n",
    "        \"\"\"\n",
    "        logger.info(f\"PixelUnshuffleChannelAveragingDownSampleLayer.__init__ {in_channels=} {out_channels=} {factor=} {temporal_downsample=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels # 128„Å™„Å©\n",
    "        self.out_channels = out_channels # 256„Å™„Å©\n",
    "        self.factor = factor # 2„Å™„Å©\n",
    "        self.temporal_downsample = temporal_downsample # False„Å™„Å©\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logger.info(f\"PixelUnshuffleChannelAveragingDownSampleLayer.forward {x.shape=}\")\n",
    "\n",
    "        # (1, 128, 4, 256, 256)„Å™„Å©\n",
    "\n",
    "        # False\n",
    "        if x.dim() == 4:\n",
    "            assert self.in_channels * self.factor**2 % self.out_channels == 0\n",
    "            group_size = self.in_channels * self.factor**2 // self.out_channels\n",
    "            x = F.pixel_unshuffle(x, self.factor)\n",
    "            B, C, H, W = x.shape\n",
    "            x = x.view(B, self.out_channels, group_size, H, W)\n",
    "            x = x.mean(dim=2)\n",
    "\n",
    "        # True\n",
    "        elif x.dim() == 5:  # [B, C, T, H, W]\n",
    "\n",
    "            # 1) „Éî„ÇØ„Çª„É´„Ç¢„É≥„Ç∑„É£„ÉÉ„Éï„É´„ÅÆÈÅ©Áî®\n",
    "\n",
    "            # 4„Å™„Å©\n",
    "            _, _, T, _, _ = x.shape\n",
    "\n",
    "            # True or False\n",
    "            if self.temporal_downsample and T != 1:\n",
    "                # 3D„Éî„ÇØ„Çª„É´„Ç¢„É≥„Ç∑„É£„ÉÉ„Éï„É´\n",
    "                x = pixel_unshuffle_3d(x, self.factor)\n",
    "\n",
    "                assert self.in_channels * self.factor**3 % self.out_channels == 0\n",
    "\n",
    "                # „ÉÅ„É£„É≥„Éç„É´Âπ≥ÂùáÂåñ„ÅÆ„Åü„ÇÅ„ÅÆ„Ç∞„É´„Éº„Éó„Çµ„Ç§„Ç∫\n",
    "                group_size = self.in_channels * self.factor**3 // self.out_channels\n",
    "            else:\n",
    "                # (B, C, T, H, W) -> (B, T, C, H, W)\n",
    "                x = x.permute(0, 2, 1, 3, 4)\n",
    "\n",
    "                # 2D„Éî„ÇØ„Çª„É´„Ç¢„É≥„Ç∑„É£„ÉÉ„Éï„É´\n",
    "                x = F.pixel_unshuffle(x, self.factor)\n",
    "\n",
    "                # (B, T, C_new, H_new, W_new) -> (B, C_new, T, H_new, W_new)\n",
    "                x = x.permute(0, 2, 1, 3, 4)\n",
    "\n",
    "                assert self.in_channels * self.factor**2 % self.out_channels == 0\n",
    "\n",
    "                # „ÉÅ„É£„É≥„Éç„É´Âπ≥ÂùáÂåñ„ÅÆ„Åü„ÇÅ„ÅÆ„Ç∞„É´„Éº„Éó„Çµ„Ç§„Ç∫\n",
    "                group_size = self.in_channels * self.factor**2 // self.out_channels\n",
    "\n",
    "            B, C, T, H, W = x.shape\n",
    "            logger.debug(f\"After pixel unshuffle: {x.shape=}\")\n",
    "\n",
    "            # (B, C, T, H, W) -> (B, C_out, group_size, T, H, W)\n",
    "            x = x.view(B, self.out_channels, group_size, T, H, W)\n",
    "\n",
    "            # 2) „ÉÅ„É£„Éç„É´Âπ≥ÂùáÂåñ\n",
    "\n",
    "            # (B, C_out, group_size, T, H, W) -> (B, C_out, T, H, W)\n",
    "            x = x.mean(dim=2)\n",
    "            logger.debug(f\"After channel averaging: {x.shape=}\")\n",
    "\n",
    "        # False\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported input dimension: {x.dim()}\")\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"PixelUnshuffleChannelAveragingDownSampleLayer(in_channels={self.in_channels}, out_channels={self.out_channels}, factor={self.factor}), temporal_downsample={self.temporal_downsample}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8248d11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_shuffle_3d(x, upscale_factor):\n",
    "    \"\"\"\n",
    "    3D„Éî„ÇØ„Çª„É´„Ç∑„É£„ÉÉ„Éï„É´\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): ÂÖ•Âäõ„ÉÜ„É≥„ÇΩ„É´ (B, C, T, H, W)\n",
    "        upscale_factor (int): „Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É™„É≥„Ç∞Âõ†Â≠ê\n",
    "    Returns:\n",
    "        torch.Tensor: Âá∫Âäõ„ÉÜ„É≥„ÇΩ„É´ (B, C / r^3, T*r, H*r, W*r)\n",
    "    \"\"\"\n",
    "    logger.info(f\"pixel_shuffle_3d {x.shape=} {upscale_factor=}\")\n",
    "\n",
    "    B, C, T, H, W = x.shape\n",
    "    r = upscale_factor\n",
    "    assert C % (r * r * r) == 0, \"ÈÄöÈÅìÊï∞ÂøÖÈ°ªÊòØ‰∏äÈááÊ†∑Âõ†Â≠êÁöÑÁ´ãÊñπÂÄçÊï∞\"\n",
    "\n",
    "    # „ÉÅ„É£„Éç„É´Êï∞„Çír^3„ÅßÊ∏õÂ∞ë\n",
    "    C_new = C // (r * r * r)\n",
    "\n",
    "    # „ÉÅ„É£„Éç„É´Ê¨°ÂÖÉ„ÇíÂàÜÂâ≤\n",
    "    # (B, C, T, H, W) -> (B, C_new, r, r, r, T, H, W)\n",
    "    x = x.view(B, C_new, r, r, r, T, H, W)\n",
    "\n",
    "    # (B, C_new, r, r, r, T, H, W) -> (B, C_new, T, r, H, r, W, r)\n",
    "    x = x.permute(0, 1, 5, 2, 6, 3, 7, 4)\n",
    "\n",
    "    # r„ÅßÁµêÂêà\n",
    "    # (B, C_new, T, r, H, r, W, r) -> (B, C_new, T * r, H * r, W * r)\n",
    "    y = x.reshape(B, C_new, T * r, H * r, W * r)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfa12d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelDuplicatingPixelShuffleUpSampleLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    „ÉÅ„É£„É≥„Éç„É´Ë§áË£Ω„Å®„Éî„ÇØ„Çª„É´„Ç∑„É£„ÉÉ„Éï„É´„Å´„Çà„Çã„Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É™„É≥„Ç∞Â±§\n",
    "    build_upsample_block, build_decoder_project_in_block„Åß‰ΩøÁî®\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, factor: int, temporal_upsample: bool = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): ÂÖ•Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "            out_channels (int): Âá∫Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "            factor (int): „Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É™„É≥„Ç∞„Éï„Ç°„ÇØ„Çø„Éº\n",
    "            temporal_upsample (bool, optional):\n",
    "                5DÂÖ•Âäõ„ÉÜ„É≥„ÇΩ„É´„Å´ÂØæ„Åó„Å¶ÊôÇÈñìÊñπÂêë„ÅÆ„Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É™„É≥„Ç∞„ÇíË°å„ÅÜ„Åã„Å©„ÅÜ„Åã\n",
    "        \"\"\"\n",
    "        logger.info(f\"ChannelDuplicatingPixelShuffleUpSampleLayer.__init__ {in_channels=} {out_channels=} {factor=} {temporal_upsample=}\")\n",
    "        super().__init__()\n",
    "\n",
    "        # ÂÖ•Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "        # ‰æã: 128\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        # Âá∫Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "        # ‰æã: 1024\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # „Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É™„É≥„Ç∞„Éï„Ç°„ÇØ„Çø„Éº\n",
    "        # ‰æã: 1\n",
    "        self.factor = factor\n",
    "\n",
    "        assert out_channels * factor**2 % in_channels == 0\n",
    "\n",
    "        # ÊôÇÈñìÊñπÂêë„ÅÆ„Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É™„É≥„Ç∞„ÇíË°å„ÅÜ„Åã„Å©„ÅÜ„Åã\n",
    "        # ‰æã: False\n",
    "        self.temporal_upsample = temporal_upsample\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logger.info(f\"ChannelDuplicatingPixelShuffleUpSampleLayer.forward {x.shape=}\")\n",
    "\n",
    "        # ‰æã: (1, 128, 1, 8, 8)\n",
    "\n",
    "        # 1) ÂÖ•Âäõ„ÇíË§áË£Ω\n",
    "\n",
    "        # True\n",
    "        if x.dim() == 5:\n",
    "            B, C, T, H, W = x.shape\n",
    "            assert C == self.in_channels\n",
    "\n",
    "        # False\n",
    "        if self.temporal_upsample and T != 1:  # video input\n",
    "            repeats = self.out_channels * self.factor**3 // self.in_channels\n",
    "            logger.debug(f\"temporal_upsample repeats (T!=1): {repeats}\")\n",
    "        else:\n",
    "            repeats = self.out_channels * self.factor**2 // self.in_channels\n",
    "            logger.debug(f\"temporal_upsample repeats (T==1): {repeats}\")\n",
    "\n",
    "        # ÂÖ•Âäõ„Çí„ÉÅ„É£„Éç„É´ÊñπÂêë„Å´Áπ∞„ÇäËøî„ÅóË§áË£Ω\n",
    "        x = x.repeat_interleave(repeats, dim=1)\n",
    "        logger.debug(f\"after repeat_interleave: {x.shape}\")\n",
    "\n",
    "        # 2) „Éî„ÇØ„Çª„É´„Ç∑„É£„ÉÉ„Éï„É´„Åß„Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É™„É≥„Ç∞\n",
    "\n",
    "        # False\n",
    "        if x.dim() == 4:  # original image-only training\n",
    "            x = F.pixel_shuffle(x, self.factor)\n",
    "\n",
    "        # True\n",
    "        elif x.dim() == 5:  # [B, C, T, H, W]\n",
    "\n",
    "            # True or False\n",
    "            if self.temporal_upsample and T != 1:\n",
    "\n",
    "                # 3D„Éî„ÇØ„Çª„É´„Ç∑„É£„ÉÉ„Éï„É´„ÇíÈÅ©Áî®\n",
    "                x = pixel_shuffle_3d(\n",
    "                    x,\n",
    "                    self.factor # 1 or 2\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                # (B, C, T, H, W) -> (B, T, C, H, W)\n",
    "                x = x.permute(0, 2, 1, 3, 4)\n",
    "\n",
    "                # 2D„Éî„ÇØ„Çª„É´„Ç∑„É£„ÉÉ„Éï„É´„ÇíÈÅ©Áî®\n",
    "                x = F.pixel_shuffle(\n",
    "                    x,\n",
    "                    self.factor # 1 or 2\n",
    "                )\n",
    "                logger.debug(f\"after pixel_shuffle: {x.shape}\")\n",
    "\n",
    "                # (B, T, C_new, H_new, W_new) -> (B, C_new, T, H_new, W_new)\n",
    "                x = x.permute(0, 2, 1, 3, 4)\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"ChannelDuplicatingPixelShuffleUpSampleLayer(in_channels={self.in_channels}, out_channels={self.out_channels}, factor={self.factor}, temporal_upsample={self.temporal_upsample})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53173154",
   "metadata": {},
   "source": [
    "### InterpolateConvUpSamplerLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04050c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "\n",
    "def chunked_interpolate(x, scale_factor, mode=\"nearest\"):\n",
    "    \"\"\"\n",
    "    „ÉÅ„É£„É≥„Éç„É´„ÅÆÊ¨°ÂÖÉ„Å´Ê≤ø„Å£„Å¶„ÉÅ„É£„É≥„ÇØÂåñ„Åó„Å¶Â§ß„Åç„Å™„ÉÜ„É≥„ÇΩ„É´„ÇíË£úÈñì„Åô„Çã\n",
    "    ÁèæÂú®„ÅØ„Äånearest„ÄçË£úÈñì„É¢„Éº„Éâ„ÅÆ„Åø„Çµ„Éù„Éº„Éà\n",
    "\n",
    "    https://discuss.pytorch.org/t/error-using-f-interpolate-for-large-3d-input/207859\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): ÂÖ•Âäõ„ÉÜ„É≥„ÇΩ„É´ (B, C, D, H, W)\n",
    "        scale_factor: „Çπ„Ç±„Éº„É™„É≥„Ç∞„Éï„Ç°„ÇØ„Çø„Éº„ÅÆ„Çø„Éó„É´ (d, h, w)\n",
    "    Returns:\n",
    "        torch.Tensor: Ë£úÈñì„Åï„Çå„Åü„ÉÜ„É≥„ÇΩ„É´\n",
    "    \"\"\"\n",
    "    logger.info(f\"chunked_interpolate {x.shape=} {scale_factor=} {mode=}\")\n",
    "\n",
    "    assert (\n",
    "        mode == \"nearest\"\n",
    "    ), \"Only the nearest mode is supported\"  # actually other modes are theoretically supported but not tested\n",
    "    if len(x.shape) != 5:\n",
    "        raise ValueError(\"Expected 5D input tensor (B, C, D, H, W)\")\n",
    "\n",
    "    # Calculate max chunk size to avoid int32 overflow. num_elements < max_int32\n",
    "    # Max int32 is 2^31 - 1\n",
    "    max_elements_per_chunk = 2**31 - 1\n",
    "\n",
    "    # Calculate output spatial dimensions\n",
    "    out_d = math.ceil(x.shape[2] * scale_factor[0])\n",
    "    out_h = math.ceil(x.shape[3] * scale_factor[1])\n",
    "    out_w = math.ceil(x.shape[4] * scale_factor[2])\n",
    "\n",
    "    # Calculate max channels per chunk to stay under limit\n",
    "    elements_per_channel = out_d * out_h * out_w\n",
    "    max_channels = max_elements_per_chunk // (x.shape[0] * elements_per_channel)\n",
    "\n",
    "    # Use smaller of max channels or input channels\n",
    "    chunk_size = min(max_channels, x.shape[1])\n",
    "\n",
    "    # Ensure at least 1 channel per chunk\n",
    "    chunk_size = max(1, chunk_size)\n",
    "    if VERBOSE:\n",
    "        print(f\"Input channels: {x.shape[1]}\")\n",
    "        print(f\"Chunk size: {chunk_size}\")\n",
    "        print(f\"max_channels: {max_channels}\")\n",
    "        print(f\"num_chunks: {math.ceil(x.shape[1] / chunk_size)}\")\n",
    "\n",
    "    chunks = []\n",
    "    for i in range(0, x.shape[1], chunk_size):\n",
    "        start_idx = i\n",
    "        end_idx = min(i + chunk_size, x.shape[1])\n",
    "\n",
    "        chunk = x[:, start_idx:end_idx, :, :, :]\n",
    "\n",
    "        interpolated_chunk = F.interpolate(chunk, scale_factor=scale_factor, mode=\"nearest\")\n",
    "\n",
    "        chunks.append(interpolated_chunk)\n",
    "\n",
    "    if not chunks:\n",
    "        raise ValueError(f\"No chunks were generated. Input shape: {x.shape}\")\n",
    "\n",
    "    # Concatenate chunks along channel dimension\n",
    "    return torch.cat(chunks, dim=1)\n",
    "\n",
    "class InterpolateConvUpSampleLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Ë£úÈñì„Å®Áï≥„ÅøËæº„Åø„ÇíÁµÑ„ÅøÂêà„Çè„Åõ„Åü„Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É™„É≥„Ç∞Â±§\n",
    "    build_upsample_block„Åß‰ΩøÁî®\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, factor: int, mode: str = \"nearest\", is_video: bool = False, temporal_upsample: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): ÂÖ•Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "            out_channels (int): Âá∫Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "            kernel_size (int): Áï≥„ÅøËæº„Åø„Ç´„Éº„Éç„É´„ÅÆ„Çµ„Ç§„Ç∫\n",
    "            factor (int): „Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É™„É≥„Ç∞„Éï„Ç°„ÇØ„Çø„Éº\n",
    "            mode (str, optional): Ë£úÈñì„É¢„Éº„Éâ\n",
    "            is_video (bool, optional): 3DÁï≥„ÅøËæº„Åø„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "            temporal_upsample (bool, optional):\n",
    "                5DÂÖ•Âäõ„ÉÜ„É≥„ÇΩ„É´„Å´ÂØæ„Åó„Å¶ÊôÇÈñìÊñπÂêë„ÅÆ„Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É™„É≥„Ç∞„ÇíË°å„ÅÜ„Åã„Å©„ÅÜ„Åã\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.factor = factor\n",
    "        self.mode = mode\n",
    "        self.temporal_upsample = temporal_upsample\n",
    "        self.conv = ConvLayer(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            use_bias=True,\n",
    "            norm=None,\n",
    "            act_func=None,\n",
    "            is_video=is_video,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logger.info(f\"InterpolateConvUpSampleLayer.forward {x.shape=}\")\n",
    "        if x.dim() == 4:\n",
    "            x = F.interpolate(x, scale_factor=self.factor, mode=self.mode)\n",
    "        elif x.dim() == 5:\n",
    "            # [B, C, T, H, W] -> [B, C, T*factor, H*factor, W*factor]\n",
    "            if self.temporal_upsample and x.size(2) != 1:  # temporal upsample for video input\n",
    "                x = chunked_interpolate(x, scale_factor=[self.factor, self.factor, self.factor], mode=self.mode)\n",
    "            else:\n",
    "                x = chunked_interpolate(x, scale_factor=[1, self.factor, self.factor], mode=self.mode)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"InterpolateConvUpSampleLayer(factor={self.factor}, mode={self.mode}, temporal_upsample={self.temporal_upsample})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ebc7f5",
   "metadata": {},
   "source": [
    "### „Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥Ê©üÊßã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2013d84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiteMLA(nn.Module):\n",
    "    \"\"\"\n",
    "    ËªΩÈáè„Å™„Éû„É´„ÉÅ„Çπ„Ç±„Éº„É´Á∑öÂΩ¢„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥Ê©üÊßãÔºàLightweight Multi-scale Linear AttentionÔºâ\n",
    "    EfficientViTBlock„ÅßÂêàË®à18Âõû‰ΩøÁî®\n",
    "    Softmax„Åß„ÅØ„Å™„ÅèReLU„Çí‰ΩøÁî®„Åó„ÄÅË°åÂàóÁ©ç„ÅÆË®àÁÆóÈ†ÜÂ∫è„ÇíÂ§â„Åà„Çã„Åì„Å®„ÅßË®àÁÆó„Ç≥„Çπ„Éà„ÇíO(N^2)„Åã„ÇâO(N)„Å´ÂâäÊ∏õ\n",
    "    „Çπ„Ç±„Éº„É´ÊôÇ„Å´Áï≥„ÅøËæº„Åø„Çí‰ΩøÁî®„Åô„Çã„Åì„Å®„Åß„ÄÅÂ±ÄÊâÄÁöÑ„Å™ÊÉÖÂ†±„ÇíÊçâ„Åà„Çã\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, heads: Optional[int] = None, heads_ratio: float = 1.0, dim=8, use_bias=False, norm=(None, \"bn2d\"), act_func=(None, None), kernel_func=\"relu\", scales: tuple[int, ...] = (5,), eps=1.0e-15, is_video=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): ÂÖ•Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "            out_channels (int): Âá∫Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "            heads (Optional[int], optional): „Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥„Éò„ÉÉ„ÉâÊï∞\n",
    "            heads_ratio (float, optional): „Éò„ÉÉ„ÉâÊï∞„ÅÆÊØîÁéá\n",
    "            dim (int, optional): ÂêÑ„Éò„ÉÉ„Éâ„ÅÆÊ¨°ÂÖÉÊï∞\n",
    "            use_bias (bool or tuple, optional): „Éê„Ç§„Ç¢„Çπ„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "            norm (str or tuple, optional): Ê≠£Ë¶èÂåñÂ±§„ÅÆÁ®ÆÈ°û\n",
    "            act_func (str or tuple, optional): Ê¥ªÊÄßÂåñÈñ¢Êï∞„ÅÆÁ®ÆÈ°û\n",
    "            kernel_func (str, optional): „Ç´„Éº„Éç„É´Èñ¢Êï∞„ÅÆÁ®ÆÈ°û\n",
    "            scales (tuple[int, ...], optional): „Éû„É´„ÉÅ„Çπ„Ç±„Éº„É´Áï≥„ÅøËæº„Åø„ÅÆ„Çπ„Ç±„Éº„É´\n",
    "            eps (float, optional): Êï∞ÂÄ§ÂÆâÂÆöÊÄß„ÅÆ„Åü„ÇÅ„ÅÆÂ∞è„Åï„Å™ÂÄ§\n",
    "            is_video (bool, optional): 3DÁï≥„ÅøËæº„Åø„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "        \"\"\"\n",
    "        logger.info(f\"LiteMLA.__init__ {in_channels=} {out_channels=} {heads=} {heads_ratio=} {dim=} {use_bias=} {norm=} {act_func=} {kernel_func=} {scales=} {eps=} {is_video=}\")\n",
    "\n",
    "        # 1) ÂàùÊúüÂåñ\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.eps = eps # 1e-15\n",
    "\n",
    "        # 512 // 32 * 1.0 = 32„Å™„Å©\n",
    "        heads = int(in_channels // dim * heads_ratio) if heads is None else heads\n",
    "\n",
    "        # 32 * 32 = 1024„Å™„Å©\n",
    "        total_dim = heads * dim\n",
    "\n",
    "        # (False, False)\n",
    "        use_bias = val2tuple(use_bias, 2)\n",
    "\n",
    "        # (rms3d, rms3d)\n",
    "        norm = val2tuple(norm, 2)\n",
    "\n",
    "        # (None, None)\n",
    "        act_func = val2tuple(act_func, 2)\n",
    "\n",
    "        # 32\n",
    "        self.dim = dim\n",
    "\n",
    "        # 2) Q, K, V„ÇíÁîüÊàê„Åô„ÇãÁï≥„ÅøËæº„ÅøÂ±§„ÇíÊßãÁØâ\n",
    "\n",
    "        self.qkv = ConvLayer(\n",
    "            in_channels,\n",
    "            3 * total_dim, # 1024 * 3 = 3072„Å™„Å©\n",
    "            1,\n",
    "            use_bias=use_bias[0], # False\n",
    "            norm=norm[0], # rms3d\n",
    "            act_func=act_func[0], # None\n",
    "            is_video=is_video, # True\n",
    "        )\n",
    "\n",
    "        # 3) „Éû„É´„ÉÅ„Çπ„Ç±„Éº„É´Áï≥„ÅøËæº„ÅøÂ±§„ÇíÊßãÁØâ\n",
    "\n",
    "        # ChannelChunkConv3d\n",
    "        conv_class = nn.Conv2d if not is_video else ChannelChunkConv3d\n",
    "\n",
    "        # „Çπ„Ç±„Éº„É´„Åî„Å®„Å´Áï≥„ÅøËæº„ÅøÂ±§„ÇíÊßãÁØâÔºàÂêàË®à1„Å§Ôºâ\n",
    "        self.aggreg = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    conv_class(\n",
    "                        3 * total_dim, # 1024 * 3 = 3072„Å™„Å©\n",
    "                        3 * total_dim, # 1024 * 3 = 3072„Å™„Å©\n",
    "                        scale, # 5 \n",
    "                        padding=get_same_padding(scale), # 2\n",
    "                        groups=3 * total_dim, # 1024 * 3 = 3072„Å™„Å©\n",
    "                        bias=use_bias[0], # False\n",
    "                    ),\n",
    "                    conv_class(\n",
    "                        3 * total_dim, # 1024 * 3 = 3072„Å™„Å©\n",
    "                        3 * total_dim, # 1024 * 3 = 3072„Å™„Å©\n",
    "                        1,\n",
    "                        groups=3 * heads, # 32 * 3 = 96„Å™„Å©\n",
    "                        bias=use_bias[0] # False\n",
    "                    ),\n",
    "                )\n",
    "                for scale in scales\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # 4) „Ç´„Éº„Éç„É´Èñ¢Êï∞„ÅÆÊßãÁØâ\n",
    "\n",
    "        # relu\n",
    "        self.kernel_func = build_act(kernel_func, inplace=False)\n",
    "\n",
    "        # 5) Âá∫ÂäõÊäïÂΩ±Â±§„ÅÆÊßãÁØâ\n",
    "\n",
    "        self.proj = ConvLayer(\n",
    "            total_dim * (1 + len(scales)), # 1024 * (1 + 1) = 2048„Å™„Å©\n",
    "            out_channels, # 1024„Å™„Å©\n",
    "            1,\n",
    "            use_bias=use_bias[1], # False\n",
    "            norm=norm[1], # rms3d\n",
    "            act_func=act_func[1], # None\n",
    "            is_video=is_video, # True\n",
    "        )\n",
    "\n",
    "    @torch.autocast(device_type=\"cuda\", enabled=False)\n",
    "    def relu_linear_att(self, qkv: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        ReLU„Ç´„Éº„Éç„É´„ÇíÁî®„ÅÑ„ÅüÁ∑öÂΩ¢„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥\n",
    "        \"\"\"\n",
    "        logger.info(f\"LiteMLA.relu_linear_att {qkv.shape=}\")\n",
    "\n",
    "        # 1) Q, K, V„ÅÆÂàÜÂâ≤„Å®Êï¥ÂΩ¢\n",
    "\n",
    "        if qkv.ndim == 5:\n",
    "            B, _, T, H, W = list(qkv.size())\n",
    "            is_video = True\n",
    "        else:\n",
    "            B, _, H, W = list(qkv.size())\n",
    "            is_video = False\n",
    "\n",
    "        if qkv.dtype == torch.float16:\n",
    "            qkv = qkv.float()\n",
    "\n",
    "        if qkv.ndim == 4:\n",
    "            qkv = torch.reshape(\n",
    "                qkv,\n",
    "                (\n",
    "                    B,\n",
    "                    -1,\n",
    "                    3 * self.dim,\n",
    "                    H * W,\n",
    "                ),\n",
    "            )\n",
    "        elif qkv.ndim == 5:\n",
    "            qkv = torch.reshape(\n",
    "                qkv,\n",
    "                (\n",
    "                    B,\n",
    "                    -1,\n",
    "                    3 * self.dim,\n",
    "                    H * W * T,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        logger.debug(f\"LiteMLA.relu_linear_att after reshape {qkv.shape=}\")\n",
    "\n",
    "        q, k, v = (\n",
    "            qkv[:, :, 0 : self.dim],\n",
    "            qkv[:, :, self.dim : 2 * self.dim],\n",
    "            qkv[:, :, 2 * self.dim :],\n",
    "        )\n",
    "\n",
    "        logger.debug(f\"LiteMLA.relu_linear_att after split {q.shape=} {k.shape=} {v.shape=}\")\n",
    "\n",
    "        # 2) ËªΩÈáèÁ∑öÂΩ¢„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥„ÅÆË®àÁÆó\n",
    "\n",
    "        q = self.kernel_func(q)\n",
    "        k = self.kernel_func(k)\n",
    "        logger.debug(f\"{q.shape=} {k.shape=} after kernel_func\")\n",
    "\n",
    "        trans_k = k.transpose(-1, -2)\n",
    "\n",
    "        v = F.pad(v, (0, 0, 0, 1), mode=\"constant\", value=1)\n",
    "        logger.debug(f\"LiteMLA.relu_linear_att after v pad {v.shape=}\")\n",
    "\n",
    "        # „Ç≠„Éº„Å®„Éê„É™„É•„Éº„ÅÆË°åÂàóÁ©ç„ÇíÂÖà„Å´Ë®àÁÆó\n",
    "        # „Éà„Éº„ÇØ„É≥Êï∞„Å´‰æùÂ≠ò„Åó„Å™„ÅÑ„Åü„ÇÅËªΩÈáè\n",
    "        vk = torch.matmul(v, trans_k)\n",
    "        logger.debug(f\"LiteMLA.relu_linear_att after vk matmul {vk.shape=}\")\n",
    "\n",
    "        # „ÇØ„Ç®„É™„Éº„Å®(vk)„ÅÆË°åÂàóÁ©ç„ÇíË®àÁÆó\n",
    "        out = torch.matmul(vk, q)\n",
    "        logger.debug(f\"LiteMLA.relu_linear_att after out matmul {out.shape=}\")\n",
    "\n",
    "        # 3) Ê≠£Ë¶èÂåñ„Å®Êï¥ÂΩ¢\n",
    "\n",
    "        if out.dtype == torch.bfloat16:\n",
    "            out = out.float()\n",
    "\n",
    "        out = out[:, :, :-1] / (out[:, :, -1:] + self.eps)\n",
    "        logger.debug(f\"LiteMLA.relu_linear_att after normalization {out.shape=}\")\n",
    "\n",
    "        # False\n",
    "        if not is_video:\n",
    "            out = torch.reshape(out, (B, -1, H, W))\n",
    "        # True\n",
    "        else:\n",
    "            out = torch.reshape(out, (B, -1, T, H, W))\n",
    "\n",
    "        logger.debug(f\"LiteMLA.relu_linear_att after reshape {out.shape=}\")\n",
    "\n",
    "        return out\n",
    "\n",
    "    @torch.autocast(device_type=\"cuda\", enabled=False)\n",
    "    def relu_quadratic_att(self, qkv: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Quadratic Attention with ReLU kernel\n",
    "        ‰ªäÂõû„ÅØ‰ΩøÁî®„Åó„Å™„ÅÑ\n",
    "        \"\"\"\n",
    "        B, _, H, W = list(qkv.size())\n",
    "\n",
    "        qkv = torch.reshape(\n",
    "            qkv,\n",
    "            (\n",
    "                B,\n",
    "                -1,\n",
    "                3 * self.dim,\n",
    "                H * W,\n",
    "            ),\n",
    "        )\n",
    "        q, k, v = (\n",
    "            qkv[:, :, 0 : self.dim],\n",
    "            qkv[:, :, self.dim : 2 * self.dim],\n",
    "            qkv[:, :, 2 * self.dim :],\n",
    "        )\n",
    "\n",
    "        q = self.kernel_func(q)\n",
    "        k = self.kernel_func(k)\n",
    "\n",
    "        att_map = torch.matmul(k.transpose(-1, -2), q)  # b h n n\n",
    "        original_dtype = att_map.dtype\n",
    "        if original_dtype in [torch.float16, torch.bfloat16]:\n",
    "            att_map = att_map.float()\n",
    "        att_map = att_map / (torch.sum(att_map, dim=2, keepdim=True) + self.eps)  # b h n n\n",
    "        att_map = att_map.to(original_dtype)\n",
    "        out = torch.matmul(v, att_map)  # b h d n\n",
    "\n",
    "        out = torch.reshape(out, (B, -1, H, W))\n",
    "        return out\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): ÂÖ•Âäõ„ÉÜ„É≥„ÇΩ„É´\n",
    "        Returns:\n",
    "            torch.Tensor: Âá∫Âäõ„ÉÜ„É≥„ÇΩ„É´\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(f\"LiteMLA.forward {x.shape=}\")\n",
    "\n",
    "        # 1) „Éû„É´„ÉÅ„Çπ„Ç±„Éº„É´QKV„ÅÆÁîüÊàê\n",
    "\n",
    "        qkv = self.qkv(x)\n",
    "\n",
    "        multi_scale_qkv = [qkv]\n",
    "\n",
    "        for op in self.aggreg:\n",
    "            multi_scale_qkv.append(op(qkv))\n",
    "\n",
    "        qkv = torch.cat(multi_scale_qkv, dim=1)\n",
    "\n",
    "        logger.debug(f\"LiteMLA.forward after multi-scale conv {qkv.shape=}\")\n",
    "\n",
    "        # 2) ReLUÁ∑öÂΩ¢„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥„ÅÆÈÅ©Áî®\n",
    "\n",
    "        out = self.relu_linear_att(qkv).to(qkv.dtype)\n",
    "        logger.debug(f\"LiteMLA.forward after relu_linear_att {out.shape=}\")\n",
    "\n",
    "        # 3) Âá∫ÂäõÊäïÂΩ±„ÅÆÈÅ©Áî®\n",
    "\n",
    "        out = self.proj(out)\n",
    "        logger.debug(f\"LiteMLA.forward after proj {out.shape=}\")\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a570c05",
   "metadata": {},
   "source": [
    "### ResidualBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9395c0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    „Éñ„É≠„ÉÉ„ÇØ„Å´ÊÆãÂ∑ÆÊé•Á∂ö„ÇíËøΩÂä†„Åô„Çã„É¢„Ç∏„É•„Éº„É´\n",
    "    build_downsample_block, build_encoder_project_out_block, build_block,\n",
    "    build_decoder_project_in_block, EfficientViTBlock„Åß‰ΩøÁî®\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        main: Optional[nn.Module],\n",
    "        shortcut: Optional[nn.Module],\n",
    "        post_act=None,\n",
    "        pre_norm: Optional[nn.Module] = None,\n",
    "    ):\n",
    "        logger.info(f\"ResidualBlock.__init__ {main=} {shortcut=} {post_act=} {pre_norm=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.pre_norm = pre_norm\n",
    "        self.main = main\n",
    "        self.shortcut = shortcut\n",
    "        self.post_act = build_act(post_act)\n",
    "\n",
    "    def forward_main(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.pre_norm is None:\n",
    "            return self.main(x)\n",
    "        else:\n",
    "            return self.main(self.pre_norm(x))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): ÂÖ•Âäõ„ÉÜ„É≥„ÇΩ„É´\n",
    "        Returns:\n",
    "            torch.Tensor: Âá∫Âäõ„ÉÜ„É≥„ÇΩ„É´\n",
    "        \"\"\"\n",
    "\n",
    "        # „É°„Ç§„É≥„Éñ„É©„É≥„ÉÅ„ÅåNone„ÅÆÂ†¥Âêà\n",
    "        if self.main is None:\n",
    "            res = x\n",
    "\n",
    "        # „Ç∑„Éß„Éº„Éà„Ç´„ÉÉ„Éà„Éñ„É©„É≥„ÉÅ„ÅåNone„ÅÆÂ†¥Âêà\n",
    "        elif self.shortcut is None:\n",
    "            res = self.forward_main(x)\n",
    "\n",
    "        # ‰∏°Êñπ„ÅÆ„Éñ„É©„É≥„ÉÅ„ÅåÂ≠òÂú®„Åô„ÇãÂ†¥Âêà\n",
    "        else:\n",
    "            # „É°„Ç§„É≥„Éñ„É©„É≥„ÉÅ„Å®„Ç∑„Éß„Éº„Éà„Ç´„ÉÉ„Éà„Éñ„É©„É≥„ÉÅ„ÅÆÂá∫Âäõ„ÇíÂä†ÁÆóÔºàÊÆãÂ∑ÆÊé•Á∂öÔºâ\n",
    "            res = self.forward_main(x) + self.shortcut(x)\n",
    "\n",
    "            # Ê¥ªÊÄßÂåñÈñ¢Êï∞„ÅåÂ≠òÂú®„Åô„ÇãÂ†¥Âêà\n",
    "            if self.post_act:\n",
    "                # Ê¥ªÊÄßÂåñÈñ¢Êï∞„ÇíÈÅ©Áî®\n",
    "                res = self.post_act(res)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca4f383",
   "metadata": {},
   "source": [
    "### EfficientViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53e3875",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientViTBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    EfficientViT„Éñ„É≠„ÉÉ„ÇØ\n",
    "    build_block„Åß18Âõû‰ΩøÁî®\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, heads_ratio: float = 1.0, dim=32, expand_ratio: float = 4, scales: tuple[int, ...] = (5,), norm: str = \"bn2d\", act_func: str = \"hswish\", context_module: str = \"LiteMLA\", local_module: str = \"MBConv\", is_video: bool = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): ÂÖ•Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "            heads_ratio (float, optional): „Éò„ÉÉ„ÉâÊï∞„ÅÆÊØîÁéá\n",
    "            dim (int, optional): ÂêÑ„Éò„ÉÉ„Éâ„ÅÆÊ¨°ÂÖÉÊï∞\n",
    "            expand_ratio (float, optional): Êã°ÂºµÊØîÁéá\n",
    "            scales (tuple[int, ...], optional): „Éû„É´„ÉÅ„Çπ„Ç±„Éº„É´Áï≥„ÅøËæº„Åø„ÅÆ„Çπ„Ç±„Éº„É´\n",
    "            norm (str, optional): Ê≠£Ë¶èÂåñÂ±§„ÅÆÁ®ÆÈ°û\n",
    "            act_func (str, optional): Ê¥ªÊÄßÂåñÈñ¢Êï∞„ÅÆÁ®ÆÈ°û\n",
    "            context_module (str, optional): „Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„É¢„Ç∏„É•„Éº„É´„ÅÆÁ®ÆÈ°û\n",
    "            local_module (str, optional): „É≠„Éº„Ç´„É´„É¢„Ç∏„É•„Éº„É´„ÅÆÁ®ÆÈ°û\n",
    "            is_video (bool, optional): 3DÁï≥„ÅøËæº„Åø„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "        \"\"\"\n",
    "        logger.info(f\"EfficientViTBlock.__init__ {in_channels=} {heads_ratio=} {dim=} {expand_ratio=} {scales=} {norm=} {act_func=} {context_module=} {local_module=} {is_video=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # 1) „Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„É¢„Ç∏„É•„Éº„É´„ÇíÊßãÁØâ\n",
    "\n",
    "        # True\n",
    "        if context_module == \"LiteMLA\":\n",
    "            self.context_module = ResidualBlock(\n",
    "\n",
    "                # ÊÆãÂ∑ÆÊé•Á∂ö„Å§„Åç„ÅÆ„Éû„É´„ÉÅ„Çπ„Ç±„Éº„É´Á∑öÂΩ¢„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥„ÇíÊßãÁØâ\n",
    "                LiteMLA(\n",
    "                    in_channels=in_channels, # 512„Å™„Å©\n",
    "                    out_channels=in_channels, # 512„Å™„Å©\n",
    "                    heads_ratio=heads_ratio, # 1.0\n",
    "                    dim=dim, # 32\n",
    "                    norm=(None, norm), # (None, rms3d)\n",
    "                    scales=scales, # (5,)\n",
    "                    is_video=is_video, # True\n",
    "                ),\n",
    "                IdentityLayer(),\n",
    "            )\n",
    "\n",
    "        # False\n",
    "        else:\n",
    "            raise ValueError(f\"context_module {context_module} is not supported\")\n",
    "\n",
    "        # False\n",
    "        if local_module == \"MBConv\":\n",
    "            self.local_module = ResidualBlock(\n",
    "                MBConv(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=in_channels,\n",
    "                    expand_ratio=expand_ratio,\n",
    "                    use_bias=(True, True, False),\n",
    "                    norm=(None, None, norm),\n",
    "                    act_func=(act_func, act_func, None),\n",
    "                    is_video=is_video,\n",
    "                ),\n",
    "                IdentityLayer(),\n",
    "            )\n",
    "\n",
    "        # 2) „É≠„Éº„Ç´„É´„É¢„Ç∏„É•„Éº„É´„ÇíÊßãÁØâ\n",
    "\n",
    "        # True\n",
    "        elif local_module == \"GLUMBConv\":\n",
    "\n",
    "             # GLU„ÇíÁµÑ„ÅøËæº„Çì„Å†„Éú„Éà„É´„Éç„ÉÉ„ÇØÊã°ÂºµÁï≥„ÅøËæº„Åø„ÇíÊßãÁØâ„Åó„ÄÅÊÆãÂ∑ÆÊé•Á∂ö„ÇíËøΩÂä†\n",
    "            self.local_module = ResidualBlock(\n",
    "                GLUMBConv(\n",
    "                    in_channels=in_channels, # 512„Å™„Å©\n",
    "                    out_channels=in_channels, # 512„Å™„Å©\n",
    "                    expand_ratio=expand_ratio, # 4\n",
    "                    use_bias=(True, True, False),\n",
    "                    norm=(None, None, norm), # (None, None, rms3d)\n",
    "                    act_func=(act_func, act_func, None), # (silu, silu, None)\n",
    "                    is_video=is_video, # True\n",
    "                ),\n",
    "                IdentityLayer(),\n",
    "            )\n",
    "\n",
    "        # False\n",
    "        else:\n",
    "            raise NotImplementedError(f\"local_module {local_module} is not supported\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logger.info(f\"EfficientViTBlock.forward {x.shape=}\")\n",
    "\n",
    "        # „Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„É¢„Ç∏„É•„Éº„É´„ÅÆÈÅ©Áî®\n",
    "        x = self.context_module(x)\n",
    "        logger.debug(f\"After context_module: {x.shape=}\")\n",
    "\n",
    "        # „É≠„Éº„Ç´„É´„É¢„Ç∏„É•„Éº„É´„ÅÆÈÅ©Áî®\n",
    "        x = self.local_module(x)\n",
    "        logger.debug(f\"After local_module: {x.shape=}\")\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6717d723",
   "metadata": {},
   "source": [
    "### ResBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f023dc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    2„Å§„ÅÆÁï≥Ëæº„ÅøÂ±§„Åã„Çâ„Å™„ÇãÊÆãÂ∑Æ„Éñ„É≠„ÉÉ„ÇØ„ÅÆ„É°„Ç§„É≥„Éñ„É©„É≥„ÉÅ\n",
    "    „Ç∑„Éß„Éº„Éà„Ç´„ÉÉ„Éà„Éë„Çπ„ÅØÂê´„Åæ„Å™„ÅÑ\n",
    "    build_block„Åß‰ΩøÁî®\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        mid_channels=None,\n",
    "        expand_ratio=1,\n",
    "        use_bias=False,\n",
    "        norm=(\"bn2d\", \"bn2d\"),\n",
    "        act_func=(\"relu6\", None),\n",
    "        is_video=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): ÂÖ•Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "            out_channels (int): Âá∫Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "            kernel_size (int, optional): Áï≥„ÅøËæº„Åø„Ç´„Éº„Éç„É´„ÅÆ„Çµ„Ç§„Ç∫\n",
    "            stride (int, optional): Áï≥„ÅøËæº„Åø„ÅÆ„Çπ„Éà„É©„Ç§„Éâ\n",
    "            mid_channels (int, optional): ‰∏≠Èñì„ÉÅ„É£„Éç„É´Êï∞\n",
    "            expand_ratio (float, optional): Êã°ÂºµÊØîÁéá\n",
    "            use_bias (bool or tuple, optional): „Éê„Ç§„Ç¢„Çπ„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "            norm (str or tuple, optional): Ê≠£Ë¶èÂåñÂ±§„ÅÆÁ®ÆÈ°û\n",
    "            act_func (str or tuple, optional): Ê¥ªÊÄßÂåñÈñ¢Êï∞„ÅÆÁ®ÆÈ°û\n",
    "            is_video (bool, optional): 3DÁï≥„ÅøËæº„Åø„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "        \"\"\"\n",
    "        logger.info(f\"ResBlock.__init__ {in_channels=} {out_channels=} {kernel_size=} {stride=} {mid_channels=} {expand_ratio=} {use_bias=} {norm=} {act_func=} {is_video=}\")\n",
    "\n",
    "        # 1) ÂàùÊúüÂåñ\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        use_bias = val2tuple(use_bias, 2) # (True, False)\n",
    "\n",
    "        norm = val2tuple(norm, 2) # (None, \"rms3d\")\n",
    "\n",
    "        act_func = val2tuple(act_func, 2) # (\"silu\", None)\n",
    "\n",
    "        # expand_ratio„ÅØÂ∏∏„Å´1„Å™„ÅÆ„Åß„ÄÅÂÖ•Âäõ„ÅÆ„ÉÅ„É£„Éç„É´Êï∞„Å®‰∏≠Èñì„ÉÅ„É£„Éç„É´Êï∞„ÅØÂêå„Åò\n",
    "        mid_channels = round(in_channels * expand_ratio) if mid_channels is None else mid_channels\n",
    "\n",
    "        # 2) 1„Å§ÁõÆ„ÅÆÁï≥„ÅøËæº„ÅøÂ±§\n",
    "\n",
    "        self.conv1 = ConvLayer(\n",
    "            in_channels, # 128„Å™„Å©\n",
    "            mid_channels, # 128„Å™„Å©\n",
    "            kernel_size, # 3\n",
    "            stride, # 1\n",
    "            use_bias=use_bias[0], # True\n",
    "            norm=norm[0], # None\n",
    "            act_func=act_func[0], # silu\n",
    "            is_video=is_video, # True\n",
    "        )\n",
    "\n",
    "        # 3) 2„Å§ÁõÆ„ÅÆÁï≥„ÅøËæº„ÅøÂ±§\n",
    "\n",
    "        self.conv2 = ConvLayer(\n",
    "            mid_channels, # 128„Å™„Å©\n",
    "            out_channels, # 128„Å™„Å©\n",
    "            kernel_size, # 3\n",
    "            1,\n",
    "            use_bias=use_bias[1], # False\n",
    "            norm=norm[1], # rms3d\n",
    "            act_func=act_func[1], # None\n",
    "            is_video=is_video, # True\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logger.info(f\"ResBlock.forward {x.shape=}\")\n",
    "\n",
    "        # 1) 1„Å§ÁõÆ„ÅÆÁï≥„ÅøËæº„ÅøÂ±§„ÇíÈÄö„Åô\n",
    "        x = self.conv1(x)\n",
    "        logger.debug(f\"After conv1: {x.shape}\")\n",
    "\n",
    "        # 2) 2„Å§ÁõÆ„ÅÆÁï≥„ÅøËæº„ÅøÂ±§„ÇíÈÄö„Åô\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        logger.debug(f\"After conv2: {x.shape}\")\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b516c0",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac36316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_downsample_block(\n",
    "    block_type: str,\n",
    "    in_channels: int,\n",
    "    out_channels: int,\n",
    "    shortcut: Optional[str],\n",
    "    is_video: bool,\n",
    "    temporal_downsample: bool = False,\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    „ÉÄ„Ç¶„É≥„Çµ„É≥„Éó„É™„É≥„Ç∞„Éñ„É≠„ÉÉ„ÇØ„ÇíÊßãÁØâ„Åô„Çã\n",
    "    Encoder.__init__„Åß5Âõû‰ΩøÁî®\n",
    "\n",
    "    Args:\n",
    "        block_type (str): „Éñ„É≠„ÉÉ„ÇØ„ÅÆÁ®ÆÈ°û (\"Conv\" „Åæ„Åü„ÅØ \"ConvPixelUnshuffle\")\n",
    "        in_channels (int): ÂÖ•Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "        out_channels (int): Âá∫Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "        shortcut (Optional[str]): „Ç∑„Éß„Éº„Éà„Ç´„ÉÉ„Éà„ÅÆÁ®ÆÈ°û (None „Åæ„Åü„ÅØ \"averaging\")\n",
    "        is_video (bool): 3DÁï≥„ÅøËæº„Åø„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "        temporal_downsample (bool, optional):\n",
    "            ÊôÇÈñìÊñπÂêë„ÅÆ„ÉÄ„Ç¶„É≥„Çµ„É≥„Éó„É™„É≥„Ç∞„ÇíË°å„ÅÜ„Åã„Å©„ÅÜ„Åã\n",
    "    Returns:\n",
    "        nn.Module: „ÉÄ„Ç¶„É≥„Çµ„É≥„Éó„É™„É≥„Ç∞„Éñ„É≠„ÉÉ„ÇØ\n",
    "    \"\"\"\n",
    "    logger.info(f\"build_downsample_block {block_type=} {in_channels=} {out_channels=} {shortcut=} {is_video=} {temporal_downsample=}\")\n",
    "\n",
    "    # 1) Áï≥Ëæº„ÅøÂ±§„Çí‰ΩúÊàê\n",
    "\n",
    "    # True\n",
    "    if block_type == \"Conv\":\n",
    "\n",
    "        # True\n",
    "        if is_video:\n",
    "            # False -> False -> False -> True -> True\n",
    "            if temporal_downsample:\n",
    "                stride = (2, 2, 2)\n",
    "            else:\n",
    "                stride = (1, 2, 2)\n",
    "        else:\n",
    "            stride = 2\n",
    "\n",
    "\n",
    "        block = ConvLayer(\n",
    "            in_channels=in_channels, # 128 -> 256 -> 256 -> 512 -> 512\n",
    "            out_channels=out_channels, # 256 -> 512 -> 512 -> 1024 -> 1024\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            use_bias=True,\n",
    "            norm=None,\n",
    "            act_func=None,\n",
    "            is_video=is_video, # True\n",
    "        )\n",
    "\n",
    "    # False\n",
    "    elif block_type == \"ConvPixelUnshuffle\":\n",
    "        if is_video:\n",
    "            raise NotImplementedError(\"ConvPixelUnshuffle downsample is not supported for video\")\n",
    "        block = ConvPixelUnshuffleDownSampleLayer(\n",
    "            in_channels=in_channels, out_channels=out_channels, kernel_size=3, factor=2\n",
    "        )\n",
    "\n",
    "    # False\n",
    "    else:\n",
    "        raise ValueError(f\"block_type {block_type} is not supported for downsampling\")\n",
    "\n",
    "    # 2) Áï≥„ÅøËæº„ÅøÂ±§„Å´ÊÆãÂ∑ÆÊé•Á∂ö„ÇíËøΩÂä†\n",
    "\n",
    "    # False\n",
    "    if shortcut is None:\n",
    "        pass\n",
    "\n",
    "    # True\n",
    "    elif shortcut == \"averaging\":\n",
    "\n",
    "        # „Éî„ÇØ„Çª„É´„Ç¢„É≥„Ç∑„É£„ÉÉ„Éï„É´„Å®„ÉÅ„É£„Éç„É´Âπ≥ÂùáÂåñ„Å´„Çà„Çã„ÉÄ„Ç¶„É≥„Çµ„É≥„Éó„É™„É≥„Ç∞Â±§„Çí‰ΩúÊàê\n",
    "        shortcut_block = PixelUnshuffleChannelAveragingDownSampleLayer(\n",
    "            in_channels=in_channels, # 128 -> 256 -> 256 -> 512 -> 512\n",
    "            out_channels=out_channels, # 256 -> 512 -> 512 -> 1024 -> 1024\n",
    "            factor=2,\n",
    "            temporal_downsample=temporal_downsample # False -> False -> False -> True -> True\n",
    "        )\n",
    "\n",
    "        # „ÉÄ„Ç¶„É≥„Çµ„É≥„Éó„É™„É≥„Ç∞Â±§„ÇíÊÆãÂ∑ÆÊé•Á∂ö„ÅÆ„Ç∑„Éß„Éº„Éà„Ç´„ÉÉ„Éà„Å®„Åó„Å¶ÁµÑ„ÅøËæº„ÇÄ\n",
    "        block = ResidualBlock(block, shortcut_block)\n",
    "\n",
    "    # False\n",
    "    else:\n",
    "        raise ValueError(f\"shortcut {shortcut} is not supported for downsample\")\n",
    "\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771ff37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder_project_in_block(in_channels: int, out_channels: int, factor: int, downsample_block_type: str, is_video: bool):\n",
    "    \"\"\"\n",
    "    „Ç®„É≥„Ç≥„Éº„ÉÄ„ÅÆÂÖ•Âäõ„Éó„É≠„Ç∏„Çß„ÇØ„Ç∑„Éß„É≥„Éñ„É≠„ÉÉ„ÇØ„ÇíÊßãÁØâ„Åô„Çã\n",
    "    Encoder.__init__„Åß1Âõû„Å†„Åë‰ΩøÁî®„Åï„Çå„Çã\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): ÂÖ•Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "        out_channels (int): Âá∫Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "        factor (int): „ÉÄ„Ç¶„É≥„Çµ„É≥„Éó„É™„É≥„Ç∞„Éï„Ç°„ÇØ„Çø„Éº\n",
    "        downsample_block_type (str): „ÉÄ„Ç¶„É≥„Çµ„É≥„Éó„É™„É≥„Ç∞„Éñ„É≠„ÉÉ„ÇØ„ÅÆÁ®ÆÈ°û\n",
    "        is_video (bool): 3DÁï≥„ÅøËæº„Åø„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "    Returns:\n",
    "        nn.Module: „Ç®„É≥„Ç≥„Éº„ÉÄ„ÅÆÂÖ•Âäõ„Éó„É≠„Ç∏„Çß„ÇØ„Ç∑„Éß„É≥„Éñ„É≠„ÉÉ„ÇØ\n",
    "    \"\"\"\n",
    "    logger.info(f\"build_encoder_project_in_block {in_channels=} {out_channels=} {factor=} {downsample_block_type=} {is_video=}\")\n",
    "\n",
    "    # 1) Áï≥„ÅøËæº„ÅøÂ±§„ÇíÊßãÁØâ\n",
    "\n",
    "    # True\n",
    "    if factor == 1:\n",
    "\n",
    "        block = ConvLayer(\n",
    "            in_channels=in_channels, # 3\n",
    "            out_channels=out_channels, # 128\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            use_bias=True,\n",
    "            norm=None,\n",
    "            act_func=None,\n",
    "            is_video=is_video, # True\n",
    "        )\n",
    "\n",
    "    # False\n",
    "    elif factor == 2:\n",
    "        if is_video:\n",
    "            raise NotImplementedError(\"Downsample during project_in is not supported for video\")\n",
    "        block = build_downsample_block(\n",
    "            block_type=downsample_block_type, in_channels=in_channels, out_channels=out_channels, shortcut=None\n",
    "        )\n",
    "\n",
    "    # False\n",
    "    else:\n",
    "        raise ValueError(f\"downsample factor {factor} is not supported for encoder project in\")\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44428bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder_project_out_block(\n",
    "    in_channels: int,\n",
    "    out_channels: int,\n",
    "    norm: Optional[str],\n",
    "    act: Optional[str],\n",
    "    shortcut: Optional[str],\n",
    "    is_video: bool,\n",
    "):\n",
    "    \"\"\"\n",
    "    „Ç®„É≥„Ç≥„Éº„ÉÄ„ÅÆÂá∫Âäõ„Éó„É≠„Ç∏„Çß„ÇØ„Ç∑„Éß„É≥„Éñ„É≠„ÉÉ„ÇØ„ÇíÊßãÁØâ„Åô„Çã\n",
    "    Encoder„Åß‰ΩøÁî®\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): ÂÖ•Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "        out_channels (int): Âá∫Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "        norm (Optional[str]): Ê≠£Ë¶èÂåñÂ±§„ÅÆÁ®ÆÈ°û\n",
    "        act (Optional[str]): Ê¥ªÊÄßÂåñÈñ¢Êï∞„ÅÆÁ®ÆÈ°û\n",
    "        shortcut (Optional[str]): „Ç∑„Éß„Éº„Éà„Ç´„ÉÉ„Éà„ÅÆÁ®ÆÈ°û (None „Åæ„Åü„ÅØ \"averaging\")\n",
    "        is_video (bool): 3DÁï≥„ÅøËæº„Åø„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "    Returns:\n",
    "        nn.Module: „Ç®„É≥„Ç≥„Éº„ÉÄ„ÅÆÂá∫Âäõ„Éó„É≠„Ç∏„Çß„ÇØ„Ç∑„Éß„É≥„Éñ„É≠„ÉÉ„ÇØ\n",
    "    \"\"\"\n",
    "    logger.info(f\"build_encoder_project_out_block {in_channels=} {out_channels=} {norm=} {act=} {shortcut=} {is_video=}\")\n",
    "\n",
    "    # 1) Áï≥„ÅøËæº„ÅøÂ±§„ÅÆ„Éñ„É≠„ÉÉ„ÇØ„Çí‰ΩúÊàê\n",
    "\n",
    "    block = OpSequential(\n",
    "        [\n",
    "            build_norm(norm), # None\n",
    "            build_act(act), # None\n",
    "            ConvLayer(\n",
    "                in_channels=in_channels, # 1024\n",
    "                out_channels=out_channels, # 128\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                use_bias=True,\n",
    "                norm=None,\n",
    "                act_func=None,\n",
    "                is_video=is_video, # True\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 2) „Éñ„É≠„ÉÉ„ÇØ„Å´ÊÆãÂ∑ÆÊé•Á∂ö„ÇíËøΩÂä†\n",
    "\n",
    "    # False\n",
    "    if shortcut is None:\n",
    "        pass\n",
    "\n",
    "    # True\n",
    "    elif shortcut == \"averaging\":\n",
    "\n",
    "        # „Éî„ÇØ„Çª„É´„Ç¢„É≥„Ç∑„É£„ÉÉ„Éï„É´„Å®„ÉÅ„É£„Éç„É´Âπ≥ÂùáÂåñ„Å´„Çà„Çã„ÉÄ„Ç¶„É≥„Çµ„É≥„Éó„É™„É≥„Ç∞Â±§„Çí‰ΩúÊàê\n",
    "        shortcut_block = PixelUnshuffleChannelAveragingDownSampleLayer(\n",
    "            in_channels=in_channels, # 1024\n",
    "            out_channels=out_channels, # 128\n",
    "            factor=1\n",
    "        )\n",
    "\n",
    "        # „ÉÄ„Ç¶„É≥„Çµ„É≥„Éó„É™„É≥„Ç∞Â±§„ÇíÊÆãÂ∑ÆÊé•Á∂ö„ÅÆ„Ç∑„Éß„Éº„Éà„Ç´„ÉÉ„Éà„Å®„Åó„Å¶ÁµÑ„ÅøËæº„ÇÄ\n",
    "        block = ResidualBlock(block, shortcut_block)\n",
    "\n",
    "    # False\n",
    "    else:\n",
    "        raise ValueError(f\"shortcut {shortcut} is not supported for encoder project out\")\n",
    "\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3a77b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_block(\n",
    "    block_type: str, in_channels: int, out_channels: int, norm: Optional[str], act: Optional[str], is_video: bool\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    „Éñ„É≠„ÉÉ„ÇØ„ÇíÊßãÁØâ„Åô„Çã\n",
    "    build_stage_main„Åß‰ΩøÁî®\n",
    "    Args:\n",
    "        block_type (str): „Éñ„É≠„ÉÉ„ÇØ„ÅÆÁ®ÆÈ°û (\"ResBlock\", \"EViT_GLU\", \"EViTS5_GLU\")\n",
    "        in_channels (int): ÂÖ•Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "        out_channels (int): Âá∫Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "        norm (Optional[str]): Ê≠£Ë¶èÂåñÂ±§„ÅÆÁ®ÆÈ°û\n",
    "        act (Optional[str]): Ê¥ªÊÄßÂåñÈñ¢Êï∞„ÅÆÁ®ÆÈ°û\n",
    "        is_video (bool): 3DÁï≥„ÅøËæº„Åø„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "    Returns:\n",
    "        nn.Module: „Éñ„É≠„ÉÉ„ÇØ\n",
    "    \"\"\"\n",
    "    logger.info(f\"build_block {block_type=} {in_channels=} {out_channels=} {norm=} {act=} {is_video=}\")\n",
    "\n",
    "    # 1) „É°„Ç§„É≥„Éñ„É≠„ÉÉ„ÇØ„Çí‰ΩúÊàê\n",
    "\n",
    "    # True of False\n",
    "    if block_type == \"ResBlock\":\n",
    "\n",
    "        assert in_channels == out_channels\n",
    "\n",
    "        # 1-1) ResBlock„Çí‰ΩúÊàê\n",
    "        main_block = ResBlock(\n",
    "            in_channels=in_channels, # 128 or 256 or 512 or 1024\n",
    "            out_channels=out_channels, # 128 or 256 or 512 or 1024\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            use_bias=(True, False),\n",
    "            norm=(None, norm), # rms3d\n",
    "            act_func=(act, None), # silu\n",
    "            is_video=is_video, # True\n",
    "        )\n",
    "\n",
    "        # 1-2) ResBlock„Å´ÊÆãÂ∑ÆÊé•Á∂ö„ÇíËøΩÂä†\n",
    "        block = ResidualBlock(main_block, IdentityLayer())\n",
    "\n",
    "    # False\n",
    "    elif block_type == \"EViT_GLU\":\n",
    "        assert in_channels == out_channels\n",
    "        block = EfficientViTBlock(\n",
    "            in_channels, norm=norm, act_func=act, local_module=\"GLUMBConv\", scales=(), is_video=is_video\n",
    "        )\n",
    "\n",
    "    # True or False\n",
    "    elif block_type == \"EViTS5_GLU\":\n",
    "\n",
    "        assert in_channels == out_channels\n",
    "\n",
    "        # 1-1) EfficientViT„Éñ„É≠„ÉÉ„ÇØ„Çí‰ΩúÊàê\n",
    "\n",
    "        block = EfficientViTBlock(\n",
    "            in_channels, # 128 or 256 or 512 or 1024\n",
    "            norm=norm, # rms3d\n",
    "            act_func=act, # silu\n",
    "            local_module=\"GLUMBConv\",\n",
    "            scales=(5,),\n",
    "            is_video=is_video # True\n",
    "        )\n",
    "\n",
    "    # False\n",
    "    else:\n",
    "        raise ValueError(f\"block_type {block_type} is not supported\")\n",
    "\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7b4b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_stage_main(\n",
    "    width: int, depth: int, block_type: str | list[str], norm: str, act: str, input_width: int, is_video: bool\n",
    ") -> list[nn.Module]:\n",
    "    \"\"\"\n",
    "    „Çπ„ÉÜ„Éº„Ç∏„ÅÆ„É°„Ç§„É≥„Éñ„É≠„ÉÉ„ÇØ„ÇíÊßãÁØâ„Åô„Çã\n",
    "    Encoder, Decoder„ÅßÂêàË®à12Âõû‰ΩøÁî®\n",
    "\n",
    "    Args:\n",
    "        width (int): „Éñ„É≠„ÉÉ„ÇØ„ÅÆ„ÉÅ„É£„Éç„É´Êï∞\n",
    "        depth (int): „Éñ„É≠„ÉÉ„ÇØ„ÅÆÊ∑±„Åï\n",
    "        block_type (str or list[str]): „Éñ„É≠„ÉÉ„ÇØ„ÅÆÁ®ÆÈ°û\n",
    "        norm (str): Ê≠£Ë¶èÂåñÂ±§„ÅÆÁ®ÆÈ°û\n",
    "        act (str): Ê¥ªÊÄßÂåñÈñ¢Êï∞„ÅÆÁ®ÆÈ°û\n",
    "        input_width (int): ÂÖ•Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "        is_video (bool): 3DÁï≥„ÅøËæº„Åø„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "    Returns:\n",
    "        list[nn.Module]: „Çπ„ÉÜ„Éº„Ç∏„ÅÆ„É°„Ç§„É≥„Éñ„É≠„ÉÉ„ÇØ„ÅÆ„É™„Çπ„Éà\n",
    "    \"\"\"\n",
    "    logger.info(f\"build_stage_main {width=} {depth=} {block_type=} {norm=} {act=} {input_width=} {is_video=}\")\n",
    "\n",
    "    assert isinstance(block_type, str) or (isinstance(block_type, list) and depth == len(block_type))\n",
    "\n",
    "    # 1) Ê∑±„Åï„ÅÆÊï∞„Å†„Åë„É°„Ç§„É≥„Éñ„É≠„ÉÉ„ÇØ„ÇíÊßãÁØâ\n",
    "\n",
    "    stage = []\n",
    "\n",
    "    # Ê∑±„Åï„Åß„É´„Éº„ÉóÔºà2 or 3Ôºâ\n",
    "    for d in range(depth):\n",
    "\n",
    "        # „Éñ„É≠„ÉÉ„ÇØ„Çø„Ç§„Éó„ÇíÊ±∫ÂÆö\n",
    "        # ResBlock or EViTS5_GLU\n",
    "        current_block_type = block_type[d] if isinstance(block_type, list) else block_type\n",
    "\n",
    "        # „Éñ„É≠„ÉÉ„ÇØ„ÇíÊßãÁØâ\n",
    "        block = build_block(\n",
    "            block_type=current_block_type, # ResBlock or EViTS5_GLU\n",
    "            in_channels=width if d > 0 else input_width, # 128 or 256 or 512 or 1024\n",
    "            out_channels=width, # 128 or 256 or 512 or 1024\n",
    "            norm=norm, # rms3d\n",
    "            act=act, # silu\n",
    "            is_video=is_video, # True\n",
    "        )\n",
    "\n",
    "        # „Éñ„É≠„ÉÉ„ÇØ„Çí„Çπ„ÉÜ„Éº„Ç∏„Å´ËøΩÂä†\n",
    "        stage.append(block)\n",
    "\n",
    "    return stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45544988",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EncoderConfig:\n",
    "    in_channels: int = MISSING\n",
    "    latent_channels: int = MISSING\n",
    "    width_list: tuple[int, ...] = (128, 256, 512, 512, 1024, 1024)\n",
    "    depth_list: tuple[int, ...] = (2, 2, 2, 2, 2, 2)\n",
    "    block_type: Any = \"ResBlock\"\n",
    "    norm: str = \"rms2d\"\n",
    "    act: str = \"silu\"\n",
    "    downsample_block_type: str = \"ConvPixelUnshuffle\"\n",
    "    downsample_match_channel: bool = True\n",
    "    downsample_shortcut: Optional[str] = \"averaging\"\n",
    "    out_norm: Optional[str] = None\n",
    "    out_act: Optional[str] = None\n",
    "    out_shortcut: Optional[str] = \"averaging\"\n",
    "    double_latent: bool = False\n",
    "    is_video: bool = False\n",
    "    temporal_downsample: tuple[bool, ...] = ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8abf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    6„Å§„ÅÆ„Çπ„ÉÜ„Éº„Ç∏„ÇíÊåÅ„Å§DCAE„ÅÆ„Ç®„É≥„Ç≥„Éº„ÉÄ\n",
    "    DCAE.__init__„Åß‰ΩøÁî®\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: EncoderConfig):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cfg (EncoderConfig): „Ç®„É≥„Ç≥„Éº„ÉÄ„ÅÆË®≠ÂÆö\n",
    "        \"\"\"\n",
    "        logger.info(f\"Encoder.__init__ {cfg=}\")\n",
    "\n",
    "        # 1) ÂàùÊúüÂåñ\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # 6„Å§„ÅÆ„Çπ„ÉÜ„Éº„Ç∏\n",
    "        # [128, 256, 512, 512, 1024, 1024] -> 6\n",
    "        num_stages = len(cfg.width_list)\n",
    "\n",
    "        self.num_stages = num_stages\n",
    "\n",
    "        assert len(cfg.depth_list) == num_stages\n",
    "        assert len(cfg.width_list) == num_stages\n",
    "\n",
    "        assert isinstance(cfg.block_type, str) or (\n",
    "            isinstance(cfg.block_type, list) and len(cfg.block_type) == num_stages\n",
    "        )\n",
    "\n",
    "        # 2) ÂÖ•Âäõ„Éó„É≠„Ç∏„Çß„ÇØ„Ç∑„Éß„É≥„Éñ„É≠„ÉÉ„ÇØ„Çí‰ΩúÊàê\n",
    "\n",
    "        self.project_in = build_encoder_project_in_block(\n",
    "            in_channels=cfg.in_channels, # 128\n",
    "            out_channels=cfg.width_list[0] if cfg.depth_list[0] > 0 else cfg.width_list[1], # 128\n",
    "            factor=1 if cfg.depth_list[0] > 0 else 2, # 1\n",
    "            downsample_block_type=cfg.downsample_block_type, # Conv\n",
    "            is_video=cfg.is_video, # True\n",
    "        )\n",
    "\n",
    "        # 3) 6„Å§„ÅÆ„Çπ„ÉÜ„Éº„Ç∏„Çí‰ΩúÊàê\n",
    "\n",
    "        self.stages: list[OpSequential] = []\n",
    "\n",
    "        # [128, 256, 512, 512, 1024, 1024]„ÅÆÂπÖ„Å®[2, 2, 2, 3, 3, 3]„ÅÆÊ∑±„Åï„ÇíÊåÅ„Å§6„Å§„ÅÆ„Çπ„ÉÜ„Éº„Ç∏„ÇíÊßãÁØâ\n",
    "        for stage_id, (width, depth) in enumerate(zip(cfg.width_list, cfg.depth_list)):\n",
    "\n",
    "            # „Éñ„É≠„ÉÉ„ÇØ„ÅÆÁ®ÆÈ°û„ÇíÊ±∫ÂÆö\n",
    "            # ['ResBlock', 'ResBlock', 'ResBlock', 'EViTS5_GLU', 'EViTS5_GLU', 'EViTS5_GLU']\n",
    "            block_type = cfg.block_type[stage_id] \\\n",
    "                if isinstance(cfg.block_type, list) else cfg.block_type\n",
    "\n",
    "            # „Çπ„ÉÜ„Éº„Ç∏„ÅÆ„É°„Ç§„É≥„Éñ„É≠„ÉÉ„ÇØ„ÇíÊßãÁØâ\n",
    "            stage = build_stage_main(\n",
    "                width=width, # 128 -> 256 -> 512 -> 512 -> 1024 -> 1024\n",
    "                depth=depth, # 2 -> 2 -> 2 -> 3 -> 3 -> 3\n",
    "                # ResBlock -> ResBlock -> ResBlock -> EViTS5_GLU -> EViTS5_GLU -> EViTS5_GLU\n",
    "                block_type=block_type,\n",
    "                norm=cfg.norm, # rms3d\n",
    "                act=cfg.act, # silu\n",
    "                input_width=width, # 128 -> 256 -> 512 -> 512 -> 1024 -> 1024\n",
    "                is_video=cfg.is_video, # True\n",
    "            )\n",
    "\n",
    "            # ÊúÄÂæå„ÅÆ„Çπ„ÉÜ„Éº„Ç∏„Çà„ÇäÂâç„ÅÆ„Çπ„ÉÜ„Éº„Ç∏„ÅÆÂ†¥Âêà\n",
    "            if stage_id < num_stages - 1 and depth > 0:\n",
    "\n",
    "                # „ÉÄ„Ç¶„É≥„Çµ„É≥„Éó„É´„Éñ„É≠„ÉÉ„ÇØ„ÇíÊßãÁØâ\n",
    "                downsample_block = build_downsample_block(\n",
    "                    block_type=cfg.downsample_block_type, # Conv\n",
    "                    in_channels=width, # 128 -> 256 -> 512 -> 512 -> 1024\n",
    "                    # 256 -> 512 -> 512 -> 1024 -> 1024\n",
    "                    out_channels=cfg.width_list[stage_id + 1] \\\n",
    "                        if cfg.downsample_match_channel else width,\n",
    "                    shortcut=cfg.downsample_shortcut, # averaging\n",
    "                    is_video=cfg.is_video, # True\n",
    "                    # False -> False -> False -> True -> True\n",
    "                    temporal_downsample=cfg.temporal_downsample[stage_id] \\\n",
    "                        if cfg.temporal_downsample != [] else False,\n",
    "                )\n",
    "\n",
    "                # „Çπ„ÉÜ„Éº„Ç∏„Å´„ÉÄ„Ç¶„É≥„Çµ„É≥„Éó„É´„Éñ„É≠„ÉÉ„ÇØ„ÇíËøΩÂä†\n",
    "                stage.append(downsample_block)\n",
    "\n",
    "            # „Çπ„ÉÜ„Éº„Ç∏„ÇíOpSequential„Åß„É©„ÉÉ„Éó„Åó„Å¶„É™„Çπ„Éà„Å´ËøΩÂä†\n",
    "            self.stages.append(OpSequential(stage))\n",
    "\n",
    "        # „Çπ„ÉÜ„Éº„Ç∏„ÅÆ„É™„Çπ„Éà„Çínn.ModuleList„Å´Â§âÊèõ\n",
    "        self.stages = nn.ModuleList(self.stages)\n",
    "\n",
    "        # 4) Âá∫Âäõ„Éó„É≠„Ç∏„Çß„ÇØ„Ç∑„Éß„É≥„Éñ„É≠„ÉÉ„ÇØ„Çí‰ΩúÊàê\n",
    "\n",
    "        self.project_out = build_encoder_project_out_block(\n",
    "            in_channels=cfg.width_list[-1], # 1024\n",
    "            out_channels=2 * cfg.latent_channels if cfg.double_latent else cfg.latent_channels, # 128\n",
    "            norm=cfg.out_norm, # None\n",
    "            act=cfg.out_act, # None\n",
    "            shortcut=cfg.out_shortcut, # averaging\n",
    "            is_video=cfg.is_video, # True\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logger.info(f\"Encoder.forward {x.shape=}\")\n",
    "\n",
    "        # 1) ÂÖ•Âäõ„Éó„É≠„Ç∏„Çß„ÇØ„Ç∑„Éß„É≥„Éñ„É≠„ÉÉ„ÇØ„ÇíÈÄöÈÅé\n",
    "        x = self.project_in(x)\n",
    "        logger.debug(f\"Encoder.forward after project_in {x.shape=}\")\n",
    "\n",
    "        # x = auto_grad_checkpoint(self.project_in, x)\n",
    "\n",
    "        # 2) 6„Å§„ÅÆ„Çπ„ÉÜ„Éº„Ç∏„ÇíÈ†ÜÁï™„Å´ÈÄöÈÅé\n",
    "\n",
    "        for i, stage in enumerate(self.stages):\n",
    "            if len(stage.op_list) == 0:\n",
    "                continue\n",
    "            x = auto_grad_checkpoint(stage, x)\n",
    "            logger.debug(f\"Encoder.forward after stage {i} {x.shape=}\")\n",
    "\n",
    "        # x = self.project_out(x)\n",
    "\n",
    "        # 3) Âá∫Âäõ„Éó„É≠„Ç∏„Çß„ÇØ„Ç∑„Éß„É≥„Éñ„É≠„ÉÉ„ÇØ„ÇíÈÄöÈÅé    \n",
    "\n",
    "        # Ëá™ÂãïÂãæÈÖç„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà„Çí‰ΩøÁî®„Åó„Å¶„É°„É¢„É™‰ΩøÁî®Èáè„ÇíÂâäÊ∏õ\n",
    "        x = auto_grad_checkpoint(self.project_out, x)\n",
    "        logger.debug(f\"Encoder.forward after project_out {x.shape=}\")\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e844bb",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366f5c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decoder_project_in_block(in_channels: int, out_channels: int, shortcut: Optional[str], is_video: bool):\n",
    "    \"\"\"\n",
    "    „Éá„Ç≥„Éº„ÉÄ„ÅÆÂÖ•Âäõ„Éó„É≠„Ç∏„Çß„ÇØ„Ç∑„Éß„É≥„Éñ„É≠„ÉÉ„ÇØ„ÇíÊßãÁØâ„Åô„Çã\n",
    "    Decoder.__init__„Åß‰ΩøÁî®\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): ÂÖ•Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "        out_channels (int): Âá∫Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "        shortcut (Optional[str]): „Ç∑„Éß„Éº„Éà„Ç´„ÉÉ„Éà„ÅÆÁ®ÆÈ°û (None „Åæ„Åü„ÅØ \"duplicating\")\n",
    "        is_video (bool): 3DÁï≥„ÅøËæº„Åø„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "    Returns:\n",
    "        nn.Module: „Éá„Ç≥„Éº„ÉÄ„ÅÆÂÖ•Âäõ„Éó„É≠„Ç∏„Çß„ÇØ„Ç∑„Éß„É≥„Éñ„É≠„ÉÉ„ÇØ\n",
    "    \"\"\"\n",
    "    logger.info(f\"build_decoder_project_in_block {in_channels=} {out_channels=} {shortcut=} {is_video=}\")\n",
    "\n",
    "    # 1) Áï≥„ÅøËæº„Åø„Éñ„É≠„ÉÉ„ÇØ„Çí‰ΩúÊàê\n",
    "\n",
    "    block = ConvLayer(\n",
    "        in_channels=in_channels, # 128\n",
    "        out_channels=out_channels, # 1024\n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        use_bias=True,\n",
    "        norm=None,\n",
    "        act_func=None,\n",
    "        is_video=is_video, # True\n",
    "    )\n",
    "\n",
    "    # 2) ÊÆãÂ∑ÆÊé•Á∂öÁî®„ÅÆ„Ç∑„Éß„Éº„Éà„Ç´„ÉÉ„Éà„Éñ„É≠„ÉÉ„ÇØ„Çí‰ΩúÊàê„Åó„ÄÅÊé•Á∂ö\n",
    "\n",
    "    # False\n",
    "    if shortcut is None:\n",
    "        pass\n",
    "\n",
    "    # True\n",
    "    elif shortcut == \"duplicating\":\n",
    "\n",
    "        # „Éî„ÇØ„Çª„É´„Ç∑„É£„ÉÉ„Éï„É´„Åß„ÉÅ„É£„Éç„É´„ÇíË§áË£Ω„Åó„Å¶„Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É™„É≥„Ç∞„Åô„Çã„Ç∑„Éß„Éº„Éà„Ç´„ÉÉ„Éà„Éñ„É≠„ÉÉ„ÇØ„Çí‰ΩúÊàê\n",
    "        shortcut_block = ChannelDuplicatingPixelShuffleUpSampleLayer(\n",
    "            in_channels=in_channels, # 128\n",
    "            out_channels=out_channels, # 1024\n",
    "            factor=1\n",
    "        )\n",
    "\n",
    "        # Áï≥„ÅøËæº„Åø„Éñ„É≠„ÉÉ„ÇØ„Å®„Ç∑„Éß„Éº„Éà„Ç´„ÉÉ„Éà„Éñ„É≠„ÉÉ„ÇØ„ÇíÊÆãÂ∑ÆÊé•Á∂ö„ÅßÁµêÂêà\n",
    "        block = ResidualBlock(block, shortcut_block)\n",
    "\n",
    "    # False\n",
    "    else:\n",
    "        raise ValueError(f\"shortcut {shortcut} is not supported for decoder project in\")\n",
    "\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ec9105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_upsample_block(\n",
    "    block_type: str,\n",
    "    in_channels: int,\n",
    "    out_channels: int,\n",
    "    shortcut: Optional[str],\n",
    "    is_video: bool,\n",
    "    temporal_upsample: bool = False,\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    „Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É™„É≥„Ç∞„Éñ„É≠„ÉÉ„ÇØ„ÇíÊßãÁØâ„Åô„Çã\n",
    "    build_decoder_project_out_block, Decoder.__init__„ÅßÂêàË®à5Âõû‰ΩøÁî®\n",
    "\n",
    "    Args:\n",
    "        block_type (str): „Éñ„É≠„ÉÉ„ÇØ„ÅÆÁ®ÆÈ°û (\"ConvPixelShuffle\" „Åæ„Åü„ÅØ \"InterpolateConv\")\n",
    "        in_channels (int): ÂÖ•Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "        out_channels (int): Âá∫Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "        shortcut (Optional[str]): „Ç∑„Éß„Éº„Éà„Ç´„ÉÉ„Éà„ÅÆÁ®ÆÈ°û (None „Åæ„Åü„ÅØ \"duplicating\")\n",
    "        is_video (bool): 3DÁï≥„ÅøËæº„Åø„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "        temporal_upsample (bool, optional):\n",
    "            ÊôÇÈñìÊñπÂêë„ÅÆ„Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É™„É≥„Ç∞„ÇíË°å„ÅÜ„Åã„Å©„ÅÜ„Åã\n",
    "    Returns:\n",
    "        nn.Module: „Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É™„É≥„Ç∞„Éñ„É≠„ÉÉ„ÇØ\n",
    "    \"\"\"\n",
    "    logger.info(f\"build_upsample_block {block_type=} {in_channels=} {out_channels=} {shortcut=} {is_video=} {temporal_upsample=}\")\n",
    "\n",
    "    # 1) „Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É™„É≥„Ç∞„Éñ„É≠„ÉÉ„ÇØ„Çí‰ΩúÊàê\n",
    "\n",
    "    # False\n",
    "    if block_type == \"ConvPixelShuffle\":\n",
    "        if is_video:\n",
    "            raise NotImplementedError(\"ConvPixelShuffle upsample is not supported for video\")\n",
    "        block = ConvPixelShuffleUpSampleLayer(\n",
    "            in_channels=in_channels, out_channels=out_channels, kernel_size=3, factor=2\n",
    "        )\n",
    "\n",
    "    # True\n",
    "    elif block_type == \"InterpolateConv\":\n",
    "        block = InterpolateConvUpSampleLayer(\n",
    "            in_channels=in_channels, # 1024 -> 1024 -> 512 -> 512 -> 256\n",
    "            out_channels=out_channels, # 1024 -> 512 -> 512 -> 256 -> 128\n",
    "            kernel_size=3,\n",
    "            factor=2,\n",
    "            is_video=is_video, # True\n",
    "            temporal_upsample=temporal_upsample, # True -> True -> False -> False -> False\n",
    "        )\n",
    "\n",
    "    # False\n",
    "    else:\n",
    "        raise ValueError(f\"block_type {block_type} is not supported for upsampling\")\n",
    "\n",
    "    # 2) ÊÆãÂ∑ÆÊé•Á∂öÁî®„ÅÆ„Ç∑„Éß„Éº„Éà„Ç´„ÉÉ„Éà„Éñ„É≠„ÉÉ„ÇØ„Çí‰ΩúÊàê„Åó„ÄÅÊé•Á∂ö\n",
    "\n",
    "    # False\n",
    "    if shortcut is None:\n",
    "        pass\n",
    "\n",
    "    # True\n",
    "    elif shortcut == \"duplicating\":\n",
    "\n",
    "        # „Éî„ÇØ„Çª„É´„Ç∑„É£„ÉÉ„Éï„É´„Åß„ÉÅ„É£„Éç„É´„ÇíË§áË£Ω„Åó„Å¶„Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É™„É≥„Ç∞„Åô„Çã„Ç∑„Éß„Éº„Éà„Ç´„ÉÉ„Éà„Éñ„É≠„ÉÉ„ÇØ„Çí‰ΩúÊàê\n",
    "        shortcut_block = ChannelDuplicatingPixelShuffleUpSampleLayer(\n",
    "            in_channels=in_channels, # 1024 -> 1024 -> 512 -> 512 -> 256\n",
    "            out_channels=out_channels, # 1024 -> 512 -> 512 -> 256 -> 128\n",
    "            factor=2,\n",
    "            temporal_upsample=temporal_upsample # True -> True -> False -> False -> False\n",
    "        )\n",
    "\n",
    "        # „Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É™„É≥„Ç∞„Éñ„É≠„ÉÉ„ÇØ„Å®„Ç∑„Éß„Éº„Éà„Ç´„ÉÉ„Éà„Éñ„É≠„ÉÉ„ÇØ„ÇíÊÆãÂ∑ÆÊé•Á∂ö„ÅßÁµêÂêà\n",
    "        block = ResidualBlock(block, shortcut_block)\n",
    "\n",
    "    # False\n",
    "    else:\n",
    "        raise ValueError(f\"shortcut {shortcut} is not supported for upsample\")\n",
    "\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2080afad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decoder_project_out_block(\n",
    "    in_channels: int,\n",
    "    out_channels: int,\n",
    "    factor: int,\n",
    "    upsample_block_type: str,\n",
    "    norm: Optional[str],\n",
    "    act: Optional[str],\n",
    "    is_video: bool,\n",
    "):\n",
    "    \"\"\"\n",
    "    „Éá„Ç≥„Éº„ÉÄ„ÅÆÂá∫Âäõ„Éó„É≠„Ç∏„Çß„ÇØ„Ç∑„Éß„É≥„Éñ„É≠„ÉÉ„ÇØ„ÇíÊßãÁØâ„Åô„Çã\n",
    "    Decoder.__init__„Åß‰ΩøÁî®\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): ÂÖ•Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "        out_channels (int): Âá∫Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "        factor (int): „Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É™„É≥„Ç∞„Éï„Ç°„ÇØ„Çø„Éº\n",
    "        upsample_block_type (str): „Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É™„É≥„Ç∞„Éñ„É≠„ÉÉ„ÇØ„ÅÆÁ®ÆÈ°û\n",
    "        norm (Optional[str]): Ê≠£Ë¶èÂåñÂ±§„ÅÆÁ®ÆÈ°û\n",
    "        act (Optional[str]): Ê¥ªÊÄßÂåñÈñ¢Êï∞„ÅÆÁ®ÆÈ°û\n",
    "        is_video (bool): 3DÁï≥„ÅøËæº„Åø„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "    Returns:\n",
    "        nn.Module: „Éá„Ç≥„Éº„ÉÄ„ÅÆÂá∫Âäõ„Éó„É≠„Ç∏„Çß„ÇØ„Ç∑„Éß„É≥„Éñ„É≠„ÉÉ„ÇØ\n",
    "    \"\"\"\n",
    "    logger.info(f\"build_decoder_project_out_block {in_channels=} {out_channels=} {factor=} {upsample_block_type=} {norm=} {act=} {is_video=}\")\n",
    "\n",
    "    # 1) „É¨„Ç§„É§„ÉºÊ≠£Ë¶èÂåñÂ±§„Å®Ê¥ªÊÄßÂåñÈñ¢Êï∞„Çí‰ΩúÊàê„Åó„ÄÅ„É™„Çπ„Éà„Å´ËøΩÂä†\n",
    "\n",
    "    layers: list[nn.Module] = [\n",
    "        build_norm(\n",
    "            norm, # rms3d\n",
    "            in_channels # 128\n",
    "        ),\n",
    "        build_act(act), # relu\n",
    "    ]\n",
    "\n",
    "    # 2) „Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É™„É≥„Ç∞„Éñ„É≠„ÉÉ„ÇØ„Çí‰ΩúÊàê„Åó„ÄÅ„É™„Çπ„Éà„Å´ËøΩÂä†\n",
    "\n",
    "    # True\n",
    "    if factor == 1:\n",
    "\n",
    "        # Áï≥„ÅøËæº„Åø„É¨„Ç§„É§„Éº„Çí‰ΩúÊàê„Åó„ÄÅ„É™„Çπ„Éà„Å´ËøΩÂä†\n",
    "        layers.append(\n",
    "            ConvLayer(\n",
    "                in_channels=in_channels, # 128\n",
    "                out_channels=out_channels, # 3\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                use_bias=True,\n",
    "                norm=None,\n",
    "                act_func=None,\n",
    "                is_video=is_video, # True\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # False\n",
    "    elif factor == 2:\n",
    "        if is_video:\n",
    "            raise NotImplementedError(\"Upsample during project_out is not supported for video\")\n",
    "        layers.append(\n",
    "            build_upsample_block(\n",
    "                block_type=upsample_block_type, in_channels=in_channels, out_channels=out_channels, shortcut=None\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # False\n",
    "    else:\n",
    "        raise ValueError(f\"upsample factor {factor} is not supported for decoder project out\")\n",
    "\n",
    "    # 3) OpSequential„Åß„É©„ÉÉ„Éó„Åó„Å¶Ëøî„Åô\n",
    "\n",
    "    return OpSequential(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf6a03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DecoderConfig:\n",
    "    in_channels: int = MISSING\n",
    "    latent_channels: int = MISSING\n",
    "    in_shortcut: Optional[str] = \"duplicating\"\n",
    "    width_list: tuple[int, ...] = (128, 256, 512, 512, 1024, 1024)\n",
    "    depth_list: tuple[int, ...] = (2, 2, 2, 2, 2, 2)\n",
    "    block_type: Any = \"ResBlock\"\n",
    "    norm: Any = \"rms2d\"\n",
    "    act: Any = \"silu\"\n",
    "    upsample_block_type: str = \"ConvPixelShuffle\"\n",
    "    upsample_match_channel: bool = True\n",
    "    upsample_shortcut: str = \"duplicating\"\n",
    "    out_norm: str = \"rms2d\"\n",
    "    out_act: str = \"relu\"\n",
    "    is_video: bool = False\n",
    "    temporal_upsample: tuple[bool, ...] = ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454056a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    „Éá„Ç≥„Éº„ÉÄ\n",
    "    DCAE„Åß‰ΩøÁî®\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: DecoderConfig):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cfg (DecoderConfig): „Éá„Ç≥„Éº„ÉÄ„ÅÆË®≠ÂÆö\n",
    "        \"\"\"\n",
    "        logger.info(f\"Decoder.__init__ {cfg=}\")\n",
    "\n",
    "        # 1) ÂàùÊúüÂåñ\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # 6„Å§„ÅÆ„Çπ„ÉÜ„Éº„Ç∏\n",
    "        num_stages = len(cfg.width_list)\n",
    "\n",
    "        self.num_stages = num_stages\n",
    "\n",
    "        assert len(cfg.depth_list) == num_stages\n",
    "        assert len(cfg.width_list) == num_stages\n",
    "        assert isinstance(cfg.block_type, str) or (\n",
    "            isinstance(cfg.block_type, list) and len(cfg.block_type) == num_stages\n",
    "        )\n",
    "        assert isinstance(cfg.norm, str) or (isinstance(cfg.norm, list) and len(cfg.norm) == num_stages)\n",
    "        assert isinstance(cfg.act, str) or (isinstance(cfg.act, list) and len(cfg.act) == num_stages)\n",
    "\n",
    "        # 2) ÂÖ•Âäõ„Éó„É≠„Ç∏„Çß„ÇØ„Ç∑„Éß„É≥„Éñ„É≠„ÉÉ„ÇØ„Çí‰ΩúÊàê\n",
    "\n",
    "        self.project_in = build_decoder_project_in_block(\n",
    "            in_channels=cfg.latent_channels,\n",
    "            out_channels=cfg.width_list[-1],\n",
    "            shortcut=cfg.in_shortcut,\n",
    "            is_video=cfg.is_video,\n",
    "        )\n",
    "\n",
    "        # 3) 6„Å§„ÅÆ„Çπ„ÉÜ„Éº„Ç∏„Çí‰ΩúÊàê\n",
    "\n",
    "        self.stages: list[OpSequential] = []\n",
    "\n",
    "        # [128, 256, 512, 512, 1024, 1024]„ÅÆÂπÖ„Å®[2, 2, 2, 3, 3, 3]„ÅÆÊ∑±„Åï„ÇíÊåÅ„Å§6„Å§„ÅÆ„Çπ„ÉÜ„Éº„Ç∏„Çí„ÄåÈÄÜÈ†Ü„Äç„Å´ÊßãÁØâ\n",
    "        for stage_id, (width, depth) in reversed(list(enumerate(zip(cfg.width_list, cfg.depth_list)))):\n",
    "            stage = []\n",
    "\n",
    "            # ÊúÄÂæå„ÅÆ„Çπ„ÉÜ„Éº„Ç∏„Çà„ÇäÂâç„ÅÆ„Çπ„ÉÜ„Éº„Ç∏„ÅÆÂ†¥Âêà\n",
    "            if stage_id < num_stages - 1 and depth > 0:\n",
    "\n",
    "                # „Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É´„Éñ„É≠„ÉÉ„ÇØ„ÇíÊßãÁØâ\n",
    "                upsample_block = build_upsample_block(\n",
    "                    block_type=cfg.upsample_block_type,\n",
    "                    in_channels=cfg.width_list[stage_id + 1],\n",
    "                    out_channels=width if cfg.upsample_match_channel else cfg.width_list[stage_id + 1],\n",
    "                    shortcut=cfg.upsample_shortcut,\n",
    "                    is_video=cfg.is_video,\n",
    "                    temporal_upsample=cfg.temporal_upsample[stage_id] if cfg.temporal_upsample != [] else False,\n",
    "                )\n",
    "\n",
    "                # „Çπ„ÉÜ„Éº„Ç∏„Å´„Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É´„Éñ„É≠„ÉÉ„ÇØ„ÇíËøΩÂä†\n",
    "                stage.append(upsample_block)\n",
    "\n",
    "            # „Éñ„É≠„ÉÉ„ÇØ„ÅÆÁ®ÆÈ°û„ÇíÊ±∫ÂÆö\n",
    "            # ['ResBlock', 'ResBlock', 'ResBlock', 'EViTS5_GLU', 'EViTS5_GLU', 'EViTS5_GLU']„ÅÆÈÄÜÈ†Ü\n",
    "            block_type = cfg.block_type[stage_id] \\\n",
    "                if isinstance(cfg.block_type, list) else cfg.block_type\n",
    "\n",
    "\n",
    "            # rms3d\n",
    "            norm = cfg.norm[stage_id] if isinstance(cfg.norm, list) else cfg.norm\n",
    "\n",
    "            # silu\n",
    "            act = cfg.act[stage_id] if isinstance(cfg.act, list) else cfg.act\n",
    "\n",
    "            # „Çπ„ÉÜ„Éº„Ç∏„ÅÆ„É°„Ç§„É≥„Éñ„É≠„ÉÉ„ÇØ„ÇíÊßãÁØâ„Åó„ÄÅ„Çπ„ÉÜ„Éº„Ç∏„Å´ËøΩÂä†\n",
    "            stage.extend(\n",
    "                build_stage_main(\n",
    "                    width=width,\n",
    "                    depth=depth,\n",
    "                    block_type=block_type,\n",
    "                    norm=norm,\n",
    "                    act=act,\n",
    "                    input_width=(\n",
    "                        width if cfg.upsample_match_channel else cfg.width_list[min(stage_id + 1, num_stages - 1)]\n",
    "                    ),\n",
    "                    is_video=cfg.is_video,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # „Çπ„ÉÜ„Éº„Ç∏„ÇíOpSequential„Åß„É©„ÉÉ„Éó„Åó„Å¶„É™„Çπ„Éà„Å´ËøΩÂä†\n",
    "            self.stages.insert(0, OpSequential(stage))\n",
    "\n",
    "        # „Çπ„ÉÜ„Éº„Ç∏„ÅÆ„É™„Çπ„Éà„Çínn.ModuleList„Å´Â§âÊèõ\n",
    "        self.stages = nn.ModuleList(self.stages)\n",
    "\n",
    "        # 4) Âá∫Âäõ„Éó„É≠„Ç∏„Çß„ÇØ„Ç∑„Éß„É≥„Éñ„É≠„ÉÉ„ÇØ„Çí‰ΩúÊàê\n",
    "\n",
    "        self.project_out = build_decoder_project_out_block(\n",
    "            in_channels=cfg.width_list[0] if cfg.depth_list[0] > 0 else cfg.width_list[1],\n",
    "            out_channels=cfg.in_channels,\n",
    "            factor=1 if cfg.depth_list[0] > 0 else 2,\n",
    "            upsample_block_type=cfg.upsample_block_type,\n",
    "            norm=cfg.out_norm,\n",
    "            act=cfg.out_act,\n",
    "            is_video=cfg.is_video,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logger.info(f\"Decoder.forward {x.shape=}\")\n",
    "\n",
    "        # 1) ÂÖ•Âäõ„Éó„É≠„Ç∏„Çß„ÇØ„Ç∑„Éß„É≥„Éñ„É≠„ÉÉ„ÇØ„ÇíÈÄöÈÅé\n",
    "\n",
    "        # Ëá™ÂãïÂãæÈÖç„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà„Çí‰ΩøÁî®„Åó„Å¶„É°„É¢„É™‰ΩøÁî®Èáè„ÇíÂâäÊ∏õ\n",
    "        x = auto_grad_checkpoint(self.project_in, x)\n",
    "        logger.debug(f\"Decoder.forward after project_in {x.shape=}\")\n",
    "\n",
    "        # 2) 6„Å§„ÅÆ„Çπ„ÉÜ„Éº„Ç∏„ÇíÈÄÜÈ†Ü„Å´ÈÄöÈÅé\n",
    "        for i, stage in enumerate(reversed(self.stages)):\n",
    "\n",
    "            if len(stage.op_list) == 0:\n",
    "                continue\n",
    "\n",
    "            # x = stage(x)\n",
    "\n",
    "            # Ëá™ÂãïÂãæÈÖç„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà„Çí‰ΩøÁî®„Åó„Å¶„É°„É¢„É™‰ΩøÁî®Èáè„ÇíÂâäÊ∏õ\n",
    "            x = auto_grad_checkpoint(stage, x)\n",
    "            logger.debug(f\"Decoder.forward after stage {i=} {x.shape=}\")\n",
    "\n",
    "        # 3) Âá∫Âäõ„Éó„É≠„Ç∏„Çß„ÇØ„Ç∑„Éß„É≥„Éñ„É≠„ÉÉ„ÇØ„ÇíÈÄöÈÅé\n",
    "\n",
    "        # Discriminator„ÇíÁî®„ÅÑ„ÅüÂ≠¶ÁøíÊôÇ„ÅÆÂ†¥Âêà\n",
    "        # False\n",
    "        if self.disc_off_grad_ckpt:\n",
    "            x = self.project_out(x)\n",
    "        else:\n",
    "            x = auto_grad_checkpoint(self.project_out, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e1fed3",
   "metadata": {},
   "source": [
    "### DC-AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cae01e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DCAEConfig:\n",
    "    in_channels: int = 3\n",
    "    latent_channels: int = 32\n",
    "    time_compression_ratio: int = 1\n",
    "    spatial_compression_ratio: int = 32\n",
    "    encoder: EncoderConfig = field(\n",
    "        default_factory=lambda: EncoderConfig(in_channels=\"${..in_channels}\", latent_channels=\"${..latent_channels}\")\n",
    "    )\n",
    "    decoder: DecoderConfig = field(\n",
    "        default_factory=lambda: DecoderConfig(in_channels=\"${..in_channels}\", latent_channels=\"${..latent_channels}\")\n",
    "    )\n",
    "    use_quant_conv: bool = False\n",
    "\n",
    "    pretrained_path: Optional[str] = None\n",
    "    pretrained_source: str = \"dc-ae\"\n",
    "\n",
    "    scaling_factor: Optional[float] = None\n",
    "    is_image_model: bool = False\n",
    "\n",
    "    is_training: bool = False  # NOTE: set to True in vae train config\n",
    "\n",
    "    use_spatial_tiling: bool = False\n",
    "    use_temporal_tiling: bool = False\n",
    "    spatial_tile_size: int = 256\n",
    "    temporal_tile_size: int = 32\n",
    "    tile_overlap_factor: float = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af7dd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_modules(model: Union[nn.Module, list[nn.Module]], init_type=\"trunc_normal\") -> None:\n",
    "    \"\"\"\n",
    "    „É¢„Éá„É´„ÅÆÈáç„Åø„ÇíÂàùÊúüÂåñ„Åô„Çã\n",
    "    1Âõû„Å†„Åë‰ΩøÁî®\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module or list[nn.Module]): ÂàùÊúüÂåñ„Åô„Çã„É¢„Éá„É´„Åæ„Åü„ÅØ„É¢„Éá„É´„ÅÆ„É™„Çπ„Éà\n",
    "        init_type (str, optional): ÂàùÊúüÂåñ„ÅÆÁ®ÆÈ°û (\"trunc_normal@std\" „Åæ„Åü„ÅØ \"normal@std\")\n",
    "    \"\"\"\n",
    "    logger.info(f\"init_modules {model=} {init_type=}\")\n",
    "\n",
    "    _DEFAULT_INIT_PARAM = {\"trunc_normal\": 0.02}\n",
    "\n",
    "    if isinstance(model, list):\n",
    "        for sub_module in model:\n",
    "            init_modules(sub_module, init_type)\n",
    "    else:\n",
    "        init_params = init_type.split(\"@\")\n",
    "        init_params = float(init_params[1]) if len(init_params) > 1 else None\n",
    "\n",
    "        if init_type.startswith(\"trunc_normal\"):\n",
    "            init_func = lambda param: nn.init.trunc_normal_(\n",
    "                param, std=(_DEFAULT_INIT_PARAM[\"trunc_normal\"] if init_params is None else init_params)\n",
    "            )\n",
    "        elif init_type.startswith(\"normal\"):\n",
    "            init_func = lambda param: nn.init.normal_(\n",
    "                param, std=(_DEFAULT_INIT_PARAM[\"trunc_normal\"] if init_params is None else init_params)\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        for m in model.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear, nn.ConvTranspose2d)):\n",
    "                init_func(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Embedding):\n",
    "                init_func(m.weight)\n",
    "            elif isinstance(m, (_BatchNorm, nn.GroupNorm, nn.LayerNorm)):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            else:\n",
    "                weight = getattr(m, \"weight\", None)\n",
    "                bias = getattr(m, \"bias\", None)\n",
    "                if isinstance(weight, torch.nn.Parameter):\n",
    "                    init_func(weight)\n",
    "                if isinstance(bias, torch.nn.Parameter):\n",
    "                    bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cea200",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Compressive Autoencoder (DCAE)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: DCAEConfig):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cfg (DCAEConfig): DCAE„ÅÆË®≠ÂÆö\n",
    "        \"\"\"\n",
    "        logger.info(f\"DCAE.__init__ {cfg=}\")\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # „Ç®„É≥„Ç≥„Éº„ÉÄ„ÇíÂàùÊúüÂåñ\n",
    "        self.encoder = Encoder(cfg.encoder)\n",
    "\n",
    "        # „Éá„Ç≥„Éº„ÉÄ„ÇíÂàùÊúüÂåñ\n",
    "        self.decoder = Decoder(cfg.decoder)\n",
    "\n",
    "        self.scaling_factor = cfg.scaling_factor\n",
    "        self.time_compression_ratio = cfg.time_compression_ratio\n",
    "        self.spatial_compression_ratio = cfg.spatial_compression_ratio\n",
    "        self.use_spatial_tiling = cfg.use_spatial_tiling\n",
    "        self.use_temporal_tiling = cfg.use_temporal_tiling\n",
    "        self.spatial_tile_size = cfg.spatial_tile_size\n",
    "        self.temporal_tile_size = cfg.temporal_tile_size\n",
    "        assert (\n",
    "            cfg.spatial_tile_size // cfg.spatial_compression_ratio\n",
    "        ), f\"spatial tile size {cfg.spatial_tile_size} must be divisible by spatial compression of {cfg.spatial_compression_ratio}\"\n",
    "        self.spatial_tile_latent_size = cfg.spatial_tile_size // cfg.spatial_compression_ratio\n",
    "        assert (\n",
    "            cfg.temporal_tile_size // cfg.time_compression_ratio\n",
    "        ), f\"temporal tile size {cfg.temporal_tile_size} must be divisible by temporal compression of {cfg.time_compression_ratio}\"\n",
    "        self.temporal_tile_latent_size = cfg.temporal_tile_size // cfg.time_compression_ratio\n",
    "        self.tile_overlap_factor = cfg.tile_overlap_factor\n",
    "        if self.cfg.pretrained_path is not None:\n",
    "            self.load_model()\n",
    "\n",
    "        self.to(torch.float32)\n",
    "        init_modules(self, init_type=\"trunc_normal\")\n",
    "\n",
    "    def load_model(self):\n",
    "        if self.cfg.pretrained_source == \"dc-ae\":\n",
    "            state_dict = torch.load(self.cfg.pretrained_path, map_location=\"cpu\", weights_only=True)[\"state_dict\"]\n",
    "            self.load_state_dict(state_dict)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def get_last_layer(self):\n",
    "        return self.decoder.project_out.op_list[2].conv.weight\n",
    "\n",
    "    # @property\n",
    "    # def spatial_compression_ratio(self) -> int:\n",
    "    #     return 2 ** (self.decoder.num_stages - 1)\n",
    "\n",
    "    def encode_single(self, x: torch.Tensor, is_video_encoder: bool = False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Âçò‰∏Ä„Çµ„É≥„Éó„É´„ÅÆ„Ç®„É≥„Ç≥„Éº„Éâ„ÇíÂÆüË°å\n",
    "        encode„É°„ÇΩ„ÉÉ„Éâ„Åã„ÇâÂëº„Å≥Âá∫„Åï„Çå„Çã\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): ÂÖ•Âäõ„ÉÜ„É≥„ÇΩ„É´ (1, C, H, W) „Åæ„Åü„ÅØ (1, C, F, H, W)\n",
    "            is_video_encoder (bool, optional): „Ç®„É≥„Ç≥„Éº„ÉÄ„ÅåÂãïÁîªÁî®„Åã„Å©„ÅÜ„Åã\n",
    "        Returns:\n",
    "            torch.Tensor: ÊΩúÂú®Ë°®Áèæ„ÉÜ„É≥„ÇΩ„É´\n",
    "        \"\"\"\n",
    "        logger.info(f\"DCAE.encode_single {x.shape=} {is_video_encoder=}\")\n",
    "\n",
    "        # (1, 3, 4, 256, 256)\n",
    "\n",
    "        # „Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫„Åå1„Åß„ÅÇ„Çã„Åì„Å®„ÇíÁ¢∫Ë™ç\n",
    "        assert x.shape[0] == 1\n",
    "\n",
    "        # ÂÖ•Âäõ„ÅåÂãïÁîª„Åã„Å©„ÅÜ„Åã„ÇíÁ¢∫Ë™ç\n",
    "        # True\n",
    "        is_video = x.dim() == 5\n",
    "\n",
    "        # „Ç®„É≥„Ç≥„Éº„ÉÄ„ÅåÁîªÂÉèÁî®„ÅÆÂ†¥Âêà\n",
    "        # False\n",
    "        if is_video and not is_video_encoder:\n",
    "            b, c, f, h, w = x.shape\n",
    "            x = x.permute(0, 2, 1, 3, 4).reshape(-1, c, h, w)\n",
    "\n",
    "        # „Ç®„É≥„Ç≥„Éº„ÉÄ„Éº„Åß„Ç®„É≥„Ç≥„Éº„Éâ„ÇíÂÆüË°å\n",
    "        z = self.encoder(x)\n",
    "\n",
    "        # „Ç®„É≥„Ç≥„Éº„ÉÄ„Éº„ÅåÁîªÂÉèÁî®„ÅÆÂ†¥Âêà\n",
    "        # False\n",
    "        if is_video and not is_video_encoder:\n",
    "            z = z.unsqueeze(dim=0).permute(0, 2, 1, 3, 4)\n",
    "\n",
    "        # „Çπ„Ç±„Éº„É™„É≥„Ç∞„Éï„Ç°„ÇØ„Çø„Éº„ÅåË®≠ÂÆö„Åï„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà\n",
    "        # None\n",
    "        if self.scaling_factor is not None:\n",
    "            # „Çπ„Ç±„Éº„É™„É≥„Ç∞„ÇíÈÅ©Áî®\n",
    "            z = z / self.scaling_factor\n",
    "\n",
    "        return z\n",
    "\n",
    "    def _encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        „Ç®„É≥„Ç≥„Éº„Éâ„ÅÆÂÜÖÈÉ®Âá¶ÁêÜ\n",
    "        encode„É°„ÇΩ„ÉÉ„Éâ„Åã„ÇâÂëº„Å≥Âá∫„Åï„Çå„Çã\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): ÂÖ•Âäõ„ÉÜ„É≥„ÇΩ„É´\n",
    "        Returns:\n",
    "            torch.Tensor: ÊΩúÂú®Ë°®Áèæ„ÉÜ„É≥„ÇΩ„É´\n",
    "        \"\"\"\n",
    "        logger.info(f\"DCAE._encode {x.shape=}\")\n",
    "\n",
    "        # (1, 3, 4, 256, 256)\n",
    "\n",
    "        # Ë®ìÁ∑¥ÊôÇ„ÅØ„Åù„ÅÆ„Åæ„Åæ„Ç®„É≥„Ç≥„Éº„Éâ\n",
    "        # False\n",
    "        if self.cfg.is_training:\n",
    "            return self.encoder(x)\n",
    "\n",
    "        # ÂÖ•Âäõ„ÅåÂãïÁîª„Åã\n",
    "        # True\n",
    "        is_video_encoder = self.encoder.cfg.is_video if self.encoder.cfg.is_video is not None else False\n",
    "\n",
    "        x_ret = []\n",
    "\n",
    "        # „Éê„ÉÉ„ÉÅ„Åî„Å®„Å´„É´„Éº„ÉóÔºà1Ôºâ\n",
    "        for i in range(x.shape[0]):\n",
    "\n",
    "            # Âçò‰∏Ä„Çµ„É≥„Éó„É´„ÅÆ„Ç®„É≥„Ç≥„Éº„Éâ„ÇíÂÆüË°å\n",
    "            x_ret.append(self.encode_single(x[i : i + 1], is_video_encoder))\n",
    "\n",
    "        # ÁµêÊûú„ÇíÁµêÂêà„Åó„Å¶Ëøî„Åô\n",
    "        return torch.cat(x_ret, dim=0)\n",
    "\n",
    "    def blend_v(self, a: torch.Tensor, b: torch.Tensor, blend_extent: int) -> torch.Tensor:\n",
    "        blend_extent = min(a.shape[-2], b.shape[-2], blend_extent)\n",
    "        for y in range(blend_extent):\n",
    "            b[:, :, :, y, :] = a[:, :, :, -blend_extent + y, :] * (1 - y / blend_extent) + b[:, :, :, y, :] * (\n",
    "                y / blend_extent\n",
    "            )\n",
    "        return b\n",
    "\n",
    "    def blend_h(self, a: torch.Tensor, b: torch.Tensor, blend_extent: int) -> torch.Tensor:\n",
    "        blend_extent = min(a.shape[-1], b.shape[-1], blend_extent)\n",
    "        for x in range(blend_extent):\n",
    "            b[:, :, :, :, x] = a[:, :, :, :, -blend_extent + x] * (1 - x / blend_extent) + b[:, :, :, :, x] * (\n",
    "                x / blend_extent\n",
    "            )\n",
    "        return b\n",
    "\n",
    "    def blend_t(self, a: torch.Tensor, b: torch.Tensor, blend_extent: int) -> torch.Tensor:\n",
    "        blend_extent = min(a.shape[-3], b.shape[-3], blend_extent)\n",
    "        for x in range(blend_extent):\n",
    "            b[:, :, x, :, :] = a[:, :, -blend_extent + x, :, :] * (1 - x / blend_extent) + b[:, :, x, :, :] * (\n",
    "                x / blend_extent\n",
    "            )\n",
    "        return b\n",
    "\n",
    "    def spatial_tiled_encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        net_size = int(self.spatial_tile_size * (1 - self.tile_overlap_factor))\n",
    "        blend_extent = int(self.spatial_tile_latent_size * self.tile_overlap_factor)\n",
    "        row_limit = self.spatial_tile_latent_size - blend_extent\n",
    "\n",
    "        # Split video into tiles and encode them separately.\n",
    "        rows = []\n",
    "        for i in range(0, x.shape[-2], net_size):\n",
    "            row = []\n",
    "            for j in range(0, x.shape[-1], net_size):\n",
    "                tile = x[:, :, :, i : i + self.spatial_tile_size, j : j + self.spatial_tile_size]\n",
    "                tile = self._encode(tile)\n",
    "                row.append(tile)\n",
    "            rows.append(row)\n",
    "        result_rows = []\n",
    "        for i, row in enumerate(rows):\n",
    "            result_row = []\n",
    "            for j, tile in enumerate(row):\n",
    "                # blend the above tile and the left tile\n",
    "                # to the current tile and add the current tile to the result row\n",
    "                if i > 0:\n",
    "                    tile = self.blend_v(rows[i - 1][j], tile, blend_extent)\n",
    "                if j > 0:\n",
    "                    tile = self.blend_h(row[j - 1], tile, blend_extent)\n",
    "                result_row.append(tile[:, :, :, :row_limit, :row_limit])\n",
    "            result_rows.append(torch.cat(result_row, dim=-1))\n",
    "\n",
    "        return torch.cat(result_rows, dim=-2)\n",
    "\n",
    "    def temporal_tiled_encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        overlap_size = int(self.temporal_tile_size * (1 - self.tile_overlap_factor))\n",
    "        blend_extent = int(self.temporal_tile_latent_size * self.tile_overlap_factor)\n",
    "        t_limit = self.temporal_tile_latent_size - blend_extent\n",
    "\n",
    "        # Split the video into tiles and encode them separately.\n",
    "        row = []\n",
    "        for i in range(0, x.shape[2], overlap_size):\n",
    "            tile = x[:, :, i : i + self.temporal_tile_size, :, :]\n",
    "            if self.use_spatial_tiling and (\n",
    "                tile.shape[-1] > self.spatial_tile_size or tile.shape[-2] > self.spatial_tile_size\n",
    "            ):\n",
    "                tile = self.spatial_tiled_encode(tile)\n",
    "            else:\n",
    "                tile = self._encode(tile)\n",
    "            row.append(tile)\n",
    "        result_row = []\n",
    "        for i, tile in enumerate(row):\n",
    "            if i > 0:\n",
    "                tile = self.blend_t(row[i - 1], tile, blend_extent)\n",
    "            result_row.append(tile[:, :, :t_limit, :, :])\n",
    "\n",
    "        return torch.cat(result_row, dim=2)\n",
    "\n",
    "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        DCAE„ÅÆ„Ç®„É≥„Ç≥„Éº„ÉâÂá¶ÁêÜ\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): ÂÖ•Âäõ„ÉÜ„É≥„ÇΩ„É´\n",
    "        Returns:\n",
    "            torch.Tensor: ÊΩúÂú®Ë°®Áèæ„ÉÜ„É≥„ÇΩ„É´\n",
    "        \"\"\"\n",
    "        logger.info(f\"DCAE.encode {x.shape=}\")\n",
    "\n",
    "        # ÊôÇÈñìÊñπÂêë„Å´„Çø„Ç§„É´ÂàÜÂâ≤„Åô„ÇãÂ†¥ÂêàÔºà16„Éï„É¨„Éº„É†„Çà„ÇäÂ§ß„Åç„ÅÑÔºâ\n",
    "        if self.use_temporal_tiling and x.shape[2] > self.temporal_tile_size:\n",
    "            return self.temporal_tiled_encode(x)\n",
    "\n",
    "        # Á©∫ÈñìÊñπÂêë„Å´„Çø„Ç§„É´ÂàÜÂâ≤„Åô„ÇãÂ†¥ÂêàÔºà256„Éî„ÇØ„Çª„É´„Çà„ÇäÂ§ß„Åç„ÅÑÔºâ\n",
    "        elif self.use_spatial_tiling and (x.shape[-1] > self.spatial_tile_size or x.shape[-2] > self.spatial_tile_size):\n",
    "\n",
    "            return self.spatial_tiled_encode(x)\n",
    "\n",
    "        # „Çø„Ç§„É´Âá¶ÁêÜ„Åó„Å™„ÅÑÂ†¥Âêà\n",
    "        # True\n",
    "        else:\n",
    "            return self._encode(x)\n",
    "\n",
    "    def spatial_tiled_decode(self, z: torch.FloatTensor) -> torch.Tensor:\n",
    "        net_size = int(self.spatial_tile_latent_size * (1 - self.tile_overlap_factor))\n",
    "        blend_extent = int(self.spatial_tile_size * self.tile_overlap_factor)\n",
    "        row_limit = self.spatial_tile_size - blend_extent\n",
    "\n",
    "        # Split z into overlapping tiles and decode them separately.\n",
    "        # The tiles have an overlap to avoid seams between tiles.\n",
    "        rows = []\n",
    "        for i in range(0, z.shape[-2], net_size):\n",
    "            row = []\n",
    "            for j in range(0, z.shape[-1], net_size):\n",
    "                tile = z[:, :, :, i : i + self.spatial_tile_latent_size, j : j + self.spatial_tile_latent_size]\n",
    "                decoded = self._decode(tile)\n",
    "                row.append(decoded)\n",
    "            rows.append(row)\n",
    "        result_rows = []\n",
    "        for i, row in enumerate(rows):\n",
    "            result_row = []\n",
    "            for j, tile in enumerate(row):\n",
    "                # blend the above tile and the left tile\n",
    "                # to the current tile and add the current tile to the result row\n",
    "                if i > 0:\n",
    "                    tile = self.blend_v(rows[i - 1][j], tile, blend_extent)\n",
    "                if j > 0:\n",
    "                    tile = self.blend_h(row[j - 1], tile, blend_extent)\n",
    "                result_row.append(tile[:, :, :, :row_limit, :row_limit])\n",
    "            result_rows.append(torch.cat(result_row, dim=-1))\n",
    "\n",
    "        return torch.cat(result_rows, dim=-2)\n",
    "\n",
    "    def temporal_tiled_decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        overlap_size = int(self.temporal_tile_latent_size * (1 - self.tile_overlap_factor))\n",
    "        blend_extent = int(self.temporal_tile_size * self.tile_overlap_factor)\n",
    "        t_limit = self.temporal_tile_size - blend_extent\n",
    "\n",
    "        row = []\n",
    "        for i in range(0, z.shape[2], overlap_size):\n",
    "            tile = z[:, :, i : i + self.temporal_tile_latent_size, :, :]\n",
    "            if self.use_spatial_tiling and (\n",
    "                tile.shape[-1] > self.spatial_tile_latent_size or tile.shape[-2] > self.spatial_tile_latent_size\n",
    "            ):\n",
    "                decoded = self.spatial_tiled_decode(tile)\n",
    "            else:\n",
    "                decoded = self._decode(tile)\n",
    "            row.append(decoded)\n",
    "        result_row = []\n",
    "        for i, tile in enumerate(row):\n",
    "            if i > 0:\n",
    "                tile = self.blend_t(row[i - 1], tile, blend_extent)\n",
    "            result_row.append(tile[:, :, :t_limit, :, :])\n",
    "\n",
    "        return torch.cat(result_row, dim=2)\n",
    "\n",
    "    def decode_single(self, z: torch.Tensor, is_video_decoder: bool = False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Âçò‰∏Ä„Çµ„É≥„Éó„É´„ÅÆ„Éá„Ç≥„Éº„Éâ„ÇíÂÆüË°å\n",
    "        _decode„É°„ÇΩ„ÉÉ„Éâ„Åã„ÇâÂëº„Å≥Âá∫„Åï„Çå„Çã\n",
    "\n",
    "        Args:\n",
    "            z (torch.Tensor): ÊΩúÂú®Ë°®Áèæ„ÉÜ„É≥„ÇΩ„É´ (1, C, H, W) „Åæ„Åü„ÅØ (1, C, F, H, W)\n",
    "            is_video_decoder (bool, optional): „Éá„Ç≥„Éº„ÉÄ„ÅåÂãïÁîªÁî®„Åã„Å©„ÅÜ„Åã\n",
    "        Returns:\n",
    "            torch.Tensor: Âæ©ÂÖÉ„Åï„Çå„Åü„ÉÜ„É≥„ÇΩ„É´\n",
    "        \"\"\"\n",
    "        logger.info(f\"DCAE.decode_single {z.shape=} {is_video_decoder=}\")\n",
    "\n",
    "        # „Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫„Åå1„Åß„ÅÇ„Çã„Åì„Å®„ÇíÁ¢∫Ë™ç\n",
    "        assert z.shape[0] == 1\n",
    "\n",
    "        # ÂÖ•Âäõ„ÅåÂãïÁîª„Åã„Å©„ÅÜ„Åã„ÇíÁ¢∫Ë™ç\n",
    "        # True\n",
    "        is_video = z.dim() == 5\n",
    "\n",
    "        # „Éá„Ç≥„Éº„ÉÄ„ÅåÁîªÂÉèÁî®„ÅÆÂ†¥Âêà\n",
    "        # False\n",
    "        if is_video and not is_video_decoder:\n",
    "            b, c, f, h, w = z.shape\n",
    "            z = z.permute(0, 2, 1, 3, 4).reshape(-1, c, h, w)\n",
    "\n",
    "        # „Çπ„Ç±„Éº„É™„É≥„Ç∞„Éï„Ç°„ÇØ„Çø„Éº„ÅåË®≠ÂÆö„Åï„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà\n",
    "        # None\n",
    "        if self.scaling_factor is not None:\n",
    "            # „Çπ„Ç±„Éº„É™„É≥„Ç∞„ÇíÈÅ©Áî®\n",
    "            z = z * self.scaling_factor\n",
    "\n",
    "        # „Éá„Ç≥„Éº„ÉÄ„Éº„Åß„Éá„Ç≥„Éº„Éâ„ÇíÂÆüË°å\n",
    "        x = self.decoder(z)\n",
    "\n",
    "        # „Éá„Ç≥„Éº„ÉÄ„Éº„ÅåÁîªÂÉèÁî®„ÅÆÂ†¥Âêà\n",
    "        # False\n",
    "        if is_video and not is_video_decoder:\n",
    "            x = x.unsqueeze(dim=0).permute(0, 2, 1, 3, 4)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        „Éá„Ç≥„Éº„Éâ„ÅÆÂÜÖÈÉ®Âá¶ÁêÜ\n",
    "        decode„É°„ÇΩ„ÉÉ„Éâ„Åã„ÇâÂëº„Å≥Âá∫„Åï„Çå„Çã\n",
    "\n",
    "        Args:\n",
    "            z (torch.Tensor): ÊΩúÂú®Ë°®Áèæ„ÉÜ„É≥„ÇΩ„É´\n",
    "        \"\"\"\n",
    "        logger.info(f\"DCAE._decode {z.shape=}\")\n",
    "\n",
    "        # Ë®ìÁ∑¥ÊôÇ„ÅØ„Åù„ÅÆ„Åæ„Åæ„Éá„Ç≥„Éº„Éâ\n",
    "        # False\n",
    "        if self.cfg.is_training:\n",
    "            return self.decoder(z)\n",
    "\n",
    "        # ÂÖ•Âäõ„ÅåÂãïÁîª„Åã\n",
    "        # True\n",
    "        is_video_decoder = self.decoder.cfg.is_video \\\n",
    "            if self.decoder.cfg.is_video is not None else False\n",
    "\n",
    "\n",
    "        x_ret = []\n",
    "\n",
    "        # „Éê„ÉÉ„ÉÅ„Åî„Å®„Å´„É´„Éº„ÉóÔºà1Ôºâ\n",
    "        for i in range(z.shape[0]):\n",
    "            x_ret.append(self.decode_single(z[i : i + 1], is_video_decoder))\n",
    "\n",
    "        return torch.cat(x_ret, dim=0)\n",
    "\n",
    "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        DCAE„ÅÆ„Éá„Ç≥„Éº„ÉâÂá¶ÁêÜ\n",
    "        forward„É°„ÇΩ„ÉÉ„Éâ„Åã„ÇâÂëº„Å≥Âá∫„Åï„Çå„Çã\n",
    "\n",
    "        Args:\n",
    "            z (torch.Tensor): ÊΩúÂú®Ë°®Áèæ„ÉÜ„É≥„ÇΩ„É´\n",
    "        \"\"\"\n",
    "        logger.info(f\"DCAE.decode {z.shape=}\")\n",
    "\n",
    "        # ÊôÇÈñìÊñπÂêë„Å´„Çø„Ç§„É´ÂàÜÂâ≤„Åô„ÇãÂ†¥Âêà\n",
    "        # False\n",
    "        if self.use_temporal_tiling and z.shape[2] > self.temporal_tile_latent_size:\n",
    "            return self.temporal_tiled_decode(z)\n",
    "\n",
    "        # Á©∫ÈñìÊñπÂêë„Å´„Çø„Ç§„É´ÂàÜÂâ≤„Åô„ÇãÂ†¥Âêà\n",
    "        # False\n",
    "        elif self.use_spatial_tiling and (\n",
    "            z.shape[-1] > self.spatial_tile_latent_size or z.shape[-2] > self.spatial_tile_latent_size\n",
    "        ):\n",
    "            return self.spatial_tiled_decode(z)\n",
    "\n",
    "        # „Çø„Ç§„É´Âá¶ÁêÜ„Åó„Å™„ÅÑÂ†¥Âêà\n",
    "        # True\n",
    "        else:\n",
    "            return self._decode(z)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[Any, Tensor, dict[Any, Any]]:\n",
    "        \"\"\"\n",
    "        DCAE„ÅÆÈ†Ü‰ºùÊê¨\n",
    "        DC-AE„ÅÆ„Ç®„É≥„Éà„É™„Éº„Éù„Ç§„É≥„Éà\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): ÂÖ•Âäõ„ÉÜ„É≥„ÇΩ„É´\n",
    "        Returns:\n",
    "            tuple[Any, Tensor, dict[Any, Any]]:\n",
    "                - torch.Tensor: Âæ©ÂÖÉ„Åï„Çå„Åü„ÉÜ„É≥„ÇΩ„É´\n",
    "                - Tensor: None (ÈáèÂ≠êÂåñÊÉÖÂ†±Áî®„ÅÆ„Éó„É¨„Éº„Çπ„Éõ„É´„ÉÄ„Éº)\n",
    "                - dict[Any, Any]: ÊΩúÂú®Ë°®Áèæ\n",
    "        \"\"\"\n",
    "        logger.info(f\"DCAE.forward {x.shape=}\")\n",
    "\n",
    "        # ÂÖÉ„ÅÆ„Éá„Éº„ÇøÂûã„Çí‰øùÂ≠ò\n",
    "        x_type = x.dtype\n",
    "\n",
    "        # ÁîªÂÉèÂÖ•Âäõ„Åã„ÇíÂèñÂæó\n",
    "        # False\n",
    "        is_image_model = self.cfg.__dict__.get(\"is_image_model\", False)\n",
    "\n",
    "        # „Ç®„É≥„Ç≥„Éº„ÉÄ„ÅÆÈáç„Åø„ÅÆ„Éá„Éº„ÇøÂûã„Å´Â§âÊèõ\n",
    "        x = x.to(self.encoder.project_in.conv.weight.dtype)\n",
    "\n",
    "        # ÁîªÂÉèÂÖ•Âäõ„ÅÆÂ†¥Âêà\n",
    "        # False\n",
    "        if is_image_model:\n",
    "            b, c, _, h, w = x.shape\n",
    "            x = x.permute(0, 2, 1, 3, 4).reshape(-1, c, h, w)\n",
    "\n",
    "        # „Ç®„É≥„Ç≥„Éº„Éâ\n",
    "        z = self.encode(x)\n",
    "\n",
    "        # „Éá„Ç≥„Éº„Éâ\n",
    "        dec = self.decode(z)\n",
    "\n",
    "        # ÁîªÂÉèÂÖ•Âäõ„ÅÆÂ†¥Âêà\n",
    "        # False\n",
    "        if is_image_model:\n",
    "            dec = dec.reshape(b, 1, c, h, w).permute(0, 2, 1, 3, 4)\n",
    "            z = z.unsqueeze(dim=0).permute(0, 2, 1, 3, 4)\n",
    "\n",
    "        # ÂÖÉ„ÅÆ„Éá„Éº„ÇøÂûã„Å´Êàª„Åô\n",
    "        dec = dec.to(x_type)\n",
    "        return dec, None, z\n",
    "\n",
    "    def get_latent_size(self, input_size: list[int]) -> list[int]:\n",
    "        latent_size = []\n",
    "        # T\n",
    "        latent_size.append((input_size[0] - 1) // self.time_compression_ratio + 1)\n",
    "        # H, w\n",
    "        for i in range(1, 3):\n",
    "            latent_size.append((input_size[i] - 1) // self.spatial_compression_ratio + 1)\n",
    "        return latent_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63711488",
   "metadata": {},
   "source": [
    "### HuggingFace HubÂØæÂøúDC-AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08c3d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dc_ae_f32(name: str, pretrained_path: str) -> DCAEConfig:\n",
    "    \"\"\"\n",
    "    DCAE„ÅÆË®≠ÂÆö„ÇíÂàùÊúüÂåñ„Åô„Çã\n",
    "\n",
    "    Args:\n",
    "        name (str): „É¢„Éá„É´„ÅÆÂêçÂâç\n",
    "        pretrained_path (str): ‰∫ãÂâçÂ≠¶ÁøíÊ∏à„Åø„É¢„Éá„É´„ÅÆ„Éë„Çπ\n",
    "    Returns:\n",
    "        DCAEConfig: DCAE„ÅÆË®≠ÂÆö\n",
    "    \"\"\"\n",
    "\n",
    "    if name in [\"dc-ae-f32t4c128\"]:\n",
    "        cfg_str = (\n",
    "            \"time_compression_ratio=4 \"\n",
    "            \"spatial_compression_ratio=32 \"\n",
    "            \"encoder.block_type=[ResBlock,ResBlock,ResBlock,EViTS5_GLU,EViTS5_GLU,EViTS5_GLU] \"\n",
    "            \"encoder.width_list=[128,256,512,512,1024,1024] encoder.depth_list=[2,2,2,3,3,3] \"\n",
    "            \"encoder.downsample_block_type=Conv \"\n",
    "            \"encoder.norm=rms3d \"\n",
    "            \"encoder.is_video=True \"\n",
    "            \"decoder.block_type=[ResBlock,ResBlock,ResBlock,EViTS5_GLU,EViTS5_GLU,EViTS5_GLU] \"\n",
    "            \"decoder.width_list=[128,256,512,512,1024,1024] decoder.depth_list=[3,3,3,3,3,3] \"\n",
    "            \"decoder.upsample_block_type=InterpolateConv \"\n",
    "            \"decoder.norm=rms3d decoder.act=silu decoder.out_norm=rms3d \"\n",
    "            \"decoder.is_video=True \"\n",
    "            \"encoder.temporal_downsample=[False,False,False,True,True,False] \"\n",
    "            \"decoder.temporal_upsample=[False,False,False,True,True,False] \"\n",
    "            \"latent_channels=128\"\n",
    "        )  # make sure there is no trailing blankspace in the last line\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # ÊñáÂ≠óÂàó„Åã„ÇâË®≠ÂÆö„ÇíË™≠„ÅøËæº„ÇÄ\n",
    "    cfg = OmegaConf.from_dotlist(cfg_str.split(\" \"))\n",
    "\n",
    "    # DCAEConfig„Å´ÊµÅ„ÅóËæº„ÇÄ\n",
    "    cfg: DCAEConfig = OmegaConf.to_object(\n",
    "        OmegaConf.merge(OmegaConf.structured(DCAEConfig), cfg)\n",
    "    )\n",
    "\n",
    "    # ‰∫ãÂâçÂ≠¶ÁøíÊ∏à„Åø„É¢„Éá„É´„ÅÆ„Éë„Çπ„ÇíË®≠ÂÆö„Åô„Çã\n",
    "    cfg.pretrained_path = pretrained_path\n",
    "\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267e5839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Âà©Áî®ÂèØËÉΩ„Å™DCAE„É¢„Éá„É´„ÅÆÂÆöÁæ©\n",
    "REGISTERED_DCAE_MODEL: dict[str, tuple[Callable, Optional[str]]] = {\n",
    "    \"dc-ae-f32t4c128\": (dc_ae_f32, None),\n",
    "}\n",
    "\n",
    "def create_dc_ae_model_cfg(name: str, pretrained_path: Optional[str] = None) -> DCAEConfig:\n",
    "    \"\"\"\n",
    "    ÁôªÈå≤„Åï„Çå„ÅüDCAE„É¢„Éá„É´Ë®≠ÂÆö„Çí‰ΩúÊàê„Åô„Çã\n",
    "\n",
    "    Args:\n",
    "        name (str): ÁôªÈå≤„Åï„Çå„ÅüDCAE„É¢„Éá„É´„ÅÆÂêçÂâç\n",
    "        pretrained_path (Optional[str], optional): ‰∫ãÂâçÂ≠¶ÁøíÊ∏à„Åø„É¢„Éá„É´„ÅÆ„Éë„Çπ\n",
    "    Returns:\n",
    "        DCAEConfig: DCAE„É¢„Éá„É´„ÅÆË®≠ÂÆö\n",
    "    \"\"\"\n",
    "    logger.info(f\"create_dc_ae_model_cfg {name=} {pretrained_path=}\")\n",
    "\n",
    "    assert name in REGISTERED_DCAE_MODEL, f\"{name} is not supported\"\n",
    "\n",
    "    # Ë®≠ÂÆö„Çí‰ΩúÊàê„Åô„ÇãÈñ¢Êï∞„Å®„Éá„Éï„Ç©„É´„Éà„ÅÆ‰∫ãÂâçÂ≠¶ÁøíÊ∏à„Åø„É¢„Éá„É´„ÅÆ„Éë„Çπ„ÇíÂèñÂæó\n",
    "    dc_ae_cls, default_pt_path = REGISTERED_DCAE_MODEL[name]\n",
    "\n",
    "    pretrained_path = default_pt_path if pretrained_path is None else pretrained_path\n",
    "\n",
    "    # DCAE„É¢„Éá„É´„ÅÆË®≠ÂÆö„Çí‰ΩúÊàê\n",
    "    model_cfg = dc_ae_cls(name, pretrained_path)\n",
    "\n",
    "    return model_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8d3234",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCAE_HF(DCAE, PyTorchModelHubMixin):\n",
    "    \"\"\"\n",
    "    HuggingFace HubÂØæÂøúDCAE„É¢„Éá„É´\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str):\n",
    "        logger.info(f\"DCAE_HF.__init__ {model_name=}\")\n",
    "\n",
    "        # 1) DCAE„É¢„Éá„É´„ÅÆË®≠ÂÆö„Çí‰ΩúÊàê\n",
    "        # dc-ae-f32t4c128\n",
    "        cfg = create_dc_ae_model_cfg(model_name)\n",
    "\n",
    "        # 2) DCAE„ÇíÂàùÊúüÂåñ\n",
    "        DCAE.__init__(self, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8679561a",
   "metadata": {},
   "source": [
    "### Ê§úË®º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85f81ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @MODELS.register_module(\"dc_ae\")\n",
    "def DC_AE(\n",
    "    model_name: str,\n",
    "    device_map: str | torch.device = \"cuda\",\n",
    "    torch_dtype: torch.dtype = torch.bfloat16,\n",
    "    from_scratch: bool = False,\n",
    "    from_pretrained: str | None = None,\n",
    "    is_training: bool = False,\n",
    "    use_spatial_tiling: bool = False,\n",
    "    use_temporal_tiling: bool = False,\n",
    "    spatial_tile_size: int = 256,\n",
    "    temporal_tile_size: int = 32,\n",
    "    tile_overlap_factor: float = 0.25,\n",
    "    scaling_factor: float = None,\n",
    "    disc_off_grad_ckpt: bool = False,\n",
    ") -> DCAE_HF:\n",
    "    \"\"\"\n",
    "    Deep Compressive Autoencoder (DCAE)„É¢„Éá„É´„Çí‰ΩúÊàê„Åô„Çã\n",
    "\n",
    "    Args:\n",
    "        model_name (str): „É¢„Éá„É´„ÅÆÂêçÂâç\n",
    "        device_map (str or torch.device, optional): „É¢„Éá„É´„ÇíÈÖçÁΩÆ„Åô„Çã„Éá„Éê„Ç§„Çπ\n",
    "        torch_dtype (torch.dtype, optional): „É¢„Éá„É´„ÅÆ„Éá„Éº„ÇøÂûã\n",
    "        from_scratch (bool, optional): „É©„É≥„ÉÄ„É†ÂàùÊúüÂåñ„Åã„Çâ„É¢„Éá„É´„Çí‰ΩúÊàê„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "        from_pretrained (str or None, optional): ‰∫ãÂâçÂ≠¶ÁøíÊ∏à„Åø„É¢„Éá„É´„ÅÆ„Éë„Çπ\n",
    "        is_training (bool, optional): „É¢„Éá„É´„Çí„Éà„É¨„Éº„Éã„É≥„Ç∞„É¢„Éº„Éâ„Å´Ë®≠ÂÆö„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "        use_spatial_tiling (bool, optional): Á©∫Èñì„Çø„Ç§„É´Âá¶ÁêÜ„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "        use_temporal_tiling (bool, optional): ÊôÇÈñì„Çø„Ç§„É´Âá¶ÁêÜ„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "        spatial_tile_size (int, optional): Á©∫Èñì„Çø„Ç§„É´„ÅÆ„Çµ„Ç§„Ç∫\n",
    "        temporal_tile_size (int, optional): ÊôÇÈñì„Çø„Ç§„É´„ÅÆ„Çµ„Ç§„Ç∫\n",
    "        tile_overlap_factor (float, optional): „Çø„Ç§„É´„ÅÆÈáç„Å™„Çä‰øÇÊï∞\n",
    "        scaling_factor (float, optional): „Çπ„Ç±„Éº„É™„É≥„Ç∞„Éï„Ç°„ÇØ„Çø„Éº\n",
    "        disc_off_grad_ckpt (bool, optional): ÂãæÈÖç„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà„ÇíÁÑ°Âäπ„Å´„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "    Returns:\n",
    "        DCAE_HF: DCAE„É¢„Éá„É´\n",
    "    \"\"\"\n",
    "    logger.info(f\"DC_AE {model_name=} {device_map=} {torch_dtype=} {from_scratch=} {from_pretrained=} {is_training=} {use_spatial_tiling=} {use_temporal_tiling=} {spatial_tile_size=} {temporal_tile_size=} {tile_overlap_factor=} {scaling_factor=} {disc_off_grad_ckpt=}\")\n",
    "\n",
    "    # False\n",
    "    if not from_scratch:\n",
    "        model = DCAE_HF.from_pretrained(model_name).to(device_map, torch_dtype)\n",
    "\n",
    "    # True\n",
    "    else:\n",
    "        model = DCAE_HF(model_name).to(device_map, torch_dtype)\n",
    "\n",
    "    # False\n",
    "    if from_pretrained is not None:\n",
    "        model = load_checkpoint(model, from_pretrained, device_map=device_map)\n",
    "        print(f\"loaded dc_ae from ckpt path: {from_pretrained}\")\n",
    "\n",
    "    model.cfg.is_training = is_training\n",
    "    model.use_spatial_tiling = use_spatial_tiling\n",
    "    model.use_temporal_tiling = use_temporal_tiling\n",
    "    model.spatial_tile_size = spatial_tile_size\n",
    "    model.temporal_tile_size = temporal_tile_size\n",
    "    model.tile_overlap_factor = tile_overlap_factor\n",
    "\n",
    "    if scaling_factor is not None:\n",
    "        model.scaling_factor = scaling_factor\n",
    "\n",
    "    model.decoder.disc_off_grad_ckpt = disc_off_grad_ckpt\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fabd6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True: \n",
    "    logger.info(f\"DCAE„ÇíÊ§úË®º\")\n",
    "    # (batch_size, channels, frames, height, width)\n",
    "    # (1, 3, 96, 512, 512)\n",
    "    sample_input = torch.randn(1, 3, 4, 256, 256).to(\"cuda\")\n",
    "\n",
    "    # video_dc_ae.py„ÅÆ„Éá„Éï„Ç©„É´„ÉàË®≠ÂÆö\n",
    "    model = DC_AE(\n",
    "        model_name=\"dc-ae-f32t4c128\",\n",
    "        from_scratch=True,\n",
    "        from_pretrained=None,\n",
    "        is_training=False,\n",
    "        use_spatial_tiling=True,\n",
    "        use_temporal_tiling=True,\n",
    "        spatial_tile_size=256,\n",
    "        temporal_tile_size=16,\n",
    "        tile_overlap_factor=0.25,\n",
    "    )\n",
    "    output = model(sample_input)\n",
    "    print(output[0].shape)  # Decoded output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41770ec",
   "metadata": {},
   "source": [
    "## MMDiT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08bcb7f",
   "metadata": {},
   "source": [
    "### LigerEmbedND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49838b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def liger_rope(pos: Tensor, dim: int, theta: int) -> Tuple:\n",
    "    \"\"\"\n",
    "    Liger RoPE\n",
    "    Triton„ÅßÊúÄÈÅ©Âåñ„Åï„Çå„Åüliger-kernel\n",
    "    LigerEmbedND„Åß‰ΩøÁî®\n",
    "\n",
    "    Args:\n",
    "        pos (Tensor): ‰ΩçÁΩÆ„Ç®„É≥„Ç≥„Éº„Éá„Ç£„É≥„Ç∞„ÅÆ‰ΩçÁΩÆ„ÉÜ„É≥„ÇΩ„É´ (..., n)\n",
    "        dim (int): Âüã„ÇÅËæº„ÅøÊ¨°ÂÖÉÊï∞\n",
    "        theta (int): RoPE„ÅÆ„Çπ„Ç±„Éº„É™„É≥„Ç∞„Éë„É©„É°„Éº„Çø\n",
    "    Returns:\n",
    "        Tuple: „Ç≥„Çµ„Ç§„É≥„Å®„Çµ„Ç§„É≥„ÅÆ„ÉÜ„É≥„ÇΩ„É´ (..., n, dim//2)\n",
    "    \"\"\"\n",
    "    logger.info(f\"liger_rope {pos.shape=} {dim=} {theta=}\")\n",
    "\n",
    "    assert dim % 2 == 0\n",
    "\n",
    "    # 1) ÈÄÜÂë®Ê≥¢Êï∞„ÇíË®àÁÆó\n",
    "\n",
    "    scale = torch.arange(\n",
    "        0,\n",
    "        dim,\n",
    "        2,\n",
    "        dtype=torch.float32,\n",
    "        device=pos.device\n",
    "    ) / dim\n",
    "\n",
    "    omega = 1.0 / (theta**scale)\n",
    "\n",
    "    out = torch.einsum(\"...n,d->...nd\", pos, omega)  # (b, seq, dim//2)\n",
    "\n",
    "    # 2) „Ç≥„Çµ„Ç§„É≥„Å®„Çµ„Ç§„É≥„ÇíË®àÁÆó\n",
    "\n",
    "    cos = out.cos()\n",
    "    sin = out.sin()\n",
    "\n",
    "    return (cos, sin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36150305",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LigerEmbedND(nn.Module):\n",
    "    \"\"\"\n",
    "    Liger Multi-dimensional RoPE Embedding\n",
    "    MMDiTModel„Åß‰ΩøÁî®\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, theta: int, axes_dim: list[int]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): Âüã„ÇÅËæº„ÅøÊ¨°ÂÖÉÊï∞\n",
    "            theta (int): RoPE„ÅÆ„Çπ„Ç±„Éº„É™„É≥„Ç∞„Éë„É©„É°„Éº„Çø\n",
    "            axes_dim (list[int]): ÂêÑËª∏„ÅÆÊ¨°ÂÖÉÊï∞\n",
    "        \"\"\"\n",
    "        logger.info(f\"LigerEmbedND.__init__ {dim=} {theta=} {axes_dim=}\")\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.theta = theta\n",
    "        self.axes_dim = axes_dim\n",
    "\n",
    "    def forward(self, ids: Tensor) -> Tensor:\n",
    "        logger.info(f\"LigerEmbedND.forward {ids.shape=}\")\n",
    "        n_axes = ids.shape[-1]\n",
    "        cos_list = []\n",
    "        sin_list = []\n",
    "        for i in range(n_axes):\n",
    "            cos, sin = liger_rope(ids[..., i], self.axes_dim[i], self.theta)\n",
    "            cos_list.append(cos)\n",
    "            sin_list.append(sin)\n",
    "        cos_emb = torch.cat(cos_list, dim=-1).repeat(1, 1, 2).contiguous()\n",
    "        sin_emb = torch.cat(sin_list, dim=-1).repeat(1, 1, 2).contiguous()\n",
    "\n",
    "        return (cos_emb, sin_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e418d284",
   "metadata": {},
   "source": [
    "### MLPEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bc8a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPEmbedder(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP Embedder\n",
    "    MMDiTModel„Åß‰ΩøÁî®\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int, hidden_dim: int):\n",
    "        logger.info(f\"MLPEmbedder.__init__ {in_dim=} {hidden_dim=}\")\n",
    "        super().__init__()\n",
    "        self.in_layer = nn.Linear(in_dim, hidden_dim, bias=True)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.out_layer = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        logger.info(f\"MLPEmbedder.forward {x.shape=}\")\n",
    "        return self.out_layer(self.silu(self.in_layer(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68deecd",
   "metadata": {},
   "source": [
    "### QKNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf50457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    RMS Normalization\n",
    "    FusedRMSNorm„ÅßÁ∂ôÊâø\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): Âüã„ÇÅËæº„ÅøÊ¨°ÂÖÉÊï∞\n",
    "        \"\"\"\n",
    "        logger.info(f\"RMSNorm.__init__ {dim=}\")\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        logger.info(f\"RMSNorm.forward {x.shape=}\")\n",
    "        x_dtype = x.dtype\n",
    "        x = x.float()\n",
    "        rrms = torch.rsqrt(torch.mean(x**2, dim=-1, keepdim=True) + 1e-6)\n",
    "        return (x * rrms).to(dtype=x_dtype) * self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3175d11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusedRMSNorm(RMSNorm):\n",
    "    \"\"\"\n",
    "    Fused RMS Normalization\n",
    "    QKNorm„Åß‰ΩøÁî®\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        logger.info(f\"FusedRMSNorm.forward {x.shape=}\")\n",
    "        return LigerRMSNormFunction.apply(\n",
    "            x,\n",
    "            self.scale,\n",
    "            1e-6,\n",
    "            0.0,\n",
    "            \"llama\",\n",
    "            False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8675753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QKNorm(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Query-Key Normalization\n",
    "    Self-Attention„Åß‰ΩøÁî®\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): Âüã„ÇÅËæº„ÅøÊ¨°ÂÖÉÊï∞\n",
    "        \"\"\"\n",
    "        logger.info(f\"QKNorm.__init__ {dim=}\")\n",
    "        super().__init__()\n",
    "        self.query_norm = FusedRMSNorm(dim)\n",
    "        self.key_norm = FusedRMSNorm(dim)\n",
    "\n",
    "    def forward(self, q: Tensor, k: Tensor, v: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            q (Tensor): „ÇØ„Ç®„É™„ÉÜ„É≥„ÇΩ„É´ (..., dim)\n",
    "            k (Tensor): „Ç≠„Éº„ÉÜ„É≥„ÇΩ„É´ (..., dim)\n",
    "            v (Tensor): „Éê„É™„É•„Éº„ÉÜ„É≥„ÇΩ„É´ (..., dim)\n",
    "        Returns:\n",
    "            tuple[Tensor, Tensor]: Ê≠£Ë¶èÂåñ„Åï„Çå„Åü„ÇØ„Ç®„É™„Å®„Ç≠„Éº„ÅÆ„ÉÜ„É≥„ÇΩ„É´\n",
    "        \"\"\"\n",
    "        logger.info(f\"QKNorm.forward {q.shape=} {k.shape=} {v.shape=}\")\n",
    "        q = self.query_norm(q)\n",
    "        k = self.key_norm(k)\n",
    "        return q.to(v), k.to(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ad6464",
   "metadata": {},
   "source": [
    "### „Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥Ê©üÊßã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6511eb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flash_attn_func(q: Tensor, k: Tensor, v: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Flash Attention„Çí‰ΩøÁî®„Åó„Åü„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥Ë®àÁÆó\n",
    "    attentionÈñ¢Êï∞„Åß‰ΩøÁî®\n",
    "\n",
    "    Args:\n",
    "        q (Tensor): „ÇØ„Ç®„É™„ÉÜ„É≥„ÇΩ„É´ (B, H, L, D)\n",
    "        k (Tensor): „Ç≠„Éº„ÉÜ„É≥„ÇΩ„É´ (B, H, L, D)\n",
    "        v (Tensor): „Éê„É™„É•„Éº„ÉÜ„É≥„ÇΩ„É´ (B, H, L, D)\n",
    "    Returns:\n",
    "        Tensor: „Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥Âá∫Âäõ„ÉÜ„É≥„ÇΩ„É´ (B, L, H, D)\n",
    "    \"\"\"\n",
    "    logger.info(f\"flash_attn_func {q.shape=} {k.shape=} {v.shape=}\")\n",
    "\n",
    "    if SUPPORT_FA3:\n",
    "        return flash_attn_func_v3(q, k, v)[0]\n",
    "    return flash_attn_func_v2(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8654fb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(q: Tensor, k: Tensor, v: Tensor, pe) -> Tensor:\n",
    "    \"\"\"\n",
    "    „Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥Ë®àÁÆó\n",
    "    SingleStreamBlockProcessor, DoubleSreamBlockProcessor,\n",
    "    SelfAttention„Åß‰ΩøÁî®\n",
    "\n",
    "    Args:\n",
    "        q (Tensor): „ÇØ„Ç®„É™„ÉÜ„É≥„ÇΩ„É´ (B, L, H, D)\n",
    "        k (Tensor): „Ç≠„Éº„ÉÜ„É≥„ÇΩ„É´ (B, L, H, D)\n",
    "        v (Tensor): „Éê„É™„É•„Éº„ÉÜ„É≥„ÇΩ„É´ (B, L, H, D)\n",
    "        pe: ‰ΩçÁΩÆ„Ç®„É≥„Ç≥„Éº„Éá„Ç£„É≥„Ç∞„ÉÜ„É≥„ÇΩ„É´„Åæ„Åü„ÅØ„Ç≥„Çµ„Ç§„É≥„Éª„Çµ„Ç§„É≥„ÉÜ„É≥„ÇΩ„É´„ÅÆ„Çø„Éó„É´\n",
    "    Returns:\n",
    "        Tensor: „Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥Âá∫Âäõ„ÉÜ„É≥„ÇΩ„É´ (B, L, H*D)\n",
    "    \"\"\"\n",
    "    logger.info(f\"attention {q.shape=} {k.shape=} {v.shape=} {type(pe)=}\")\n",
    "\n",
    "    # 1) RoPE„ÇíÈÅ©Áî®\n",
    "\n",
    "    if isinstance(pe, torch.Tensor):\n",
    "        q, k = apply_rope(q, k, pe)\n",
    "    else:\n",
    "        cos, sin = pe\n",
    "        logger.debug(f\"LigerRope„ÇíÈÅ©Áî® {cos.shape=} {sin.shape=}\")\n",
    "        q, k = LigerRopeFunction.apply(q, k, cos, sin)\n",
    "\n",
    "    # 2) Flash Attention„ÇíË®àÁÆó\n",
    "\n",
    "    q = rearrange(q, \"B H L D -> B L H D\")\n",
    "    k = rearrange(k, \"B H L D -> B L H D\")\n",
    "    v = rearrange(v, \"B H L D -> B L H D\")\n",
    "\n",
    "    x = flash_attn_func(q, k, v)\n",
    "\n",
    "    # 3) „Éò„ÉÉ„Éâ„ÇíÁµêÂêà\n",
    "\n",
    "    x = rearrange(x, \"B L H D -> B L (H D)\")\n",
    "\n",
    "    logger.debug(f\"attention output {x.shape=}\")\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c07f7d",
   "metadata": {},
   "source": [
    "### „Çª„É´„Éï„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3045aa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    „Çª„É´„Éï„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥\n",
    "    DoubleStreamBlock„ÅßÂêàË®à2Âõû‰ΩøÁî®\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, num_heads: int = 8, qkv_bias: bool = False, fused_qkv: bool = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): Âüã„ÇÅËæº„ÅøÊ¨°ÂÖÉÊï∞\n",
    "            num_heads (int, optional): „Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥„Éò„ÉÉ„ÉâÊï∞. Defaults to 8.\n",
    "            qkv_bias (bool, optional): QKV„ÅÆ„Éê„Ç§„Ç¢„Çπ„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã. Defaults to False.\n",
    "            fused_qkv (bool, optional): QKV„ÇíËûçÂêà„Åó„ÅüÁ∑öÂΩ¢Â±§„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã. Defaults to True.\n",
    "        \"\"\"\n",
    "        logger.info(f\"SelfAttention.__init__ {dim=} {num_heads=} {qkv_bias=} {fused_qkv=}\")\n",
    "\n",
    "        # 1) ÂàùÊúüÂåñ\n",
    "\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads # 3\n",
    "        self.fused_qkv = fused_qkv # False\n",
    "        head_dim = dim // num_heads # 384 // 3 = 128\n",
    "\n",
    "        # 2) QKV„ÅÆÁ∑öÂΩ¢Â±§„ÇíÂÆöÁæ©\n",
    "\n",
    "        # False\n",
    "        if fused_qkv:\n",
    "            self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "\n",
    "        # True\n",
    "        else:\n",
    "            self.q_proj = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "            self.k_proj = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "            self.v_proj = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "\n",
    "        # 3) QKNorm„ÇíÂÆöÁæ©\n",
    "\n",
    "        self.norm = QKNorm(head_dim)\n",
    "\n",
    "        # 4) Âá∫Âäõ„ÅÆÁ∑öÂΩ¢Â±§„ÇíÂÆöÁæ©\n",
    "\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x: Tensor, pe: Tensor) -> Tensor:\n",
    "        logger.info(f\"SelfAttention.forward {x.shape=} {pe.shape=}\")\n",
    "\n",
    "        # 1) QKV„ÇíË®àÁÆó\n",
    "\n",
    "        # False\n",
    "        if self.fused_qkv:\n",
    "            qkv = self.qkv(x)\n",
    "            q, k, v = rearrange(qkv, \"B L (K H D) -> K B H L D\", K=3, H=self.num_heads)\n",
    "\n",
    "        # True\n",
    "        else:\n",
    "            q = rearrange(self.q_proj(x), \"B L (H D) -> B L H D\", H=self.num_heads)\n",
    "            k = rearrange(self.k_proj(x), \"B L (H D) -> B L H D\", H=self.num_heads)\n",
    "            v = rearrange(self.v_proj(x), \"B L (H D) -> B L H D\", H=self.num_heads)\n",
    "            logger.debug(f\"Before QKNorm {q.shape=} {k.shape=} {v.shape=}\")\n",
    "\n",
    "        q, k = self.norm(q, k, v)\n",
    "\n",
    "        # False\n",
    "        if not self.fused_qkv:\n",
    "            q = rearrange(q, \"B L H D -> B H L D\")\n",
    "            k = rearrange(k, \"B L H D -> B H L D\")\n",
    "            v = rearrange(v, \"B L H D -> B H L D\")\n",
    "            logger.debug(f\"After QKNorm {q.shape=} {k.shape=} {v.shape=}\")\n",
    "\n",
    "        # 2) „Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥„ÇíË®àÁÆó\n",
    "        x = attention(q, k, v, pe=pe)\n",
    "\n",
    "        # 3) Âá∫Âäõ„ÇíË®àÁÆó\n",
    "        x = self.proj(x)\n",
    "        logger.debug(f\"SelfAttention output {x.shape=}\")\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e9da89",
   "metadata": {},
   "source": [
    "### Modulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee5fbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModulationOut:\n",
    "    shift: Tensor\n",
    "    scale: Tensor\n",
    "    gate: Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c045c4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modulation(nn.Module):\n",
    "    \"\"\"\n",
    "    „É¢„Ç∏„É•„É¨„Éº„Ç∑„Éß„É≥Â±§\n",
    "    AdaLNÂ±§„ÅÆÂ§âË™øÔºà„É¢„Ç∏„É•„É¨„Éº„Ç∑„Éß„É≥Ôºâ„Åß‰ΩøÁî®„Åô„Çã„Ç∑„Éï„Éà„Éª„Çπ„Ç±„Éº„É´„Éª„Ç≤„Éº„Éà„ÇíÁîüÊàê„Åô„Çã\n",
    "    DoubleStreamBlock„Åß‰ΩøÁî®\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, double: bool):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): Âüã„ÇÅËæº„ÅøÊ¨°ÂÖÉÊï∞\n",
    "            double (bool):\n",
    "                „ÉÄ„Éñ„É´„É¢„Ç∏„É•„É¨„Éº„Ç∑„Éß„É≥„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "                „ÉÄ„Éñ„É´„É¢„Ç∏„É•„É¨„Éº„Ç∑„Éß„É≥„ÅÆÂ†¥Âêà„ÅØ„ÄÅ2„Çª„ÉÉ„Éà„ÅÆ„Ç∑„Éï„Éà„Éª„Çπ„Ç±„Éº„É´„Éª„Ç≤„Éº„Éà„ÇíÁîüÊàê„Åô„Çã\n",
    "        \"\"\"\n",
    "        logger.info(f\"Modulation.__init__ {dim=} {double=}\")\n",
    "        super().__init__()\n",
    "\n",
    "        # True or False\n",
    "        self.is_double = double\n",
    "\n",
    "        # 6 or 3\n",
    "        self.multiplier = 6 if double else 3\n",
    "\n",
    "        # 384 -> 384*6 or 384 -> 384*3\n",
    "        self.lin = nn.Linear(dim, self.multiplier * dim, bias=True)\n",
    "\n",
    "    def forward(self, vec: Tensor) -> tuple[ModulationOut, ModulationOut | None]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vec (Tensor): ÂÖ•Âäõ„ÉÜ„É≥„ÇΩ„É´ (B, L, D)\n",
    "        Returns:\n",
    "            tuple[ModulationOut, ModulationOut | None]:\n",
    "                „É¢„Ç∏„É•„É¨„Éº„Ç∑„Éß„É≥Âá∫ÂäõÔºàshift, scale, gateÔºâ„ÅÆ„Çø„Éó„É´\n",
    "                „ÉÄ„Éñ„É´„É¢„Ç∏„É•„É¨„Éº„Ç∑„Éß„É≥„ÅÆÂ†¥Âêà„ÅØ2„Å§ÁõÆ„ÅÆModulationOut„ÇÇËøî„Åô\n",
    "        \"\"\"\n",
    "        logger.info(f\"Modulation.forward {vec.shape=}\")\n",
    "\n",
    "        # (1, 384) -> (1, 384*6) or (1, 384) -> (1, 384*3)\n",
    "        out = self.lin(nn.functional.silu(vec))[:, None, :].chunk(self.multiplier, dim=-1)\n",
    "        logger.debug(f\"Modulation.forward out shapes: {[o.shape for o in out]}\")\n",
    "\n",
    "        return (\n",
    "            ModulationOut(*out[:3]),\n",
    "            ModulationOut(*out[3:]) if self.is_double else None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd63fafd",
   "metadata": {},
   "source": [
    "### DoubleStreamBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3eafe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleStreamBlockProcessor:\n",
    "    \"\"\"\n",
    "    „ÉÄ„Éñ„É´„Çπ„Éà„É™„Éº„É†„Éñ„É≠„ÉÉ„ÇØ„Éó„É≠„Çª„ÉÉ„Çµ\n",
    "    DoubleStreamBlock„Åß‰ΩøÁî®\n",
    "    ÁîªÂÉè„Å®„ÉÜ„Ç≠„Çπ„Éà„ÅÆ‰∏°Êñπ„ÅÆ„Çπ„Éà„É™„Éº„É†„ÇíÂá¶ÁêÜ„Åô„Çã\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, attn: nn.Module, img: Tensor, txt: Tensor, vec: Tensor, pe: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        ÂãïÁîª„Å®„ÉÜ„Ç≠„Çπ„ÉàÂüã„ÇÅËæº„Åø„ÇíÂà•„ÄÖ„Å´Âá¶ÁêÜ„Åô„Çã\n",
    "        Âá¶ÁêÜ„ÅÆÈÅéÁ®ã„Åß„Éô„ÇØ„Éà„É´„ÉÜ„É≥„ÇΩ„É´„Åå‰∏°Êñπ„Å´ÂΩ±Èüø„Çí‰∏é„Åà„Çã\n",
    "\n",
    "        Args:\n",
    "            attn (nn.Module): „ÉÄ„Éñ„É´„Çπ„Éà„É™„Éº„É†„Éñ„É≠„ÉÉ„ÇØ„É¢„Ç∏„É•„Éº„É´\n",
    "            img (Tensor): ÂãïÁîª„ÉÜ„É≥„ÇΩ„É´ (B, L_img, C)\n",
    "            txt (Tensor): „ÉÜ„Ç≠„Çπ„Éà„ÉÜ„É≥„ÇΩ„É´ (B, L_txt, C)\n",
    "            vec (Tensor): „Éô„ÇØ„Éà„É´„ÉÜ„É≥„ÇΩ„É´ (B, C)\n",
    "            pe (Tensor): ‰ΩçÁΩÆ„Ç®„É≥„Ç≥„Éº„Éá„Ç£„É≥„Ç∞„ÉÜ„É≥„ÇΩ„É´„Åæ„Åü„ÅØ„Ç≥„Çµ„Ç§„É≥„Éª„Çµ„Ç§„É≥„ÉÜ„É≥„ÇΩ„É´„ÅÆ„Çø„Éó„É´\n",
    "        Returns:\n",
    "            tuple[Tensor, Tensor]: Âá¶ÁêÜ„Åï„Çå„ÅüÂãïÁîª„ÉÜ„É≥„ÇΩ„É´„Å®„ÉÜ„Ç≠„Çπ„Éà„ÉÜ„É≥„ÇΩ„É´\n",
    "        \"\"\"\n",
    "        logger.info(f\"DoubleStreamBlockProcessor.__call__ {img.shape=} {txt.shape=} {vec.shape=} {pe[0].shape=} {pe[1].shape=}\")\n",
    "\n",
    "        # 1) „Éô„ÇØ„Éà„É´„ÉÜ„É≥„ÇΩ„É´„Åã„Çâ„Ç∑„Éï„Éà„Éª„Çπ„Ç±„Éº„É´„Éª„Ç≤„Éº„Éà„ÇíÂèñÂæó\n",
    "\n",
    "        img_mod1, img_mod2 = attn.img_mod(vec)\n",
    "        txt_mod1, txt_mod2 = attn.txt_mod(vec)\n",
    "\n",
    "        # 2) ÂãïÁîª„ÉÜ„É≥„ÇΩ„É´„Å´AdaLN„ÇíÈÅ©Áî®\n",
    "\n",
    "        # „É¨„Ç§„É§„ÉºÊ≠£Ë¶èÂåñ„ÇíÈÅ©Áî®\n",
    "        img_modulated = attn.img_norm1(img)\n",
    "\n",
    "        # „Çπ„Ç±„Éº„É´„Å®„Ç∑„Éï„Éà„ÅßÂ§âË™ø\n",
    "        img_modulated = (1 + img_mod1.scale) * img_modulated + img_mod1.shift\n",
    "\n",
    "        # False\n",
    "        if attn.img_attn.fused_qkv:\n",
    "            img_qkv = attn.img_attn.qkv(img_modulated)\n",
    "            img_q, img_k, img_v = rearrange(img_qkv, \"B L (K H D) -> K B H L D\", K=3, H=attn.num_heads, D=attn.head_dim)\n",
    "        else:\n",
    "\n",
    "            # „ÇØ„Ç®„É™„ÇíË®àÁÆó\n",
    "            img_q = rearrange(\n",
    "                attn.img_attn.q_proj(img_modulated),\n",
    "                \"B L (H D) -> B L H D\",\n",
    "                H=attn.num_heads\n",
    "            )\n",
    "\n",
    "            # „Ç≠„Éº„ÇíË®àÁÆó\n",
    "            img_k = rearrange(\n",
    "                attn.img_attn.k_proj(img_modulated),\n",
    "                \"B L (H D) -> B L H D\",\n",
    "                H=attn.num_heads\n",
    "            )\n",
    "\n",
    "            # „Éê„É™„É•„Éº„ÇíË®àÁÆó\n",
    "            img_v = rearrange(\n",
    "                attn.img_attn.v_proj(img_modulated),\n",
    "                \"B L (H D) -> B L H D\",\n",
    "                H=attn.num_heads\n",
    "            )\n",
    "\n",
    "            logger.debug(f\"{img_q.shape=} {img_k.shape=} {img_v.shape=}\")\n",
    "\n",
    "\n",
    "        # Stable Diffusion 3Ë´ñÊñá„ÅÆQKÊ≠£Ë¶èÂåñ„ÇíÈÅ©Áî®\n",
    "        img_q, img_k = attn.img_attn.norm(img_q, img_k, img_v)\n",
    "\n",
    "        # True\n",
    "        if not attn.img_attn.fused_qkv:\n",
    "            img_q = rearrange(img_q, \"B L H D -> B H L D\")\n",
    "            img_k = rearrange(img_k, \"B L H D -> B H L D\")\n",
    "            img_v = rearrange(img_v, \"B L H D -> B H L D\")\n",
    "            logger.debug(f\"{img_q.shape=} {img_k.shape=} {img_v.shape=}\")\n",
    "\n",
    "        # 3) „ÉÜ„Ç≠„Çπ„Éà„ÉÜ„É≥„ÇΩ„É´„Å´AdaLN„ÇíÈÅ©Áî®\n",
    "\n",
    "        # „É¨„Ç§„É§„ÉºÊ≠£Ë¶èÂåñ„ÇíÈÅ©Áî®\n",
    "        txt_modulated = attn.txt_norm1(txt)\n",
    "\n",
    "        # „Çπ„Ç±„Éº„É´„Å®„Ç∑„Éï„Éà„ÅßÂ§âË™ø\n",
    "        txt_modulated = (1 + txt_mod1.scale) * txt_modulated + txt_mod1.shift\n",
    "\n",
    "        # False\n",
    "        if attn.txt_attn.fused_qkv:\n",
    "            txt_qkv = attn.txt_attn.qkv(txt_modulated)\n",
    "            txt_q, txt_k, txt_v = rearrange(txt_qkv, \"B L (K H D) -> K B H L D\", K=3, H=attn.num_heads, D=attn.head_dim)\n",
    "        else:\n",
    "            txt_q = rearrange(\n",
    "                attn.txt_attn.q_proj(txt_modulated),\n",
    "                \"B L (H D) -> B L H D\",\n",
    "                H=attn.num_heads\n",
    "            )\n",
    "\n",
    "            txt_k = rearrange(\n",
    "                attn.txt_attn.k_proj(txt_modulated),\n",
    "                \"B L (H D) -> B L H D\",\n",
    "                H=attn.num_heads\n",
    "            )\n",
    "\n",
    "            txt_v = rearrange(\n",
    "                attn.txt_attn.v_proj(txt_modulated),\n",
    "                \"B L (H D) -> B L H D\",\n",
    "                H=attn.num_heads\n",
    "            )\n",
    "            logger.debug(f\"Before QKNorm {txt_q.shape=} {txt_k.shape=} {txt_v.shape=}\")\n",
    "\n",
    "        # Stable Diffusion 3Ë´ñÊñá„ÅÆQKÊ≠£Ë¶èÂåñ„ÇíÈÅ©Áî®\n",
    "        txt_q, txt_k = attn.txt_attn.norm(txt_q, txt_k, txt_v)\n",
    "\n",
    "        # True\n",
    "        if not attn.txt_attn.fused_qkv:\n",
    "            txt_q = rearrange(txt_q, \"B L H D -> B H L D\")\n",
    "            txt_k = rearrange(txt_k, \"B L H D -> B H L D\")\n",
    "            txt_v = rearrange(txt_v, \"B L H D -> B H L D\")\n",
    "            logger.debug(f\"After QKNorm {txt_q.shape=} {txt_k.shape=} {txt_v.shape=}\")\n",
    "\n",
    "        # 4) „Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥„ÇíË®àÁÆó\n",
    "\n",
    "        # ÁîªÂÉè„Å®„ÉÜ„Ç≠„Çπ„Éà„ÅÆ„ÇØ„Ç®„É™„ÄÅ„Ç≠„Éº„ÄÅ„Éê„É™„É•„Éº„ÇíÈÄ£Áµê\n",
    "\n",
    "        q = torch.cat((txt_q, img_q), dim=2)\n",
    "        k = torch.cat((txt_k, img_k), dim=2)\n",
    "        v = torch.cat((txt_v, img_v), dim=2)\n",
    "\n",
    "        logger.debug(f\"Before Attention {q.shape=} {k.shape=} {v.shape=}\")\n",
    "\n",
    "        # „Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥„ÇíË®àÁÆó\n",
    "        attn1 = attention(q, k, v, pe=pe)\n",
    "\n",
    "        # 5) „Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥Âá∫Âäõ„ÇíÁîªÂÉè„Å®„ÉÜ„Ç≠„Çπ„Éà„Å´ÂàÜÂâ≤\n",
    "\n",
    "        txt_attn, img_attn = attn1[:, : txt_q.shape[2]], attn1[:, txt_q.shape[2] :]\n",
    "\n",
    "        # 6) Âá∫Âäõ„ÇíË®àÁÆó \n",
    "\n",
    "        img = img + img_mod1.gate * attn.img_attn.proj(img_attn)\n",
    "        img = img + img_mod2.gate * attn.img_mlp(\n",
    "            (1 + img_mod2.scale) * attn.img_norm2(img) + img_mod2.shift\n",
    "        )\n",
    "\n",
    "        txt = txt + txt_mod1.gate * attn.txt_attn.proj(txt_attn)\n",
    "        txt = txt + txt_mod2.gate * attn.txt_mlp(\n",
    "            (1 + txt_mod2.scale) * attn.txt_norm2(txt) + txt_mod2.shift\n",
    "        )\n",
    "\n",
    "        return img, txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82098a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleStreamBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    „ÉÄ„Éñ„É´„Çπ„Éà„É™„Éº„É†„Éñ„É≠„ÉÉ„ÇØ\n",
    "    MMDiTModel„Åß1Âõû‰ΩøÁî®\n",
    "    ÁîªÂÉè„Å®„ÉÜ„Ç≠„Çπ„Éà„ÅÆ‰∏°Êñπ„ÅÆ„Çπ„Éà„É™„Éº„É†„ÇíÂá¶ÁêÜ„Åô„Çã\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size: int, num_heads: int, mlp_ratio: float, qkv_bias: bool = False, fused_qkv: bool = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_size (int): Âüã„ÇÅËæº„ÅøÊ¨°ÂÖÉÊï∞\n",
    "            num_heads (int): „Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥„Éò„ÉÉ„ÉâÊï∞\n",
    "            mlp_ratio (float): MLP„ÅÆÈö†„ÇåÂ±§„ÅÆÊ¨°ÂÖÉÊï∞„ÅÆÊØîÁéá\n",
    "            qkv_bias (bool, optional): QKV„ÅÆ„Éê„Ç§„Ç¢„Çπ„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "            fused_qkv (bool, optional): QKV„ÇíËûçÂêà„Åó„ÅüÁ∑öÂΩ¢Â±§„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "        \"\"\"\n",
    "        logger.info(f\"DoubleStreamBlock.__init__ {hidden_size=} {num_heads=} {mlp_ratio=} {qkv_bias=} {fused_qkv=}\")\n",
    "\n",
    "        # 1) ÂàùÊúüÂåñ\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # 384 * 4 = 1536\n",
    "        mlp_hidden_dim = int(hidden_size * mlp_ratio)\n",
    "\n",
    "        # 3\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # 384\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # 384 // 3 = 128\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "\n",
    "        # 2) ÂãïÁîª„Çπ„Éà„É™„Éº„É†„ÇíÂÆöÁæ©\n",
    "\n",
    "        # image stream\n",
    "\n",
    "        # AdaLNÁî®„ÅÆ„É¢„Ç∏„É•„É¨„Éº„Ç∑„Éß„É≥Â±§\n",
    "        self.img_mod = Modulation(hidden_size, double=True)\n",
    "\n",
    "        # AdaLNÁî®„ÅÆÂâçÂçä„ÅÆ„É¨„Ç§„É§„ÉºÊ≠£Ë¶èÂåñÂ±§    \n",
    "        self.img_norm1 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
    "\n",
    "        # „Çª„É´„Éï„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥Â±§\n",
    "        self.img_attn = SelfAttention(\n",
    "            dim=hidden_size, # 384\n",
    "            num_heads=num_heads, # 3\n",
    "            qkv_bias=qkv_bias, # True\n",
    "            fused_qkv=fused_qkv # False\n",
    "        )\n",
    "\n",
    "        # AdaLNÁî®„ÅÆÂæåÂçä„ÅÆ„É¨„Ç§„É§„ÉºÊ≠£Ë¶èÂåñÂ±§\n",
    "        self.img_norm2 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
    "\n",
    "        # MLPÂ±§\n",
    "        self.img_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, mlp_hidden_dim, bias=True),\n",
    "            nn.GELU(approximate=\"tanh\"),\n",
    "            nn.Linear(mlp_hidden_dim, hidden_size, bias=True),\n",
    "        )\n",
    "\n",
    "        # 3) „ÉÜ„Ç≠„Çπ„Éà„Çπ„Éà„É™„Éº„É†„ÇíÂÆöÁæ©\n",
    "\n",
    "        # AdaLNÁî®„ÅÆ„É¢„Ç∏„É•„É¨„Éº„Ç∑„Éß„É≥Â±§\n",
    "        self.txt_mod = Modulation(hidden_size, double=True)\n",
    "\n",
    "        # AdaLNÁî®„ÅÆÂâçÂçä„ÅÆ„É¨„Ç§„É§„ÉºÊ≠£Ë¶èÂåñÂ±§\n",
    "        self.txt_norm1 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
    "\n",
    "        # „Çª„É´„Éï„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥Â±§\n",
    "        self.txt_attn = SelfAttention(\n",
    "            dim=hidden_size,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            fused_qkv=fused_qkv\n",
    "        )\n",
    "\n",
    "        # AdaLNÁî®„ÅÆÂæåÂçä„ÅÆ„É¨„Ç§„É§„ÉºÊ≠£Ë¶èÂåñÂ±§\n",
    "        self.txt_norm2 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
    "\n",
    "        # MLPÂ±§\n",
    "        self.txt_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, mlp_hidden_dim, bias=True),\n",
    "            nn.GELU(approximate=\"tanh\"),\n",
    "            nn.Linear(mlp_hidden_dim, hidden_size, bias=True),\n",
    "        )\n",
    "\n",
    "        # 4) ÂãïÁîª„Å®„ÉÜ„Ç≠„Çπ„Éà„ÅÆ‰∏°Êñπ„ÅÆ„Çπ„Éà„É™„Éº„É†„ÇíÂá¶ÁêÜ„Åô„Çã„Éó„É≠„Çª„ÉÉ„Çµ„Çí‰ΩúÊàê\n",
    "\n",
    "        processor = DoubleStreamBlockProcessor()\n",
    "        self.set_processor(processor)\n",
    "\n",
    "    def set_processor(self, processor) -> None:\n",
    "        self.processor = processor\n",
    "\n",
    "    def get_processor(self):\n",
    "        return self.processor\n",
    "\n",
    "    def forward(self, img: Tensor, txt: Tensor, vec: Tensor, pe: Tensor, **kwargs) -> tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        ÂãïÁîª„Å®„ÉÜ„Ç≠„Çπ„ÉàÂüã„ÇÅËæº„Åø„ÇíÂà•„ÄÖ„Å´Âá¶ÁêÜ„Åô„Çã\n",
    "        Âá¶ÁêÜ„ÅÆÈÅéÁ®ã„Åß„Éô„ÇØ„Éà„É´„ÉÜ„É≥„ÇΩ„É´„Åå‰∏°Êñπ„Å´ÂΩ±Èüø„Çí‰∏é„Åà„Çã\n",
    "\n",
    "        Args:\n",
    "            img (Tensor): ÂãïÁîª„ÉÜ„É≥„ÇΩ„É´ (B, L_img, C)\n",
    "            txt (Tensor): „ÉÜ„Ç≠„Çπ„Éà„ÉÜ„É≥„ÇΩ„É´ (B, L_txt, C)\n",
    "            vec (Tensor): „Éô„ÇØ„Éà„É´„ÉÜ„É≥„ÇΩ„É´ (B, C)\n",
    "            pe (Tensor): ‰ΩçÁΩÆ„Ç®„É≥„Ç≥„Éº„Éá„Ç£„É≥„Ç∞„ÉÜ„É≥„ÇΩ„É´„Åæ„Åü„ÅØ„Ç≥„Çµ„Ç§„É≥„Éª„Çµ„Ç§„É≥„ÉÜ„É≥„ÇΩ„É´„ÅÆ„Çø„Éó„É´\n",
    "        Returns:\n",
    "            tuple[Tensor, Tensor]: Âá¶ÁêÜ„Åï„Çå„ÅüÂãïÁîª„ÉÜ„É≥„ÇΩ„É´„Å®„ÉÜ„Ç≠„Çπ„Éà„ÉÜ„É≥„ÇΩ„É´\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(f\"DoubleStreamBlock.forward {img.shape=} {txt.shape=} {vec.shape=} {pe[0].shape=} {pe[1].shape=}\")\n",
    "        return self.processor(self, img, txt, vec, pe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43837fc",
   "metadata": {},
   "source": [
    "### SingleStreamBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc340020",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleStreamBlockProcessor:\n",
    "    \"\"\"\n",
    "    „Ç∑„É≥„Ç∞„É´„Çπ„Éà„É™„Éº„É†„Éñ„É≠„ÉÉ„ÇØ„Éó„É≠„Çª„ÉÉ„Çµ\n",
    "    SingleStreamBlock„Åß‰ΩøÁî®\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, attn: nn.Module, x: Tensor, vec: Tensor, pe: Tensor) -> Tensor:\n",
    "        logger.info(f\"SingleStreamBlockProcessor.__call__ {x.shape=} {vec.shape=} {pe[0].shape=} {pe[1].shape=}\")\n",
    "\n",
    "        # 1) AdaLN„ÇíÈÅ©Áî®\n",
    "\n",
    "        # AdaLN„ÅÆ„Ç∑„Éï„Éà„Éª„Çπ„Ç±„Éº„É´„Éª„Ç≤„Éº„Éà„Çí‰ΩúÊàê„Åô„Çã\n",
    "        mod, _ = attn.modulation(vec)\n",
    "\n",
    "        # „É¨„Ç§„É§„ÉºÊ≠£Ë¶èÂåñ„ÇíÈÅ©Áî®„Åó„ÄÅÂ§âË™ø„Åï„Åõ„Çã\n",
    "        x_mod = (1 + mod.scale) * attn.pre_norm(x) + mod.shift\n",
    "\n",
    "        # 2) „ÇØ„Ç®„É™„ÄÅ„Ç≠„Éº„ÄÅ„Éê„É™„É•„Éº„ÄÅMLP„ÇíË®àÁÆó\n",
    "\n",
    "        # False\n",
    "        if attn.fused_qkv:\n",
    "            qkv, mlp = torch.split(attn.linear1(x_mod), [3 * attn.hidden_size, attn.mlp_hidden_dim], dim=-1)\n",
    "            q, k, v = rearrange(qkv, \"B L (K H D) -> K B H L D\", K=3, H=attn.num_heads)\n",
    "        # True\n",
    "        else:\n",
    "            q = rearrange(attn.q_proj(x_mod), \"B L (H D) -> B L H D\", H=attn.num_heads)\n",
    "            k = rearrange(attn.k_proj(x_mod), \"B L (H D) -> B L H D\", H=attn.num_heads)\n",
    "            v, mlp = torch.split(attn.v_mlp(x_mod), [attn.hidden_size, attn.mlp_hidden_dim], dim=-1)\n",
    "            logger.debug(f\"Before QKNorm {q.shape=} {k.shape=} {v.shape=} {mlp.shape=}\")\n",
    "\n",
    "            v = rearrange(v, \"B L (H D) -> B L H D\", H=attn.num_heads)\n",
    "\n",
    "        # 3) Stable Diffusion 3Ë´ñÊñá„ÅÆQKÊ≠£Ë¶èÂåñ„ÇíÈÅ©Áî®\n",
    "        q, k = attn.norm(q, k, v)\n",
    "\n",
    "        # True\n",
    "        if not attn.fused_qkv:\n",
    "            q = rearrange(q, \"B L H D -> B H L D\")\n",
    "            k = rearrange(k, \"B L H D -> B H L D\")\n",
    "            v = rearrange(v, \"B L H D -> B H L D\")\n",
    "\n",
    "            logger.debug(f\"After QKNorm {q.shape=} {k.shape=} {v.shape=}\")\n",
    "\n",
    "        # 4) „Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥„ÇíË®àÁÆó\n",
    "\n",
    "        attn_1 = attention(q, k, v, pe=pe)\n",
    "\n",
    "        # 5) Âá∫Âäõ„ÇíË®àÁÆó\n",
    "\n",
    "        # MLP„Çπ„Éà„É™„Éº„É†„ÅßÊ¥ªÊÄßÂåñ„ÇíË®àÁÆó„Åó„ÄÅÂÜçÂ∫¶ÈÄ£Áµê„Åó„Å¶2Áï™ÁõÆ„ÅÆÁ∑öÂΩ¢Â±§„ÇíÂÆüË°å\n",
    "        output = attn.linear2(torch.cat((attn_1, attn.mlp_act(mlp)), 2))\n",
    "\n",
    "        # „Ç∑„Éï„Éà„Éª„Çπ„Ç±„Éº„É´„Éª„Ç≤„Éº„Éà„ÅßÂ§âË™ø\n",
    "        output = x + mod.gate * output\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d3021b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleStreamBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    „Ç∑„É≥„Ç∞„É´„Çπ„Éà„É™„Éº„É†„Éñ„É≠„ÉÉ„ÇØ\n",
    "    MMDiTModel„Åß38ÂÄãÊßãÁØâ„Åó„Å¶‰ΩøÁî®\n",
    "\n",
    "    ‰∏¶ÂàóÁ∑öÂΩ¢Â±§„ÇíÊåÅ„Å§DiT„Éñ„É≠„ÉÉ„ÇØ„Åß„ÄÅÂ§âË™ø„Ç§„É≥„Çø„Éº„Éï„Çß„Éº„Çπ„ÅåÈÅ©Âøú\n",
    "    https://arxiv.org/abs/2302.05442\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        num_heads: int,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        qk_scale: float | None = None,\n",
    "        fused_qkv: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_size (int): Âüã„ÇÅËæº„ÅøÊ¨°ÂÖÉÊï∞\n",
    "            num_heads (int): „Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥„Éò„ÉÉ„ÉâÊï∞\n",
    "            mlp_ratio (float, optional): MLP„ÅÆÈö†„ÇåÂ±§„ÅÆÊ¨°ÂÖÉÊï∞„ÅÆÊØîÁéá\n",
    "            qk_scale (float | None, optional): QK„Çπ„Ç±„Éº„É™„É≥„Ç∞„Éï„Ç°„ÇØ„Çø„Éº\n",
    "            fused_qkv (bool, optional): QKV„ÇíËûçÂêà„Åó„ÅüÁ∑öÂΩ¢Â±§„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "        \"\"\"\n",
    "        logger.info(f\"SingleStreamBlock.__init__ {hidden_size=} {num_heads=} {mlp_ratio=} {qk_scale=} {fused_qkv=}\")\n",
    "\n",
    "        # 1) ÂàùÊúüÂåñ\n",
    "\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_size # 384\n",
    "        self.num_heads = num_heads # 3\n",
    "        self.head_dim = hidden_size // num_heads # 384 // 3 = 128\n",
    "        self.scale = qk_scale or self.head_dim**-0.5 # 128**-0.5\n",
    "        self.fused_qkv = fused_qkv # False\n",
    "\n",
    "        self.mlp_hidden_dim = int(hidden_size * mlp_ratio) # 384 * 4.0 = 1536\n",
    "\n",
    "        # 2) „ÇØ„Ç®„É™„Éª„Ç≠„Éº„Éª„Éê„É™„É•„Éº„Å®MLP„ÅÆÁ∑öÂΩ¢Â±§„ÇíÊßãÁØâ\n",
    "\n",
    "        # False\n",
    "        if fused_qkv:\n",
    "            # qkv and mlp_in\n",
    "            self.linear1 = nn.Linear(hidden_size, hidden_size * 3 + self.mlp_hidden_dim)\n",
    "\n",
    "        # True\n",
    "        else:\n",
    "\n",
    "            # 384 -> 384\n",
    "            self.q_proj = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "            # 384 -> 384\n",
    "            self.k_proj = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "            # 384 -> 384 + 1536\n",
    "            self.v_mlp = nn.Linear(hidden_size, hidden_size + self.mlp_hidden_dim)\n",
    "\n",
    "        # 3) „Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥Âá∫Âäõ„Å®MLPÂá∫Âäõ„ÇíÁµêÂêà„Åó„Å¶ÊúÄÁµÇÁ∑öÂΩ¢Â±§„ÇíÊßãÁØâ\n",
    "\n",
    "        # 384 + 1536 -> 384\n",
    "        self.linear2 = nn.Linear(hidden_size + self.mlp_hidden_dim, hidden_size)\n",
    "\n",
    "        # 4) QKNorm„ÇíÊßãÁØâ\n",
    "\n",
    "        self.norm = QKNorm(self.head_dim)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # 5) „É¨„Ç§„É§„ÉºÊ≠£Ë¶èÂåñÂ±§„ÇíÊßãÁØâ\n",
    "\n",
    "        self.pre_norm = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
    "\n",
    "        # 6) GELUÊ¥ªÊÄßÂåñÈñ¢Êï∞„ÇíÊßãÁØâ\n",
    "\n",
    "        self.mlp_act = nn.GELU(approximate=\"tanh\")\n",
    "\n",
    "        # 7) „É¢„Ç∏„É•„É¨„Éº„Ç∑„Éß„É≥Â±§„ÇíÊßãÁØâ\n",
    "\n",
    "        self.modulation = Modulation(hidden_size, double=False)\n",
    "\n",
    "        # 8) „Ç∑„É≥„Ç∞„É´„Çπ„Éà„É™„Éº„É†„Éñ„É≠„ÉÉ„ÇØ„Éó„É≠„Çª„ÉÉ„Çµ„Çí‰ΩúÊàê\n",
    "\n",
    "        processor = SingleStreamBlockProcessor()\n",
    "        self.set_processor(processor)\n",
    "\n",
    "    def set_processor(self, processor) -> None:\n",
    "        self.processor = processor\n",
    "\n",
    "    def get_processor(self):\n",
    "        return self.processor\n",
    "\n",
    "    def forward(self, x: Tensor, vec: Tensor, pe: Tensor, **kwargs) -> Tensor:\n",
    "        logger.info(f\"SingleStreamBlock.forward {x.shape=} {vec.shape=} {pe[0].shape=} {pe[1].shape=}\")\n",
    "        return self.processor(self, x, vec, pe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1b6d7a",
   "metadata": {},
   "source": [
    "### MMDiT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7a53e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile(mode=\"max-autotune-no-cudagraphs\", dynamic=True)\n",
    "def timestep_embedding(t: Tensor, dim, max_period=10000, time_factor: float = 1000.0):\n",
    "    \"\"\"\n",
    "    „Çµ„Ç§„É≥Ê≥¢„Çø„Ç§„É†„Çπ„ÉÜ„ÉÉ„ÉóÂüã„ÇÅËæº„Åø„Çí‰ΩúÊàê„Åô„Çã\n",
    "\n",
    "    Args:\n",
    "        t (Tensor): „Éê„ÉÉ„ÉÅË¶ÅÁ¥†„Åî„Å®„Å´1„Å§„ÅÆN„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„ÇíÊåÅ„Å§1-D„ÉÜ„É≥„ÇΩ„É´„ÄÇ„Åì„Çå„Çâ„ÅØÂàÜÊï∞ÂÄ§„Åß„ÅÇ„ÇãÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ\n",
    "        dim (int): Âá∫Âäõ„ÅÆÊ¨°ÂÖÉ„ÄÇ\n",
    "        max_period (int, optional): Âüã„ÇÅËæº„Åø„ÅÆÊúÄÂ∞èÂë®Ê≥¢Êï∞„ÇíÂà∂Âæ°„Åó„Åæ„Åô„ÄÇ„Éá„Éï„Ç©„É´„Éà„ÅØ10000„ÄÇ\n",
    "        time_factor (float, optional): ÊôÇÈñì„Çπ„Ç±„Éº„É™„É≥„Ç∞„Éï„Ç°„ÇØ„Çø„Éº„ÄÇ„Éá„Éï„Ç©„É´„Éà„ÅØ1000.0„ÄÇ\n",
    "    Returns:\n",
    "        Tensor: Âüã„ÇÅËæº„Åø„ÉÜ„É≥„ÇΩ„É´„ÅÆÂΩ¢Áä∂„ÅØ(t.shape[0], dim)\n",
    "    \"\"\"\n",
    "    logger.info(f\"timestep_embedding {t.shape=} {dim=} {max_period=} {time_factor=}\")\n",
    "\n",
    "    t = time_factor * t\n",
    "    half = dim // 2\n",
    "    freqs = torch.exp(-math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half).to(t.device)\n",
    "\n",
    "    args = t[:, None].float() * freqs[None]\n",
    "    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "    if dim % 2:\n",
    "        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
    "    if torch.is_floating_point(t):\n",
    "        embedding = embedding.to(t)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74746fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    DCAE„Éá„Ç≥„Éº„ÉÄ„ÅÆÊúÄÂæå„ÅÆÂ±§\n",
    "    ÁîªÂÉè„Éë„ÉÉ„ÉÅ„ÇíÁîüÊàê„Åô„Çã„Åü„ÇÅ„Å´‰ΩøÁî®„Åï„Çå„Çã\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size: int, patch_size: int, out_channels: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_size (int): Èö†„ÇåÂ±§„ÅÆÊ¨°ÂÖÉÊï∞\n",
    "            patch_size (int): „Éë„ÉÉ„ÉÅ„ÅÆ„Çµ„Ç§„Ç∫\n",
    "            out_channels (int): Âá∫Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "        \"\"\"\n",
    "        logger.info(f\"LastLayer.__init__ {hidden_size=} {patch_size=} {out_channels=}\")\n",
    "\n",
    "        super().__init__()\n",
    "        self.norm_final = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
    "        self.linear = nn.Linear(hidden_size, patch_size * patch_size * out_channels, bias=True)\n",
    "        self.adaLN_modulation = nn.Sequential(nn.SiLU(), nn.Linear(hidden_size, 2 * hidden_size, bias=True))\n",
    "\n",
    "    def forward(self, x: Tensor, vec: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): ÂÖ•Âäõ„ÉÜ„É≥„ÇΩ„É´ (B, N, hidden_size)\n",
    "            vec (Tensor): „É¢„Ç∏„É•„É¨„Éº„Ç∑„Éß„É≥„Éô„ÇØ„Éà„É´ (B, hidden_size)\n",
    "        Returns:\n",
    "            Tensor: Âá∫Âäõ„ÉÜ„É≥„ÇΩ„É´ (B, N, patch_size * patch_size * out_channels)\n",
    "        \"\"\"\n",
    "        logger.info(f\"LastLayer.forward {x.shape=} {vec.shape=}\")\n",
    "        shift, scale = self.adaLN_modulation(vec).chunk(2, dim=1)\n",
    "        x = (1 + scale[:, None, :]) * self.norm_final(x) + shift[:, None, :]\n",
    "        x = self.linear(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8179a144",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MMDiTConfig:\n",
    "    model_type = \"MMDiT\"\n",
    "    from_pretrained: str\n",
    "    cache_dir: str\n",
    "    in_channels: int\n",
    "    vec_in_dim: int\n",
    "    context_in_dim: int\n",
    "    hidden_size: int\n",
    "    mlp_ratio: float\n",
    "    num_heads: int\n",
    "    depth: int\n",
    "    depth_single_blocks: int\n",
    "    axes_dim: list[int]\n",
    "    theta: int\n",
    "    qkv_bias: bool\n",
    "    guidance_embed: bool\n",
    "    cond_embed: bool = False\n",
    "    fused_qkv: bool = True\n",
    "    grad_ckpt_settings: tuple[int, int] | None = None\n",
    "    use_liger_rope: bool = False\n",
    "    patch_size: int = 2\n",
    "\n",
    "    def get(self, attribute_name, default=None):\n",
    "        return getattr(self, attribute_name, default)\n",
    "\n",
    "    def __contains__(self, attribute_name):\n",
    "        return hasattr(self, attribute_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d538c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMDiTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    „Éû„É´„ÉÅ„É¢„Éº„ÉÄ„É´„Éá„Ç£„Éï„É•„Éº„Ç∏„Éß„É≥„Éà„É©„É≥„Çπ„Éï„Ç©„Éº„Éû„Éº„É¢„Éá„É´\n",
    "    ÁîªÂÉè„Å®„ÉÜ„Ç≠„Çπ„Éà„ÅÆ‰∏°Êñπ„ÅÆ„Çπ„Éà„É™„Éº„É†„ÇíÂá¶ÁêÜ„Åô„Çã\n",
    "    FluxÈñ¢Êï∞„Åß‰ΩøÁî®\n",
    "\n",
    "    \"\"\"\n",
    "    config_class = MMDiTConfig\n",
    "\n",
    "    def __init__(self, config: MMDiTConfig):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            config (MMDiTConfig): „É¢„Éá„É´„ÅÆË®≠ÂÆö\n",
    "        \"\"\"\n",
    "        logger.info(f\"MMDiTModel.__init__ {config=}\")\n",
    "\n",
    "        # 1) ÂàùÊúüÂåñ\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        self.in_channels = config.in_channels # 64\n",
    "\n",
    "        self.out_channels = self.in_channels # 64\n",
    "\n",
    "        self.patch_size = config.patch_size # 2\n",
    "\n",
    "        # False\n",
    "        # 384 % 3 = 0\n",
    "        if config.hidden_size % config.num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"Hidden size {config.hidden_size} must be divisible by num_heads {config.num_heads}\"\n",
    "            )\n",
    "\n",
    "        # 384 // 3 = 128\n",
    "        pe_dim = config.hidden_size // config.num_heads\n",
    "\n",
    "        # False\n",
    "        # 16 + 56 + 56 = 128\n",
    "        if sum(config.axes_dim) != pe_dim:\n",
    "            raise ValueError(\n",
    "                f\"Got {config.axes_dim} but expected positional dim {pe_dim}\"\n",
    "            )\n",
    "\n",
    "        # 384\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        # 3\n",
    "        self.num_heads = config.num_heads\n",
    "\n",
    "        # LigerEmbedND\n",
    "        pe_embedder_cls = LigerEmbedND if config.use_liger_rope else EmbedND\n",
    "\n",
    "        # 2) ‰ΩçÁΩÆÂüã„ÇÅËæº„ÅøÂ±§„ÇíÊßãÁØâ\n",
    "\n",
    "        self.pe_embedder = pe_embedder_cls(\n",
    "            dim=pe_dim, # 128\n",
    "            theta=config.theta, # 10000\n",
    "            axes_dim=config.axes_dim # [16, 56, 56]\n",
    "        )\n",
    "\n",
    "        # 3) ÂãïÁîªÂÉè„ÅÆÊΩúÂú®Â§âÊï∞„ÅÆÂüã„ÇÅËæº„ÅøÂ±§„ÇíÊßãÁØâ\n",
    "\n",
    "        # 64 -> 384\n",
    "        self.img_in = nn.Linear(\n",
    "            self.in_channels, # 64\n",
    "            self.hidden_size, # 384\n",
    "            bias=True\n",
    "        )\n",
    "\n",
    "        # 3) „Çø„Ç§„É†„Çπ„ÉÜ„ÉÉ„ÉóÁâπÂæ¥Èáè„ÅÆMLPÂüã„ÇÅËæº„ÅøÂ±§„ÇíÊßãÁØâ\n",
    "\n",
    "        # 256 -> 384\n",
    "        self.time_in = MLPEmbedder(\n",
    "            in_dim=256,\n",
    "            hidden_dim=self.hidden_size # 384\n",
    "        )\n",
    "\n",
    "        # 4) „Ç∞„É≠„Éº„Éê„É´„ÉÜ„Ç≠„Çπ„ÉàÁâπÂæ¥ÈáèÔºàCLIPÁâπÂæ¥ÈáèÔºâ„ÅÆMLPÂüã„ÇÅËæº„ÅøÂ±§„ÇíÊßãÁØâ\n",
    "\n",
    "        self.vector_in = MLPEmbedder(\n",
    "            config.vec_in_dim, # 768\n",
    "            self.hidden_size # 384\n",
    "        )\n",
    "\n",
    "        # 5) „Ç¨„Ç§„ÉÄ„É≥„ÇπÂº∑Â∫¶„ÅÆMLPÂüã„ÇÅËæº„ÅøÂ±§„ÇíÊßãÁØâ\n",
    "\n",
    "        self.guidance_in = (\n",
    "            MLPEmbedder(in_dim=256, hidden_dim=self.hidden_size)\n",
    "            if config.guidance_embed # False\n",
    "            else nn.Identity()\n",
    "        )\n",
    "\n",
    "        # 6) I2V„ÅÆÁîªÂÉèÊù°‰ª∂‰ªò„ÅçÂÖ•Âäõ„ÅÆÂüã„ÇÅËæº„ÅøÂ±§„ÇíÊßãÁØâ\n",
    "\n",
    "        self.cond_in = (\n",
    "            nn.Linear(\n",
    "                self.in_channels + self.patch_size**2,\n",
    "                self.hidden_size,\n",
    "                bias=True\n",
    "            )\n",
    "            if config.cond_embed # False\n",
    "            else nn.Identity()\n",
    "        )\n",
    "\n",
    "        # 7) ÂçòË™û„É¨„Éô„É´„ÅÆ„ÉÜ„Ç≠„Çπ„ÉàÁâπÂæ¥ÈáèÔºàT5ÁâπÂæ¥ÈáèÔºâ„ÅÆÂüã„ÇÅËæº„ÅøÂ±§„ÇíÊßãÁØâ\n",
    "\n",
    "        self.txt_in = nn.Linear(\n",
    "            config.context_in_dim, # 4096\n",
    "            self.hidden_size # 384\n",
    "        )\n",
    "\n",
    "        # 8) „ÉÄ„Éñ„É´„Çπ„Éà„É™„Éº„É†„Éñ„É≠„ÉÉ„ÇØ„ÇíÊßãÁØâ(1ÂÄã)\n",
    "\n",
    "        self.double_blocks = nn.ModuleList(\n",
    "            [\n",
    "                DoubleStreamBlock(\n",
    "                    self.hidden_size, # 384\n",
    "                    self.num_heads, # 3\n",
    "                    mlp_ratio=config.mlp_ratio, # 4.0\n",
    "                    qkv_bias=config.qkv_bias, # True\n",
    "                    fused_qkv=config.fused_qkv, # False\n",
    "                )\n",
    "                for _ in range(config.depth) # 1\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # 9) „Ç∑„É≥„Ç∞„É´„Çπ„Éà„É™„Éº„É†„Éñ„É≠„ÉÉ„ÇØ„ÇíÊßãÁØâ(38ÂÄã)\n",
    "\n",
    "        self.single_blocks = nn.ModuleList(\n",
    "            [\n",
    "                SingleStreamBlock(\n",
    "                    self.hidden_size, # 384\n",
    "                    self.num_heads, # 3\n",
    "                    mlp_ratio=config.mlp_ratio, # 4.0\n",
    "                    fused_qkv=config.fused_qkv, # False\n",
    "                )\n",
    "                for _ in range(config.depth_single_blocks) # 38\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # 10) ÊúÄÁµÇÂ±§„ÇíÊßãÁØâ\n",
    "\n",
    "        self.final_layer = LastLayer(\n",
    "            self.hidden_size, # 384\n",
    "            1,\n",
    "            self.out_channels # 64\n",
    "        )\n",
    "\n",
    "        # 11) Èáç„Åø„ÇíÂàùÊúüÂåñ\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "        # 12) È†Ü‰ºùÊê¨Èñ¢Êï∞„ÇíË®≠ÂÆö\n",
    "\n",
    "        # True\n",
    "        if self.config.grad_ckpt_settings:\n",
    "            self.forward = self.forward_selective_ckpt\n",
    "        # False\n",
    "        else:\n",
    "            self.forward = self.forward_ckpt\n",
    "\n",
    "        self._input_requires_grad = False\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        if self.config.cond_embed:\n",
    "            nn.init.zeros_(self.cond_in.weight)\n",
    "            nn.init.zeros_(self.cond_in.bias)\n",
    "\n",
    "    def prepare_block_inputs(\n",
    "        self,\n",
    "        img: Tensor,\n",
    "        img_ids: Tensor,\n",
    "        txt: Tensor,  # t5 encoded vec\n",
    "        txt_ids: Tensor,\n",
    "        timesteps: Tensor,\n",
    "        y_vec: Tensor,  # clip encoded vec\n",
    "        cond: Tensor = None,\n",
    "        guidance: Tensor | None = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        ÂÖ•Âäõ„ÇíÂêÑ„Éñ„É≠„ÉÉ„ÇØ„Å´ÈÅ©„Åó„ÅüÂΩ¢Âºè„Å´Ê∫ñÂÇô„Åô„Çã\n",
    "\n",
    "        Args:\n",
    "            img (Tensor): ÂãïÁîªÊΩúÂú®Â§âÊï∞„ÉÜ„É≥„ÇΩ„É´ (B, T, N, in_channels)\n",
    "            img_ids (Tensor): ÂãïÁîª‰ΩçÁΩÆID„ÉÜ„É≥„ÇΩ„É´ (B, T, N)\n",
    "            txt (Tensor): „ÉÜ„Ç≠„Çπ„Éà„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„ÉÜ„É≥„ÇΩ„É´ (B, L, context_in_dim)\n",
    "            txt_ids (Tensor): „ÉÜ„Ç≠„Çπ„Éà‰ΩçÁΩÆID„ÉÜ„É≥„ÇΩ„É´ (B, L)\n",
    "            timesteps (Tensor): „Çø„Ç§„É†„Çπ„ÉÜ„ÉÉ„Éó„ÉÜ„É≥„ÇΩ„É´ (B,)\n",
    "            y_vec (Tensor): CLIP„Ç®„É≥„Ç≥„Éº„Éâ„Éô„ÇØ„Éà„É´„ÉÜ„É≥„ÇΩ„É´ (B, vec_in_dim)\n",
    "            cond (Tensor, optional): Êù°‰ª∂ÁîªÂÉè„ÉÜ„É≥„ÇΩ„É´ (B, T, N, in_channels + patch_size**2)\n",
    "            guidance (Tensor | None, optional): „Ç¨„Ç§„ÉÄ„É≥„ÇπÂº∑Â∫¶„ÉÜ„É≥„ÇΩ„É´ (B,)\n",
    "        Returns:\n",
    "            tuple[Tensor, Tensor, Tensor, Tensor]:\n",
    "                „Éñ„É≠„ÉÉ„ÇØ„Å´Ê∏°„Åô„Åü„ÇÅ„Å´Ê∫ñÂÇô„Åï„Çå„ÅüÂÖ•Âäõ„ÉÜ„É≥„ÇΩ„É´„ÅÆ„Çø„Éó„É´\n",
    "        \"\"\"\n",
    "        logger.info(f\"MMDiTModel.prepare_block_inputs {img.shape=} {txt.shape=} {timesteps.shape=} {y_vec.shape=}\")\n",
    "\n",
    "        if img.ndim != 3 or txt.ndim != 3:\n",
    "            raise ValueError(\"Input img and txt tensors must have 3 dimensions.\")\n",
    "\n",
    "        # 1) ÂãïÁîª„ÅÆÊΩúÂú®Â§âÊï∞„Å´Âüã„ÇÅËæº„ÅøÂ±§„ÇíÈÅ©Áî®\n",
    "\n",
    "        img = self.img_in(img)\n",
    "        logger.debug(f\"After img_in: {img.shape=}\")\n",
    "\n",
    "        # 2) Êù°‰ª∂ÁîªÂÉè„ÅÆÂüã„ÇÅËæº„Åø„ÇíÈÅ©Áî®„Åó„ÄÅÂãïÁîª„ÅÆÊΩúÂú®Â§âÊï∞„Å´Âä†ÁÆó\n",
    "\n",
    "        # Êù°‰ª∂ÁîªÂÉè„Åå„ÅÇ„ÇãÂ†¥Âêà\n",
    "        # False\n",
    "        if self.config.cond_embed:\n",
    "            if cond is None:\n",
    "                raise ValueError(\"Didn't get conditional input for conditional model.\")\n",
    "\n",
    "            # Êù°‰ª∂ÁîªÂÉè„ÅÆÂüã„ÇÅËæº„Åø„ÇíÂãïÁîª„ÅÆÊΩúÂú®Â§âÊï∞„Å´Âä†ÁÆó\n",
    "            img = img + self.cond_in(cond)\n",
    "\n",
    "        # 3) „Çø„Ç§„É†„Çπ„ÉÜ„ÉÉ„ÉóÁâπÂæ¥Èáè„Å´Âüã„ÇÅËæº„ÅøÂ±§„ÇíÈÅ©Áî®\n",
    "\n",
    "        vec = self.time_in(timestep_embedding(timesteps, 256))\n",
    "\n",
    "        # 4) „Ç¨„Ç§„ÉÄ„É≥„ÇπÂº∑Â∫¶„Å´Âüã„ÇÅËæº„ÅøÂ±§„ÇíÈÅ©Áî®„Åó„ÄÅ„Çø„Ç§„É†„Çπ„ÉÜ„ÉÉ„ÉóÂüã„ÇÅËæº„Åø„Å´Âä†ÁÆó\n",
    "\n",
    "        # „Ç¨„Ç§„ÉÄ„É≥„ÇπÂº∑Â∫¶„ÅÆÂüã„ÇÅËæº„Åø„Çí‰ΩøÁî®„Åô„ÇãÂ†¥Âêà\n",
    "        # False\n",
    "        if self.config.guidance_embed:\n",
    "            if guidance is None:\n",
    "                raise ValueError(\n",
    "                    \"Didn't get guidance strength for guidance distilled model.\"\n",
    "                )\n",
    "\n",
    "            # „Ç¨„Ç§„ÉÄ„É≥„ÇπÂº∑Â∫¶„ÅÆÂüã„ÇÅËæº„Åø„Çí„Çø„Ç§„É†„Çπ„ÉÜ„ÉÉ„ÉóÂüã„ÇÅËæº„Åø„Å´Âä†ÁÆó \n",
    "            vec = vec + self.guidance_in(timestep_embedding(guidance, 256))\n",
    "\n",
    "        # 5) CLIPÁâπÂæ¥Èáè„Å´Âüã„ÇÅËæº„Åø„ÇíÈÅ©Áî®„Åó„ÄÅ„Çø„Ç§„É†„Çπ„ÉÜ„ÉÉ„ÉóÂüã„ÇÅËæº„Åø„Å´Âä†ÁÆó\n",
    "\n",
    "        vec = vec + self.vector_in(y_vec)\n",
    "\n",
    "        # 6) T5ÁâπÂæ¥Èáè„Å´Âüã„ÇÅËæº„Åø„ÇíÈÅ©Áî®\n",
    "\n",
    "        txt = self.txt_in(txt)\n",
    "\n",
    "        # 7) ‰ΩçÁΩÆ„Ç®„É≥„Ç≥„Éº„Éá„Ç£„É≥„Ç∞„Å´Âüã„ÇÅËæº„Åø„ÇíÈÅ©Áî®\n",
    "\n",
    "        # „ÉÜ„Ç≠„Çπ„Éà„Å®ÁîªÂÉè„ÅÆ‰ΩçÁΩÆID„ÇíÈÄ£Áµê\n",
    "        # concat: 4096 + t*h*2/4\n",
    "        ids = torch.cat((txt_ids, img_ids), dim=1)\n",
    "\n",
    "        # ‰ΩçÁΩÆ„Ç®„É≥„Ç≥„Éº„Éá„Ç£„É≥„Ç∞„Å´Âüã„ÇÅËæº„Åø„ÇíÈÅ©Áî®\n",
    "        pe = self.pe_embedder(ids)\n",
    "\n",
    "        # False\n",
    "        if self._input_requires_grad:\n",
    "            # we only apply lora to double/single blocks, thus we only need to enable grad for these inputs\n",
    "            img.requires_grad_()\n",
    "            txt.requires_grad_()\n",
    "\n",
    "        return img, txt, vec, pe\n",
    "\n",
    "    def enable_input_require_grads(self):\n",
    "        \"\"\"Fit peft lora. This method should not be called manually.\"\"\"\n",
    "        self._input_requires_grad = True\n",
    "\n",
    "    def forward_ckpt(\n",
    "        self,\n",
    "        img: Tensor,\n",
    "        img_ids: Tensor,\n",
    "        txt: Tensor,\n",
    "        txt_ids: Tensor,\n",
    "        timesteps: Tensor,\n",
    "        y_vec: Tensor,\n",
    "        cond: Tensor = None,\n",
    "        guidance: Tensor | None = None,\n",
    "        **kwargs,\n",
    "    ) -> Tensor:\n",
    "\n",
    "        # 1) ÂÖ•Âäõ„ÅÆÁâπÂæ¥Èáè„ÇíÂüã„ÇÅËæº„Åø„ÄÅÈõÜÁ¥Ñ\n",
    "\n",
    "        # ÂãïÁîªÂüã„ÇÅËæº„Åø, T5„ÉÜ„Ç≠„Çπ„ÉàÂüã„ÇÅËæº„Åø, „Çø„Ç§„É†„Çπ„ÉÜ„ÉÉ„ÉóÂüã„ÇÅËæº„Åø, ‰ΩçÁΩÆ„Ç®„É≥„Ç≥„Éº„Éá„Ç£„É≥„Ç∞\n",
    "        # „Çø„Ç§„É†„Çπ„ÉÜ„ÉÉ„ÉóÂüã„ÇÅËæº„Åø„ÅØ„ÄÅCLIPÁâπÂæ¥Èáè„Å®„Ç¨„Ç§„ÉÄ„É≥„ÇπÂº∑Â∫¶„ÇíÂä†ÁÆó„Åó„ÅüÂÄ§\n",
    "        img, txt, vec, pe = self.prepare_block_inputs(\n",
    "            img, img_ids, txt, txt_ids, timesteps, y_vec, cond, guidance\n",
    "        )\n",
    "        logger.debug(f\"After prepare_block_inputs: {img.shape=} {txt.shape=} {vec.shape=} {pe[0].shape=} {pe[1].shape=}\")\n",
    "\n",
    "        # 2) „ÉÄ„Éñ„É´„Çπ„Éà„É™„Éº„É†„Éñ„É≠„ÉÉ„ÇØ„ÇíÈ†Ü‰ºùÊê¨\n",
    "\n",
    "        # 1„Å§„ÅÆ„ÉÄ„Éñ„É´„Çπ„Éà„É™„Éº„É†„Éñ„É≠„ÉÉ„ÇØ„ÇíÈ†Ü‰ºùÊê¨\n",
    "        for block in self.double_blocks:\n",
    "\n",
    "            # Ëá™ÂãïÂãæÈÖç„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà„Çí‰ΩøÁî®\n",
    "            img, txt = auto_grad_checkpoint(block, img, txt, vec, pe)\n",
    "\n",
    "        logger.debug(f\"After double_blocks: {img.shape=} {txt.shape=}\")\n",
    "\n",
    "        # 3) „ÉÜ„Ç≠„Çπ„ÉàÂüã„ÇÅËæº„Åø„Å®ÁîªÂÉèÂüã„ÇÅËæº„Åø„ÇíÈÄ£Áµê\n",
    "\n",
    "        img = torch.cat((txt, img), 1)\n",
    "        logger.debug(f\"After concat txt and img: {img.shape=}\")\n",
    "\n",
    "        # 4) „Ç∑„É≥„Ç∞„É´„Çπ„Éà„É™„Éº„É†„Éñ„É≠„ÉÉ„ÇØ„ÇíÈ†Ü‰ºùÊê¨\n",
    "\n",
    "        # 38ÂÄã„ÅÆ„Ç∑„É≥„Ç∞„É´„Çπ„Éà„É™„Éº„É†„Éñ„É≠„ÉÉ„ÇØ„ÇíÈ†Ü‰ºùÊê¨\n",
    "        for block in self.single_blocks:\n",
    "\n",
    "            # Ëá™ÂãïÂãæÈÖç„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà„Çí‰ΩøÁî®\n",
    "            img = auto_grad_checkpoint(block, img, vec, pe)\n",
    "\n",
    "        logger.debug(f\"After single_blocks: {img.shape=}\")\n",
    "\n",
    "        # 5) „ÉÜ„Ç≠„Çπ„ÉàÂüã„ÇÅËæº„Åø„ÇíÂàá„ÇäÈõ¢„Åó„ÄÅÁîªÂÉèÂüã„ÇÅËæº„Åø„ÇíÂèñÂæó\n",
    "\n",
    "        img = img[:, txt.shape[1] :, ...]\n",
    "        logger.debug(f\"After single_blocks: {img.shape=}\")\n",
    "\n",
    "        # 6) ÊúÄÁµÇÂ±§„ÇíÈ†Ü‰ºùÊê¨„Åó„ÄÅÁîªÂÉè„Éë„ÉÉ„ÉÅ„ÇíÁîüÊàê\n",
    "\n",
    "        img = self.final_layer(img, vec)  # (N, T, patch_size ** 2 * out_channels)\n",
    "\n",
    "        logger.debug(f\"After final_layer: {img.shape=}\")\n",
    "\n",
    "        return img\n",
    "\n",
    "    def forward_selective_ckpt(\n",
    "        self,\n",
    "        img: Tensor,\n",
    "        img_ids: Tensor,\n",
    "        txt: Tensor,\n",
    "        txt_ids: Tensor,\n",
    "        timesteps: Tensor,\n",
    "        y_vec: Tensor,\n",
    "        cond: Tensor = None,\n",
    "        guidance: Tensor | None = None,\n",
    "        **kwargs,\n",
    "    ) -> Tensor:\n",
    "        logger.info(f\"MMDiTModel.forward_selective_ckpt {img.shape=} {txt.shape=} {timesteps.shape=} {y_vec.shape=}\")\n",
    "\n",
    "        # 1) ÂÖ•Âäõ„ÅÆÁâπÂæ¥Èáè„ÇíÂüã„ÇÅËæº„Åø„ÄÅÈõÜÁ¥Ñ\n",
    "\n",
    "        # ÂãïÁîªÂüã„ÇÅËæº„Åø, T5„ÉÜ„Ç≠„Çπ„ÉàÂüã„ÇÅËæº„Åø, „Çø„Ç§„É†„Çπ„ÉÜ„ÉÉ„ÉóÂüã„ÇÅËæº„Åø, ‰ΩçÁΩÆ„Ç®„É≥„Ç≥„Éº„Éá„Ç£„É≥„Ç∞\n",
    "        # „Çø„Ç§„É†„Çπ„ÉÜ„ÉÉ„ÉóÂüã„ÇÅËæº„Åø„ÅØ„ÄÅCLIPÁâπÂæ¥Èáè„Å®„Ç¨„Ç§„ÉÄ„É≥„ÇπÂº∑Â∫¶„ÇíÂä†ÁÆó„Åó„ÅüÂÄ§\n",
    "        img, txt, vec, pe = self.prepare_block_inputs(\n",
    "            img, img_ids, txt, txt_ids, timesteps, y_vec, cond, guidance\n",
    "        )\n",
    "        logger.debug(f\"After prepare_block_inputs: {img.shape=} {txt.shape=} {vec.shape=} {pe[0].shape=} {pe[1].shape=}\")\n",
    "\n",
    "        # 1) „ÉÄ„Éñ„É´„Çπ„Éà„É™„Éº„É†„Éñ„É≠„ÉÉ„ÇØ„ÇíÈ†Ü‰ºùÊê¨\n",
    "\n",
    "        # 8\n",
    "        ckpt_depth_double = self.config.grad_ckpt_settings[0]\n",
    "\n",
    "        # 1„Å§„ÅÆ„ÉÄ„Éñ„É´„Çπ„Éà„É™„Éº„É†„Éñ„É≠„ÉÉ„ÇØ„ÇíÈ†Ü‰ºùÊê¨\n",
    "        for block in self.double_blocks[:ckpt_depth_double]:\n",
    "            img, txt = auto_grad_checkpoint(block, img, txt, vec, pe)\n",
    "\n",
    "        for block in self.double_blocks[ckpt_depth_double:]:\n",
    "            img, txt = block(img, txt, vec, pe)\n",
    "\n",
    "        logger.debug(f\"After double_blocks: {img.shape=} {txt.shape=}\")\n",
    "\n",
    "        # 2) „ÉÜ„Ç≠„Çπ„ÉàÂüã„ÇÅËæº„Åø„Å®ÁîªÂÉèÂüã„ÇÅËæº„Åø„ÇíÈÄ£Áµê\n",
    "\n",
    "        img = torch.cat((txt, img), 1)\n",
    "        logger.debug(f\"After concat txt and img: {img.shape=}\")\n",
    "\n",
    "        # 3) „Ç∑„É≥„Ç∞„É´„Çπ„Éà„É™„Éº„É†„Éñ„É≠„ÉÉ„ÇØ„ÇíÈ†Ü‰ºùÊê¨\n",
    "\n",
    "        ckpt_depth_single = self.config.grad_ckpt_settings[1]\n",
    "\n",
    "        for block in self.single_blocks[:ckpt_depth_single]:\n",
    "            img = auto_grad_checkpoint(block, img, vec, pe)\n",
    "\n",
    "        for block in self.single_blocks[ckpt_depth_single:]:\n",
    "            img = block(img, vec, pe)\n",
    "\n",
    "        logger.debug(f\"After single_blocks: {img.shape=}\")\n",
    "\n",
    "        # 4) „ÉÜ„Ç≠„Çπ„ÉàÂüã„ÇÅËæº„Åø„ÇíÂàá„ÇäÈõ¢„Åó„ÄÅÁîªÂÉèÂüã„ÇÅËæº„Åø„ÇíÂèñÂæó\n",
    "\n",
    "        img = img[:, txt.shape[1] :, ...]\n",
    "        logger.debug(f\"After single_blocks: {img.shape=}\")\n",
    "\n",
    "        # 5) ÊúÄÁµÇÂ±§„ÇíÈ†Ü‰ºùÊê¨„Åó„ÄÅÁîªÂÉè„Éë„ÉÉ„ÉÅ„ÇíÁîüÊàê\n",
    "\n",
    "        img = self.final_layer(img, vec)  # (N, T, patch_size ** 2 * out_channels)\n",
    "        logger.debug(f\"After final_layer: {img.shape=}\")\n",
    "\n",
    "\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b629da9e",
   "metadata": {},
   "source": [
    "### Ê§úË®º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e309fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @MODELS.register_module(\"flux\")\n",
    "def Flux(\n",
    "    cache_dir: str = None,\n",
    "    from_pretrained: str = None,\n",
    "    device_map: str | torch.device = \"cuda\",\n",
    "    torch_dtype: torch.dtype = torch.bfloat16,\n",
    "    strict_load: bool = False,\n",
    "    **kwargs,\n",
    ") -> MMDiTModel:\n",
    "    \"\"\"\n",
    "    MMDiT„É¢„Éá„É´„Çí‰ΩúÊàê„Åô„Çã„Éï„Ç°„ÇØ„Éà„É™Èñ¢Êï∞\n",
    "\n",
    "    Args:\n",
    "        cache_dir (str, optional): ‰∫ãÂâçÂ≠¶ÁøíÊ∏à„Åø„É¢„Éá„É´„ÅÆ„Ç≠„É£„ÉÉ„Ç∑„É•„Éá„Ç£„É¨„ÇØ„Éà„É™. Defaults to None.\n",
    "        from_pretrained (str, optional): ‰∫ãÂâçÂ≠¶ÁøíÊ∏à„Åø„É¢„Éá„É´„ÅÆ„Éë„Çπ„Åæ„Åü„ÅØÂêçÂâç. Defaults to None.\n",
    "        device_map (str | torch.device, optional): „É¢„Éá„É´„ÇíÈÖçÁΩÆ„Åô„Çã„Éá„Éê„Ç§„Çπ. Defaults to \"cuda\".\n",
    "        torch_dtype (torch.dtype, optional): „É¢„Éá„É´„ÅÆ„Éá„Éï„Ç©„É´„Éà„Éá„Éº„ÇøÂûã. Defaults to torch.bfloat16.\n",
    "        strict_load (bool, optional): „ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà„ÅÆÂé≥ÂØÜ„Å™Ë™≠„ÅøËæº„Åø„ÇíË°å„ÅÜ„Åã„Å©„ÅÜ„Åã. Defaults to False.\n",
    "    Returns:\n",
    "        MMDiTModel: ÂàùÊúüÂåñ„Åï„Çå„ÅüMMDiT„É¢„Éá„É´\n",
    "    \"\"\"\n",
    "    logger.info(f\"Initializing MMDiT Model with {from_pretrained=}, {cache_dir=}, {device_map=}, {torch_dtype=}, {strict_load=}, {kwargs=}\")\n",
    "\n",
    "    # 1) „É¢„Éá„É´Ë®≠ÂÆö„Çí‰ΩúÊàê\n",
    "\n",
    "    config = MMDiTConfig(\n",
    "        from_pretrained=from_pretrained, # None\n",
    "        cache_dir=cache_dir, # None\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    # False\n",
    "    low_precision_init = from_pretrained is not None and len(from_pretrained) > 0\n",
    "\n",
    "    # False\n",
    "    if low_precision_init:\n",
    "        default_dtype = torch.get_default_dtype()\n",
    "        torch.set_default_dtype(torch_dtype)\n",
    "\n",
    "    # 2) „É¢„Éá„É´„Çí„Éá„Éê„Ç§„Çπ„Å´ÈÖçÁΩÆ„Åó„Å¶ÂàùÊúüÂåñ\n",
    "\n",
    "    with torch.device(device_map):\n",
    "        model = MMDiTModel(config)\n",
    "\n",
    "    # False\n",
    "    if low_precision_init:\n",
    "        torch.set_default_dtype(default_dtype)\n",
    "\n",
    "    # 3) „É¢„Éá„É´„ÇíÈÅ©Âàá„Å™„Éá„Éº„ÇøÂûã„Å´Â§âÊèõ\n",
    "\n",
    "    # True\n",
    "    else:\n",
    "        # bfloat16„Å´Â§âÊèõ\n",
    "        model = model.to(torch_dtype)\n",
    "\n",
    "    # False\n",
    "    if from_pretrained:\n",
    "        model = load_checkpoint(\n",
    "            model,\n",
    "            from_pretrained,\n",
    "            cache_dir=cache_dir,\n",
    "            device_map=device_map,\n",
    "            strict=strict_load,\n",
    "        )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fb91e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    logger.info(f\"MMDiT„ÇíÊ§úË®º\")\n",
    "    # {'type': 'flux',\n",
    "    #  'from_pretrained': None,\n",
    "    #  'strict_load': False,\n",
    "    #  'guidance_embed': False,\n",
    "    #  'fused_qkv': False,\n",
    "    #  'use_liger_rope': True,\n",
    "    #  'grad_ckpt_settings': (8, 100),\n",
    "    #  'in_channels': 64,\n",
    "    #  'vec_in_dim': 768,\n",
    "    #  'context_in_dim': 4096,\n",
    "    #  'hidden_size': 384,\n",
    "    #  'mlp_ratio': 4.0,\n",
    "    #  'num_heads': 3,\n",
    "    #  'depth': 1,\n",
    "    #  'depth_single_blocks': 38,\n",
    "    #  'axes_dim': [16, 56, 56],\n",
    "    #  'theta': 10000,\n",
    "    #  'qkv_bias': True}\n",
    "\n",
    "    model = Flux(\n",
    "        from_pretrained=None,\n",
    "        strict_load=False,\n",
    "        guidance_embed=False,\n",
    "        fused_qkv=False,\n",
    "        use_liger_rope=True,\n",
    "        grad_ckpt_settings=(8, 100),\n",
    "        in_channels=64,\n",
    "        vec_in_dim=768,\n",
    "        context_in_dim=4096,\n",
    "        hidden_size=384,\n",
    "        mlp_ratio=4.0,\n",
    "        num_heads=3,\n",
    "        depth=1,\n",
    "        depth_single_blocks=38,\n",
    "        axes_dim=[16, 56, 56],\n",
    "        theta=10000,\n",
    "        qkv_bias=True,\n",
    "    )\n",
    "\n",
    "    # (batch_size, seq_len, in_channels)\n",
    "    sample_input = torch.randn(1, 16, 64).to(\"cuda\").to(torch.bfloat16)\n",
    "\n",
    "    # (batch_size, seq_len)\n",
    "    sample_img_ids = torch.randint(0, 10000, (1, 16, 3)).to(\"cuda\")\n",
    "\n",
    "    # (batch_size, seq_len, context_in_dim)\n",
    "    sample_txt = torch.randn(1, 16, 4096).to(\"cuda\").to(torch.bfloat16)\n",
    "\n",
    "    # (batch_size, seq_len)\n",
    "    sample_txt_ids = torch.randint(0, 10000, (1, 16, 3)).to(\"cuda\")\n",
    "\n",
    "    # (batch_size,)\n",
    "    sample_timesteps = torch.randint(0, 1000, (1,)).to(\"cuda\").to(torch.bfloat16)\n",
    "\n",
    "    # (batch_size, vec_in_dim)\n",
    "    sample_y_vec = torch.randn(1, 768).to(\"cuda\").to(torch.bfloat16)\n",
    "\n",
    "    output = model(\n",
    "        img=sample_input,\n",
    "        img_ids=sample_img_ids,\n",
    "        txt=sample_txt,\n",
    "        txt_ids=sample_txt_ids,\n",
    "        timesteps=sample_timesteps,\n",
    "        y_vec=sample_y_vec,\n",
    "    )\n",
    "\n",
    "    # (1, 16, patch_size ** 2 * out_channels)\n",
    "    logger.info(output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
