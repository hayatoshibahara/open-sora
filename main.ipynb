{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e54053b",
   "metadata": {},
   "source": [
    "## æ¦‚è¦"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53236702",
   "metadata": {},
   "source": [
    "Open-Sora 2.0ã¯ã€3000ä¸‡å††ã§å®Ÿç¾å¯èƒ½ãªå•†ç”¨ãƒ¬ãƒ™ãƒ«ã®å‹•ç”»ç”Ÿæˆãƒ¢ãƒ‡ãƒ«\n",
    "\n",
    "è¨“ç·´ã‚³ã‚¹ãƒˆã¯ã€åŒç­‰ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆMovieGenã‚„Step-Video-T2Vï¼‰ã‚ˆã‚Šã‚‚5~10å€ä½ã„\n",
    "\n",
    "äººã®è©•ä¾¡ã¨VBenchã®ã‚¹ã‚³ã‚¢ã§ã¯ã€Huyyuan Videoã‚„Runway Gen-3 Alphaã«åŒ¹æ•µ:\n",
    "\n",
    "![](image/fig1.png)\n",
    "\n",
    "- Visual Quality: è¦–è¦šå“è³ª\n",
    "- Prompt Following: æŒ‡ç¤ºè¿½å¾“æ€§\n",
    "- Motion Quality: å‹•ãã®å“è³ª"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12386bd",
   "metadata": {},
   "source": [
    "## ãƒ‡ãƒ¼ã‚¿"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1780041",
   "metadata": {},
   "source": [
    "ç›®çš„ã¯ã€å­¦ç¿’ã®é€²æ—ã«åˆã‚ã›ãŸãƒ‡ãƒ¼ã‚¿ãƒ”ãƒ©ãƒŸãƒƒãƒ‰ï¼ˆhierarchical data pyramidï¼‰ã®æ§‹ç¯‰\n",
    "\n",
    "æ§˜ã€…ãªç¨®é¡ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œå‡ºå¯èƒ½ãªãªãƒ•ã‚£ãƒ«ã‚¿ã‚’é–‹ç™º\n",
    "\n",
    "å­¦ç¿’ã®é€²æ—ã«å¿œã˜ã¦ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®å¼·åº¦ã‚’é«˜ã‚ã€ç´”åº¦ã¨å“è³ªã®é«˜ã„å°ã•ã„ã‚µãƒ–ã‚»ãƒƒãƒˆã§è¨“ç·´\n",
    "\n",
    "ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®å…¨ä½“åƒ:\n",
    "\n",
    "![](image/fig2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7093517c",
   "metadata": {},
   "source": [
    "- ç´«: ç”Ÿã®å‹•ç”»ã®å‰å‡¦ç†\n",
    "    1. ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "        - ç ´æã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®é™¤å»\n",
    "        - æ¥µç«¯ãªå‹•ç”»ã®é™¤å»\n",
    "            - å†ç”Ÿæ™‚é–“ãŒ2ç§’æœªæº€\n",
    "            - 1ç”»åƒã‚ãŸã‚Šã®ãƒ‡ãƒ¼ã‚¿é‡ï¼ˆBit per pixelï¼‰ãŒ0.02æœªæº€\n",
    "            - ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ¼ãƒˆï¼ˆfpsï¼‰ãŒ16æœªæº€\n",
    "            - ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ãŒç¯„å›²å¤–ï¼ˆ1/3, 3ï¼‰\n",
    "            - ç‰¹å®šã®ä½å“è³ªãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰è¨­å®šï¼ˆConstrained Baseline profileï¼‰\n",
    "    2. é€£ç¶šã—ãŸæ˜ åƒã‚’æ¤œå‡ºã—ã€çŸ­ã„ã‚¯ãƒªãƒƒãƒ—ã«åˆ†å‰²\n",
    "        - FFmpegã®libavfilterã‚’ä½¿ç”¨ã—ã€ã‚·ãƒ¼ãƒ³ã‚¹ã‚³ã‚¢ï¼ˆãƒ•ãƒ¬ãƒ¼ãƒ é–“ã®è¦–è¦šå·®åˆ†ï¼‰ã‚’è¨ˆç®—\n",
    "    3. å‹•ç”»ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ\n",
    "        - ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ¼ãƒˆã¯30fpsä»¥ä¸‹\n",
    "        - é•·è¾ºã¯1080pxä»¥ä¸‹\n",
    "        - ã‚³ãƒ¼ãƒ‡ãƒƒã‚¯ï¼ˆåœ§ç¸®å½¢å¼ï¼‰ã¯H.264\n",
    "        - å‹•ç”»ã®é»’å¸¯ã‚’å‰Šé™¤\n",
    "        - 8ç§’ã‚’è¶…ãˆã‚‹ã‚·ãƒ§ãƒƒãƒˆã¯ã€8ç§’ã®ã‚¯ãƒªãƒƒãƒ—ã«åˆ†å‰²ã—ã€2ç§’æœªæº€ã¯ç ´æ£„\n",
    "- é’: ã‚¯ãƒªãƒƒãƒ—å‹•ç”»ã®ã‚¹ã‚³ã‚¢ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "    - ç¾çš„ã‚¹ã‚³ã‚¢ã§ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "        - [CLIPã¨MLP][1]ã§ç¾çš„ã‚¹ã‚³ã‚¢ã‚’äºˆæ¸¬\n",
    "            - æœ€åˆãƒ»ä¸­é–“ãƒ»æœ€çµ‚ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æŠ½å‡ºã—ã€ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã—ã€å¹³å‡\n",
    "    - é®®æ˜ã•ã®ä½ã„å‹•ç”»ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "        - OpenCVã®ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³æ¼”ç®—å­ã§ç”»åƒã®åˆ†æ•£ãŒä½ã„ï¼ˆã¼ã‚„ã‘ã¦ã„ã‚‹ï¼‰å‹•ç”»ã‚’é™¤å»\n",
    "    - æ‰‹ãƒ–ãƒ¬ã®å¤šã„å‹•ç”»ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "        - PySceneDetectã‚’ä½¿ç”¨ã—ã¦ã€ãƒ•ãƒ¬ãƒ¼ãƒ é–“ã®å¤‰åŒ–ãŒå¤§ãã„å‹•ç”»ã‚’é™¤å»\n",
    "    - é‡è¤‡ã‚¯ãƒªãƒƒãƒ—ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "- ç·‘: 256pxã®ä½è§£åƒåº¦å‹•ç”»\n",
    "    - å¤šãã®æ–‡ç« ãŒå«ã¾ã‚Œã‚‹å‹•ç”»ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "        - PaddleOCRã‚’ä½¿ç”¨ã—ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆã®ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’æ¤œå‡º\n",
    "        - ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ãŒ0.7ã‚’è¶…ãˆã‚‹ãƒœãƒƒã‚¯ã‚¹ã®ç·é¢ç©ãŒå¤šã„å‹•ç”»ã‚’é™¤å»\n",
    "    - ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã§ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "        - libavfilterã®VMAFã‚’ä½¿ç”¨ã—ã¦å‹•ç”»ã®å‹•ãã®æ¿€ã—ã•ã‚’æ¸¬å®š\n",
    "        - ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ãŒæ¥µç«¯ã«ä½ã„ãƒ»é«˜ã„å‹•ç”»ã‚’é™¤å»\n",
    "- é»„è‰²: 768pxã®é«˜è§£åƒåº¦å‹•ç”»\n",
    "\n",
    "[1]: https://github.com/christophschuhmann/improved-aesthetic-predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa995eea",
   "metadata": {},
   "source": [
    "å‹•ç”»ã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½œæˆã™ã‚‹ãŸã‚ã«ã€è¦–è¦šè¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨:\n",
    "\n",
    "- 256pxå‹•ç”»ã«ã¯ã€LLaVA-Video\n",
    "- 778pxå‹•ç”»ã«ã¯ã€Qwen 2.5 Maxï¼ˆãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ãŒå°‘ãªã„ãƒ—ãƒ­ãƒ—ãƒ©ã‚¤ã‚¨ã‚¿ãƒªãƒ¢ãƒ‡ãƒ«ï¼‰\n",
    "\n",
    "ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹æˆ:\n",
    "\n",
    "- ä¸»ãªè¢«å†™ä½“\n",
    "- è¢«å†™ä½“ã®å‹•ã\n",
    "- èƒŒæ™¯ã‚„ç’°å¢ƒ\n",
    "- è¨¼æ˜æ¡ä»¶ã‚„é›°å›²æ°—\n",
    "- ã‚«ãƒ¡ãƒ©ãƒ¯ãƒ¼ã‚¯\n",
    "- ãƒªã‚¢ãƒ«ãƒ»ã‚·ãƒãƒãƒ†ã‚£ãƒƒã‚¯ãƒ»3Dãƒ»ã‚¢ãƒ‹ãƒ¡ãªã©ã®å‹•ç”»ã®ã‚¹ã‚¿ã‚¤ãƒ«\n",
    "\n",
    "ç”Ÿæˆæ™‚ã«å‹•ãã®å¼·åº¦ã‚’èª¿æ•´å¯èƒ½ã«ã™ã‚‹ãŸã‚ã€ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã®æœ€å¾Œã«ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’è¿½è¨˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7587fc55",
   "metadata": {},
   "source": [
    "ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆ:\n",
    "\n",
    "![](image/fig3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff7e82d",
   "metadata": {},
   "source": [
    "- ç¾çš„ã‚¹ã‚³ã‚¢ã¯4.5~5.5ã§ä¸­ç¨‹åº¦\n",
    "- å‹•ç”»ã®é•·ã•ã¯2~8ç§’ã§ã€åŠåˆ†è¿‘ããŒ6~8ç§’\n",
    "- ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã¯ã€å¤§éƒ¨åˆ†ãŒ0.5~0.75ï¼ˆ16:9ã®æ¨ªé•·å‹•ç”»ï¼‰\n",
    "- ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã®70%ã¯75å˜èªã‚’è¶…ãˆã¦ã„ã¦æƒ…å ±é‡ãŒå¤šã„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1277b7",
   "metadata": {},
   "source": [
    "ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã®ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab60e0b",
   "metadata": {},
   "source": [
    "![](image/fig4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1d547b",
   "metadata": {},
   "source": [
    "- èƒŒæ™¯ã‚„ç…§æ˜æ¡ä»¶ã‚‚å«ã¾ã‚Œã¦ã„ã¦ã€è¢«å†™ä½“ã¯äººç‰©ãŒå¤šã„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c25823",
   "metadata": {},
   "source": [
    "## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346c05aa",
   "metadata": {},
   "source": [
    "### 3æ¬¡å…ƒã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a577fd",
   "metadata": {},
   "source": [
    "[Hunyuan Video VAE][1]ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’åŠ¹ç‡åŒ–ã—ãŸVideo DC-AEã‚’é–‹ç™º\n",
    "\n",
    "DC-AEã¯ã€[Deep Compression Autoencoder][2]ã®ç•¥\n",
    "\n",
    "åœ§ç¸®ç‡ã¯ã€$4\\times 32\\times 32$ï¼ˆæ™‚é–“ã¯ $\\frac{1}{4}$ã€ç¸¦ã¨æ¨ªã¯$\\frac{1}{32}$ã«åœ§ç¸®ï¼‰\n",
    "\n",
    "ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¯32ãƒ•ãƒ¬ãƒ¼ãƒ ã€256pxã®ãŸã‚ã€æ½œåœ¨è¡¨ç¾ã¯$8\\times 8\\times 8$\n",
    "\n",
    "[1]: https://arxiv.org/abs/2412.03603\n",
    "[2]: https://arxiv.org/abs/2410.10733"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef5fb66",
   "metadata": {},
   "source": [
    "Video DC-AEã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£:\n",
    "\n",
    "![](image/fig5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346d9939",
   "metadata": {},
   "source": [
    "- ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€\n",
    "    - 3å±¤ã®ResBlockã¨3å±¤ã®EfficientViT Blockã§æ§‹æˆã•ã‚Œã‚‹\n",
    "    - æœ€åˆã®5ã¤ã®ãƒ–ãƒ­ãƒƒã‚¯ã¯ã€ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç”¨\n",
    "    - å­¦ç¿’ã‚’å¯èƒ½ã«ã™ã‚‹ãŸã‚ã€ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒ«ãƒ–ãƒ­ãƒƒã‚¯ã«ã¯ã€æ®‹å·®æ¥ç¶šãŒå°å…¥\n",
    "    - æ®‹å·®æ¥ç¶šã¯ãƒ”ã‚¯ã‚»ãƒ«ã‚¢ãƒ³ã‚·ãƒ£ãƒƒãƒ•ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨ï¼ˆSpace&Time->Channelï¼‰\n",
    "- ãƒ‡ã‚³ãƒ¼ãƒ€\n",
    "    - 3å±¤ã®EfficientViT Blockã¨3å±¤ã®ResBlockã§æ§‹æˆã•ã‚Œã‚‹\n",
    "    - æœ€å¾Œã®5ã¤ã®ãƒ–ãƒ­ãƒƒã‚¯ã¯ã€ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç”¨\n",
    "    - å­¦ç¿’ã‚’å¯èƒ½ã«ã™ã‚‹ãŸã‚ã€ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒ«ãƒ–ãƒ­ãƒƒã‚¯ã«ã¯ã€æ®‹å·®æ¥ç¶šãŒå°å…¥\n",
    "    - æ®‹å·®æ¥ç¶šã¯ãƒ”ã‚¯ã‚»ãƒ«ã‚·ãƒ£ãƒƒãƒ•ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨ï¼ˆChannel->Space&Timeï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb61f01",
   "metadata": {},
   "source": [
    "Video DC-AEã‚’ã‚¹ã‚¯ãƒ©ãƒƒãƒã‹ã‚‰å­¦ç¿’ã—ã€å†æ§‹æˆå“è³ªã‚’è©•ä¾¡:\n",
    "\n",
    "![](image/table1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df750de",
   "metadata": {},
   "source": [
    "- LPIPS: äººé–“ã®çŸ¥è¦šã«è¿‘ã„ç”»è³ªè©•ä¾¡æŒ‡æ¨™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adeda3cd",
   "metadata": {},
   "source": [
    "## DiTã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759c3421",
   "metadata": {},
   "source": [
    "é›¢ã‚ŒãŸãƒ•ãƒ¬ãƒ¼ãƒ ã‚„ç”»ç´ åŒå£«ã®é–¢ä¿‚ã‚’åŠ¹æœçš„ã«æ‰ãˆã‚‹ãƒ•ãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’æ¡ç”¨\n",
    "\n",
    "å‹•ç”»ã¯Video DC-AEã§åœ§ç¸®å¾Œã€ãƒ‘ãƒƒãƒã‚µã‚¤ã‚º1ï¼ˆ=ãƒ‘ãƒƒãƒåŒ–ç„¡ã—ï¼‰ã§ãƒ•ãƒ©ãƒƒãƒˆåŒ–\n",
    "\n",
    "Hunyuan Videoã®ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€ãƒ‘ãƒƒãƒã‚µã‚¤ã‚º2ãŒå¿…è¦"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df79314e",
   "metadata": {},
   "source": [
    "FLUXã®[MMDiT][1]ã‚’å‚è€ƒã«ã€ãƒ‡ãƒ¥ã‚¢ãƒ«ã‚¹ãƒˆãƒªãƒ¼ãƒ ã¨ã‚·ãƒ³ã‚°ãƒ«ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‹ã‚‰ãªã‚‹æ§‹é€ ã‚’æ¡ç”¨:\n",
    "\n",
    "![](image/fig6.png)\n",
    "\n",
    "[1]: https://github.com/black-forest-labs/flux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968e2d5c",
   "metadata": {},
   "source": [
    "- ãƒ‡ãƒ¥ã‚¢ãƒ«ã‚¹ãƒˆãƒªãƒ¼ãƒ ãƒ–ãƒ­ãƒƒã‚¯ã§ã€å‹•ç”»ã¨ãƒ†ã‚­ã‚¹ãƒˆãŒåˆ¥ã€…ã«ç‰¹å¾´æŠ½å‡ºã•ã‚Œã‚‹\n",
    "- ã‚·ãƒ³ã‚°ãƒ«ã‚¹ãƒˆãƒªãƒ¼ãƒ ãƒ–ãƒ­ãƒƒã‚¯ã§ã€ç‰¹å¾´ã‚’çµ±åˆã™ã‚‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f3e053",
   "metadata": {},
   "source": [
    "ç©ºé–“ã¨æ™‚é–“æƒ…å ±ã‚’æ‰ãˆã‚‹ãŸã‚ã«ã€3D RoPEã‚’æ¡ç”¨\n",
    "\n",
    "ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã¯ã€2ã¤ã®äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’æ¡ç”¨:\n",
    "\n",
    "- T5-XXL: è¤‡é›‘ãªãƒ†ã‚­ã‚¹ãƒˆã®æ„å‘³ã‚’æ‰ãˆã‚‹\n",
    "- CLIP-Large: ãƒ†ã‚­ã‚¹ãƒˆã¨è¦–è¦šæ¦‚å¿µã®æ•´åˆæ€§ã‚’æ‰ãˆã‚‹ï¼ˆ=æŒ‡ç¤ºè¿½å¾“æ€§ã‚’é«˜ã‚ã‚‹ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6548ce9c",
   "metadata": {},
   "source": [
    "![](image/table2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baaba03",
   "metadata": {},
   "source": [
    "## å®Ÿè£…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf13bf8",
   "metadata": {},
   "source": [
    "- image.py: ç”»åƒã®ã¿ã§å­¦ç¿’ã€‚\n",
    "- stage1.py: 256pxè§£åƒåº¦ã®å‹•ç”»ã§å­¦ç¿’ã€‚\n",
    "- stage2.py: 768pxè§£åƒåº¦ã®å‹•ç”»ã§å­¦ç¿’ï¼ˆã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä¸¦åˆ—åŒ–ã‚’ä½¿ç”¨ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯4ï¼‰ã€‚\n",
    "- stage1_i2v.py: 256pxè§£åƒåº¦ã§T2Vï¼ˆãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å‹•ç”»ï¼‰ã¨I2Vï¼ˆç”»åƒã‹ã‚‰å‹•ç”»ï¼‰ã‚’å­¦ç¿’ã€‚\n",
    "- stage2_i2v.py: 768pxè§£åƒåº¦ã§T2Vã¨I2Vã‚’å­¦ç¿’ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e895226e",
   "metadata": {},
   "source": [
    "## ç’°å¢ƒæ§‹ç¯‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9f1318",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd /workspaces/open-sora/Open-Sora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52ec244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import platform\n",
    "import subprocess\n",
    "\n",
    "if os.path.exists(\"debug.log\"):\n",
    "    os.remove(\"debug.log\")\n",
    "\n",
    "def custom_format(record):\n",
    "    match record.levelno:\n",
    "        case logging.DEBUG:\n",
    "            level = \"ğŸŸ¦\"\n",
    "        case logging.INFO:\n",
    "            level = \"ğŸŸ©\"\n",
    "        case logging.WARNING:\n",
    "            level = \"ğŸŸ¨\"\n",
    "        case logging.ERROR:\n",
    "            level = \"ğŸŸ¥\"\n",
    "        case logging.CRITICAL:\n",
    "            level = \"ğŸ›‘\"\n",
    "    return f\"{level} {record.getMessage()}\"\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "for handler in logger.handlers:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "formatter = logging.Formatter()\n",
    "formatter.format = custom_format\n",
    "\n",
    "file_handler = logging.FileHandler(\"debug.log\")\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "PYTHON_VERSION = platform.python_version()\n",
    "logger.info(f\"Python {PYTHON_VERSION}\")\n",
    "\n",
    "NVIDIA_SMI = subprocess.run(\"nvidia-smi\", capture_output=True, text=True).stdout\n",
    "logger.info(f\"{NVIDIA_SMI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57815940",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    %pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "    %pip install \\\n",
    "        accelerate \\\n",
    "        av \\\n",
    "        colossalai \\\n",
    "        ftfy \\\n",
    "        liger-kernel \\\n",
    "        omegaconf \\\n",
    "        mmengine \\\n",
    "        openai \\\n",
    "        pandas \\\n",
    "        pandarallel \\\n",
    "        pyarrow \\\n",
    "        tensorboard \\\n",
    "        wandb \\\n",
    "        --extra-index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "    %pip install flash-attn --no-build-isolation\n",
    "\n",
    "    %pip install -e . --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a745e96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from einops import rearrange\n",
    "from flash_attn import flash_attn_func as flash_attn_func_v2\n",
    "from functools import partial\n",
    "from huggingface_hub import PyTorchModelHubMixin\n",
    "from inspect import signature\n",
    "from liger_kernel.ops.rms_norm import LigerRMSNormFunction\n",
    "from liger_kernel.ops.rope import LigerRopeFunction\n",
    "from omegaconf import MISSING, OmegaConf\n",
    "from torch import Tensor, nn\n",
    "from torch.nn.modules.batchnorm import _BatchNorm\n",
    "from typing import Any, Callable, Optional, Union, Tuple\n",
    "import diffusers\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "try:\n",
    "    from flash_attn_interface import flash_attn_func as flash_attn_func_v3\n",
    "    SUPPORT_FA3 = True\n",
    "except:\n",
    "    SUPPORT_FA3 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14a0a26",
   "metadata": {},
   "source": [
    "## DC-AE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c2122b",
   "metadata": {},
   "source": [
    "### ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c352dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val2list(x: Union[list, tuple, Any], repeat_time=1) -> list:\n",
    "    \"\"\"\n",
    "    å€¤ã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        x (Union[list, tuple, Any]): å¤‰æ›ã™ã‚‹å€¤\n",
    "        repeat_time (int, optional): xãŒãƒªã‚¹ãƒˆã‚„ã‚¿ãƒ—ãƒ«ã§ãªã„å ´åˆã®ç¹°ã‚Šè¿”ã—å›æ•°\n",
    "    Returns:\n",
    "        list: å¤‰æ›å¾Œã®ãƒªã‚¹ãƒˆ\n",
    "    \"\"\"\n",
    "\n",
    "    # ãƒªã‚¹ãƒˆã‹ã‚¿ãƒ—ãƒ«ã®å ´åˆ\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return list(x)\n",
    "\n",
    "    # ãã‚Œä»¥å¤–ã®å ´åˆ\n",
    "    return [x for _ in range(repeat_time)]\n",
    "\n",
    "# æ¤œè¨¼\n",
    "val2list([1,2,3]), val2list((4,5)), val2list(5, repeat_time=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ed0c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val2tuple(x: Union[list, tuple, Any], min_len: int = 1, idx_repeat: int = -1) -> tuple:\n",
    "    \"\"\"\n",
    "    å€¤ã‚’ã‚¿ãƒ—ãƒ«ã«å¤‰æ›ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        x (Union[list, tuple, Any]): å¤‰æ›ã™ã‚‹å€¤\n",
    "        min_len (int, optional): ã‚¿ãƒ—ãƒ«ã®æœ€å°é•·ã•\n",
    "        idx_repeat (int, optional): ç¹°ã‚Šè¿”ã—æŒ¿å…¥ã™ã‚‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "    Returns:\n",
    "        tuple: å¤‰æ›å¾Œã®ã‚¿ãƒ—ãƒ«\n",
    "    \"\"\"\n",
    "\n",
    "    # å€¤ã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›\n",
    "    x = val2list(x)\n",
    "\n",
    "    # å¿…è¦ã«å¿œã˜ã¦è¦ç´ ã‚’ç¹°ã‚Šè¿”ã—\n",
    "    if len(x) > 0:\n",
    "        x[idx_repeat:idx_repeat] = [\n",
    "            x[idx_repeat] for _ in range(min_len - len(x))\n",
    "        ]\n",
    "\n",
    "    return tuple(x)\n",
    "\n",
    "# æ¤œè¨¼\n",
    "val2tuple(False, 2), val2tuple((None, \"bnb2d\"), 2), val2tuple((None, None), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fd9c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_kwargs_from_config(config: dict, target_func: Callable) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    è¨­å®šã‹ã‚‰é–¢æ•°ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¼•æ•°ã‚’æ§‹ç¯‰ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        config (dict): è¨­å®šè¾æ›¸\n",
    "        target_func (Callable): å¯¾è±¡ã®é–¢æ•°\n",
    "    Returns:\n",
    "        dict[str, Any]: é–¢æ•°ã«æ¸¡ã™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¼•æ•°\n",
    "    \"\"\"\n",
    "    logger.info(f\"build_kwargs_from_config {config=} {target_func=}\")\n",
    "\n",
    "    valid_keys = list(signature(target_func).parameters)\n",
    "    kwargs = {}\n",
    "    for key in config:\n",
    "        if key in valid_keys:\n",
    "            kwargs[key] = config[key]\n",
    "    return kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8397c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_same_padding(kernel_size: Union[int, tuple[int, ...]]) -> Union[int, tuple[int, ...]]:\n",
    "    \"\"\"\n",
    "    ã‚«ãƒ¼ãƒãƒ«ã‚µã‚¤ã‚ºã«åŸºã¥ã„ã¦åŒã˜ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¨ˆç®—ã™ã‚‹\n",
    "    ConvLayer, LiteMLAã§ä½¿ç”¨\n",
    "\n",
    "    Args:\n",
    "        kernel_size (Union[int, tuple[int, ...]]): ã‚«ãƒ¼ãƒãƒ«ã‚µã‚¤ã‚º\n",
    "    Returns:\n",
    "        Union[int, tuple[int, ...]]: åŒã˜ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚µã‚¤ã‚º\n",
    "    \"\"\"\n",
    "    if isinstance(kernel_size, tuple):\n",
    "        return tuple([get_same_padding(ks) for ks in kernel_size])\n",
    "    else:\n",
    "        assert kernel_size % 2 > 0, \"kernel size should be odd number\"\n",
    "        return kernel_size // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68670c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_grad_checkpoint(module, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    è‡ªå‹•å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆé–¢æ•°\n",
    "    å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒæœ‰åŠ¹ãªå ´åˆã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹\n",
    "    ãã†ã§ãªã„å ´åˆã€é€šå¸¸ã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã‚’å®Ÿè¡Œã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        module: nn.Moduleã¾ãŸã¯nn.Moduleã®Iterable\n",
    "        *args: ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¸ã®å¼•æ•°\n",
    "        **kwargs: ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¸ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¼•æ•°\n",
    "    Returns:\n",
    "        ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å‡ºåŠ›\n",
    "    \"\"\"\n",
    "    logger.info(f\"auto_grad_checkpoint {len(args)=} {args[0].shape=} {kwargs=}\")\n",
    "    if getattr(module, \"grad_checkpointing\", False):\n",
    "        if not isinstance(module, Iterable):\n",
    "            return checkpoint(module, *args, use_reentrant=True, **kwargs)\n",
    "        gc_step = module[0].grad_checkpointing_step\n",
    "        return checkpoint_sequential(module, gc_step, *args, use_reentrant=False, **kwargs)\n",
    "    return module(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fcbe88",
   "metadata": {},
   "source": [
    "### ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒªã‚¹ãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaebc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpSequential(nn.Module):\n",
    "    \"\"\"\n",
    "    è¤‡æ•°ã®æ“ä½œã‚’é †æ¬¡é©ç”¨ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    "    build_encoder_project_out_block, Encoder\n",
    "    build_decoder_project_out_block, Decoder\n",
    "    \"\"\"\n",
    "    def __init__(self, op_list: list[Optional[nn.Module]]):\n",
    "        super().__init__()\n",
    "        valid_op_list = []\n",
    "        for op in op_list:\n",
    "            if op is not None:\n",
    "                valid_op_list.append(op)\n",
    "        self.op_list = nn.ModuleList(valid_op_list)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for op in self.op_list:\n",
    "            x = op(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cd1bca",
   "metadata": {},
   "source": [
    "### æ’ç­‰å±¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f403bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    æ’ç­‰å±¤\n",
    "    build_block, EfficientViTBlockã§ä½¿ç”¨\n",
    "    \"\"\"\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5208b6",
   "metadata": {},
   "source": [
    "### æ­£è¦åŒ–å±¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f3527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register normalization function here\n",
    "REGISTERED_NORM_DICT: dict[str, type] = {\n",
    "    \"bn2d\": nn.BatchNorm2d,\n",
    "    \"ln\": nn.LayerNorm,\n",
    "    # \"ln2d\": LayerNorm2d,\n",
    "    # \"rms2d\": RMSNorm2d,\n",
    "    # \"rms3d\": RMSNorm3d,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4289a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_norm(name=\"bn2d\", num_features=None, **kwargs) -> Optional[nn.Module]:\n",
    "    \"\"\"\n",
    "    æ­£è¦åŒ–å±¤ã‚’æ§‹ç¯‰ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        name (str, optional): æ­£è¦åŒ–å±¤ã®åå‰\n",
    "        num_features (int, optional): æ­£è¦åŒ–å±¤ã®ç‰¹å¾´æ•°\n",
    "        **kwargs: æ­£è¦åŒ–å±¤ã®è¿½åŠ å¼•æ•°\n",
    "    Returns:\n",
    "        Optional[nn.Module]: æ§‹ç¯‰ã•ã‚ŒãŸæ­£è¦åŒ–å±¤ã€ã¾ãŸã¯None\n",
    "    \"\"\"\n",
    "    logger.info(f\"build_norm {name=} {num_features=} {kwargs=}\")\n",
    "\n",
    "    if name in [\"ln\", \"ln2d\"]:\n",
    "        kwargs[\"normalized_shape\"] = num_features\n",
    "    else:\n",
    "        kwargs[\"num_features\"] = num_features\n",
    "    if name in REGISTERED_NORM_DICT:\n",
    "        norm_cls = REGISTERED_NORM_DICT[name]\n",
    "        args = build_kwargs_from_config(kwargs, norm_cls)\n",
    "        return norm_cls(**args)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc58644",
   "metadata": {},
   "source": [
    "### æ´»æ€§åŒ–é–¢æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e510b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGISTERED_ACT_DICT: dict[str, type] = {\n",
    "    \"relu\": nn.ReLU,\n",
    "    \"relu6\": nn.ReLU6,\n",
    "    \"hswish\": nn.Hardswish,\n",
    "    \"silu\": nn.SiLU,\n",
    "    \"gelu\": partial(nn.GELU, approximate=\"tanh\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e410569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_act(name: str, **kwargs) -> Optional[nn.Module]:\n",
    "    \"\"\"\n",
    "    æ´»æ€§åŒ–é–¢æ•°ã‚’æ§‹ç¯‰ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        name (str): æ´»æ€§åŒ–é–¢æ•°ã®åå‰\n",
    "        **kwargs: æ´»æ€§åŒ–é–¢æ•°ã®è¿½åŠ å¼•æ•°\n",
    "    Returns:\n",
    "        Optional[nn.Module]: æ§‹ç¯‰ã•ã‚ŒãŸæ´»æ€§åŒ–é–¢æ•°ã€ã¾ãŸã¯None\n",
    "    \"\"\"\n",
    "    logger.info(f\"build_act {name=} {kwargs=}\")\n",
    "    if name in REGISTERED_ACT_DICT:\n",
    "        act_cls = REGISTERED_ACT_DICT[name]\n",
    "        args = build_kwargs_from_config(kwargs, act_cls)\n",
    "        return act_cls(**args)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7188dbd0",
   "metadata": {},
   "source": [
    "### ç•³ã¿è¾¼ã¿å±¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca76b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelChunkConv3d(nn.Conv3d):\n",
    "    \"\"\"\n",
    "    ãƒãƒ£ãƒ³ãƒãƒ«ã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¦Conv3dã‚’å®Ÿè¡Œã™ã‚‹ã‚¯ãƒ©ã‚¹\n",
    "    ConvLayerã¨LiteMLAã§ä½¿ç”¨\n",
    "    \"\"\"\n",
    "\n",
    "    CONV3D_NUMEL_LIMIT = 2**31\n",
    "\n",
    "    def _get_output_numel(self, input_shape: torch.Size) -> int:\n",
    "        numel = self.out_channels\n",
    "        if len(input_shape) == 5:\n",
    "            numel *= input_shape[0]\n",
    "        for i, d in enumerate(input_shape[-3:]):\n",
    "            d_out = math.floor(\n",
    "                (d + 2 * self.padding[i] - self.dilation[i] * (self.kernel_size[i] - 1) - 1) / self.stride[i] + 1\n",
    "            )\n",
    "            numel *= d_out\n",
    "        return numel\n",
    "\n",
    "    def _get_n_chunks(self, numel: int, n_channels: int):\n",
    "        n_chunks = math.ceil(numel / ChannelChunkConv3d.CONV3D_NUMEL_LIMIT)\n",
    "        n_chunks = ceil_to_divisible(n_chunks, n_channels)\n",
    "        return n_chunks\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã«å¯¾ã—ã¦ãƒãƒ£ãƒ³ã‚¯åŒ–ã•ã‚ŒãŸConv3dã‚’å®Ÿè¡Œã™ã‚‹\n",
    "        \"\"\"\n",
    "        logger.info(f\"ChannelChunkConv3d.forward {input.shape=}\")\n",
    "        if input.numel() // input.size(0) < ChannelChunkConv3d.CONV3D_NUMEL_LIMIT:\n",
    "            return super().forward(input)\n",
    "        n_in_chunks = self._get_n_chunks(input.numel(), self.in_channels)\n",
    "        n_out_chunks = self._get_n_chunks(self._get_output_numel(input.shape), self.out_channels)\n",
    "        if n_in_chunks == 1 and n_out_chunks == 1:\n",
    "            return super().forward(input)\n",
    "        outputs = []\n",
    "        input_shards = input.chunk(n_in_chunks, dim=1)\n",
    "        for weight, bias in zip(self.weight.chunk(n_out_chunks), self.bias.chunk(n_out_chunks)):\n",
    "            weight_shards = weight.chunk(n_in_chunks, dim=1)\n",
    "            o = None\n",
    "            for x, w in zip(input_shards, weight_shards):\n",
    "                if o is None:\n",
    "                    o = F.conv3d(x, w, bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "                else:\n",
    "                    o += F.conv3d(x, w, None, self.stride, self.padding, self.dilation, self.groups)\n",
    "            outputs.append(o)\n",
    "        return torch.cat(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1339b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    2Dã¾ãŸã¯3Dç•³ã¿è¾¼ã¿å±¤ã€æ­£è¦åŒ–ã€æ´»æ€§åŒ–é–¢æ•°ã‚’çµ„ã¿åˆã‚ã›ãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    "    - build_downsample_block\n",
    "    - build_encoder_project_in_block\n",
    "    - GLUMBConv\n",
    "    - InterpolateConvUpSamplerLayer\n",
    "    - LiteMLA\n",
    "    - ResBlock\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size=3, stride=1, dilation=1, groups=1, use_bias=False, dropout=0, norm=\"bn2d\", act_func=\"relu\", is_video=False, pad_mode_3d=\"constant\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): å…¥åŠ›ãƒãƒ£ãƒãƒ«æ•°\n",
    "            out_channels (int): å‡ºåŠ›ãƒãƒ£ãƒãƒ«æ•°\n",
    "            kernel_size (int, optional): ç•³ã¿è¾¼ã¿ã‚«ãƒ¼ãƒãƒ«ã®ã‚µã‚¤ã‚º\n",
    "            stride (int, optional): ç•³ã¿è¾¼ã¿ã®ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰\n",
    "            dilation (int, optional): ç•³ã¿è¾¼ã¿ã®ãƒ€ã‚¤ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "            groups (int, optional): ç•³ã¿è¾¼ã¿ã®ã‚°ãƒ«ãƒ¼ãƒ—æ•°\n",
    "            use_bias (bool, optional): ãƒã‚¤ã‚¢ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "            dropout (float, optional): ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡\n",
    "            norm (str, optional): æ­£è¦åŒ–å±¤ã®ç¨®é¡\n",
    "            act_func (str, optional): æ´»æ€§åŒ–é–¢æ•°ã®ç¨®é¡\n",
    "            is_video (bool, optional): 3Dç•³ã¿è¾¼ã¿ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "            pad_mode_3d (str, optional): 3Dç•³ã¿è¾¼ã¿ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰\n",
    "        \"\"\"\n",
    "        logger.info(f\"ConvLayer.__init__ {in_channels=} {out_channels=} {kernel_size=} {stride=} {dilation=} {groups=} {use_bias=} {dropout=} {norm=} {act_func=} {is_video=} {pad_mode_3d=}\")\n",
    "\n",
    "        super().__init__()\n",
    "        self.is_video = is_video\n",
    "\n",
    "        if self.is_video:\n",
    "            assert dilation == 1, \"only support dilation=1 for 3d conv\"\n",
    "            assert kernel_size % 2 == 1, \"only support odd kernel size for 3d conv\"\n",
    "            self.pad_mode_3d = pad_mode_3d  # 3d padding follows CausalConv3d by Hunyuan\n",
    "            # padding = (\n",
    "            #     kernel_size // 2,\n",
    "            #     kernel_size // 2,\n",
    "            #     kernel_size // 2,\n",
    "            #     kernel_size // 2,\n",
    "            #     kernel_size - 1,\n",
    "            #     0,\n",
    "            # )  # W, H, T\n",
    "            # non-causal padding\n",
    "            padding = (\n",
    "                kernel_size // 2,\n",
    "                kernel_size // 2,\n",
    "                kernel_size // 2,\n",
    "                kernel_size // 2,\n",
    "                kernel_size // 2,\n",
    "                kernel_size // 2,\n",
    "            )\n",
    "            self.padding = padding\n",
    "            self.dropout = nn.Dropout3d(dropout, inplace=False) if dropout > 0 else None\n",
    "            assert isinstance(stride, (int, tuple)), \"stride must be an integer or 3-tuple for 3d conv\"\n",
    "            self.conv = ChannelChunkConv3d(  # padding is handled by F.pad() in forward()\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=(kernel_size, kernel_size, kernel_size),\n",
    "                stride=(stride, stride, stride) if isinstance(stride, int) else stride,\n",
    "                groups=groups,\n",
    "                bias=use_bias,\n",
    "            )\n",
    "        else:\n",
    "            padding = get_same_padding(kernel_size)\n",
    "            padding *= dilation\n",
    "            self.dropout = nn.Dropout2d(dropout, inplace=False) if dropout > 0 else None\n",
    "            self.conv = nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=(kernel_size, kernel_size),\n",
    "                stride=(stride, stride),\n",
    "                padding=padding,\n",
    "                dilation=(dilation, dilation),\n",
    "                groups=groups,\n",
    "                bias=use_bias,\n",
    "            )\n",
    "\n",
    "        self.norm = build_norm(norm, num_features=out_channels)\n",
    "        self.act = build_act(act_func)\n",
    "        self.pad = F.pad\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        Returns:\n",
    "            torch.Tensor: å‡ºåŠ›ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        \"\"\"\n",
    "        logger.info(f\"ConvLayer.forward {x.shape=}\")\n",
    "        if self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "        if self.is_video:  # custom padding for 3d conv\n",
    "            x = self.pad(x, self.padding, mode=self.pad_mode_3d)  # \"constant\" padding defaults to 0\n",
    "        x = self.conv(x)\n",
    "        if self.norm:\n",
    "            x = self.norm(x)\n",
    "        if self.act:\n",
    "            x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8478f9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLUMBConv(nn.Module):\n",
    "    \"\"\"\n",
    "    GLUã‚’ç”¨ã„ãŸMBConvãƒ–ãƒ­ãƒƒã‚¯\n",
    "    EfficientViTBlockã§ä½¿ç”¨\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size=3, stride=1, mid_channels=None, expand_ratio=6, use_bias=False, norm=(None, None, \"ln2d\"), act_func=(\"silu\", \"silu\", None), is_video=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): å…¥åŠ›ãƒãƒ£ãƒãƒ«æ•°\n",
    "            out_channels (int): å‡ºåŠ›ãƒãƒ£ãƒãƒ«æ•°\n",
    "            kernel_size (int, optional): ç•³ã¿è¾¼ã¿ã‚«ãƒ¼ãƒãƒ«ã®ã‚µã‚¤ã‚º\n",
    "            stride (int, optional): ç•³ã¿è¾¼ã¿ã®ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰\n",
    "            mid_channels (int, optional): ä¸­é–“ãƒãƒ£ãƒãƒ«æ•°\n",
    "            expand_ratio (float, optional): æ‹¡å¼µæ¯”ç‡\n",
    "            use_bias (bool or tuple, optional): ãƒã‚¤ã‚¢ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "            norm (str or tuple, optional): æ­£è¦åŒ–å±¤ã®ç¨®é¡\n",
    "            act_func (str or tuple, optional): æ´»æ€§åŒ–é–¢æ•°ã®ç¨®é¡\n",
    "            is_video (bool, optional): 3Dç•³ã¿è¾¼ã¿ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "        \"\"\"\n",
    "        logger.info(f\"GLUMBConv.__init__ {in_channels=} {out_channels=} {kernel_size=} {stride=} {mid_channels=} {expand_ratio=} {use_bias=} {norm=} {act_func=} {is_video=}\")\n",
    "\n",
    "        super().__init__()\n",
    "        use_bias = val2tuple(use_bias, 3)\n",
    "        norm = val2tuple(norm, 3)\n",
    "        act_func = val2tuple(act_func, 3)\n",
    "\n",
    "        mid_channels = round(in_channels * expand_ratio) if mid_channels is None else mid_channels\n",
    "\n",
    "        self.glu_act = build_act(act_func[1], inplace=False)\n",
    "        self.inverted_conv = ConvLayer(\n",
    "            in_channels,\n",
    "            mid_channels * 2,\n",
    "            1,\n",
    "            use_bias=use_bias[0],\n",
    "            norm=norm[0],\n",
    "            act_func=act_func[0],\n",
    "            is_video=is_video,\n",
    "        )\n",
    "        self.depth_conv = ConvLayer(\n",
    "            mid_channels * 2,\n",
    "            mid_channels * 2,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            groups=mid_channels * 2,\n",
    "            use_bias=use_bias[1],\n",
    "            norm=norm[1],\n",
    "            act_func=None,\n",
    "            is_video=is_video,\n",
    "        )\n",
    "        self.point_conv = ConvLayer(\n",
    "            mid_channels,\n",
    "            out_channels,\n",
    "            1,\n",
    "            use_bias=use_bias[2],\n",
    "            norm=norm[2],\n",
    "            act_func=act_func[2],\n",
    "            is_video=is_video,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logger.info(f\"GLUMBConv.forward {x.shape=}\")\n",
    "        x = self.inverted_conv(x)\n",
    "        x = self.depth_conv(x)\n",
    "\n",
    "        x, gate = torch.chunk(x, 2, dim=1)\n",
    "        gate = self.glu_act(gate)\n",
    "        x = x * gate\n",
    "\n",
    "        x = self.point_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aecd9ea",
   "metadata": {},
   "source": [
    "### ãƒ”ã‚¯ã‚»ãƒ«ã‚·ãƒ£ãƒƒãƒ•ãƒªãƒ³ã‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02842da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelUnshuffleChannelAveragingDownSampleLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    ãƒ”ã‚¯ã‚»ãƒ«ã‚¢ãƒ³ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã¨ãƒãƒ£ãƒãƒ«å¹³å‡åŒ–ã«ã‚ˆã‚‹ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å±¤\n",
    "    - build_downsample_block\n",
    "    - build_encoder_project_out_block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, factor: int, temporal_downsample: bool = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): å…¥åŠ›ãƒãƒ£ãƒãƒ«æ•°\n",
    "            out_channels (int): å‡ºåŠ›ãƒãƒ£ãƒãƒ«æ•°\n",
    "            factor (int): ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼\n",
    "            temporal_downsample (bool, optional):\n",
    "                5Då…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã«å¯¾ã—ã¦æ™‚é–“æ–¹å‘ã®ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’è¡Œã†ã‹ã©ã†ã‹\n",
    "        \"\"\"\n",
    "        logger.info(f\"PixelUnshuffleChannelAveragingDownSampleLayer.__init__ {in_channels=} {out_channels=} {factor=} {temporal_downsample=}\")\n",
    "\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.factor = factor\n",
    "        self.temporal_downsample = temporal_downsample\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logger.info(f\"PixelUnshuffleChannelAveragingDownSampleLayer.forward {x.shape=}\")\n",
    "\n",
    "        if x.dim() == 4:\n",
    "            assert self.in_channels * self.factor**2 % self.out_channels == 0\n",
    "            group_size = self.in_channels * self.factor**2 // self.out_channels\n",
    "            x = F.pixel_unshuffle(x, self.factor)\n",
    "            B, C, H, W = x.shape\n",
    "            x = x.view(B, self.out_channels, group_size, H, W)\n",
    "            x = x.mean(dim=2)\n",
    "        elif x.dim() == 5:  # [B, C, T, H, W]\n",
    "            _, _, T, _, _ = x.shape\n",
    "            if self.temporal_downsample and T != 1:  # 3d pixel unshuffle\n",
    "                x = pixel_unshuffle_3d(x, self.factor)\n",
    "                assert self.in_channels * self.factor**3 % self.out_channels == 0\n",
    "                group_size = self.in_channels * self.factor**3 // self.out_channels\n",
    "            else:  # 2d pixel unshuffle\n",
    "                x = x.permute(0, 2, 1, 3, 4)  # [B, T, C, H, W]\n",
    "                x = F.pixel_unshuffle(x, self.factor)\n",
    "                x = x.permute(0, 2, 1, 3, 4)  # [B, C, T, H, W]\n",
    "                assert self.in_channels * self.factor**2 % self.out_channels == 0\n",
    "                group_size = self.in_channels * self.factor**2 // self.out_channels\n",
    "            B, C, T, H, W = x.shape\n",
    "            x = x.view(B, self.out_channels, group_size, T, H, W)\n",
    "            x = x.mean(dim=2)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported input dimension: {x.dim()}\")\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"PixelUnshuffleChannelAveragingDownSampleLayer(in_channels={self.in_channels}, out_channels={self.out_channels}, factor={self.factor}), temporal_downsample={self.temporal_downsample}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfa12d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelDuplicatingPixelShuffleUpSampleLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    ãƒãƒ£ãƒ³ãƒãƒ«è¤‡è£½ã¨ãƒ”ã‚¯ã‚»ãƒ«ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã«ã‚ˆã‚‹ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å±¤\n",
    "    build_upsample_block, build_decoder_project_in_blockã§ä½¿ç”¨\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, factor: int, temporal_upsample: bool = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): å…¥åŠ›ãƒãƒ£ãƒãƒ«æ•°\n",
    "            out_channels (int): å‡ºåŠ›ãƒãƒ£ãƒãƒ«æ•°\n",
    "            factor (int): ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼\n",
    "            temporal_upsample (bool, optional):\n",
    "                5Då…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã«å¯¾ã—ã¦æ™‚é–“æ–¹å‘ã®ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’è¡Œã†ã‹ã©ã†ã‹\n",
    "        \"\"\"\n",
    "        logger.info(f\"ChannelDuplicatingPixelShuffleUpSampleLayer.__init__ {in_channels=} {out_channels=} {factor=} {temporal_upsample=}\")\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.factor = factor\n",
    "        assert out_channels * factor**2 % in_channels == 0\n",
    "        self.temporal_upsample = temporal_upsample\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logger.info(f\"ChannelDuplicatingPixelShuffleUpSampleLayer.forward {x.shape=}\")\n",
    "        if x.dim() == 5:\n",
    "            B, C, T, H, W = x.shape\n",
    "            assert C == self.in_channels\n",
    "\n",
    "        if self.temporal_upsample and T != 1:  # video input\n",
    "            repeats = self.out_channels * self.factor**3 // self.in_channels\n",
    "        else:\n",
    "            repeats = self.out_channels * self.factor**2 // self.in_channels\n",
    "\n",
    "        x = x.repeat_interleave(repeats, dim=1)\n",
    "\n",
    "        if x.dim() == 4:  # original image-only training\n",
    "            x = F.pixel_shuffle(x, self.factor)\n",
    "        elif x.dim() == 5:  # [B, C, T, H, W]\n",
    "            if self.temporal_upsample and T != 1:  # video input\n",
    "                x = pixel_shuffle_3d(x, self.factor)\n",
    "            else:\n",
    "                x = x.permute(0, 2, 1, 3, 4)  # [B, T, C, H, W]\n",
    "                x = F.pixel_shuffle(x, self.factor)  # on H and W only\n",
    "                x = x.permute(0, 2, 1, 3, 4)  # [B, C, T, H, W]\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"ChannelDuplicatingPixelShuffleUpSampleLayer(in_channels={self.in_channels}, out_channels={self.out_channels}, factor={self.factor}, temporal_upsample={self.temporal_upsample})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53173154",
   "metadata": {},
   "source": [
    "### InterpolateConvUpSamplerLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04050c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "VERBOSE = False\n",
    "\n",
    "def chunked_interpolate(x, scale_factor, mode=\"nearest\"):\n",
    "    \"\"\"\n",
    "    ãƒãƒ£ãƒ³ãƒãƒ«ã®æ¬¡å…ƒã«æ²¿ã£ã¦ãƒãƒ£ãƒ³ã‚¯åŒ–ã—ã¦å¤§ããªãƒ†ãƒ³ã‚½ãƒ«ã‚’è£œé–“ã™ã‚‹\n",
    "    ç¾åœ¨ã¯ã€Œnearestã€è£œé–“ãƒ¢ãƒ¼ãƒ‰ã®ã¿ã‚µãƒãƒ¼ãƒˆ\n",
    "\n",
    "    https://discuss.pytorch.org/t/error-using-f-interpolate-for-large-3d-input/207859\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ« (B, C, D, H, W)\n",
    "        scale_factor: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã®ã‚¿ãƒ—ãƒ« (d, h, w)\n",
    "    Returns:\n",
    "        torch.Tensor: è£œé–“ã•ã‚ŒãŸãƒ†ãƒ³ã‚½ãƒ«\n",
    "    \"\"\"\n",
    "    logger.info(f\"chunked_interpolate {x.shape=} {scale_factor=} {mode=}\")\n",
    "\n",
    "    assert (\n",
    "        mode == \"nearest\"\n",
    "    ), \"Only the nearest mode is supported\"  # actually other modes are theoretically supported but not tested\n",
    "    if len(x.shape) != 5:\n",
    "        raise ValueError(\"Expected 5D input tensor (B, C, D, H, W)\")\n",
    "\n",
    "    # Calculate max chunk size to avoid int32 overflow. num_elements < max_int32\n",
    "    # Max int32 is 2^31 - 1\n",
    "    max_elements_per_chunk = 2**31 - 1\n",
    "\n",
    "    # Calculate output spatial dimensions\n",
    "    out_d = math.ceil(x.shape[2] * scale_factor[0])\n",
    "    out_h = math.ceil(x.shape[3] * scale_factor[1])\n",
    "    out_w = math.ceil(x.shape[4] * scale_factor[2])\n",
    "\n",
    "    # Calculate max channels per chunk to stay under limit\n",
    "    elements_per_channel = out_d * out_h * out_w\n",
    "    max_channels = max_elements_per_chunk // (x.shape[0] * elements_per_channel)\n",
    "\n",
    "    # Use smaller of max channels or input channels\n",
    "    chunk_size = min(max_channels, x.shape[1])\n",
    "\n",
    "    # Ensure at least 1 channel per chunk\n",
    "    chunk_size = max(1, chunk_size)\n",
    "    if VERBOSE:\n",
    "        print(f\"Input channels: {x.shape[1]}\")\n",
    "        print(f\"Chunk size: {chunk_size}\")\n",
    "        print(f\"max_channels: {max_channels}\")\n",
    "        print(f\"num_chunks: {math.ceil(x.shape[1] / chunk_size)}\")\n",
    "\n",
    "    chunks = []\n",
    "    for i in range(0, x.shape[1], chunk_size):\n",
    "        start_idx = i\n",
    "        end_idx = min(i + chunk_size, x.shape[1])\n",
    "\n",
    "        chunk = x[:, start_idx:end_idx, :, :, :]\n",
    "\n",
    "        interpolated_chunk = F.interpolate(chunk, scale_factor=scale_factor, mode=\"nearest\")\n",
    "\n",
    "        chunks.append(interpolated_chunk)\n",
    "\n",
    "    if not chunks:\n",
    "        raise ValueError(f\"No chunks were generated. Input shape: {x.shape}\")\n",
    "\n",
    "    # Concatenate chunks along channel dimension\n",
    "    return torch.cat(chunks, dim=1)\n",
    "\n",
    "class InterpolateConvUpSampleLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    è£œé–“ã¨ç•³ã¿è¾¼ã¿ã‚’çµ„ã¿åˆã‚ã›ãŸã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å±¤\n",
    "    build_upsample_blockã§ä½¿ç”¨\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, factor: int, mode: str = \"nearest\", is_video: bool = False, temporal_upsample: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): å…¥åŠ›ãƒãƒ£ãƒãƒ«æ•°\n",
    "            out_channels (int): å‡ºåŠ›ãƒãƒ£ãƒãƒ«æ•°\n",
    "            kernel_size (int): ç•³ã¿è¾¼ã¿ã‚«ãƒ¼ãƒãƒ«ã®ã‚µã‚¤ã‚º\n",
    "            factor (int): ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼\n",
    "            mode (str, optional): è£œé–“ãƒ¢ãƒ¼ãƒ‰\n",
    "            is_video (bool, optional): 3Dç•³ã¿è¾¼ã¿ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "            temporal_upsample (bool, optional):\n",
    "                5Då…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã«å¯¾ã—ã¦æ™‚é–“æ–¹å‘ã®ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’è¡Œã†ã‹ã©ã†ã‹\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.factor = factor\n",
    "        self.mode = mode\n",
    "        self.temporal_upsample = temporal_upsample\n",
    "        self.conv = ConvLayer(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            use_bias=True,\n",
    "            norm=None,\n",
    "            act_func=None,\n",
    "            is_video=is_video,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logger.info(f\"InterpolateConvUpSampleLayer.forward {x.shape=}\")\n",
    "        if x.dim() == 4:\n",
    "            x = F.interpolate(x, scale_factor=self.factor, mode=self.mode)\n",
    "        elif x.dim() == 5:\n",
    "            # [B, C, T, H, W] -> [B, C, T*factor, H*factor, W*factor]\n",
    "            if self.temporal_upsample and x.size(2) != 1:  # temporal upsample for video input\n",
    "                x = chunked_interpolate(x, scale_factor=[self.factor, self.factor, self.factor], mode=self.mode)\n",
    "            else:\n",
    "                x = chunked_interpolate(x, scale_factor=[1, self.factor, self.factor], mode=self.mode)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"InterpolateConvUpSampleLayer(factor={self.factor}, mode={self.mode}, temporal_upsample={self.temporal_upsample})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ebc7f5",
   "metadata": {},
   "source": [
    "### ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ©Ÿæ§‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2013d84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiteMLA(nn.Module):\n",
    "    \"\"\"\n",
    "    è»½é‡ãªãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ç·šå½¢ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ©Ÿæ§‹\n",
    "    EfficientViTBlockã§ä½¿ç”¨\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, heads: Optional[int] = None, heads_ratio: float = 1.0, dim=8, use_bias=False, norm=(None, \"bn2d\"), act_func=(None, None), kernel_func=\"relu\", scales: tuple[int, ...] = (5,), eps=1.0e-15, is_video=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): å…¥åŠ›ãƒãƒ£ãƒãƒ«æ•°\n",
    "            out_channels (int): å‡ºåŠ›ãƒãƒ£ãƒãƒ«æ•°\n",
    "            heads (Optional[int], optional): ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰æ•°\n",
    "            heads_ratio (float, optional): ãƒ˜ãƒƒãƒ‰æ•°ã®æ¯”ç‡\n",
    "            dim (int, optional): å„ãƒ˜ãƒƒãƒ‰ã®æ¬¡å…ƒæ•°\n",
    "            use_bias (bool or tuple, optional): ãƒã‚¤ã‚¢ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "            norm (str or tuple, optional): æ­£è¦åŒ–å±¤ã®ç¨®é¡\n",
    "            act_func (str or tuple, optional): æ´»æ€§åŒ–é–¢æ•°ã®ç¨®é¡\n",
    "            kernel_func (str, optional): ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ã®ç¨®é¡\n",
    "            scales (tuple[int, ...], optional): ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ç•³ã¿è¾¼ã¿ã®ã‚¹ã‚±ãƒ¼ãƒ«\n",
    "            eps (float, optional): æ•°å€¤å®‰å®šæ€§ã®ãŸã‚ã®å°ã•ãªå€¤\n",
    "            is_video (bool, optional): 3Dç•³ã¿è¾¼ã¿ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "        \"\"\"\n",
    "        logger.info(f\"LiteMLA.__init__ {in_channels=} {out_channels=} {heads=} {heads_ratio=} {dim=} {use_bias=} {norm=} {act_func=} {kernel_func=} {scales=} {eps=} {is_video=}\")\n",
    "\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        heads = int(in_channels // dim * heads_ratio) if heads is None else heads\n",
    "\n",
    "        total_dim = heads * dim\n",
    "\n",
    "        use_bias = val2tuple(use_bias, 2)\n",
    "        norm = val2tuple(norm, 2)\n",
    "        act_func = val2tuple(act_func, 2)\n",
    "\n",
    "        self.dim = dim\n",
    "        self.qkv = ConvLayer(\n",
    "            in_channels,\n",
    "            3 * total_dim,\n",
    "            1,\n",
    "            use_bias=use_bias[0],\n",
    "            norm=norm[0],\n",
    "            act_func=act_func[0],\n",
    "            is_video=is_video,\n",
    "        )\n",
    "        conv_class = nn.Conv2d if not is_video else ChannelChunkConv3d\n",
    "        self.aggreg = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    conv_class(\n",
    "                        3 * total_dim,\n",
    "                        3 * total_dim,\n",
    "                        scale,\n",
    "                        padding=get_same_padding(scale),\n",
    "                        groups=3 * total_dim,\n",
    "                        bias=use_bias[0],\n",
    "                    ),\n",
    "                    conv_class(3 * total_dim, 3 * total_dim, 1, groups=3 * heads, bias=use_bias[0]),\n",
    "                )\n",
    "                for scale in scales\n",
    "            ]\n",
    "        )\n",
    "        self.kernel_func = build_act(kernel_func, inplace=False)\n",
    "\n",
    "        self.proj = ConvLayer(\n",
    "            total_dim * (1 + len(scales)),\n",
    "            out_channels,\n",
    "            1,\n",
    "            use_bias=use_bias[1],\n",
    "            norm=norm[1],\n",
    "            act_func=act_func[1],\n",
    "            is_video=is_video,\n",
    "        )\n",
    "\n",
    "    @torch.autocast(device_type=\"cuda\", enabled=False)\n",
    "    def relu_linear_att(self, qkv: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        ReLUã‚«ãƒ¼ãƒãƒ«ã‚’ç”¨ã„ãŸç·šå½¢ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³\n",
    "        \"\"\"\n",
    "        logger.info(f\"LiteMLA.relu_linear_att {qkv.shape=}\")\n",
    "\n",
    "        if qkv.ndim == 5:\n",
    "            B, _, T, H, W = list(qkv.size())\n",
    "            is_video = True\n",
    "        else:\n",
    "            B, _, H, W = list(qkv.size())\n",
    "            is_video = False\n",
    "\n",
    "        if qkv.dtype == torch.float16:\n",
    "            qkv = qkv.float()\n",
    "\n",
    "        if qkv.ndim == 4:\n",
    "            qkv = torch.reshape(\n",
    "                qkv,\n",
    "                (\n",
    "                    B,\n",
    "                    -1,\n",
    "                    3 * self.dim,\n",
    "                    H * W,\n",
    "                ),\n",
    "            )\n",
    "        elif qkv.ndim == 5:\n",
    "            qkv = torch.reshape(\n",
    "                qkv,\n",
    "                (\n",
    "                    B,\n",
    "                    -1,\n",
    "                    3 * self.dim,\n",
    "                    H * W * T,\n",
    "                ),\n",
    "            )\n",
    "        q, k, v = (\n",
    "            qkv[:, :, 0 : self.dim],\n",
    "            qkv[:, :, self.dim : 2 * self.dim],\n",
    "            qkv[:, :, 2 * self.dim :],\n",
    "        )\n",
    "\n",
    "        # lightweight linear attention\n",
    "        q = self.kernel_func(q)\n",
    "        k = self.kernel_func(k)\n",
    "\n",
    "        # linear matmul\n",
    "        trans_k = k.transpose(-1, -2)\n",
    "\n",
    "        v = F.pad(v, (0, 0, 0, 1), mode=\"constant\", value=1)\n",
    "        vk = torch.matmul(v, trans_k)\n",
    "        out = torch.matmul(vk, q)\n",
    "        if out.dtype == torch.bfloat16:\n",
    "            out = out.float()\n",
    "        out = out[:, :, :-1] / (out[:, :, -1:] + self.eps)\n",
    "\n",
    "        if not is_video:\n",
    "            out = torch.reshape(out, (B, -1, H, W))\n",
    "        else:\n",
    "            out = torch.reshape(out, (B, -1, T, H, W))\n",
    "        return out\n",
    "\n",
    "    @torch.autocast(device_type=\"cuda\", enabled=False)\n",
    "    def relu_quadratic_att(self, qkv: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Quadratic Attention with ReLU kernel\n",
    "        ä»Šå›ã¯ä½¿ç”¨ã—ãªã„\n",
    "        \"\"\"\n",
    "        B, _, H, W = list(qkv.size())\n",
    "\n",
    "        qkv = torch.reshape(\n",
    "            qkv,\n",
    "            (\n",
    "                B,\n",
    "                -1,\n",
    "                3 * self.dim,\n",
    "                H * W,\n",
    "            ),\n",
    "        )\n",
    "        q, k, v = (\n",
    "            qkv[:, :, 0 : self.dim],\n",
    "            qkv[:, :, self.dim : 2 * self.dim],\n",
    "            qkv[:, :, 2 * self.dim :],\n",
    "        )\n",
    "\n",
    "        q = self.kernel_func(q)\n",
    "        k = self.kernel_func(k)\n",
    "\n",
    "        att_map = torch.matmul(k.transpose(-1, -2), q)  # b h n n\n",
    "        original_dtype = att_map.dtype\n",
    "        if original_dtype in [torch.float16, torch.bfloat16]:\n",
    "            att_map = att_map.float()\n",
    "        att_map = att_map / (torch.sum(att_map, dim=2, keepdim=True) + self.eps)  # b h n n\n",
    "        att_map = att_map.to(original_dtype)\n",
    "        out = torch.matmul(v, att_map)  # b h d n\n",
    "\n",
    "        out = torch.reshape(out, (B, -1, H, W))\n",
    "        return out\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logger.info(f\"LiteMLA.forward {x.shape=}\")\n",
    "        # generate multi-scale q, k, v\n",
    "        qkv = self.qkv(x)\n",
    "        multi_scale_qkv = [qkv]\n",
    "        for op in self.aggreg:\n",
    "            multi_scale_qkv.append(op(qkv))\n",
    "        qkv = torch.cat(multi_scale_qkv, dim=1)\n",
    "\n",
    "        if qkv.ndim == 4:\n",
    "            H, W = list(qkv.size())[-2:]\n",
    "            # num_tokens = H * W\n",
    "        elif qkv.ndim == 5:\n",
    "            _, _, T, H, W = list(qkv.size())\n",
    "            # num_tokens = H * W * T\n",
    "\n",
    "        # if num_tokens > self.dim:\n",
    "        out = self.relu_linear_att(qkv).to(qkv.dtype)\n",
    "        # else:\n",
    "        #     if self.is_video:\n",
    "        #         raise NotImplementedError(\"Video is not supported for quadratic attention\")\n",
    "        #     out = self.relu_quadratic_att(qkv)\n",
    "        out = self.proj(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a570c05",
   "metadata": {},
   "source": [
    "### ResidualBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9395c0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    æ®‹å·®æ¥ç¶šã‚’æŒã¤ãƒ–ãƒ­ãƒƒã‚¯\n",
    "    EfficientViTBlockã§ä½¿ç”¨\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        main: Optional[nn.Module],\n",
    "        shortcut: Optional[nn.Module],\n",
    "        post_act=None,\n",
    "        pre_norm: Optional[nn.Module] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pre_norm = pre_norm\n",
    "        self.main = main\n",
    "        self.shortcut = shortcut\n",
    "        self.post_act = build_act(post_act)\n",
    "\n",
    "    def forward_main(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.pre_norm is None:\n",
    "            return self.main(x)\n",
    "        else:\n",
    "            return self.main(self.pre_norm(x))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.main is None:\n",
    "            res = x\n",
    "        elif self.shortcut is None:\n",
    "            res = self.forward_main(x)\n",
    "        else:\n",
    "            res = self.forward_main(x) + self.shortcut(x)\n",
    "            if self.post_act:\n",
    "                res = self.post_act(res)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca4f383",
   "metadata": {},
   "source": [
    "### EfficientViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53e3875",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientViTBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    EfficientViTãƒ–ãƒ­ãƒƒã‚¯\n",
    "    build_blockã§ä½¿ç”¨\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, heads_ratio: float = 1.0, dim=32, expand_ratio: float = 4, scales: tuple[int, ...] = (5,), norm: str = \"bn2d\", act_func: str = \"hswish\", context_module: str = \"LiteMLA\", local_module: str = \"MBConv\", is_video: bool = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): å…¥åŠ›ãƒãƒ£ãƒãƒ«æ•°\n",
    "            heads_ratio (float, optional): ãƒ˜ãƒƒãƒ‰æ•°ã®æ¯”ç‡\n",
    "            dim (int, optional): å„ãƒ˜ãƒƒãƒ‰ã®æ¬¡å…ƒæ•°\n",
    "            expand_ratio (float, optional): æ‹¡å¼µæ¯”ç‡\n",
    "            scales (tuple[int, ...], optional): ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ç•³ã¿è¾¼ã¿ã®ã‚¹ã‚±ãƒ¼ãƒ«\n",
    "            norm (str, optional): æ­£è¦åŒ–å±¤ã®ç¨®é¡\n",
    "            act_func (str, optional): æ´»æ€§åŒ–é–¢æ•°ã®ç¨®é¡\n",
    "            context_module (str, optional): ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ç¨®é¡\n",
    "            local_module (str, optional): ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ç¨®é¡\n",
    "            is_video (bool, optional): 3Dç•³ã¿è¾¼ã¿ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "        \"\"\"\n",
    "        logger.info(f\"EfficientViTBlock.__init__ {in_channels=} {heads_ratio=} {dim=} {expand_ratio=} {scales=} {norm=} {act_func=} {context_module=} {local_module=} {is_video=}\")\n",
    "\n",
    "        super().__init__()\n",
    "        if context_module == \"LiteMLA\":\n",
    "            self.context_module = ResidualBlock(\n",
    "                LiteMLA(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=in_channels,\n",
    "                    heads_ratio=heads_ratio,\n",
    "                    dim=dim,\n",
    "                    norm=(None, norm),\n",
    "                    scales=scales,\n",
    "                    is_video=is_video,\n",
    "                ),\n",
    "                IdentityLayer(),\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"context_module {context_module} is not supported\")\n",
    "        if local_module == \"MBConv\":\n",
    "            self.local_module = ResidualBlock(\n",
    "                MBConv(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=in_channels,\n",
    "                    expand_ratio=expand_ratio,\n",
    "                    use_bias=(True, True, False),\n",
    "                    norm=(None, None, norm),\n",
    "                    act_func=(act_func, act_func, None),\n",
    "                    is_video=is_video,\n",
    "                ),\n",
    "                IdentityLayer(),\n",
    "            )\n",
    "        elif local_module == \"GLUMBConv\":\n",
    "            self.local_module = ResidualBlock(\n",
    "                GLUMBConv(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=in_channels,\n",
    "                    expand_ratio=expand_ratio,\n",
    "                    use_bias=(True, True, False),\n",
    "                    norm=(None, None, norm),\n",
    "                    act_func=(act_func, act_func, None),\n",
    "                    is_video=is_video,\n",
    "                ),\n",
    "                IdentityLayer(),\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(f\"local_module {local_module} is not supported\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logger.info(f\"EfficientViTBlock.forward {x.shape=}\")\n",
    "        x = self.context_module(x)\n",
    "        x = self.local_module(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6717d723",
   "metadata": {},
   "source": [
    "### ResBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f023dc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    æ®‹å·®ãƒ–ãƒ­ãƒƒã‚¯\n",
    "    build_blockã§ä½¿ç”¨\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        mid_channels=None,\n",
    "        expand_ratio=1,\n",
    "        use_bias=False,\n",
    "        norm=(\"bn2d\", \"bn2d\"),\n",
    "        act_func=(\"relu6\", None),\n",
    "        is_video=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): å…¥åŠ›ãƒãƒ£ãƒãƒ«æ•°\n",
    "            out_channels (int): å‡ºåŠ›ãƒãƒ£ãƒãƒ«æ•°\n",
    "            kernel_size (int, optional): ç•³ã¿è¾¼ã¿ã‚«ãƒ¼ãƒãƒ«ã®ã‚µã‚¤ã‚º\n",
    "            stride (int, optional): ç•³ã¿è¾¼ã¿ã®ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰\n",
    "            mid_channels (int, optional): ä¸­é–“ãƒãƒ£ãƒãƒ«æ•°\n",
    "            expand_ratio (float, optional): æ‹¡å¼µæ¯”ç‡\n",
    "            use_bias (bool or tuple, optional): ãƒã‚¤ã‚¢ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "            norm (str or tuple, optional): æ­£è¦åŒ–å±¤ã®ç¨®é¡\n",
    "            act_func (str or tuple, optional): æ´»æ€§åŒ–é–¢æ•°ã®ç¨®é¡\n",
    "            is_video (bool, optional): 3Dç•³ã¿è¾¼ã¿ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "        \"\"\"\n",
    "        logger.info(f\"ResBlock.__init__ {in_channels=} {out_channels=} {kernel_size=} {stride=} {mid_channels=} {expand_ratio=} {use_bias=} {norm=} {act_func=} {is_video=}\")\n",
    "\n",
    "        super().__init__()\n",
    "        use_bias = val2tuple(use_bias, 2)\n",
    "        norm = val2tuple(norm, 2)\n",
    "        act_func = val2tuple(act_func, 2)\n",
    "\n",
    "        mid_channels = round(in_channels * expand_ratio) if mid_channels is None else mid_channels\n",
    "\n",
    "        self.conv1 = ConvLayer(\n",
    "            in_channels,\n",
    "            mid_channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            use_bias=use_bias[0],\n",
    "            norm=norm[0],\n",
    "            act_func=act_func[0],\n",
    "            is_video=is_video,\n",
    "        )\n",
    "        self.conv2 = ConvLayer(\n",
    "            mid_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            1,\n",
    "            use_bias=use_bias[1],\n",
    "            norm=norm[1],\n",
    "            act_func=act_func[1],\n",
    "            is_video=is_video,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logger.info(f\"ResBlock.forward {x.shape=}\")\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b516c0",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac36316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_downsample_block(\n",
    "    block_type: str,\n",
    "    in_channels: int,\n",
    "    out_channels: int,\n",
    "    shortcut: Optional[str],\n",
    "    is_video: bool,\n",
    "    temporal_downsample: bool = False,\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ§‹ç¯‰ã™ã‚‹\n",
    "    ç©ºé–“çš„ãªãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¯å¸¸ã«è¡Œã‚ã‚Œã‚‹\n",
    "    æ™‚é–“çš„ãªãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³\n",
    "    build_encoder_project_in_block, Encoderã§ä½¿ç”¨\n",
    "\n",
    "    Args:\n",
    "        block_type (str): ãƒ–ãƒ­ãƒƒã‚¯ã®ç¨®é¡ (\"Conv\" ã¾ãŸã¯ \"ConvPixelUnshuffle\")\n",
    "        in_channels (int): å…¥åŠ›ãƒãƒ£ãƒãƒ«æ•°\n",
    "        out_channels (int): å‡ºåŠ›ãƒãƒ£ãƒãƒ«æ•°\n",
    "        shortcut (Optional[str]): ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆã®ç¨®é¡ (None ã¾ãŸã¯ \"averaging\")\n",
    "        is_video (bool): 3Dç•³ã¿è¾¼ã¿ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "        temporal_downsample (bool, optional):\n",
    "            æ™‚é–“æ–¹å‘ã®ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’è¡Œã†ã‹ã©ã†ã‹\n",
    "    Returns:\n",
    "        nn.Module: ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ–ãƒ­ãƒƒã‚¯\n",
    "    \"\"\"\n",
    "    logger.info(f\"build_downsample_block {block_type=} {in_channels=} {out_channels=} {shortcut=} {is_video=} {temporal_downsample=}\")\n",
    "\n",
    "    if block_type == \"Conv\":\n",
    "        if is_video:\n",
    "            if temporal_downsample:\n",
    "                stride = (2, 2, 2)\n",
    "            else:\n",
    "                stride = (1, 2, 2)\n",
    "        else:\n",
    "            stride = 2\n",
    "        block = ConvLayer(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            use_bias=True,\n",
    "            norm=None,\n",
    "            act_func=None,\n",
    "            is_video=is_video,\n",
    "        )\n",
    "    elif block_type == \"ConvPixelUnshuffle\":\n",
    "        if is_video:\n",
    "            raise NotImplementedError(\"ConvPixelUnshuffle downsample is not supported for video\")\n",
    "        block = ConvPixelUnshuffleDownSampleLayer(\n",
    "            in_channels=in_channels, out_channels=out_channels, kernel_size=3, factor=2\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"block_type {block_type} is not supported for downsampling\")\n",
    "    if shortcut is None:\n",
    "        pass\n",
    "    elif shortcut == \"averaging\":\n",
    "        shortcut_block = PixelUnshuffleChannelAveragingDownSampleLayer(\n",
    "            in_channels=in_channels, out_channels=out_channels, factor=2, temporal_downsample=temporal_downsample\n",
    "        )\n",
    "        block = ResidualBlock(block, shortcut_block)\n",
    "    else:\n",
    "        raise ValueError(f\"shortcut {shortcut} is not supported for downsample\")\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771ff37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder_project_in_block(in_channels: int, out_channels: int, factor: int, downsample_block_type: str, is_video: bool):\n",
    "    \"\"\"\n",
    "    ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å…¥åŠ›ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ§‹ç¯‰ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): å…¥åŠ›ãƒãƒ£ãƒãƒ«æ•°\n",
    "        out_channels (int): å‡ºåŠ›ãƒãƒ£ãƒãƒ«æ•°\n",
    "        factor (int): ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼\n",
    "        downsample_block_type (str): ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ–ãƒ­ãƒƒã‚¯ã®ç¨®é¡\n",
    "        is_video (bool): 3Dç•³ã¿è¾¼ã¿ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "    Returns:\n",
    "        nn.Module: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å…¥åŠ›ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ãƒ–ãƒ­ãƒƒã‚¯\n",
    "    \"\"\"\n",
    "    logger.info(f\"build_encoder_project_in_block {in_channels=} {out_channels=} {factor=} {downsample_block_type=} {is_video=}\")\n",
    "\n",
    "    if factor == 1:\n",
    "        block = ConvLayer(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            use_bias=True,\n",
    "            norm=None,\n",
    "            act_func=None,\n",
    "            is_video=is_video,\n",
    "        )\n",
    "    elif factor == 2:\n",
    "        if is_video:\n",
    "            raise NotImplementedError(\"Downsample during project_in is not supported for video\")\n",
    "        block = build_downsample_block(\n",
    "            block_type=downsample_block_type, in_channels=in_channels, out_channels=out_channels, shortcut=None\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"downsample factor {factor} is not supported for encoder project in\")\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44428bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder_project_out_block(\n",
    "    in_channels: int,\n",
    "    out_channels: int,\n",
    "    norm: Optional[str],\n",
    "    act: Optional[str],\n",
    "    shortcut: Optional[str],\n",
    "    is_video: bool,\n",
    "):\n",
    "    \"\"\"\n",
    "    ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ›ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ§‹ç¯‰ã™ã‚‹\n",
    "    Encoderã§ä½¿ç”¨\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): å…¥åŠ›ãƒãƒ£ãƒãƒ«æ•°\n",
    "        out_channels (int): å‡ºåŠ›ãƒãƒ£ãƒãƒ«æ•°\n",
    "        norm (Optional[str]): æ­£è¦åŒ–å±¤ã®ç¨®é¡\n",
    "        act (Optional[str]): æ´»æ€§åŒ–é–¢æ•°ã®ç¨®é¡\n",
    "        shortcut (Optional[str]): ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆã®ç¨®é¡ (None ã¾ãŸã¯ \"averaging\")\n",
    "        is_video (bool): 3Dç•³ã¿è¾¼ã¿ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "    Returns:\n",
    "        nn.Module: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ›ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ãƒ–ãƒ­ãƒƒã‚¯\n",
    "    \"\"\"\n",
    "    logger.info(f\"build_encoder_project_out_block {in_channels=} {out_channels=} {norm=} {act=} {shortcut=} {is_video=}\")\n",
    "\n",
    "    block = OpSequential(\n",
    "        [\n",
    "            build_norm(norm),\n",
    "            build_act(act),\n",
    "            ConvLayer(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                use_bias=True,\n",
    "                norm=None,\n",
    "                act_func=None,\n",
    "                is_video=is_video,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    if shortcut is None:\n",
    "        pass\n",
    "    elif shortcut == \"averaging\":\n",
    "        shortcut_block = PixelUnshuffleChannelAveragingDownSampleLayer(\n",
    "            in_channels=in_channels, out_channels=out_channels, factor=1\n",
    "        )\n",
    "        block = ResidualBlock(block, shortcut_block)\n",
    "    else:\n",
    "        raise ValueError(f\"shortcut {shortcut} is not supported for encoder project out\")\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3a77b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_block(\n",
    "    block_type: str, in_channels: int, out_channels: int, norm: Optional[str], act: Optional[str], is_video: bool\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ§‹ç¯‰ã™ã‚‹\n",
    "    build_stage_mainã§ä½¿ç”¨\n",
    "    Args:\n",
    "        block_type (str): ãƒ–ãƒ­ãƒƒã‚¯ã®ç¨®é¡ (\"ResBlock\", \"EViT_GLU\", \"EViTS5_GLU\")\n",
    "        in_channels (int): å…¥åŠ›ãƒãƒ£ãƒãƒ«æ•°\n",
    "        out_channels (int): å‡ºåŠ›ãƒãƒ£ãƒãƒ«æ•°\n",
    "        norm (Optional[str]): æ­£è¦åŒ–å±¤ã®ç¨®é¡\n",
    "        act (Optional[str]): æ´»æ€§åŒ–é–¢æ•°ã®ç¨®é¡\n",
    "        is_video (bool): 3Dç•³ã¿è¾¼ã¿ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "    Returns:\n",
    "        nn.Module: ãƒ–ãƒ­ãƒƒã‚¯\n",
    "    \"\"\"\n",
    "    logger.info(f\"build_block {block_type=} {in_channels=} {out_channels=} {norm=} {act=} {is_video=}\")\n",
    "\n",
    "    if block_type == \"ResBlock\":\n",
    "        assert in_channels == out_channels\n",
    "        main_block = ResBlock(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            use_bias=(True, False),\n",
    "            norm=(None, norm),\n",
    "            act_func=(act, None),\n",
    "            is_video=is_video,\n",
    "        )\n",
    "        block = ResidualBlock(main_block, IdentityLayer())\n",
    "    elif block_type == \"EViT_GLU\":\n",
    "        assert in_channels == out_channels\n",
    "        block = EfficientViTBlock(\n",
    "            in_channels, norm=norm, act_func=act, local_module=\"GLUMBConv\", scales=(), is_video=is_video\n",
    "        )\n",
    "    elif block_type == \"EViTS5_GLU\":\n",
    "        assert in_channels == out_channels\n",
    "        block = EfficientViTBlock(\n",
    "            in_channels, norm=norm, act_func=act, local_module=\"GLUMBConv\", scales=(5,), is_video=is_video\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"block_type {block_type} is not supported\")\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7b4b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_stage_main(\n",
    "    width: int, depth: int, block_type: str | list[str], norm: str, act: str, input_width: int, is_video: bool\n",
    ") -> list[nn.Module]:\n",
    "    \"\"\"\n",
    "    ã‚¹ãƒ†ãƒ¼ã‚¸ã®ãƒ¡ã‚¤ãƒ³ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ§‹ç¯‰ã™ã‚‹\n",
    "    Encoder, Decoderã§ä½¿ç”¨\n",
    "    Args:\n",
    "        width (int): ãƒ–ãƒ­ãƒƒã‚¯ã®ãƒãƒ£ãƒãƒ«æ•°\n",
    "        depth (int): ãƒ–ãƒ­ãƒƒã‚¯ã®æ·±ã•\n",
    "        block_type (str or list[str]): ãƒ–ãƒ­ãƒƒã‚¯ã®ç¨®é¡\n",
    "        norm (str): æ­£è¦åŒ–å±¤ã®ç¨®é¡\n",
    "        act (str): æ´»æ€§åŒ–é–¢æ•°ã®ç¨®é¡\n",
    "        input_width (int): å…¥åŠ›ãƒãƒ£ãƒãƒ«æ•°\n",
    "        is_video (bool): 3Dç•³ã¿è¾¼ã¿ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "    Returns:\n",
    "        list[nn.Module]: ã‚¹ãƒ†ãƒ¼ã‚¸ã®ãƒ¡ã‚¤ãƒ³ãƒ–ãƒ­ãƒƒã‚¯ã®ãƒªã‚¹ãƒˆ\n",
    "    \"\"\"\n",
    "    logger.info(f\"build_stage_main {width=} {depth=} {block_type=} {norm=} {act=} {input_width=} {is_video=}\")\n",
    "\n",
    "    assert isinstance(block_type, str) or (isinstance(block_type, list) and depth == len(block_type))\n",
    "    stage = []\n",
    "    for d in range(depth):\n",
    "        current_block_type = block_type[d] if isinstance(block_type, list) else block_type\n",
    "        block = build_block(\n",
    "            block_type=current_block_type,\n",
    "            in_channels=width if d > 0 else input_width,\n",
    "            out_channels=width,\n",
    "            norm=norm,\n",
    "            act=act,\n",
    "            is_video=is_video,\n",
    "        )\n",
    "        stage.append(block)\n",
    "    return stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45544988",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EncoderConfig:\n",
    "    in_channels: int = MISSING\n",
    "    latent_channels: int = MISSING\n",
    "    width_list: tuple[int, ...] = (128, 256, 512, 512, 1024, 1024)\n",
    "    depth_list: tuple[int, ...] = (2, 2, 2, 2, 2, 2)\n",
    "    block_type: Any = \"ResBlock\"\n",
    "    norm: str = \"rms2d\"\n",
    "    act: str = \"silu\"\n",
    "    downsample_block_type: str = \"ConvPixelUnshuffle\"\n",
    "    downsample_match_channel: bool = True\n",
    "    downsample_shortcut: Optional[str] = \"averaging\"\n",
    "    out_norm: Optional[str] = None\n",
    "    out_act: Optional[str] = None\n",
    "    out_shortcut: Optional[str] = \"averaging\"\n",
    "    double_latent: bool = False\n",
    "    is_video: bool = False\n",
    "    temporal_downsample: tuple[bool, ...] = ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8abf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€\n",
    "    DCAEã§ä½¿ç”¨\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: EncoderConfig):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cfg (EncoderConfig): ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®è¨­å®š\n",
    "        \"\"\"\n",
    "        logger.info(f\"Encoder.__init__ {cfg=}\")\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        num_stages = len(cfg.width_list)\n",
    "        self.num_stages = num_stages\n",
    "        assert len(cfg.depth_list) == num_stages\n",
    "        assert len(cfg.width_list) == num_stages\n",
    "        assert isinstance(cfg.block_type, str) or (\n",
    "            isinstance(cfg.block_type, list) and len(cfg.block_type) == num_stages\n",
    "        )\n",
    "\n",
    "        self.project_in = build_encoder_project_in_block(\n",
    "            in_channels=cfg.in_channels,\n",
    "            out_channels=cfg.width_list[0] if cfg.depth_list[0] > 0 else cfg.width_list[1],\n",
    "            factor=1 if cfg.depth_list[0] > 0 else 2,\n",
    "            downsample_block_type=cfg.downsample_block_type,\n",
    "            is_video=cfg.is_video,\n",
    "        )\n",
    "\n",
    "        self.stages: list[OpSequential] = []\n",
    "        for stage_id, (width, depth) in enumerate(zip(cfg.width_list, cfg.depth_list)):\n",
    "            block_type = cfg.block_type[stage_id] if isinstance(cfg.block_type, list) else cfg.block_type\n",
    "            stage = build_stage_main(\n",
    "                width=width,\n",
    "                depth=depth,\n",
    "                block_type=block_type,\n",
    "                norm=cfg.norm,\n",
    "                act=cfg.act,\n",
    "                input_width=width,\n",
    "                is_video=cfg.is_video,\n",
    "            )\n",
    "\n",
    "            if stage_id < num_stages - 1 and depth > 0:\n",
    "                downsample_block = build_downsample_block(\n",
    "                    block_type=cfg.downsample_block_type,\n",
    "                    in_channels=width,\n",
    "                    out_channels=cfg.width_list[stage_id + 1] if cfg.downsample_match_channel else width,\n",
    "                    shortcut=cfg.downsample_shortcut,\n",
    "                    is_video=cfg.is_video,\n",
    "                    temporal_downsample=cfg.temporal_downsample[stage_id] if cfg.temporal_downsample != [] else False,\n",
    "                )\n",
    "                stage.append(downsample_block)\n",
    "            self.stages.append(OpSequential(stage))\n",
    "        self.stages = nn.ModuleList(self.stages)\n",
    "\n",
    "        self.project_out = build_encoder_project_out_block(\n",
    "            in_channels=cfg.width_list[-1],\n",
    "            out_channels=2 * cfg.latent_channels if cfg.double_latent else cfg.latent_channels,\n",
    "            norm=cfg.out_norm,\n",
    "            act=cfg.out_act,\n",
    "            shortcut=cfg.out_shortcut,\n",
    "            is_video=cfg.is_video,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logger.info(f\"Encoder.forward {x.shape=}\")\n",
    "        x = self.project_in(x)\n",
    "        # x = auto_grad_checkpoint(self.project_in, x)\n",
    "        for stage in self.stages:\n",
    "            if len(stage.op_list) == 0:\n",
    "                continue\n",
    "            x = auto_grad_checkpoint(stage, x)\n",
    "        # x = self.project_out(x)\n",
    "        x = auto_grad_checkpoint(self.project_out, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e844bb",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366f5c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decoder_project_in_block(in_channels: int, out_channels: int, shortcut: Optional[str], is_video: bool):\n",
    "    \"\"\"\n",
    "    ãƒ‡ã‚³ãƒ¼ãƒ€ã®å…¥åŠ›ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ§‹ç¯‰ã™ã‚‹\n",
    "    Decoderã§ä½¿ç”¨\n",
    "    Args:\n",
    "        in_channels (int): å…¥åŠ›ãƒãƒ£ãƒãƒ«æ•°\n",
    "        out_channels (int): å‡ºåŠ›ãƒãƒ£ãƒãƒ«æ•°\n",
    "        shortcut (Optional[str]): ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆã®ç¨®é¡ (None ã¾ãŸã¯ \"duplicating\")\n",
    "        is_video (bool): 3Dç•³ã¿è¾¼ã¿ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "    Returns:\n",
    "        nn.Module: ãƒ‡ã‚³ãƒ¼ãƒ€ã®å…¥åŠ›ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ãƒ–ãƒ­ãƒƒã‚¯\n",
    "    \"\"\"\n",
    "    logger.info(f\"build_decoder_project_in_block {in_channels=} {out_channels=} {shortcut=} {is_video=}\")\n",
    "\n",
    "    block = ConvLayer(\n",
    "        in_channels=in_channels,\n",
    "        out_channels=out_channels,\n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        use_bias=True,\n",
    "        norm=None,\n",
    "        act_func=None,\n",
    "        is_video=is_video,\n",
    "    )\n",
    "    if shortcut is None:\n",
    "        pass\n",
    "    elif shortcut == \"duplicating\":\n",
    "        shortcut_block = ChannelDuplicatingPixelShuffleUpSampleLayer(\n",
    "            in_channels=in_channels, out_channels=out_channels, factor=1\n",
    "        )\n",
    "        block = ResidualBlock(block, shortcut_block)\n",
    "    else:\n",
    "        raise ValueError(f\"shortcut {shortcut} is not supported for decoder project in\")\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ec9105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_upsample_block(\n",
    "    block_type: str,\n",
    "    in_channels: int,\n",
    "    out_channels: int,\n",
    "    shortcut: Optional[str],\n",
    "    is_video: bool,\n",
    "    temporal_upsample: bool = False,\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ§‹ç¯‰ã™ã‚‹\n",
    "    build_decoder_project_out_block, Decoderã§ä½¿ç”¨\n",
    "\n",
    "    Args:\n",
    "        block_type (str): ãƒ–ãƒ­ãƒƒã‚¯ã®ç¨®é¡ (\"ConvPixelShuffle\" ã¾ãŸã¯ \"InterpolateConv\")\n",
    "        in_channels (int): å…¥åŠ›ãƒãƒ£ãƒãƒ«æ•°\n",
    "        out_channels (int): å‡ºåŠ›ãƒãƒ£ãƒãƒ«æ•°\n",
    "        shortcut (Optional[str]): ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆã®ç¨®é¡ (None ã¾ãŸã¯ \"duplicating\")\n",
    "        is_video (bool): 3Dç•³ã¿è¾¼ã¿ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "        temporal_upsample (bool, optional):\n",
    "            æ™‚é–“æ–¹å‘ã®ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’è¡Œã†ã‹ã©ã†ã‹\n",
    "    Returns:\n",
    "        nn.Module: ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ–ãƒ­ãƒƒã‚¯\n",
    "    \"\"\"\n",
    "    logger.info(f\"build_upsample_block {block_type=} {in_channels=} {out_channels=} {shortcut=} {is_video=} {temporal_upsample=}\")\n",
    "    if block_type == \"ConvPixelShuffle\":\n",
    "        if is_video:\n",
    "            raise NotImplementedError(\"ConvPixelShuffle upsample is not supported for video\")\n",
    "        block = ConvPixelShuffleUpSampleLayer(\n",
    "            in_channels=in_channels, out_channels=out_channels, kernel_size=3, factor=2\n",
    "        )\n",
    "    elif block_type == \"InterpolateConv\":\n",
    "        block = InterpolateConvUpSampleLayer(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            factor=2,\n",
    "            is_video=is_video,\n",
    "            temporal_upsample=temporal_upsample,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"block_type {block_type} is not supported for upsampling\")\n",
    "    if shortcut is None:\n",
    "        pass\n",
    "    elif shortcut == \"duplicating\":\n",
    "        shortcut_block = ChannelDuplicatingPixelShuffleUpSampleLayer(\n",
    "            in_channels=in_channels, out_channels=out_channels, factor=2, temporal_upsample=temporal_upsample\n",
    "        )\n",
    "        block = ResidualBlock(block, shortcut_block)\n",
    "    else:\n",
    "        raise ValueError(f\"shortcut {shortcut} is not supported for upsample\")\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2080afad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decoder_project_out_block(\n",
    "    in_channels: int,\n",
    "    out_channels: int,\n",
    "    factor: int,\n",
    "    upsample_block_type: str,\n",
    "    norm: Optional[str],\n",
    "    act: Optional[str],\n",
    "    is_video: bool,\n",
    "):\n",
    "    \"\"\"\n",
    "    ãƒ‡ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ›ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ§‹ç¯‰ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): å…¥åŠ›ãƒãƒ£ãƒãƒ«æ•°\n",
    "        out_channels (int): å‡ºåŠ›ãƒãƒ£ãƒãƒ«æ•°\n",
    "        factor (int): ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼\n",
    "        upsample_block_type (str): ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ–ãƒ­ãƒƒã‚¯ã®ç¨®é¡\n",
    "        norm (Optional[str]): æ­£è¦åŒ–å±¤ã®ç¨®é¡\n",
    "        act (Optional[str]): æ´»æ€§åŒ–é–¢æ•°ã®ç¨®é¡\n",
    "        is_video (bool): 3Dç•³ã¿è¾¼ã¿ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "    Returns:\n",
    "        nn.Module: ãƒ‡ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ›ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ãƒ–ãƒ­ãƒƒã‚¯\n",
    "    \"\"\"\n",
    "    logger.info(f\"build_decoder_project_out_block {in_channels=} {out_channels=} {factor=} {upsample_block_type=} {norm=} {act=} {is_video=}\")\n",
    "\n",
    "    layers: list[nn.Module] = [\n",
    "        build_norm(norm, in_channels),\n",
    "        build_act(act),\n",
    "    ]\n",
    "    if factor == 1:\n",
    "        layers.append(\n",
    "            ConvLayer(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                use_bias=True,\n",
    "                norm=None,\n",
    "                act_func=None,\n",
    "                is_video=is_video,\n",
    "            )\n",
    "        )\n",
    "    elif factor == 2:\n",
    "        if is_video:\n",
    "            raise NotImplementedError(\"Upsample during project_out is not supported for video\")\n",
    "        layers.append(\n",
    "            build_upsample_block(\n",
    "                block_type=upsample_block_type, in_channels=in_channels, out_channels=out_channels, shortcut=None\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"upsample factor {factor} is not supported for decoder project out\")\n",
    "    return OpSequential(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf6a03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DecoderConfig:\n",
    "    in_channels: int = MISSING\n",
    "    latent_channels: int = MISSING\n",
    "    in_shortcut: Optional[str] = \"duplicating\"\n",
    "    width_list: tuple[int, ...] = (128, 256, 512, 512, 1024, 1024)\n",
    "    depth_list: tuple[int, ...] = (2, 2, 2, 2, 2, 2)\n",
    "    block_type: Any = \"ResBlock\"\n",
    "    norm: Any = \"rms2d\"\n",
    "    act: Any = \"silu\"\n",
    "    upsample_block_type: str = \"ConvPixelShuffle\"\n",
    "    upsample_match_channel: bool = True\n",
    "    upsample_shortcut: str = \"duplicating\"\n",
    "    out_norm: str = \"rms2d\"\n",
    "    out_act: str = \"relu\"\n",
    "    is_video: bool = False\n",
    "    temporal_upsample: tuple[bool, ...] = ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454056a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    ãƒ‡ã‚³ãƒ¼ãƒ€\n",
    "    DCAEã§ä½¿ç”¨\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: DecoderConfig):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cfg (DecoderConfig): ãƒ‡ã‚³ãƒ¼ãƒ€ã®è¨­å®š\n",
    "        \"\"\"\n",
    "        logger.info(f\"Decoder.__init__ {cfg=}\")\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        num_stages = len(cfg.width_list)\n",
    "        self.num_stages = num_stages\n",
    "        assert len(cfg.depth_list) == num_stages\n",
    "        assert len(cfg.width_list) == num_stages\n",
    "        assert isinstance(cfg.block_type, str) or (\n",
    "            isinstance(cfg.block_type, list) and len(cfg.block_type) == num_stages\n",
    "        )\n",
    "        assert isinstance(cfg.norm, str) or (isinstance(cfg.norm, list) and len(cfg.norm) == num_stages)\n",
    "        assert isinstance(cfg.act, str) or (isinstance(cfg.act, list) and len(cfg.act) == num_stages)\n",
    "\n",
    "        self.project_in = build_decoder_project_in_block(\n",
    "            in_channels=cfg.latent_channels,\n",
    "            out_channels=cfg.width_list[-1],\n",
    "            shortcut=cfg.in_shortcut,\n",
    "            is_video=cfg.is_video,\n",
    "        )\n",
    "\n",
    "        self.stages: list[OpSequential] = []\n",
    "        for stage_id, (width, depth) in reversed(list(enumerate(zip(cfg.width_list, cfg.depth_list)))):\n",
    "            stage = []\n",
    "            if stage_id < num_stages - 1 and depth > 0:\n",
    "                upsample_block = build_upsample_block(\n",
    "                    block_type=cfg.upsample_block_type,\n",
    "                    in_channels=cfg.width_list[stage_id + 1],\n",
    "                    out_channels=width if cfg.upsample_match_channel else cfg.width_list[stage_id + 1],\n",
    "                    shortcut=cfg.upsample_shortcut,\n",
    "                    is_video=cfg.is_video,\n",
    "                    temporal_upsample=cfg.temporal_upsample[stage_id] if cfg.temporal_upsample != [] else False,\n",
    "                )\n",
    "                stage.append(upsample_block)\n",
    "\n",
    "            block_type = cfg.block_type[stage_id] if isinstance(cfg.block_type, list) else cfg.block_type\n",
    "            norm = cfg.norm[stage_id] if isinstance(cfg.norm, list) else cfg.norm\n",
    "            act = cfg.act[stage_id] if isinstance(cfg.act, list) else cfg.act\n",
    "            stage.extend(\n",
    "                build_stage_main(\n",
    "                    width=width,\n",
    "                    depth=depth,\n",
    "                    block_type=block_type,\n",
    "                    norm=norm,\n",
    "                    act=act,\n",
    "                    input_width=(\n",
    "                        width if cfg.upsample_match_channel else cfg.width_list[min(stage_id + 1, num_stages - 1)]\n",
    "                    ),\n",
    "                    is_video=cfg.is_video,\n",
    "                )\n",
    "            )\n",
    "            self.stages.insert(0, OpSequential(stage))\n",
    "        self.stages = nn.ModuleList(self.stages)\n",
    "\n",
    "        self.project_out = build_decoder_project_out_block(\n",
    "            in_channels=cfg.width_list[0] if cfg.depth_list[0] > 0 else cfg.width_list[1],\n",
    "            out_channels=cfg.in_channels,\n",
    "            factor=1 if cfg.depth_list[0] > 0 else 2,\n",
    "            upsample_block_type=cfg.upsample_block_type,\n",
    "            norm=cfg.out_norm,\n",
    "            act=cfg.out_act,\n",
    "            is_video=cfg.is_video,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logger.info(f\"Decoder.forward {x.shape=}\")\n",
    "        x = auto_grad_checkpoint(self.project_in, x)\n",
    "        for stage in reversed(self.stages):\n",
    "            if len(stage.op_list) == 0:\n",
    "                continue\n",
    "            # x = stage(x)\n",
    "            x = auto_grad_checkpoint(stage, x)\n",
    "\n",
    "        if self.disc_off_grad_ckpt:\n",
    "            x = self.project_out(x)\n",
    "        else:\n",
    "            x = auto_grad_checkpoint(self.project_out, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e1fed3",
   "metadata": {},
   "source": [
    "### DC-AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cae01e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DCAEConfig:\n",
    "    in_channels: int = 3\n",
    "    latent_channels: int = 32\n",
    "    time_compression_ratio: int = 1\n",
    "    spatial_compression_ratio: int = 32\n",
    "    encoder: EncoderConfig = field(\n",
    "        default_factory=lambda: EncoderConfig(in_channels=\"${..in_channels}\", latent_channels=\"${..latent_channels}\")\n",
    "    )\n",
    "    decoder: DecoderConfig = field(\n",
    "        default_factory=lambda: DecoderConfig(in_channels=\"${..in_channels}\", latent_channels=\"${..latent_channels}\")\n",
    "    )\n",
    "    use_quant_conv: bool = False\n",
    "\n",
    "    pretrained_path: Optional[str] = None\n",
    "    pretrained_source: str = \"dc-ae\"\n",
    "\n",
    "    scaling_factor: Optional[float] = None\n",
    "    is_image_model: bool = False\n",
    "\n",
    "    is_training: bool = False  # NOTE: set to True in vae train config\n",
    "\n",
    "    use_spatial_tiling: bool = False\n",
    "    use_temporal_tiling: bool = False\n",
    "    spatial_tile_size: int = 256\n",
    "    temporal_tile_size: int = 32\n",
    "    tile_overlap_factor: float = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af7dd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_modules(model: Union[nn.Module, list[nn.Module]], init_type=\"trunc_normal\") -> None:\n",
    "    \"\"\"\n",
    "    ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’åˆæœŸåŒ–ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module or list[nn.Module]): åˆæœŸåŒ–ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã¾ãŸã¯ãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆ\n",
    "        init_type (str, optional): åˆæœŸåŒ–ã®ç¨®é¡ (\"trunc_normal@std\" ã¾ãŸã¯ \"normal@std\")\n",
    "    \"\"\"\n",
    "    logger.info(f\"init_modules {init_type=}\")\n",
    "\n",
    "    _DEFAULT_INIT_PARAM = {\"trunc_normal\": 0.02}\n",
    "\n",
    "    if isinstance(model, list):\n",
    "        for sub_module in model:\n",
    "            init_modules(sub_module, init_type)\n",
    "    else:\n",
    "        init_params = init_type.split(\"@\")\n",
    "        init_params = float(init_params[1]) if len(init_params) > 1 else None\n",
    "\n",
    "        if init_type.startswith(\"trunc_normal\"):\n",
    "            init_func = lambda param: nn.init.trunc_normal_(\n",
    "                param, std=(_DEFAULT_INIT_PARAM[\"trunc_normal\"] if init_params is None else init_params)\n",
    "            )\n",
    "        elif init_type.startswith(\"normal\"):\n",
    "            init_func = lambda param: nn.init.normal_(\n",
    "                param, std=(_DEFAULT_INIT_PARAM[\"trunc_normal\"] if init_params is None else init_params)\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        for m in model.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear, nn.ConvTranspose2d)):\n",
    "                init_func(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Embedding):\n",
    "                init_func(m.weight)\n",
    "            elif isinstance(m, (_BatchNorm, nn.GroupNorm, nn.LayerNorm)):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            else:\n",
    "                weight = getattr(m, \"weight\", None)\n",
    "                bias = getattr(m, \"bias\", None)\n",
    "                if isinstance(weight, torch.nn.Parameter):\n",
    "                    init_func(weight)\n",
    "                if isinstance(bias, torch.nn.Parameter):\n",
    "                    bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cea200",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Compressive Autoencoder (DCAE)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: DCAEConfig):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cfg (DCAEConfig): DCAEã®è¨­å®š\n",
    "        \"\"\"\n",
    "        logger.info(f\"DCAE.__init__ {cfg=}\")\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.encoder = Encoder(cfg.encoder)\n",
    "        self.decoder = Decoder(cfg.decoder)\n",
    "        self.scaling_factor = cfg.scaling_factor\n",
    "        self.time_compression_ratio = cfg.time_compression_ratio\n",
    "        self.spatial_compression_ratio = cfg.spatial_compression_ratio\n",
    "        self.use_spatial_tiling = cfg.use_spatial_tiling\n",
    "        self.use_temporal_tiling = cfg.use_temporal_tiling\n",
    "        self.spatial_tile_size = cfg.spatial_tile_size\n",
    "        self.temporal_tile_size = cfg.temporal_tile_size\n",
    "        assert (\n",
    "            cfg.spatial_tile_size // cfg.spatial_compression_ratio\n",
    "        ), f\"spatial tile size {cfg.spatial_tile_size} must be divisible by spatial compression of {cfg.spatial_compression_ratio}\"\n",
    "        self.spatial_tile_latent_size = cfg.spatial_tile_size // cfg.spatial_compression_ratio\n",
    "        assert (\n",
    "            cfg.temporal_tile_size // cfg.time_compression_ratio\n",
    "        ), f\"temporal tile size {cfg.temporal_tile_size} must be divisible by temporal compression of {cfg.time_compression_ratio}\"\n",
    "        self.temporal_tile_latent_size = cfg.temporal_tile_size // cfg.time_compression_ratio\n",
    "        self.tile_overlap_factor = cfg.tile_overlap_factor\n",
    "        if self.cfg.pretrained_path is not None:\n",
    "            self.load_model()\n",
    "\n",
    "        self.to(torch.float32)\n",
    "        init_modules(self, init_type=\"trunc_normal\")\n",
    "\n",
    "    def load_model(self):\n",
    "        if self.cfg.pretrained_source == \"dc-ae\":\n",
    "            state_dict = torch.load(self.cfg.pretrained_path, map_location=\"cpu\", weights_only=True)[\"state_dict\"]\n",
    "            self.load_state_dict(state_dict)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def get_last_layer(self):\n",
    "        return self.decoder.project_out.op_list[2].conv.weight\n",
    "\n",
    "    # @property\n",
    "    # def spatial_compression_ratio(self) -> int:\n",
    "    #     return 2 ** (self.decoder.num_stages - 1)\n",
    "\n",
    "    def encode_single(self, x: torch.Tensor, is_video_encoder: bool = False) -> torch.Tensor:\n",
    "        assert x.shape[0] == 1\n",
    "        is_video = x.dim() == 5\n",
    "        if is_video and not is_video_encoder:\n",
    "            b, c, f, h, w = x.shape\n",
    "            x = x.permute(0, 2, 1, 3, 4).reshape(-1, c, h, w)\n",
    "        z = self.encoder(x)\n",
    "\n",
    "        if is_video and not is_video_encoder:\n",
    "            z = z.unsqueeze(dim=0).permute(0, 2, 1, 3, 4)\n",
    "\n",
    "        if self.scaling_factor is not None:\n",
    "            z = z / self.scaling_factor\n",
    "\n",
    "        return z\n",
    "\n",
    "    def _encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.cfg.is_training:\n",
    "            return self.encoder(x)\n",
    "        is_video_encoder = self.encoder.cfg.is_video if self.encoder.cfg.is_video is not None else False\n",
    "        x_ret = []\n",
    "        for i in range(x.shape[0]):\n",
    "            x_ret.append(self.encode_single(x[i : i + 1], is_video_encoder))\n",
    "        return torch.cat(x_ret, dim=0)\n",
    "\n",
    "    def blend_v(self, a: torch.Tensor, b: torch.Tensor, blend_extent: int) -> torch.Tensor:\n",
    "        blend_extent = min(a.shape[-2], b.shape[-2], blend_extent)\n",
    "        for y in range(blend_extent):\n",
    "            b[:, :, :, y, :] = a[:, :, :, -blend_extent + y, :] * (1 - y / blend_extent) + b[:, :, :, y, :] * (\n",
    "                y / blend_extent\n",
    "            )\n",
    "        return b\n",
    "\n",
    "    def blend_h(self, a: torch.Tensor, b: torch.Tensor, blend_extent: int) -> torch.Tensor:\n",
    "        blend_extent = min(a.shape[-1], b.shape[-1], blend_extent)\n",
    "        for x in range(blend_extent):\n",
    "            b[:, :, :, :, x] = a[:, :, :, :, -blend_extent + x] * (1 - x / blend_extent) + b[:, :, :, :, x] * (\n",
    "                x / blend_extent\n",
    "            )\n",
    "        return b\n",
    "\n",
    "    def blend_t(self, a: torch.Tensor, b: torch.Tensor, blend_extent: int) -> torch.Tensor:\n",
    "        blend_extent = min(a.shape[-3], b.shape[-3], blend_extent)\n",
    "        for x in range(blend_extent):\n",
    "            b[:, :, x, :, :] = a[:, :, -blend_extent + x, :, :] * (1 - x / blend_extent) + b[:, :, x, :, :] * (\n",
    "                x / blend_extent\n",
    "            )\n",
    "        return b\n",
    "\n",
    "    def spatial_tiled_encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        net_size = int(self.spatial_tile_size * (1 - self.tile_overlap_factor))\n",
    "        blend_extent = int(self.spatial_tile_latent_size * self.tile_overlap_factor)\n",
    "        row_limit = self.spatial_tile_latent_size - blend_extent\n",
    "\n",
    "        # Split video into tiles and encode them separately.\n",
    "        rows = []\n",
    "        for i in range(0, x.shape[-2], net_size):\n",
    "            row = []\n",
    "            for j in range(0, x.shape[-1], net_size):\n",
    "                tile = x[:, :, :, i : i + self.spatial_tile_size, j : j + self.spatial_tile_size]\n",
    "                tile = self._encode(tile)\n",
    "                row.append(tile)\n",
    "            rows.append(row)\n",
    "        result_rows = []\n",
    "        for i, row in enumerate(rows):\n",
    "            result_row = []\n",
    "            for j, tile in enumerate(row):\n",
    "                # blend the above tile and the left tile\n",
    "                # to the current tile and add the current tile to the result row\n",
    "                if i > 0:\n",
    "                    tile = self.blend_v(rows[i - 1][j], tile, blend_extent)\n",
    "                if j > 0:\n",
    "                    tile = self.blend_h(row[j - 1], tile, blend_extent)\n",
    "                result_row.append(tile[:, :, :, :row_limit, :row_limit])\n",
    "            result_rows.append(torch.cat(result_row, dim=-1))\n",
    "\n",
    "        return torch.cat(result_rows, dim=-2)\n",
    "\n",
    "    def temporal_tiled_encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        overlap_size = int(self.temporal_tile_size * (1 - self.tile_overlap_factor))\n",
    "        blend_extent = int(self.temporal_tile_latent_size * self.tile_overlap_factor)\n",
    "        t_limit = self.temporal_tile_latent_size - blend_extent\n",
    "\n",
    "        # Split the video into tiles and encode them separately.\n",
    "        row = []\n",
    "        for i in range(0, x.shape[2], overlap_size):\n",
    "            tile = x[:, :, i : i + self.temporal_tile_size, :, :]\n",
    "            if self.use_spatial_tiling and (\n",
    "                tile.shape[-1] > self.spatial_tile_size or tile.shape[-2] > self.spatial_tile_size\n",
    "            ):\n",
    "                tile = self.spatial_tiled_encode(tile)\n",
    "            else:\n",
    "                tile = self._encode(tile)\n",
    "            row.append(tile)\n",
    "        result_row = []\n",
    "        for i, tile in enumerate(row):\n",
    "            if i > 0:\n",
    "                tile = self.blend_t(row[i - 1], tile, blend_extent)\n",
    "            result_row.append(tile[:, :, :t_limit, :, :])\n",
    "\n",
    "        return torch.cat(result_row, dim=2)\n",
    "\n",
    "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.use_temporal_tiling and x.shape[2] > self.temporal_tile_size:\n",
    "            return self.temporal_tiled_encode(x)\n",
    "        elif self.use_spatial_tiling and (x.shape[-1] > self.spatial_tile_size or x.shape[-2] > self.spatial_tile_size):\n",
    "            return self.spatial_tiled_encode(x)\n",
    "        else:\n",
    "            return self._encode(x)\n",
    "\n",
    "    def spatial_tiled_decode(self, z: torch.FloatTensor) -> torch.Tensor:\n",
    "        net_size = int(self.spatial_tile_latent_size * (1 - self.tile_overlap_factor))\n",
    "        blend_extent = int(self.spatial_tile_size * self.tile_overlap_factor)\n",
    "        row_limit = self.spatial_tile_size - blend_extent\n",
    "\n",
    "        # Split z into overlapping tiles and decode them separately.\n",
    "        # The tiles have an overlap to avoid seams between tiles.\n",
    "        rows = []\n",
    "        for i in range(0, z.shape[-2], net_size):\n",
    "            row = []\n",
    "            for j in range(0, z.shape[-1], net_size):\n",
    "                tile = z[:, :, :, i : i + self.spatial_tile_latent_size, j : j + self.spatial_tile_latent_size]\n",
    "                decoded = self._decode(tile)\n",
    "                row.append(decoded)\n",
    "            rows.append(row)\n",
    "        result_rows = []\n",
    "        for i, row in enumerate(rows):\n",
    "            result_row = []\n",
    "            for j, tile in enumerate(row):\n",
    "                # blend the above tile and the left tile\n",
    "                # to the current tile and add the current tile to the result row\n",
    "                if i > 0:\n",
    "                    tile = self.blend_v(rows[i - 1][j], tile, blend_extent)\n",
    "                if j > 0:\n",
    "                    tile = self.blend_h(row[j - 1], tile, blend_extent)\n",
    "                result_row.append(tile[:, :, :, :row_limit, :row_limit])\n",
    "            result_rows.append(torch.cat(result_row, dim=-1))\n",
    "\n",
    "        return torch.cat(result_rows, dim=-2)\n",
    "\n",
    "    def temporal_tiled_decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        overlap_size = int(self.temporal_tile_latent_size * (1 - self.tile_overlap_factor))\n",
    "        blend_extent = int(self.temporal_tile_size * self.tile_overlap_factor)\n",
    "        t_limit = self.temporal_tile_size - blend_extent\n",
    "\n",
    "        row = []\n",
    "        for i in range(0, z.shape[2], overlap_size):\n",
    "            tile = z[:, :, i : i + self.temporal_tile_latent_size, :, :]\n",
    "            if self.use_spatial_tiling and (\n",
    "                tile.shape[-1] > self.spatial_tile_latent_size or tile.shape[-2] > self.spatial_tile_latent_size\n",
    "            ):\n",
    "                decoded = self.spatial_tiled_decode(tile)\n",
    "            else:\n",
    "                decoded = self._decode(tile)\n",
    "            row.append(decoded)\n",
    "        result_row = []\n",
    "        for i, tile in enumerate(row):\n",
    "            if i > 0:\n",
    "                tile = self.blend_t(row[i - 1], tile, blend_extent)\n",
    "            result_row.append(tile[:, :, :t_limit, :, :])\n",
    "\n",
    "        return torch.cat(result_row, dim=2)\n",
    "\n",
    "    def decode_single(self, z: torch.Tensor, is_video_decoder: bool = False) -> torch.Tensor:\n",
    "        assert z.shape[0] == 1\n",
    "        is_video = z.dim() == 5\n",
    "        if is_video and not is_video_decoder:\n",
    "            b, c, f, h, w = z.shape\n",
    "            z = z.permute(0, 2, 1, 3, 4).reshape(-1, c, h, w)\n",
    "        if self.scaling_factor is not None:\n",
    "            z = z * self.scaling_factor\n",
    "\n",
    "        x = self.decoder(z)\n",
    "\n",
    "        if is_video and not is_video_decoder:\n",
    "            x = x.unsqueeze(dim=0).permute(0, 2, 1, 3, 4)\n",
    "        return x\n",
    "\n",
    "    def _decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        if self.cfg.is_training:\n",
    "            return self.decoder(z)\n",
    "        is_video_decoder = self.decoder.cfg.is_video if self.decoder.cfg.is_video is not None else False\n",
    "        x_ret = []\n",
    "        for i in range(z.shape[0]):\n",
    "            x_ret.append(self.decode_single(z[i : i + 1], is_video_decoder))\n",
    "        return torch.cat(x_ret, dim=0)\n",
    "\n",
    "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        if self.use_temporal_tiling and z.shape[2] > self.temporal_tile_latent_size:\n",
    "            return self.temporal_tiled_decode(z)\n",
    "        elif self.use_spatial_tiling and (\n",
    "            z.shape[-1] > self.spatial_tile_latent_size or z.shape[-2] > self.spatial_tile_latent_size\n",
    "        ):\n",
    "            return self.spatial_tiled_decode(z)\n",
    "        else:\n",
    "            return self._decode(z)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[Any, Tensor, dict[Any, Any]]:\n",
    "        x_type = x.dtype\n",
    "        is_image_model = self.cfg.__dict__.get(\"is_image_model\", False)\n",
    "        x = x.to(self.encoder.project_in.conv.weight.dtype)\n",
    "\n",
    "        if is_image_model:\n",
    "            b, c, _, h, w = x.shape\n",
    "            x = x.permute(0, 2, 1, 3, 4).reshape(-1, c, h, w)\n",
    "\n",
    "        z = self.encode(x)\n",
    "        dec = self.decode(z)\n",
    "\n",
    "        if is_image_model:\n",
    "            dec = dec.reshape(b, 1, c, h, w).permute(0, 2, 1, 3, 4)\n",
    "            z = z.unsqueeze(dim=0).permute(0, 2, 1, 3, 4)\n",
    "\n",
    "        dec = dec.to(x_type)\n",
    "        return dec, None, z\n",
    "\n",
    "    def get_latent_size(self, input_size: list[int]) -> list[int]:\n",
    "        latent_size = []\n",
    "        # T\n",
    "        latent_size.append((input_size[0] - 1) // self.time_compression_ratio + 1)\n",
    "        # H, w\n",
    "        for i in range(1, 3):\n",
    "            latent_size.append((input_size[i] - 1) // self.spatial_compression_ratio + 1)\n",
    "        return latent_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63711488",
   "metadata": {},
   "source": [
    "### ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08c3d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dc_ae_f32(name: str, pretrained_path: str) -> DCAEConfig:\n",
    "    if name in [\"dc-ae-f32t4c128\"]:\n",
    "        cfg_str = (\n",
    "            \"time_compression_ratio=4 \"\n",
    "            \"spatial_compression_ratio=32 \"\n",
    "            \"encoder.block_type=[ResBlock,ResBlock,ResBlock,EViTS5_GLU,EViTS5_GLU,EViTS5_GLU] \"\n",
    "            \"encoder.width_list=[128,256,512,512,1024,1024] encoder.depth_list=[2,2,2,3,3,3] \"\n",
    "            \"encoder.downsample_block_type=Conv \"\n",
    "            \"encoder.norm=rms3d \"\n",
    "            \"encoder.is_video=True \"\n",
    "            \"decoder.block_type=[ResBlock,ResBlock,ResBlock,EViTS5_GLU,EViTS5_GLU,EViTS5_GLU] \"\n",
    "            \"decoder.width_list=[128,256,512,512,1024,1024] decoder.depth_list=[3,3,3,3,3,3] \"\n",
    "            \"decoder.upsample_block_type=InterpolateConv \"\n",
    "            \"decoder.norm=rms3d decoder.act=silu decoder.out_norm=rms3d \"\n",
    "            \"decoder.is_video=True \"\n",
    "            \"encoder.temporal_downsample=[False,False,False,True,True,False] \"\n",
    "            \"decoder.temporal_upsample=[False,False,False,True,True,False] \"\n",
    "            \"latent_channels=128\"\n",
    "        )  # make sure there is no trailing blankspace in the last line\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    cfg = OmegaConf.from_dotlist(cfg_str.split(\" \"))\n",
    "    cfg: DCAEConfig = OmegaConf.to_object(OmegaConf.merge(OmegaConf.structured(DCAEConfig), cfg))\n",
    "    cfg.pretrained_path = pretrained_path\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847c9fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGISTERED_DCAE_MODEL: dict[str, tuple[Callable, Optional[str]]] = {\n",
    "    \"dc-ae-f32t4c128\": (dc_ae_f32, None),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267e5839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dc_ae_model_cfg(name: str, pretrained_path: Optional[str] = None) -> DCAEConfig:\n",
    "    \"\"\"\n",
    "    ç™»éŒ²ã•ã‚ŒãŸDCAEãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’ä½œæˆã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        name (str): ç™»éŒ²ã•ã‚ŒãŸDCAEãƒ¢ãƒ‡ãƒ«ã®åå‰\n",
    "        pretrained_path (Optional[str], optional): äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹\n",
    "    Returns:\n",
    "        DCAEConfig: DCAEãƒ¢ãƒ‡ãƒ«ã®è¨­å®š\n",
    "    \"\"\"\n",
    "    assert name in REGISTERED_DCAE_MODEL, f\"{name} is not supported\"\n",
    "    dc_ae_cls, default_pt_path = REGISTERED_DCAE_MODEL[name]\n",
    "    pretrained_path = default_pt_path if pretrained_path is None else pretrained_path\n",
    "    model_cfg = dc_ae_cls(name, pretrained_path)\n",
    "    return model_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8d3234",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCAE_HF(DCAE, PyTorchModelHubMixin):\n",
    "    \"\"\"\n",
    "    HuggingFace Hubå¯¾å¿œDCAEãƒ¢ãƒ‡ãƒ«\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str):\n",
    "        logger.info(f\"DCAE_HF.__init__ {model_name=}\")\n",
    "        cfg = create_dc_ae_model_cfg(model_name)\n",
    "        DCAE.__init__(self, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85f81ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @MODELS.register_module(\"dc_ae\")\n",
    "def DC_AE(\n",
    "    model_name: str,\n",
    "    device_map: str | torch.device = \"cuda\",\n",
    "    torch_dtype: torch.dtype = torch.bfloat16,\n",
    "    from_scratch: bool = False,\n",
    "    from_pretrained: str | None = None,\n",
    "    is_training: bool = False,\n",
    "    use_spatial_tiling: bool = False,\n",
    "    use_temporal_tiling: bool = False,\n",
    "    spatial_tile_size: int = 256,\n",
    "    temporal_tile_size: int = 32,\n",
    "    tile_overlap_factor: float = 0.25,\n",
    "    scaling_factor: float = None,\n",
    "    disc_off_grad_ckpt: bool = False,\n",
    ") -> DCAE_HF:\n",
    "    \"\"\"\n",
    "    Deep Compressive Autoencoder (DCAE)ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        model_name (str): ãƒ¢ãƒ‡ãƒ«ã®åå‰\n",
    "        device_map (str or torch.device, optional): ãƒ¢ãƒ‡ãƒ«ã‚’é…ç½®ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹\n",
    "        torch_dtype (torch.dtype, optional): ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ¼ã‚¿å‹\n",
    "        from_scratch (bool, optional): ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã™ã‚‹ã‹ã©ã†ã‹\n",
    "        from_pretrained (str or None, optional): äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹\n",
    "        is_training (bool, optional): ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®šã™ã‚‹ã‹ã©ã†ã‹\n",
    "        use_spatial_tiling (bool, optional): ç©ºé–“ã‚¿ã‚¤ãƒ«å‡¦ç†ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "        use_temporal_tiling (bool, optional): æ™‚é–“ã‚¿ã‚¤ãƒ«å‡¦ç†ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "        spatial_tile_size (int, optional): ç©ºé–“ã‚¿ã‚¤ãƒ«ã®ã‚µã‚¤ã‚º\n",
    "        temporal_tile_size (int, optional): æ™‚é–“ã‚¿ã‚¤ãƒ«ã®ã‚µã‚¤ã‚º\n",
    "        tile_overlap_factor (float, optional): ã‚¿ã‚¤ãƒ«ã®é‡ãªã‚Šä¿‚æ•°\n",
    "        scaling_factor (float, optional): ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼\n",
    "        disc_off_grad_ckpt (bool, optional): å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ç„¡åŠ¹ã«ã™ã‚‹ã‹ã©ã†ã‹\n",
    "    Returns:\n",
    "        DCAE_HF: DCAEãƒ¢ãƒ‡ãƒ«\n",
    "    \"\"\"\n",
    "    logger.info(f\"DC_AE {model_name=} {device_map=} {torch_dtype=} {from_scratch=} {from_pretrained=} {is_training=} {use_spatial_tiling=} {use_temporal_tiling=} {spatial_tile_size=} {temporal_tile_size=} {tile_overlap_factor=} {scaling_factor=} {disc_off_grad_ckpt=}\")\n",
    "\n",
    "    if not from_scratch:\n",
    "        model = DCAE_HF.from_pretrained(model_name).to(device_map, torch_dtype)\n",
    "    else:\n",
    "        model = DCAE_HF(model_name).to(device_map, torch_dtype)\n",
    "\n",
    "    if from_pretrained is not None:\n",
    "        model = load_checkpoint(model, from_pretrained, device_map=device_map)\n",
    "        print(f\"loaded dc_ae from ckpt path: {from_pretrained}\")\n",
    "\n",
    "    model.cfg.is_training = is_training\n",
    "    model.use_spatial_tiling = use_spatial_tiling\n",
    "    model.use_temporal_tiling = use_temporal_tiling\n",
    "    model.spatial_tile_size = spatial_tile_size\n",
    "    model.temporal_tile_size = temporal_tile_size\n",
    "    model.tile_overlap_factor = tile_overlap_factor\n",
    "    if scaling_factor is not None:\n",
    "        model.scaling_factor = scaling_factor\n",
    "    model.decoder.disc_off_grad_ckpt = disc_off_grad_ckpt\n",
    "    return model\n",
    "\n",
    "# {'type': 'dc_ae',\n",
    "#  'model_name': 'dc-ae-f32t4c128',\n",
    "#  'from_scratch': True,\n",
    "#  'from_pretrained': None}\n",
    "\n",
    "DC_AE(\n",
    "    model_name=\"dc-ae-f32t4c128\",\n",
    "    from_scratch=True,\n",
    "    from_pretrained=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fabd6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (batch_size, channels, frames, height, width)\n",
    "sample_input = torch.randn(1, 3, 1, 256, 256).to(\"cuda\")\n",
    "\n",
    "model = DC_AE(\n",
    "    model_name=\"dc-ae-f32t4c128\",\n",
    "    from_scratch=True,\n",
    "    from_pretrained=None,\n",
    "    is_training=False,\n",
    "    # use_spatial_tiling=True,\n",
    "    # use_temporal_tiling=True,\n",
    "    # spatial_tile_size=256,\n",
    "    # temporal_tile_size=16,\n",
    "    # tile_overlap_factor=0.25,\n",
    ")\n",
    "output = model(sample_input)\n",
    "print(output[0].shape)  # Decoded output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41770ec",
   "metadata": {},
   "source": [
    "## MMDiT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08bcb7f",
   "metadata": {},
   "source": [
    "### LigerEmbedND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49838b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def liger_rope(pos: Tensor, dim: int, theta: int) -> Tuple:\n",
    "    \"\"\"\n",
    "    Liger RoPE\n",
    "    LigerEmbedNDã§ä½¿ç”¨\n",
    "\n",
    "    Args:\n",
    "        pos (Tensor): ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®ä½ç½®ãƒ†ãƒ³ã‚½ãƒ« (..., n)\n",
    "        dim (int): åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°\n",
    "        theta (int): RoPEã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "    Returns:\n",
    "        Tuple: ã‚³ã‚µã‚¤ãƒ³ã¨ã‚µã‚¤ãƒ³ã®ãƒ†ãƒ³ã‚½ãƒ« (..., n, dim//2)\n",
    "    \"\"\"\n",
    "    logger.info(f\"liger_rope {pos.shape=} {dim=} {theta=}\")\n",
    "\n",
    "    assert dim % 2 == 0\n",
    "    scale = torch.arange(0, dim, 2, dtype=torch.float32, device=pos.device) / dim\n",
    "    omega = 1.0 / (theta**scale)\n",
    "    out = torch.einsum(\"...n,d->...nd\", pos, omega)  # (b, seq, dim//2)\n",
    "    cos = out.cos()\n",
    "    sin = out.sin()\n",
    "\n",
    "    return (cos, sin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36150305",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LigerEmbedND(nn.Module):\n",
    "    \"\"\"\n",
    "    Liger Multi-dimensional RoPE Embedding\n",
    "    MMDiTModelã§ä½¿ç”¨\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, theta: int, axes_dim: list[int]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°\n",
    "            theta (int): RoPEã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "            axes_dim (list[int]): å„è»¸ã®æ¬¡å…ƒæ•°\n",
    "        \"\"\"\n",
    "        logger.info(f\"LigerEmbedND.__init__ {dim=} {theta=} {axes_dim=}\")\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.theta = theta\n",
    "        self.axes_dim = axes_dim\n",
    "\n",
    "    def forward(self, ids: Tensor) -> Tensor:\n",
    "        logger.info(f\"LigerEmbedND.forward {ids.shape=}\")\n",
    "        n_axes = ids.shape[-1]\n",
    "        cos_list = []\n",
    "        sin_list = []\n",
    "        for i in range(n_axes):\n",
    "            cos, sin = liger_rope(ids[..., i], self.axes_dim[i], self.theta)\n",
    "            cos_list.append(cos)\n",
    "            sin_list.append(sin)\n",
    "        cos_emb = torch.cat(cos_list, dim=-1).repeat(1, 1, 2).contiguous()\n",
    "        sin_emb = torch.cat(sin_list, dim=-1).repeat(1, 1, 2).contiguous()\n",
    "\n",
    "        return (cos_emb, sin_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e418d284",
   "metadata": {},
   "source": [
    "### MLPEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bc8a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPEmbedder(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP Embedder\n",
    "    MMDiTModelã§ä½¿ç”¨\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int, hidden_dim: int):\n",
    "        logger.info(f\"MLPEmbedder.__init__ {in_dim=} {hidden_dim=}\")\n",
    "        super().__init__()\n",
    "        self.in_layer = nn.Linear(in_dim, hidden_dim, bias=True)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.out_layer = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        logger.info(f\"MLPEmbedder.forward {x.shape=}\")\n",
    "        return self.out_layer(self.silu(self.in_layer(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68deecd",
   "metadata": {},
   "source": [
    "### QKNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf50457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    RMS Normalization\n",
    "    FusedRMSNormã§ç¶™æ‰¿\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°\n",
    "        \"\"\"\n",
    "        logger.info(f\"RMSNorm.__init__ {dim=}\")\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        logger.info(f\"RMSNorm.forward {x.shape=}\")\n",
    "        x_dtype = x.dtype\n",
    "        x = x.float()\n",
    "        rrms = torch.rsqrt(torch.mean(x**2, dim=-1, keepdim=True) + 1e-6)\n",
    "        return (x * rrms).to(dtype=x_dtype) * self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3175d11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusedRMSNorm(RMSNorm):\n",
    "    \"\"\"\n",
    "    Fused RMS Normalization\n",
    "    QKNormã§ä½¿ç”¨\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        logger.info(f\"FusedRMSNorm.forward {x.shape=}\")\n",
    "        return LigerRMSNormFunction.apply(\n",
    "            x,\n",
    "            self.scale,\n",
    "            1e-6,\n",
    "            0.0,\n",
    "            \"llama\",\n",
    "            False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8675753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QKNorm(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Query-Key Normalization\n",
    "    Self-Attentionã§ä½¿ç”¨\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°\n",
    "        \"\"\"\n",
    "        logger.info(f\"QKNorm.__init__ {dim=}\")\n",
    "        super().__init__()\n",
    "        self.query_norm = FusedRMSNorm(dim)\n",
    "        self.key_norm = FusedRMSNorm(dim)\n",
    "\n",
    "    def forward(self, q: Tensor, k: Tensor, v: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            q (Tensor): ã‚¯ã‚¨ãƒªãƒ†ãƒ³ã‚½ãƒ« (..., dim)\n",
    "            k (Tensor): ã‚­ãƒ¼ãƒ†ãƒ³ã‚½ãƒ« (..., dim)\n",
    "            v (Tensor): ãƒãƒªãƒ¥ãƒ¼ãƒ†ãƒ³ã‚½ãƒ« (..., dim)\n",
    "        Returns:\n",
    "            tuple[Tensor, Tensor]: æ­£è¦åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒªã¨ã‚­ãƒ¼ã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        \"\"\"\n",
    "        logger.info(f\"QKNorm.forward {q.shape=} {k.shape=} {v.shape=}\")\n",
    "        q = self.query_norm(q)\n",
    "        k = self.key_norm(k)\n",
    "        return q.to(v), k.to(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ad6464",
   "metadata": {},
   "source": [
    "### ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ©Ÿæ§‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6511eb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flash_attn_func(q: Tensor, k: Tensor, v: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Flash Attentionã‚’ä½¿ç”¨ã—ãŸã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¨ˆç®—\n",
    "    Args:\n",
    "        q (Tensor): ã‚¯ã‚¨ãƒªãƒ†ãƒ³ã‚½ãƒ« (B, H, L, D)\n",
    "        k (Tensor): ã‚­ãƒ¼ãƒ†ãƒ³ã‚½ãƒ« (B, H, L, D)\n",
    "        v (Tensor): ãƒãƒªãƒ¥ãƒ¼ãƒ†ãƒ³ã‚½ãƒ« (B, H, L, D)\n",
    "    Returns:\n",
    "        Tensor: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‡ºåŠ›ãƒ†ãƒ³ã‚½ãƒ« (B, L, H, D)\n",
    "    \"\"\"\n",
    "    logger.info(f\"flash_attn_func {q.shape=} {k.shape=} {v.shape=}\")\n",
    "\n",
    "    if SUPPORT_FA3:\n",
    "        return flash_attn_func_v3(q, k, v)[0]\n",
    "    return flash_attn_func_v2(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8654fb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(q: Tensor, k: Tensor, v: Tensor, pe) -> Tensor:\n",
    "    \"\"\"\n",
    "    ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¨ˆç®—\n",
    "    SingleStreamBlockProcessor, DoubleSreamBlockProcessor,\n",
    "    SelfAttentionã§ä½¿ç”¨\n",
    "    Args:\n",
    "        q (Tensor): ã‚¯ã‚¨ãƒªãƒ†ãƒ³ã‚½ãƒ« (B, L, H, D)\n",
    "        k (Tensor): ã‚­ãƒ¼ãƒ†ãƒ³ã‚½ãƒ« (B, L, H, D)\n",
    "        v (Tensor): ãƒãƒªãƒ¥ãƒ¼ãƒ†ãƒ³ã‚½ãƒ« (B, L, H, D)\n",
    "        pe: ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ†ãƒ³ã‚½ãƒ«ã¾ãŸã¯ã‚³ã‚µã‚¤ãƒ³ãƒ»ã‚µã‚¤ãƒ³ãƒ†ãƒ³ã‚½ãƒ«ã®ã‚¿ãƒ—ãƒ«\n",
    "    Returns:\n",
    "        Tensor: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‡ºåŠ›ãƒ†ãƒ³ã‚½ãƒ« (B, L, H*D)\n",
    "    \"\"\"\n",
    "    if isinstance(pe, torch.Tensor):\n",
    "        q, k = apply_rope(q, k, pe)\n",
    "    else:\n",
    "        cos, sin = pe\n",
    "        q, k = LigerRopeFunction.apply(q, k, cos, sin)\n",
    "        # to compare with the original implementation\n",
    "        # k = reverse_rearrange_tensor(k)\n",
    "    q = rearrange(q, \"B H L D -> B L H D\")\n",
    "    k = rearrange(k, \"B H L D -> B L H D\")\n",
    "    v = rearrange(v, \"B H L D -> B L H D\")\n",
    "    x = flash_attn_func(q, k, v)\n",
    "    x = rearrange(x, \"B L H D -> B L (H D)\")\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3045aa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    ã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³\n",
    "    DoubleStreamBlockã§ä½¿ç”¨\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, num_heads: int = 8, qkv_bias: bool = False, fused_qkv: bool = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°\n",
    "            num_heads (int, optional): ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰æ•°. Defaults to 8.\n",
    "            qkv_bias (bool, optional): QKVã®ãƒã‚¤ã‚¢ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹. Defaults to False.\n",
    "            fused_qkv (bool, optional): QKVã‚’èåˆã—ãŸç·šå½¢å±¤ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹. Defaults to True.\n",
    "        \"\"\"\n",
    "        logger.info(f\"SelfAttention.__init__ {dim=} {num_heads=} {qkv_bias=} {fused_qkv=}\")\n",
    "\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.fused_qkv = fused_qkv\n",
    "        head_dim = dim // num_heads\n",
    "\n",
    "        if fused_qkv:\n",
    "            self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        else:\n",
    "            self.q_proj = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "            self.k_proj = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "            self.v_proj = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.norm = QKNorm(head_dim)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x: Tensor, pe: Tensor) -> Tensor:\n",
    "        logger.info(f\"SelfAttention.forward {x.shape=} {pe.shape=}\")\n",
    "        if self.fused_qkv:\n",
    "            qkv = self.qkv(x)\n",
    "            q, k, v = rearrange(qkv, \"B L (K H D) -> K B H L D\", K=3, H=self.num_heads)\n",
    "        else:\n",
    "            q = rearrange(self.q_proj(x), \"B L (H D) -> B L H D\", H=self.num_heads)\n",
    "            k = rearrange(self.k_proj(x), \"B L (H D) -> B L H D\", H=self.num_heads)\n",
    "            v = rearrange(self.v_proj(x), \"B L (H D) -> B L H D\", H=self.num_heads)\n",
    "        q, k = self.norm(q, k, v)\n",
    "        if not self.fused_qkv:\n",
    "            q = rearrange(q, \"B L H D -> B H L D\")\n",
    "            k = rearrange(k, \"B L H D -> B H L D\")\n",
    "            v = rearrange(v, \"B L H D -> B H L D\")\n",
    "        x = attention(q, k, v, pe=pe)\n",
    "        x = self.proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e9da89",
   "metadata": {},
   "source": [
    "### Modulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee5fbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModulationOut:\n",
    "    shift: Tensor\n",
    "    scale: Tensor\n",
    "    gate: Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c045c4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modulation(nn.Module):\n",
    "    \"\"\"\n",
    "    ãƒ¢ã‚¸ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å±¤\n",
    "    DoubleStreamBlockã§ä½¿ç”¨\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, double: bool):\n",
    "        logger.info(f\"Modulation.__init__ {dim=} {double=}\")\n",
    "        super().__init__()\n",
    "        self.is_double = double\n",
    "        self.multiplier = 6 if double else 3\n",
    "        self.lin = nn.Linear(dim, self.multiplier * dim, bias=True)\n",
    "\n",
    "    def forward(self, vec: Tensor) -> tuple[ModulationOut, ModulationOut | None]:\n",
    "        logger.info(f\"Modulation.forward {vec.shape=}\")\n",
    "        out = self.lin(nn.functional.silu(vec))[:, None, :].chunk(self.multiplier, dim=-1)\n",
    "\n",
    "        return (\n",
    "            ModulationOut(*out[:3]),\n",
    "            ModulationOut(*out[3:]) if self.is_double else None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd63fafd",
   "metadata": {},
   "source": [
    "### DoubleStreamBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3eafe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleStreamBlockProcessor:\n",
    "    \"\"\"\n",
    "    ãƒ€ãƒ–ãƒ«ã‚¹ãƒˆãƒªãƒ¼ãƒ ãƒ–ãƒ­ãƒƒã‚¯ãƒ—ãƒ­ã‚»ãƒƒã‚µ\n",
    "    DoubleStreamBlockã§ä½¿ç”¨\n",
    "    ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã®ä¸¡æ–¹ã®ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’å‡¦ç†ã™ã‚‹\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, attn: nn.Module, img: Tensor, txt: Tensor, vec: Tensor, pe: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        logger.info(f\"DoubleStreamBlockProcessor.__call__ {img.shape=} {txt.shape=} {vec.shape=} {pe[0].shape=} {pe[1].shape=}\")\n",
    "\n",
    "        # attn is the DoubleStreamBlock;\n",
    "        # process img and txt separately while both is influenced by text vec\n",
    "\n",
    "        # vec will interact with image latent and text context\n",
    "        img_mod1, img_mod2 = attn.img_mod(vec)  # get shift, scale, gate for each mod\n",
    "        txt_mod1, txt_mod2 = attn.txt_mod(vec)\n",
    "\n",
    "        # prepare image for attention\n",
    "        img_modulated = attn.img_norm1(img)\n",
    "        img_modulated = (1 + img_mod1.scale) * img_modulated + img_mod1.shift\n",
    "\n",
    "        if attn.img_attn.fused_qkv:\n",
    "            img_qkv = attn.img_attn.qkv(img_modulated)\n",
    "            img_q, img_k, img_v = rearrange(img_qkv, \"B L (K H D) -> K B H L D\", K=3, H=attn.num_heads, D=attn.head_dim)\n",
    "        else:\n",
    "            img_q = rearrange(attn.img_attn.q_proj(img_modulated), \"B L (H D) -> B L H D\", H=attn.num_heads)\n",
    "            img_k = rearrange(attn.img_attn.k_proj(img_modulated), \"B L (H D) -> B L H D\", H=attn.num_heads)\n",
    "            img_v = rearrange(attn.img_attn.v_proj(img_modulated), \"B L (H D) -> B L H D\", H=attn.num_heads)\n",
    "\n",
    "        img_q, img_k = attn.img_attn.norm(img_q, img_k, img_v)  # RMSNorm for QK Norm as in SD3 paper\n",
    "        if not attn.img_attn.fused_qkv:\n",
    "            img_q = rearrange(img_q, \"B L H D -> B H L D\")\n",
    "            img_k = rearrange(img_k, \"B L H D -> B H L D\")\n",
    "            img_v = rearrange(img_v, \"B L H D -> B H L D\")\n",
    "\n",
    "        # prepare txt for attention\n",
    "        txt_modulated = attn.txt_norm1(txt)\n",
    "        txt_modulated = (1 + txt_mod1.scale) * txt_modulated + txt_mod1.shift\n",
    "        if attn.txt_attn.fused_qkv:\n",
    "            txt_qkv = attn.txt_attn.qkv(txt_modulated)\n",
    "            txt_q, txt_k, txt_v = rearrange(txt_qkv, \"B L (K H D) -> K B H L D\", K=3, H=attn.num_heads, D=attn.head_dim)\n",
    "        else:\n",
    "            txt_q = rearrange(attn.txt_attn.q_proj(txt_modulated), \"B L (H D) -> B L H D\", H=attn.num_heads)\n",
    "            txt_k = rearrange(attn.txt_attn.k_proj(txt_modulated), \"B L (H D) -> B L H D\", H=attn.num_heads)\n",
    "            txt_v = rearrange(attn.txt_attn.v_proj(txt_modulated), \"B L (H D) -> B L H D\", H=attn.num_heads)\n",
    "        txt_q, txt_k = attn.txt_attn.norm(txt_q, txt_k, txt_v)\n",
    "        if not attn.txt_attn.fused_qkv:\n",
    "            txt_q = rearrange(txt_q, \"B L H D -> B H L D\")\n",
    "            txt_k = rearrange(txt_k, \"B L H D -> B H L D\")\n",
    "            txt_v = rearrange(txt_v, \"B L H D -> B H L D\")\n",
    "\n",
    "        # run actual attention, image and text attention are calculated together by concat different attn heads\n",
    "        q = torch.cat((txt_q, img_q), dim=2)\n",
    "        k = torch.cat((txt_k, img_k), dim=2)\n",
    "        v = torch.cat((txt_v, img_v), dim=2)\n",
    "\n",
    "        attn1 = attention(q, k, v, pe=pe)\n",
    "        txt_attn, img_attn = attn1[:, : txt_q.shape[2]], attn1[:, txt_q.shape[2] :]\n",
    "\n",
    "        # calculate the img bloks\n",
    "        img = img + img_mod1.gate * attn.img_attn.proj(img_attn)\n",
    "        img = img + img_mod2.gate * attn.img_mlp((1 + img_mod2.scale) * attn.img_norm2(img) + img_mod2.shift)\n",
    "\n",
    "        # calculate the txt bloks\n",
    "        txt = txt + txt_mod1.gate * attn.txt_attn.proj(txt_attn)\n",
    "        txt = txt + txt_mod2.gate * attn.txt_mlp((1 + txt_mod2.scale) * attn.txt_norm2(txt) + txt_mod2.shift)\n",
    "        return img, txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82098a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleStreamBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ãƒ€ãƒ–ãƒ«ã‚¹ãƒˆãƒªãƒ¼ãƒ ãƒ–ãƒ­ãƒƒã‚¯\n",
    "    MMDiTModelã§ä½¿ç”¨\n",
    "    ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã®ä¸¡æ–¹ã®ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’å‡¦ç†ã™ã‚‹\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size: int, num_heads: int, mlp_ratio: float, qkv_bias: bool = False, fused_qkv: bool = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_size (int): åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°\n",
    "            num_heads (int): ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰æ•°\n",
    "            mlp_ratio (float): MLPã®éš ã‚Œå±¤ã®æ¬¡å…ƒæ•°ã®æ¯”ç‡\n",
    "            qkv_bias (bool, optional): QKVã®ãƒã‚¤ã‚¢ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "            fused_qkv (bool, optional): QKVã‚’èåˆã—ãŸç·šå½¢å±¤ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "        \"\"\"\n",
    "        logger.info(f\"DoubleStreamBlock.__init__ {hidden_size=} {num_heads=} {mlp_ratio=} {qkv_bias=} {fused_qkv=}\")\n",
    "\n",
    "        super().__init__()\n",
    "        mlp_hidden_dim = int(hidden_size * mlp_ratio)\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "\n",
    "        # image stream\n",
    "        self.img_mod = Modulation(hidden_size, double=True)\n",
    "        self.img_norm1 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
    "        self.img_attn = SelfAttention(dim=hidden_size, num_heads=num_heads, qkv_bias=qkv_bias, fused_qkv=fused_qkv)\n",
    "\n",
    "        self.img_norm2 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
    "        self.img_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, mlp_hidden_dim, bias=True),\n",
    "            nn.GELU(approximate=\"tanh\"),\n",
    "            nn.Linear(mlp_hidden_dim, hidden_size, bias=True),\n",
    "        )\n",
    "\n",
    "        # text stream\n",
    "        self.txt_mod = Modulation(hidden_size, double=True)\n",
    "        self.txt_norm1 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
    "        self.txt_attn = SelfAttention(dim=hidden_size, num_heads=num_heads, qkv_bias=qkv_bias, fused_qkv=fused_qkv)\n",
    "\n",
    "        self.txt_norm2 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
    "        self.txt_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, mlp_hidden_dim, bias=True),\n",
    "            nn.GELU(approximate=\"tanh\"),\n",
    "            nn.Linear(mlp_hidden_dim, hidden_size, bias=True),\n",
    "        )\n",
    "\n",
    "        # processor\n",
    "        processor = DoubleStreamBlockProcessor()\n",
    "        self.set_processor(processor)\n",
    "\n",
    "    def set_processor(self, processor) -> None:\n",
    "        self.processor = processor\n",
    "\n",
    "    def get_processor(self):\n",
    "        return self.processor\n",
    "\n",
    "    def forward(self, img: Tensor, txt: Tensor, vec: Tensor, pe: Tensor, **kwargs) -> tuple[Tensor, Tensor]:\n",
    "        logger.info(f\"DoubleStreamBlock.forward {img.shape=} {txt.shape=} {vec.shape=} {pe[0].shape=} {pe[1].shape=}\")\n",
    "        return self.processor(self, img, txt, vec, pe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43837fc",
   "metadata": {},
   "source": [
    "#### SingleStreamBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc340020",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleStreamBlockProcessor:\n",
    "    \"\"\"\n",
    "    ã‚·ãƒ³ã‚°ãƒ«ã‚¹ãƒˆãƒªãƒ¼ãƒ ãƒ–ãƒ­ãƒƒã‚¯ãƒ—ãƒ­ã‚»ãƒƒã‚µ\n",
    "    SingleStreamBlockã§ä½¿ç”¨\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, attn: nn.Module, x: Tensor, vec: Tensor, pe: Tensor) -> Tensor:\n",
    "        logger.info(f\"SingleStreamBlockProcessor.__call__ {x.shape=} {vec.shape=} {pe[0].shape=} {pe[1].shape=}\")\n",
    "\n",
    "        mod, _ = attn.modulation(vec)\n",
    "        x_mod = (1 + mod.scale) * attn.pre_norm(x) + mod.shift\n",
    "        if attn.fused_qkv:\n",
    "            qkv, mlp = torch.split(attn.linear1(x_mod), [3 * attn.hidden_size, attn.mlp_hidden_dim], dim=-1)\n",
    "            q, k, v = rearrange(qkv, \"B L (K H D) -> K B H L D\", K=3, H=attn.num_heads)\n",
    "        else:\n",
    "            q = rearrange(attn.q_proj(x_mod), \"B L (H D) -> B L H D\", H=attn.num_heads)\n",
    "            k = rearrange(attn.k_proj(x_mod), \"B L (H D) -> B L H D\", H=attn.num_heads)\n",
    "            v, mlp = torch.split(attn.v_mlp(x_mod), [attn.hidden_size, attn.mlp_hidden_dim], dim=-1)\n",
    "            v = rearrange(v, \"B L (H D) -> B L H D\", H=attn.num_heads)\n",
    "\n",
    "        q, k = attn.norm(q, k, v)\n",
    "        if not attn.fused_qkv:\n",
    "            q = rearrange(q, \"B L H D -> B H L D\")\n",
    "            k = rearrange(k, \"B L H D -> B H L D\")\n",
    "            v = rearrange(v, \"B L H D -> B H L D\")\n",
    "\n",
    "        # compute attention\n",
    "        attn_1 = attention(q, k, v, pe=pe)\n",
    "\n",
    "        # compute activation in mlp stream, cat again and run second linear layer\n",
    "        output = attn.linear2(torch.cat((attn_1, attn.mlp_act(mlp)), 2))\n",
    "        output = x + mod.gate * output\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d3021b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleStreamBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ã‚·ãƒ³ã‚°ãƒ«ã‚¹ãƒˆãƒªãƒ¼ãƒ ãƒ–ãƒ­ãƒƒã‚¯\n",
    "    MMDiTModelã§ä½¿ç”¨\n",
    "\n",
    "    ä¸¦åˆ—ç·šå½¢å±¤ã‚’æŒã¤DiTãƒ–ãƒ­ãƒƒã‚¯ã§ã€å¤‰èª¿ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ãŒé©å¿œ\n",
    "    https://arxiv.org/abs/2302.05442\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        num_heads: int,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        qk_scale: float | None = None,\n",
    "        fused_qkv: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_size (int): åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°\n",
    "            num_heads (int): ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰æ•°\n",
    "            mlp_ratio (float, optional): MLPã®éš ã‚Œå±¤ã®æ¬¡å…ƒæ•°ã®æ¯”ç‡\n",
    "            qk_scale (float | None, optional): QKã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼\n",
    "            fused_qkv (bool, optional): QKVã‚’èåˆã—ãŸç·šå½¢å±¤ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "        \"\"\"\n",
    "        logger.info(f\"SingleStreamBlock.__init__ {hidden_size=} {num_heads=} {mlp_ratio=} {qk_scale=} {fused_qkv=}\")\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        self.scale = qk_scale or self.head_dim**-0.5\n",
    "        self.fused_qkv = fused_qkv\n",
    "\n",
    "        self.mlp_hidden_dim = int(hidden_size * mlp_ratio)\n",
    "        if fused_qkv:\n",
    "            # qkv and mlp_in\n",
    "            self.linear1 = nn.Linear(hidden_size, hidden_size * 3 + self.mlp_hidden_dim)\n",
    "        else:\n",
    "            self.q_proj = nn.Linear(hidden_size, hidden_size)\n",
    "            self.k_proj = nn.Linear(hidden_size, hidden_size)\n",
    "            self.v_mlp = nn.Linear(hidden_size, hidden_size + self.mlp_hidden_dim)\n",
    "\n",
    "        # proj and mlp_out\n",
    "        self.linear2 = nn.Linear(hidden_size + self.mlp_hidden_dim, hidden_size)\n",
    "\n",
    "        self.norm = QKNorm(self.head_dim)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.pre_norm = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
    "\n",
    "        self.mlp_act = nn.GELU(approximate=\"tanh\")\n",
    "        self.modulation = Modulation(hidden_size, double=False)\n",
    "\n",
    "        processor = SingleStreamBlockProcessor()\n",
    "        self.set_processor(processor)\n",
    "\n",
    "    def set_processor(self, processor) -> None:\n",
    "        self.processor = processor\n",
    "\n",
    "    def get_processor(self):\n",
    "        return self.processor\n",
    "\n",
    "    def forward(self, x: Tensor, vec: Tensor, pe: Tensor, **kwargs) -> Tensor:\n",
    "        logger.info(f\"SingleStreamBlock.forward {x.shape=} {vec.shape=} {pe[0].shape=} {pe[1].shape=}\")\n",
    "        return self.processor(self, x, vec, pe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1b6d7a",
   "metadata": {},
   "source": [
    "#### MMDiT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7a53e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile(mode=\"max-autotune-no-cudagraphs\", dynamic=True)\n",
    "def timestep_embedding(t: Tensor, dim, max_period=10000, time_factor: float = 1000.0):\n",
    "    \"\"\"\n",
    "    ã‚µã‚¤ãƒ³æ³¢ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—åŸ‹ã‚è¾¼ã¿ã‚’ä½œæˆã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        t (Tensor): ãƒãƒƒãƒè¦ç´ ã”ã¨ã«1ã¤ã®Nã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æŒã¤1-Dãƒ†ãƒ³ã‚½ãƒ«ã€‚ã“ã‚Œã‚‰ã¯åˆ†æ•°å€¤ã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n",
    "        dim (int): å‡ºåŠ›ã®æ¬¡å…ƒã€‚\n",
    "        max_period (int, optional): åŸ‹ã‚è¾¼ã¿ã®æœ€å°å‘¨æ³¢æ•°ã‚’åˆ¶å¾¡ã—ã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯10000ã€‚\n",
    "        time_factor (float, optional): æ™‚é–“ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1000.0ã€‚\n",
    "    Returns:\n",
    "        Tensor: åŸ‹ã‚è¾¼ã¿ãƒ†ãƒ³ã‚½ãƒ«ã®å½¢çŠ¶ã¯(t.shape[0], dim)\n",
    "    \"\"\"\n",
    "    logger.info(f\"timestep_embedding {t.shape=} {dim=} {max_period=} {time_factor=}\")\n",
    "\n",
    "    t = time_factor * t\n",
    "    half = dim // 2\n",
    "    freqs = torch.exp(-math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half).to(t.device)\n",
    "\n",
    "    args = t[:, None].float() * freqs[None]\n",
    "    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "    if dim % 2:\n",
    "        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
    "    if torch.is_floating_point(t):\n",
    "        embedding = embedding.to(t)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74746fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    DCAEãƒ‡ã‚³ãƒ¼ãƒ€ã®æœ€å¾Œã®å±¤\n",
    "    ç”»åƒãƒ‘ãƒƒãƒã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size: int, patch_size: int, out_channels: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_size (int): éš ã‚Œå±¤ã®æ¬¡å…ƒæ•°\n",
    "            patch_size (int): ãƒ‘ãƒƒãƒã®ã‚µã‚¤ã‚º\n",
    "            out_channels (int): å‡ºåŠ›ãƒãƒ£ãƒãƒ«æ•°\n",
    "        \"\"\"\n",
    "        logger.info(f\"LastLayer.__init__ {hidden_size=} {patch_size=} {out_channels=}\")\n",
    "\n",
    "        super().__init__()\n",
    "        self.norm_final = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
    "        self.linear = nn.Linear(hidden_size, patch_size * patch_size * out_channels, bias=True)\n",
    "        self.adaLN_modulation = nn.Sequential(nn.SiLU(), nn.Linear(hidden_size, 2 * hidden_size, bias=True))\n",
    "\n",
    "    def forward(self, x: Tensor, vec: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ« (B, N, hidden_size)\n",
    "            vec (Tensor): ãƒ¢ã‚¸ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ™ã‚¯ãƒˆãƒ« (B, hidden_size)\n",
    "        Returns:\n",
    "            Tensor: å‡ºåŠ›ãƒ†ãƒ³ã‚½ãƒ« (B, N, patch_size * patch_size * out_channels)\n",
    "        \"\"\"\n",
    "        logger.info(f\"LastLayer.forward {x.shape=} {vec.shape=}\")\n",
    "        shift, scale = self.adaLN_modulation(vec).chunk(2, dim=1)\n",
    "        x = (1 + scale[:, None, :]) * self.norm_final(x) + shift[:, None, :]\n",
    "        x = self.linear(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8179a144",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MMDiTConfig:\n",
    "    model_type = \"MMDiT\"\n",
    "    from_pretrained: str\n",
    "    cache_dir: str\n",
    "    in_channels: int\n",
    "    vec_in_dim: int\n",
    "    context_in_dim: int\n",
    "    hidden_size: int\n",
    "    mlp_ratio: float\n",
    "    num_heads: int\n",
    "    depth: int\n",
    "    depth_single_blocks: int\n",
    "    axes_dim: list[int]\n",
    "    theta: int\n",
    "    qkv_bias: bool\n",
    "    guidance_embed: bool\n",
    "    cond_embed: bool = False\n",
    "    fused_qkv: bool = True\n",
    "    grad_ckpt_settings: tuple[int, int] | None = None\n",
    "    use_liger_rope: bool = False\n",
    "    patch_size: int = 2\n",
    "\n",
    "    def get(self, attribute_name, default=None):\n",
    "        return getattr(self, attribute_name, default)\n",
    "\n",
    "    def __contains__(self, attribute_name):\n",
    "        return hasattr(self, attribute_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d538c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMDiTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ‡ã‚£ãƒ•ãƒ¥ãƒ¼ã‚¸ãƒ§ãƒ³ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«\n",
    "    ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã®ä¸¡æ–¹ã®ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’å‡¦ç†ã™ã‚‹\n",
    "    ç”»åƒç”Ÿæˆã‚¿ã‚¹ã‚¯ã«ä½¿ç”¨ã•ã‚Œã‚‹\n",
    "    \"\"\"\n",
    "    config_class = MMDiTConfig\n",
    "\n",
    "    def __init__(self, config: MMDiTConfig):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            config (MMDiTConfig): ãƒ¢ãƒ‡ãƒ«ã®è¨­å®š\n",
    "        \"\"\"\n",
    "        logger.info(f\"MMDiTModel.__init__ {config=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.in_channels = config.in_channels\n",
    "        self.out_channels = self.in_channels\n",
    "        self.patch_size = config.patch_size\n",
    "\n",
    "        if config.hidden_size % config.num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"Hidden size {config.hidden_size} must be divisible by num_heads {config.num_heads}\"\n",
    "            )\n",
    "\n",
    "        pe_dim = config.hidden_size // config.num_heads\n",
    "        if sum(config.axes_dim) != pe_dim:\n",
    "            raise ValueError(\n",
    "                f\"Got {config.axes_dim} but expected positional dim {pe_dim}\"\n",
    "            )\n",
    "\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_heads\n",
    "        pe_embedder_cls = LigerEmbedND if config.use_liger_rope else EmbedND\n",
    "        self.pe_embedder = pe_embedder_cls(\n",
    "            dim=pe_dim, theta=config.theta, axes_dim=config.axes_dim\n",
    "        )\n",
    "\n",
    "        self.img_in = nn.Linear(self.in_channels, self.hidden_size, bias=True)\n",
    "        self.time_in = MLPEmbedder(in_dim=256, hidden_dim=self.hidden_size)\n",
    "        self.vector_in = MLPEmbedder(config.vec_in_dim, self.hidden_size)\n",
    "        self.guidance_in = (\n",
    "            MLPEmbedder(in_dim=256, hidden_dim=self.hidden_size)\n",
    "            if config.guidance_embed\n",
    "            else nn.Identity()\n",
    "        )\n",
    "        self.cond_in = (\n",
    "            nn.Linear(\n",
    "                self.in_channels + self.patch_size**2, self.hidden_size, bias=True\n",
    "            )\n",
    "            if config.cond_embed\n",
    "            else nn.Identity()\n",
    "        )\n",
    "        self.txt_in = nn.Linear(config.context_in_dim, self.hidden_size)\n",
    "\n",
    "        self.double_blocks = nn.ModuleList(\n",
    "            [\n",
    "                DoubleStreamBlock(\n",
    "                    self.hidden_size,\n",
    "                    self.num_heads,\n",
    "                    mlp_ratio=config.mlp_ratio,\n",
    "                    qkv_bias=config.qkv_bias,\n",
    "                    fused_qkv=config.fused_qkv,\n",
    "                )\n",
    "                for _ in range(config.depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.single_blocks = nn.ModuleList(\n",
    "            [\n",
    "                SingleStreamBlock(\n",
    "                    self.hidden_size,\n",
    "                    self.num_heads,\n",
    "                    mlp_ratio=config.mlp_ratio,\n",
    "                    fused_qkv=config.fused_qkv,\n",
    "                )\n",
    "                for _ in range(config.depth_single_blocks)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.final_layer = LastLayer(self.hidden_size, 1, self.out_channels)\n",
    "        self.initialize_weights()\n",
    "\n",
    "        if self.config.grad_ckpt_settings:\n",
    "            self.forward = self.forward_selective_ckpt\n",
    "        else:\n",
    "            self.forward = self.forward_ckpt\n",
    "        self._input_requires_grad = False\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        if self.config.cond_embed:\n",
    "            nn.init.zeros_(self.cond_in.weight)\n",
    "            nn.init.zeros_(self.cond_in.bias)\n",
    "\n",
    "    def prepare_block_inputs(\n",
    "        self,\n",
    "        img: Tensor,\n",
    "        img_ids: Tensor,\n",
    "        txt: Tensor,  # t5 encoded vec\n",
    "        txt_ids: Tensor,\n",
    "        timesteps: Tensor,\n",
    "        y_vec: Tensor,  # clip encoded vec\n",
    "        cond: Tensor = None,\n",
    "        guidance: Tensor | None = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        obtain the processed:\n",
    "            img: projected noisy img latent,\n",
    "            txt: text context (from t5),\n",
    "            vec: clip encoded vector,\n",
    "            pe: the positional embeddings for concatenated img and txt\n",
    "        \"\"\"\n",
    "        if img.ndim != 3 or txt.ndim != 3:\n",
    "            raise ValueError(\"Input img and txt tensors must have 3 dimensions.\")\n",
    "\n",
    "        # running on sequences img\n",
    "        print(\"img shape before img_in:\", img.shape)\n",
    "        img = self.img_in(img)\n",
    "        if self.config.cond_embed:\n",
    "            if cond is None:\n",
    "                raise ValueError(\"Didn't get conditional input for conditional model.\")\n",
    "            img = img + self.cond_in(cond)\n",
    "\n",
    "        vec = self.time_in(timestep_embedding(timesteps, 256))\n",
    "        if self.config.guidance_embed:\n",
    "            if guidance is None:\n",
    "                raise ValueError(\n",
    "                    \"Didn't get guidance strength for guidance distilled model.\"\n",
    "                )\n",
    "            vec = vec + self.guidance_in(timestep_embedding(guidance, 256))\n",
    "        vec = vec + self.vector_in(y_vec)\n",
    "\n",
    "        txt = self.txt_in(txt)\n",
    "\n",
    "        # concat: 4096 + t*h*2/4\n",
    "        ids = torch.cat((txt_ids, img_ids), dim=1)\n",
    "        pe = self.pe_embedder(ids)\n",
    "\n",
    "        if self._input_requires_grad:\n",
    "            # we only apply lora to double/single blocks, thus we only need to enable grad for these inputs\n",
    "            img.requires_grad_()\n",
    "            txt.requires_grad_()\n",
    "\n",
    "        return img, txt, vec, pe\n",
    "\n",
    "    def enable_input_require_grads(self):\n",
    "        \"\"\"Fit peft lora. This method should not be called manually.\"\"\"\n",
    "        self._input_requires_grad = True\n",
    "\n",
    "    def forward_ckpt(\n",
    "        self,\n",
    "        img: Tensor,\n",
    "        img_ids: Tensor,\n",
    "        txt: Tensor,\n",
    "        txt_ids: Tensor,\n",
    "        timesteps: Tensor,\n",
    "        y_vec: Tensor,\n",
    "        cond: Tensor = None,\n",
    "        guidance: Tensor | None = None,\n",
    "        **kwargs,\n",
    "    ) -> Tensor:\n",
    "        img, txt, vec, pe = self.prepare_block_inputs(\n",
    "            img, img_ids, txt, txt_ids, timesteps, y_vec, cond, guidance\n",
    "        )\n",
    "\n",
    "        for block in self.double_blocks:\n",
    "            img, txt = auto_grad_checkpoint(block, img, txt, vec, pe)\n",
    "\n",
    "        img = torch.cat((txt, img), 1)\n",
    "        for block in self.single_blocks:\n",
    "            img = auto_grad_checkpoint(block, img, vec, pe)\n",
    "        img = img[:, txt.shape[1] :, ...]\n",
    "\n",
    "        img = self.final_layer(img, vec)  # (N, T, patch_size ** 2 * out_channels)\n",
    "        return img\n",
    "\n",
    "    def forward_selective_ckpt(\n",
    "        self,\n",
    "        img: Tensor,\n",
    "        img_ids: Tensor,\n",
    "        txt: Tensor,\n",
    "        txt_ids: Tensor,\n",
    "        timesteps: Tensor,\n",
    "        y_vec: Tensor,\n",
    "        cond: Tensor = None,\n",
    "        guidance: Tensor | None = None,\n",
    "        **kwargs,\n",
    "    ) -> Tensor:\n",
    "        img, txt, vec, pe = self.prepare_block_inputs(\n",
    "            img, img_ids, txt, txt_ids, timesteps, y_vec, cond, guidance\n",
    "        )\n",
    "\n",
    "        ckpt_depth_double = self.config.grad_ckpt_settings[0]\n",
    "        for block in self.double_blocks[:ckpt_depth_double]:\n",
    "            img, txt = auto_grad_checkpoint(block, img, txt, vec, pe)\n",
    "\n",
    "        for block in self.double_blocks[ckpt_depth_double:]:\n",
    "            img, txt = block(img, txt, vec, pe)\n",
    "\n",
    "        ckpt_depth_single = self.config.grad_ckpt_settings[1]\n",
    "        img = torch.cat((txt, img), 1)\n",
    "        for block in self.single_blocks[:ckpt_depth_single]:\n",
    "            img = auto_grad_checkpoint(block, img, vec, pe)\n",
    "        for block in self.single_blocks[ckpt_depth_single:]:\n",
    "            img = block(img, vec, pe)\n",
    "\n",
    "        img = img[:, txt.shape[1] :, ...]\n",
    "\n",
    "        img = self.final_layer(img, vec)  # (N, T, patch_size ** 2 * out_channels)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e309fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @MODELS.register_module(\"flux\")\n",
    "def Flux(\n",
    "    cache_dir: str = None,\n",
    "    from_pretrained: str = None,\n",
    "    device_map: str | torch.device = \"cuda\",\n",
    "    torch_dtype: torch.dtype = torch.bfloat16,\n",
    "    strict_load: bool = False,\n",
    "    **kwargs,\n",
    ") -> MMDiTModel:\n",
    "    \"\"\"\n",
    "    MMDiTãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã™ã‚‹ãƒ•ã‚¡ã‚¯ãƒˆãƒªé–¢æ•°\n",
    "    Args:\n",
    "        cache_dir (str, optional): äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª. Defaults to None.\n",
    "        from_pretrained (str, optional): äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ã¾ãŸã¯åå‰. Defaults to None.\n",
    "        device_map (str | torch.device, optional): ãƒ¢ãƒ‡ãƒ«ã‚’é…ç½®ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹. Defaults to \"cuda\".\n",
    "        torch_dtype (torch.dtype, optional): ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‡ãƒ¼ã‚¿å‹. Defaults to torch.bfloat16.\n",
    "        strict_load (bool, optional): ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®å³å¯†ãªèª­ã¿è¾¼ã¿ã‚’è¡Œã†ã‹ã©ã†ã‹. Defaults to False.\n",
    "    Returns:\n",
    "        MMDiTModel: åˆæœŸåŒ–ã•ã‚ŒãŸMMDiTãƒ¢ãƒ‡ãƒ«\n",
    "    \"\"\"\n",
    "    logger.info(f\"Flux {from_pretrained=} {device_map=} {torch_dtype=} {strict_load=} {kwargs=}\")\n",
    "\n",
    "    config = MMDiTConfig(\n",
    "        from_pretrained=from_pretrained,\n",
    "        cache_dir=cache_dir,\n",
    "        **kwargs,\n",
    "    )\n",
    "    low_precision_init = from_pretrained is not None and len(from_pretrained) > 0\n",
    "    if low_precision_init:\n",
    "        default_dtype = torch.get_default_dtype()\n",
    "        torch.set_default_dtype(torch_dtype)\n",
    "    with torch.device(device_map):\n",
    "        model = MMDiTModel(config)\n",
    "    if low_precision_init:\n",
    "        torch.set_default_dtype(default_dtype)\n",
    "    else:\n",
    "        model = model.to(torch_dtype)\n",
    "    if from_pretrained:\n",
    "        model = load_checkpoint(\n",
    "            model,\n",
    "            from_pretrained,\n",
    "            cache_dir=cache_dir,\n",
    "            device_map=device_map,\n",
    "            strict=strict_load,\n",
    "        )\n",
    "    return model\n",
    "\n",
    "\n",
    "# {'type': 'flux',\n",
    "#  'from_pretrained': None,\n",
    "#  'strict_load': False,\n",
    "#  'guidance_embed': False,\n",
    "#  'fused_qkv': False,\n",
    "#  'use_liger_rope': True,\n",
    "#  'grad_ckpt_settings': (8, 100),\n",
    "#  'in_channels': 64,\n",
    "#  'vec_in_dim': 768,\n",
    "#  'context_in_dim': 4096,\n",
    "#  'hidden_size': 384,\n",
    "#  'mlp_ratio': 4.0,\n",
    "#  'num_heads': 3,\n",
    "#  'depth': 1,\n",
    "#  'depth_single_blocks': 38,\n",
    "#  'axes_dim': [16, 56, 56],\n",
    "#  'theta': 10000,\n",
    "#  'qkv_bias': True}\n",
    "\n",
    "model = Flux(\n",
    "    from_pretrained=None,\n",
    "    strict_load=False,\n",
    "    guidance_embed=False,\n",
    "    fused_qkv=False,\n",
    "    use_liger_rope=True,\n",
    "    grad_ckpt_settings=(8, 100),\n",
    "    in_channels=64,\n",
    "    vec_in_dim=768,\n",
    "    context_in_dim=4096,\n",
    "    hidden_size=384,\n",
    "    mlp_ratio=4.0,\n",
    "    num_heads=3,\n",
    "    depth=1,\n",
    "    depth_single_blocks=38,\n",
    "    axes_dim=[16, 56, 56],\n",
    "    theta=10000,\n",
    "    qkv_bias=True,\n",
    ")\n",
    "\n",
    "# (batch_size, seq_len, in_channels)\n",
    "sample_input = torch.randn(1, 16, 64).to(\"cuda\").to(torch.bfloat16)\n",
    "\n",
    "# (batch_size, seq_len)\n",
    "sample_img_ids = torch.randint(0, 10000, (1, 16, 3)).to(\"cuda\")\n",
    "\n",
    "# (batch_size, seq_len, context_in_dim)\n",
    "sample_txt = torch.randn(1, 16, 4096).to(\"cuda\").to(torch.bfloat16)\n",
    "\n",
    "# (batch_size, seq_len)\n",
    "sample_txt_ids = torch.randint(0, 10000, (1, 16, 3)).to(\"cuda\")\n",
    "\n",
    "# (batch_size,)\n",
    "sample_timesteps = torch.randint(0, 1000, (1,)).to(\"cuda\").to(torch.bfloat16)\n",
    "\n",
    "# (batch_size, vec_in_dim)\n",
    "sample_y_vec = torch.randn(1, 768).to(\"cuda\").to(torch.bfloat16)\n",
    "\n",
    "output = model(\n",
    "    img=sample_input,\n",
    "    img_ids=sample_img_ids,\n",
    "    txt=sample_txt,\n",
    "    txt_ids=sample_txt_ids,\n",
    "    timesteps=sample_timesteps,\n",
    "    y_vec=sample_y_vec,\n",
    ")\n",
    "\n",
    "# (1, 16, patch_size ** 2 * out_channels)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fe560f",
   "metadata": {},
   "source": [
    "### è¨“ç·´è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260212ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensora.utils.config import read_config, merge_args\n",
    "import argparse\n",
    "from mmengine.config import Config\n",
    "\n",
    "def parse_args(args) -> tuple[str, argparse.Namespace]:\n",
    "    \"\"\"\n",
    "    This function parses the command line arguments.\n",
    "\n",
    "    Returns:\n",
    "        tuple[str, argparse.Namespace]: The path to the configuration file and the command line arguments.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"config\", type=str, help=\"model config file path\")\n",
    "    args, unknown_args = parser.parse_known_args(args)\n",
    "    return args.config, unknown_args\n",
    "\n",
    "def parse_configs(args) -> Config:\n",
    "    \"\"\"\n",
    "    This function parses the configuration file and command line arguments.\n",
    "\n",
    "    Returns:\n",
    "        Config: The configuration object.\n",
    "    \"\"\"\n",
    "    config, args = parse_args(args)\n",
    "    cfg = read_config(config)\n",
    "    cfg = merge_args(cfg, args)\n",
    "    cfg.config_path = config\n",
    "\n",
    "    # hard-coded for spatial compression\n",
    "    if cfg.get(\"ae_spatial_compression\", None) is not None:\n",
    "        os.environ[\"AE_SPATIAL_COMPRESSION\"] = str(cfg.ae_spatial_compression)\n",
    "\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1047d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DC-AEã®è¨“ç·´\n",
    "\n",
    "# https://github.com/hpcaitech/Open-Sora/blob/main/docs/hcae.md\n",
    "\n",
    "# torchrun --nproc_per_node 8 scripts/vae/train.py configs/vae/train/video_dc_ae.py\n",
    "\n",
    "args_1 = [\n",
    "    \"configs/vae/train/video_dc_ae.py\",\n",
    "]\n",
    "\n",
    "# torchrun --nproc_per_node 8 scripts/vae/train.py configs/vae/train/video_dc_ae_disc.py --model.from_pretrained <model_ckpt>\n",
    "\n",
    "args_2 = [\n",
    "    \"configs/vae/train/video_dc_ae_disc.py\",\n",
    "    \"--model.from_pretrained\",\n",
    "    \"<model_ckpt>\"\n",
    "]\n",
    "\n",
    "# torchrun --nproc_per_node 8 scripts/diffusion/train.py configs/diffusion/train/stage1.py --dataset.data-path datasets/pexels_45k_necessary.csv\n",
    "\n",
    "args_3 = [\n",
    "    \"configs/diffusion/train/stage1.py\",\n",
    "    \"--dataset.data-path\",\n",
    "    \"datasets/pexels_45k_necessary.csv\"\n",
    "]\n",
    "\n",
    "# cfg = parse_configs(args_3)\n",
    "# cfg.model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
