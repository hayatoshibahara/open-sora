{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e54053b",
   "metadata": {},
   "source": [
    "## Ê¶ÇË¶Å"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53236702",
   "metadata": {},
   "source": [
    "Open-Sora 2.0„ÅØ„ÄÅ3000‰∏áÂÜÜ„ÅßÂÆüÁèæÂèØËÉΩ„Å™ÂïÜÁî®„É¨„Éô„É´„ÅÆÂãïÁîªÁîüÊàê„É¢„Éá„É´\n",
    "\n",
    "Ë®ìÁ∑¥„Ç≥„Çπ„Éà„ÅØ„ÄÅÂêåÁ≠â„ÅÆ„É¢„Éá„É´ÔºàMovieGen„ÇÑStep-Video-T2VÔºâ„Çà„Çä„ÇÇ5~10ÂÄç‰Ωé„ÅÑ\n",
    "\n",
    "‰∫∫„ÅÆË©ï‰æ°„Å®VBench„ÅÆ„Çπ„Ç≥„Ç¢„Åß„ÅØ„ÄÅHuyyuan Video„ÇÑRunway Gen-3 Alpha„Å´ÂåπÊïµ:\n",
    "\n",
    "![](image/fig1.png)\n",
    "\n",
    "- Visual Quality: Ë¶ñË¶öÂìÅË≥™\n",
    "- Prompt Following: ÊåáÁ§∫ËøΩÂæìÊÄß\n",
    "- Motion Quality: Âãï„Åç„ÅÆÂìÅË≥™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12386bd",
   "metadata": {},
   "source": [
    "## „Éá„Éº„Çø"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1780041",
   "metadata": {},
   "source": [
    "ÁõÆÁöÑ„ÅØ„ÄÅÂ≠¶Áøí„ÅÆÈÄ≤Êçó„Å´Âêà„Çè„Åõ„Åü„Éá„Éº„Çø„Éî„É©„Éü„ÉÉ„ÉâÔºàhierarchical data pyramidÔºâ„ÅÆÊßãÁØâ\n",
    "\n",
    "Êßò„ÄÖ„Å™Á®ÆÈ°û„ÅÆ„Éá„Éº„Çø„ÇíÊ§úÂá∫ÂèØËÉΩ„Å™„Å™„Éï„Ç£„É´„Çø„ÇíÈñãÁô∫\n",
    "\n",
    "Â≠¶Áøí„ÅÆÈÄ≤Êçó„Å´Âøú„Åò„Å¶„ÄÅ„Éï„Ç£„É´„Çø„É™„É≥„Ç∞„ÅÆÂº∑Â∫¶„ÇíÈ´ò„ÇÅ„ÄÅÁ¥îÂ∫¶„Å®ÂìÅË≥™„ÅÆÈ´ò„ÅÑÂ∞è„Åï„ÅÑ„Çµ„Éñ„Çª„ÉÉ„Éà„ÅßË®ìÁ∑¥\n",
    "\n",
    "„Éá„Éº„Çø„Éï„Ç£„É´„Çø„É™„É≥„Ç∞„ÅÆÂÖ®‰ΩìÂÉè:\n",
    "\n",
    "![](image/fig2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7093517c",
   "metadata": {},
   "source": [
    "- Á¥´: Áîü„ÅÆÂãïÁîª„ÅÆÂâçÂá¶ÁêÜ\n",
    "    1. „Éï„Ç£„É´„Çø„É™„É≥„Ç∞\n",
    "        - Á†¥Êêç„Åó„Åü„Éï„Ç°„Ç§„É´„ÅÆÈô§Âéª\n",
    "        - Ê•µÁ´Ø„Å™ÂãïÁîª„ÅÆÈô§Âéª\n",
    "            - ÂÜçÁîüÊôÇÈñì„Åå2ÁßíÊú™Ê∫Ä\n",
    "            - 1ÁîªÂÉè„ÅÇ„Åü„Çä„ÅÆ„Éá„Éº„ÇøÈáèÔºàBit per pixelÔºâ„Åå0.02Êú™Ê∫Ä\n",
    "            - „Éï„É¨„Éº„É†„É¨„Éº„ÉàÔºàfpsÔºâ„Åå16Êú™Ê∫Ä\n",
    "            - „Ç¢„Çπ„Éö„ÇØ„ÉàÊØî„ÅåÁØÑÂõ≤Â§ñÔºà1/3, 3Ôºâ\n",
    "            - ÁâπÂÆö„ÅÆ‰ΩéÂìÅË≥™„Å™„Ç®„É≥„Ç≥„Éº„ÉâË®≠ÂÆöÔºàConstrained Baseline profileÔºâ\n",
    "    2. ÈÄ£Á∂ö„Åó„ÅüÊò†ÂÉè„ÇíÊ§úÂá∫„Åó„ÄÅÁü≠„ÅÑ„ÇØ„É™„ÉÉ„Éó„Å´ÂàÜÂâ≤\n",
    "        - FFmpeg„ÅÆlibavfilter„Çí‰ΩøÁî®„Åó„ÄÅ„Ç∑„Éº„É≥„Çπ„Ç≥„Ç¢Ôºà„Éï„É¨„Éº„É†Èñì„ÅÆË¶ñË¶öÂ∑ÆÂàÜÔºâ„ÇíË®àÁÆó\n",
    "    3. ÂãïÁîª„ÅÆ„Éï„Ç©„Éº„Éû„ÉÉ„Éà\n",
    "        - „Éï„É¨„Éº„É†„É¨„Éº„Éà„ÅØ30fps‰ª•‰∏ã\n",
    "        - Èï∑Ëæ∫„ÅØ1080px‰ª•‰∏ã\n",
    "        - „Ç≥„Éº„Éá„ÉÉ„ÇØÔºàÂúßÁ∏ÆÂΩ¢ÂºèÔºâ„ÅØH.264\n",
    "        - ÂãïÁîª„ÅÆÈªíÂ∏Ø„ÇíÂâäÈô§\n",
    "        - 8Áßí„ÇíË∂Ö„Åà„Çã„Ç∑„Éß„ÉÉ„Éà„ÅØ„ÄÅ8Áßí„ÅÆ„ÇØ„É™„ÉÉ„Éó„Å´ÂàÜÂâ≤„Åó„ÄÅ2ÁßíÊú™Ê∫Ä„ÅØÁ†¥Ê£Ñ\n",
    "- Èùí: „ÇØ„É™„ÉÉ„ÉóÂãïÁîª„ÅÆ„Çπ„Ç≥„Ç¢„Éï„Ç£„É´„Çø„É™„É≥„Ç∞\n",
    "    - ÁæéÁöÑ„Çπ„Ç≥„Ç¢„Åß„ÅÆ„Éï„Ç£„É´„Çø„É™„É≥„Ç∞\n",
    "        - [CLIP„Å®MLP][1]„ÅßÁæéÁöÑ„Çπ„Ç≥„Ç¢„Çí‰∫àÊ∏¨\n",
    "            - ÊúÄÂàù„Éª‰∏≠Èñì„ÉªÊúÄÁµÇ„Éï„É¨„Éº„É†„ÇíÊäΩÂá∫„Åó„ÄÅ„Çπ„Ç≥„Ç¢„ÇíË®àÁÆó„Åó„ÄÅÂπ≥Âùá\n",
    "    - ÈÆÆÊòé„Åï„ÅÆ‰Ωé„ÅÑÂãïÁîª„ÅÆ„Éï„Ç£„É´„Çø„É™„É≥„Ç∞\n",
    "        - OpenCV„ÅÆ„É©„Éó„É©„Ç∑„Ç¢„É≥ÊºîÁÆóÂ≠ê„ÅßÁîªÂÉè„ÅÆÂàÜÊï£„Åå‰Ωé„ÅÑÔºà„Åº„ÇÑ„Åë„Å¶„ÅÑ„ÇãÔºâÂãïÁîª„ÇíÈô§Âéª\n",
    "    - Êâã„Éñ„É¨„ÅÆÂ§ö„ÅÑÂãïÁîª„ÅÆ„Éï„Ç£„É´„Çø„É™„É≥„Ç∞\n",
    "        - PySceneDetect„Çí‰ΩøÁî®„Åó„Å¶„ÄÅ„Éï„É¨„Éº„É†Èñì„ÅÆÂ§âÂåñ„ÅåÂ§ß„Åç„ÅÑÂãïÁîª„ÇíÈô§Âéª\n",
    "    - ÈáçË§á„ÇØ„É™„ÉÉ„Éó„ÅÆ„Éï„Ç£„É´„Çø„É™„É≥„Ç∞\n",
    "- Á∑ë: 256px„ÅÆ‰ΩéËß£ÂÉèÂ∫¶ÂãïÁîª\n",
    "    - Â§ö„Åè„ÅÆÊñáÁ´†„ÅåÂê´„Åæ„Çå„ÇãÂãïÁîª„ÅÆ„Éï„Ç£„É´„Çø„É™„É≥„Ç∞\n",
    "        - PaddleOCR„Çí‰ΩøÁî®„Åó„Å¶„ÄÅ„ÉÜ„Ç≠„Çπ„Éà„ÅÆ„Éê„Ç¶„É≥„Éá„Ç£„É≥„Ç∞„Éú„ÉÉ„ÇØ„Çπ„ÇíÊ§úÂá∫\n",
    "        - ‰ø°È†ºÂ∫¶„Çπ„Ç≥„Ç¢„Åå0.7„ÇíË∂Ö„Åà„Çã„Éú„ÉÉ„ÇØ„Çπ„ÅÆÁ∑èÈù¢Á©ç„ÅåÂ§ö„ÅÑÂãïÁîª„ÇíÈô§Âéª\n",
    "    - „É¢„Éº„Ç∑„Éß„É≥„Çπ„Ç≥„Ç¢„Åß„ÅÆ„Éï„Ç£„É´„Çø„É™„É≥„Ç∞\n",
    "        - libavfilter„ÅÆVMAF„Çí‰ΩøÁî®„Åó„Å¶ÂãïÁîª„ÅÆÂãï„Åç„ÅÆÊøÄ„Åó„Åï„ÇíÊ∏¨ÂÆö\n",
    "        - „É¢„Éº„Ç∑„Éß„É≥„Çπ„Ç≥„Ç¢„ÅåÊ•µÁ´Ø„Å´‰Ωé„ÅÑ„ÉªÈ´ò„ÅÑÂãïÁîª„ÇíÈô§Âéª\n",
    "- ÈªÑËâ≤: 768px„ÅÆÈ´òËß£ÂÉèÂ∫¶ÂãïÁîª\n",
    "\n",
    "[1]: https://github.com/christophschuhmann/improved-aesthetic-predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa995eea",
   "metadata": {},
   "source": [
    "ÂãïÁîª„ÅÆ„Ç≠„É£„Éó„Ç∑„Éß„É≥„Çí‰ΩúÊàê„Åô„Çã„Åü„ÇÅ„Å´„ÄÅË¶ñË¶öË®ÄË™û„É¢„Éá„É´„Çí‰ΩøÁî®:\n",
    "\n",
    "- 256pxÂãïÁîª„Å´„ÅØ„ÄÅLLaVA-Video\n",
    "- 778pxÂãïÁîª„Å´„ÅØ„ÄÅQwen 2.5 MaxÔºà„Éè„É´„Ç∑„Éç„Éº„Ç∑„Éß„É≥„ÅåÂ∞ë„Å™„ÅÑ„Éó„É≠„Éó„É©„Ç§„Ç®„Çø„É™„É¢„Éá„É´Ôºâ\n",
    "\n",
    "„Ç≠„É£„Éó„Ç∑„Éß„É≥ÁîüÊàê„ÅÆ„Éó„É≠„É≥„Éó„Éà„ÅÆÊßãÊàê:\n",
    "\n",
    "- ‰∏ª„Å™Ë¢´ÂÜô‰Ωì\n",
    "- Ë¢´ÂÜô‰Ωì„ÅÆÂãï„Åç\n",
    "- ËÉåÊôØ„ÇÑÁí∞Â¢É\n",
    "- Ë®ºÊòéÊù°‰ª∂„ÇÑÈõ∞Âõ≤Ê∞ó\n",
    "- „Ç´„É°„É©„ÉØ„Éº„ÇØ\n",
    "- „É™„Ç¢„É´„Éª„Ç∑„Éç„Éû„ÉÜ„Ç£„ÉÉ„ÇØ„Éª3D„Éª„Ç¢„Éã„É°„Å™„Å©„ÅÆÂãïÁîª„ÅÆ„Çπ„Çø„Ç§„É´\n",
    "\n",
    "ÁîüÊàêÊôÇ„Å´Âãï„Åç„ÅÆÂº∑Â∫¶„ÇíË™øÊï¥ÂèØËÉΩ„Å´„Åô„Çã„Åü„ÇÅ„ÄÅ„Ç≠„É£„Éó„Ç∑„Éß„É≥„ÅÆÊúÄÂæå„Å´„É¢„Éº„Ç∑„Éß„É≥„Çπ„Ç≥„Ç¢„ÇíËøΩË®ò"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7587fc55",
   "metadata": {},
   "source": [
    "„Éï„Ç£„É´„Çø„É™„É≥„Ç∞Âæå„ÅÆ„Éá„Éº„Çø„ÅÆÁµ±Ë®à:\n",
    "\n",
    "![](image/fig3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff7e82d",
   "metadata": {},
   "source": [
    "- ÁæéÁöÑ„Çπ„Ç≥„Ç¢„ÅØ4.5~5.5„Åß‰∏≠Á®ãÂ∫¶\n",
    "- ÂãïÁîª„ÅÆÈï∑„Åï„ÅØ2~8Áßí„Åß„ÄÅÂçäÂàÜËøë„Åè„Åå6~8Áßí\n",
    "- „Ç¢„Çπ„Éö„ÇØ„ÉàÊØî„ÅØ„ÄÅÂ§ßÈÉ®ÂàÜ„Åå0.5~0.75Ôºà16:9„ÅÆÊ®™Èï∑ÂãïÁîªÔºâ\n",
    "- „Ç≠„É£„Éó„Ç∑„Éß„É≥„ÅÆ70%„ÅØ75ÂçòË™û„ÇíË∂Ö„Åà„Å¶„ÅÑ„Å¶ÊÉÖÂ†±Èáè„ÅåÂ§ö„ÅÑ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1277b7",
   "metadata": {},
   "source": [
    "„Ç≠„É£„Éó„Ç∑„Éß„É≥„ÅÆ„ÉØ„Éº„Éâ„ÇØ„É©„Ç¶„Éâ:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab60e0b",
   "metadata": {},
   "source": [
    "![](image/fig4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1d547b",
   "metadata": {},
   "source": [
    "- ËÉåÊôØ„ÇÑÁÖßÊòéÊù°‰ª∂„ÇÇÂê´„Åæ„Çå„Å¶„ÅÑ„Å¶„ÄÅË¢´ÂÜô‰Ωì„ÅØ‰∫∫Áâ©„ÅåÂ§ö„ÅÑ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c25823",
   "metadata": {},
   "source": [
    "## „Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346c05aa",
   "metadata": {},
   "source": [
    "### 3Ê¨°ÂÖÉ„Ç™„Éº„Éà„Ç®„É≥„Ç≥„Éº„ÉÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a577fd",
   "metadata": {},
   "source": [
    "[Hunyuan Video VAE][1]„ÅÆ„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„ÇíÂäπÁéáÂåñ„Åó„ÅüVideo DC-AE„ÇíÈñãÁô∫\n",
    "\n",
    "DC-AE„ÅØ„ÄÅ[Deep Compression Autoencoder][2]„ÅÆÁï•\n",
    "\n",
    "ÂúßÁ∏ÆÁéá„ÅØ„ÄÅ$4\\times 32\\times 32$ÔºàÊôÇÈñì„ÅØ $\\frac{1}{4}$„ÄÅÁ∏¶„Å®Ê®™„ÅØ$\\frac{1}{32}$„Å´ÂúßÁ∏ÆÔºâ\n",
    "\n",
    "„Ç™„Éº„Éà„Ç®„É≥„Ç≥„Éº„ÉÄ„Éº„ÅÆÂ≠¶Áøí„Éá„Éº„Çø„ÅØ32„Éï„É¨„Éº„É†„ÄÅ256px„ÅÆ„Åü„ÇÅ„ÄÅÊΩúÂú®Ë°®Áèæ„ÅØ$8\\times 8\\times 8$\n",
    "\n",
    "[1]: https://arxiv.org/abs/2412.03603\n",
    "[2]: https://arxiv.org/abs/2410.10733"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef5fb66",
   "metadata": {},
   "source": [
    "Video DC-AE„ÅÆ„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£:\n",
    "\n",
    "![](image/fig5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346d9939",
   "metadata": {},
   "source": [
    "- „Ç®„É≥„Ç≥„Éº„ÉÄ\n",
    "    - 3Â±§„ÅÆResBlock„Å®3Â±§„ÅÆEfficientViT Block„ÅßÊßãÊàê„Åï„Çå„Çã\n",
    "    - ÊúÄÂàù„ÅÆ5„Å§„ÅÆ„Éñ„É≠„ÉÉ„ÇØ„ÅØ„ÄÅ„ÉÄ„Ç¶„É≥„Çµ„É≥„Éó„É™„É≥„Ç∞Áî®\n",
    "    - Â≠¶Áøí„ÇíÂèØËÉΩ„Å´„Åô„Çã„Åü„ÇÅ„ÄÅ„ÉÄ„Ç¶„É≥„Çµ„É≥„Éó„É´„Éñ„É≠„ÉÉ„ÇØ„Å´„ÅØ„ÄÅÊÆãÂ∑ÆÊé•Á∂ö„ÅåÂ∞éÂÖ•\n",
    "    - ÊÆãÂ∑ÆÊé•Á∂ö„ÅØ„Éî„ÇØ„Çª„É´„Ç¢„É≥„Ç∑„É£„ÉÉ„Éï„É™„É≥„Ç∞„Çí‰ΩøÁî®ÔºàSpace&Time->ChannelÔºâ\n",
    "- „Éá„Ç≥„Éº„ÉÄ\n",
    "    - 3Â±§„ÅÆEfficientViT Block„Å®3Â±§„ÅÆResBlock„ÅßÊßãÊàê„Åï„Çå„Çã\n",
    "    - ÊúÄÂæå„ÅÆ5„Å§„ÅÆ„Éñ„É≠„ÉÉ„ÇØ„ÅØ„ÄÅ„Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É™„É≥„Ç∞Áî®\n",
    "    - Â≠¶Áøí„ÇíÂèØËÉΩ„Å´„Åô„Çã„Åü„ÇÅ„ÄÅ„Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É´„Éñ„É≠„ÉÉ„ÇØ„Å´„ÅØ„ÄÅÊÆãÂ∑ÆÊé•Á∂ö„ÅåÂ∞éÂÖ•\n",
    "    - ÊÆãÂ∑ÆÊé•Á∂ö„ÅØ„Éî„ÇØ„Çª„É´„Ç∑„É£„ÉÉ„Éï„É™„É≥„Ç∞„Çí‰ΩøÁî®ÔºàChannel->Space&TimeÔºâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb61f01",
   "metadata": {},
   "source": [
    "Video DC-AE„Çí„Çπ„ÇØ„É©„ÉÉ„ÉÅ„Åã„ÇâÂ≠¶Áøí„Åó„ÄÅÂÜçÊßãÊàêÂìÅË≥™„ÇíË©ï‰æ°:\n",
    "\n",
    "![](image/table1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df750de",
   "metadata": {},
   "source": [
    "- LPIPS: ‰∫∫Èñì„ÅÆÁü•Ë¶ö„Å´Ëøë„ÅÑÁîªË≥™Ë©ï‰æ°ÊåáÊ®ô"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adeda3cd",
   "metadata": {},
   "source": [
    "## DiT„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759c3421",
   "metadata": {},
   "source": [
    "Èõ¢„Çå„Åü„Éï„É¨„Éº„É†„ÇÑÁîªÁ¥†ÂêåÂ£´„ÅÆÈñ¢‰øÇ„ÇíÂäπÊûúÁöÑ„Å´Êçâ„Åà„Çã„Éï„É´„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥„ÇíÊé°Áî®\n",
    "\n",
    "ÂãïÁîª„ÅØVideo DC-AE„ÅßÂúßÁ∏ÆÂæå„ÄÅ„Éë„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫1Ôºà=„Éë„ÉÉ„ÉÅÂåñÁÑ°„ÅóÔºâ„Åß„Éï„É©„ÉÉ„ÉàÂåñ\n",
    "\n",
    "Hunyuan Video„ÅÆ„Ç™„Éº„Éà„Ç®„É≥„Ç≥„Éº„ÉÄ„Éº„Çí‰ΩøÁî®„Åô„ÇãÂ†¥Âêà„ÅØ„ÄÅ„Éë„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫2„ÅåÂøÖË¶Å"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df79314e",
   "metadata": {},
   "source": [
    "FLUX„ÅÆ[MMDiT][1]„ÇíÂèÇËÄÉ„Å´„ÄÅ„Éá„É•„Ç¢„É´„Çπ„Éà„É™„Éº„É†„Å®„Ç∑„É≥„Ç∞„É´„Çπ„Éà„É™„Éº„É†„Åã„Çâ„Å™„ÇãÊßãÈÄ†„ÇíÊé°Áî®:\n",
    "\n",
    "![](image/fig6.png)\n",
    "\n",
    "[1]: https://github.com/black-forest-labs/flux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968e2d5c",
   "metadata": {},
   "source": [
    "- „Éá„É•„Ç¢„É´„Çπ„Éà„É™„Éº„É†„Éñ„É≠„ÉÉ„ÇØ„Åß„ÄÅÂãïÁîª„Å®„ÉÜ„Ç≠„Çπ„Éà„ÅåÂà•„ÄÖ„Å´ÁâπÂæ¥ÊäΩÂá∫„Åï„Çå„Çã\n",
    "- „Ç∑„É≥„Ç∞„É´„Çπ„Éà„É™„Éº„É†„Éñ„É≠„ÉÉ„ÇØ„Åß„ÄÅÁâπÂæ¥„ÇíÁµ±Âêà„Åô„Çã"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f3e053",
   "metadata": {},
   "source": [
    "Á©∫Èñì„Å®ÊôÇÈñìÊÉÖÂ†±„ÇíÊçâ„Åà„Çã„Åü„ÇÅ„Å´„ÄÅ3D RoPE„ÇíÊé°Áî®\n",
    "\n",
    "„ÉÜ„Ç≠„Çπ„Éà„Éé„Ç®„É≥„Ç≥„Éº„Éâ„ÅØ„ÄÅ2„Å§„ÅÆ‰∫ãÂâçÂ≠¶ÁøíÊ∏à„Åø„É¢„Éá„É´„ÇíÊé°Áî®:\n",
    "\n",
    "- T5-XXL: Ë§áÈõë„Å™„ÉÜ„Ç≠„Çπ„Éà„ÅÆÊÑèÂë≥„ÇíÊçâ„Åà„Çã\n",
    "- CLIP-Large: „ÉÜ„Ç≠„Çπ„Éà„Å®Ë¶ñË¶öÊ¶ÇÂøµ„ÅÆÊï¥ÂêàÊÄß„ÇíÊçâ„Åà„ÇãÔºà=ÊåáÁ§∫ËøΩÂæìÊÄß„ÇíÈ´ò„ÇÅ„ÇãÔºâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6548ce9c",
   "metadata": {},
   "source": [
    "![](image/table2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baaba03",
   "metadata": {},
   "source": [
    "## ÂÆüË£Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf13bf8",
   "metadata": {},
   "source": [
    "- image.py: ÁîªÂÉè„ÅÆ„Åø„ÅßÂ≠¶Áøí„ÄÇ\n",
    "- stage1.py: 256pxËß£ÂÉèÂ∫¶„ÅÆÂãïÁîª„ÅßÂ≠¶Áøí„ÄÇ\n",
    "- stage2.py: 768pxËß£ÂÉèÂ∫¶„ÅÆÂãïÁîª„ÅßÂ≠¶ÁøíÔºà„Ç∑„Éº„Ç±„É≥„Çπ‰∏¶ÂàóÂåñ„Çí‰ΩøÁî®„ÄÅ„Éá„Éï„Ç©„É´„Éà„ÅØ4Ôºâ„ÄÇ\n",
    "- stage1_i2v.py: 256pxËß£ÂÉèÂ∫¶„ÅßT2VÔºà„ÉÜ„Ç≠„Çπ„Éà„Åã„ÇâÂãïÁîªÔºâ„Å®I2VÔºàÁîªÂÉè„Åã„ÇâÂãïÁîªÔºâ„ÇíÂ≠¶Áøí„ÄÇ\n",
    "- stage2_i2v.py: 768pxËß£ÂÉèÂ∫¶„ÅßT2V„Å®I2V„ÇíÂ≠¶Áøí„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e895226e",
   "metadata": {},
   "source": [
    "## Áí∞Â¢ÉÊßãÁØâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9f1318",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd /workspaces/open-sora/Open-Sora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52ec244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import platform\n",
    "import subprocess\n",
    "\n",
    "if os.path.exists(\"debug.log\"):\n",
    "    os.remove(\"debug.log\")\n",
    "\n",
    "def custom_format(record):\n",
    "    match record.levelno:\n",
    "        case logging.DEBUG:\n",
    "            level = \"üü¶\"\n",
    "        case logging.INFO:\n",
    "            level = \"üü©\"\n",
    "        case logging.WARNING:\n",
    "            level = \"üü®\"\n",
    "        case logging.ERROR:\n",
    "            level = \"üü•\"\n",
    "        case logging.CRITICAL:\n",
    "            level = \"üõë\"\n",
    "    return f\"{level} {record.getMessage()}\"\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "for handler in logger.handlers:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "formatter = logging.Formatter()\n",
    "formatter.format = custom_format\n",
    "\n",
    "file_handler = logging.FileHandler(\"debug.log\")\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "PYTHON_VERSION = platform.python_version()\n",
    "logger.info(f\"Python {PYTHON_VERSION}\")\n",
    "\n",
    "NVIDIA_SMI = subprocess.run(\"nvidia-smi\", capture_output=True, text=True).stdout\n",
    "logger.info(f\"{NVIDIA_SMI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57815940",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    %pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "    %pip install \\\n",
    "        accelerate \\\n",
    "        av \\\n",
    "        colossalai \\\n",
    "        ftfy \\\n",
    "        liger-kernel \\\n",
    "        omegaconf \\\n",
    "        mmengine \\\n",
    "        openai \\\n",
    "        pandas \\\n",
    "        pandarallel \\\n",
    "        pyarrow \\\n",
    "        tensorboard \\\n",
    "        wandb \\\n",
    "        --extra-index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "    %pip install flash-attn --no-build-isolation\n",
    "\n",
    "    %pip install -e . --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a52b84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -qU av==13.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a745e96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from einops import rearrange\n",
    "from flash_attn import flash_attn_func as flash_attn_func_v2\n",
    "from functools import partial\n",
    "from huggingface_hub import PyTorchModelHubMixin\n",
    "from inspect import signature\n",
    "from liger_kernel.ops.rms_norm import LigerRMSNormFunction\n",
    "from liger_kernel.ops.rope import LigerRopeFunction\n",
    "from omegaconf import MISSING, OmegaConf\n",
    "from torch import Tensor, nn\n",
    "from torch.nn.modules.batchnorm import _BatchNorm\n",
    "from typing import Any, Callable, Optional, Union, Tuple\n",
    "import diffusers\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "from mmengine.config import Config\n",
    "import ast\n",
    "\n",
    "try:\n",
    "    from flash_attn_interface import flash_attn_func as flash_attn_func_v3\n",
    "    SUPPORT_FA3 = True\n",
    "except:\n",
    "    SUPPORT_FA3 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320458ef",
   "metadata": {},
   "source": [
    "## „ÉÄ„Ç¶„É≥„É≠„Éº„Éâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d5b66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/hpcai-tech/Open-Sora-v2-Video-DC-AE\n",
    "# F32T4C128_AE.safetensors 919MB\n",
    "# Open_Sora_v2_Video_DC_AE.safetensors 23.8GB\n",
    "\n",
    "if False:\n",
    "    !huggingface-cli download hpcai-tech/Open-Sora-v2-Video-DC-AE --local-dir ./ckpts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b44d43",
   "metadata": {},
   "source": [
    "## CLI„Éë„Éº„Çµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29127b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(args) -> tuple[str, argparse.Namespace]:\n",
    "    \"\"\"\n",
    "    This function parses the command line arguments.\n",
    "\n",
    "    Returns:\n",
    "        tuple[str, argparse.Namespace]: The path to the configuration file and the command line arguments.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"config\", type=str, help=\"model config file path\")\n",
    "    args, unknown_args = parser.parse_known_args(args)\n",
    "    return args.config, unknown_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a7b984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config(config_path: str) -> Config:\n",
    "    \"\"\"\n",
    "    This function reads the configuration file.\n",
    "\n",
    "    Args:\n",
    "        config_path (str): The path to the configuration file.\n",
    "\n",
    "    Returns:\n",
    "        Config: The configuration object.\n",
    "    \"\"\"\n",
    "    cfg = Config.fromfile(config_path)\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328e3ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_convert(value: str) -> int | float | bool | list | dict | None:\n",
    "    \"\"\"\n",
    "    Automatically convert a string to the appropriate Python data type,\n",
    "    including int, float, bool, list, dict, etc.\n",
    "\n",
    "    Args:\n",
    "        value (str): The string to convert.\n",
    "\n",
    "    Returns:\n",
    "        int, float, bool, list |  dict: The converted value.\n",
    "    \"\"\"\n",
    "    # Handle empty string\n",
    "    if value == \"\":\n",
    "        return value\n",
    "\n",
    "    # Handle None\n",
    "    if value.lower() == \"none\":\n",
    "        return None\n",
    "\n",
    "    # Handle boolean values\n",
    "    lower_value = value.lower()\n",
    "    if lower_value == \"true\":\n",
    "        return True\n",
    "    elif lower_value == \"false\":\n",
    "        return False\n",
    "\n",
    "    # Try to convert the string to an integer or float\n",
    "    try:\n",
    "        # Try converting to an integer\n",
    "        return int(value)\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        # Try converting to a float\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    # Try to convert the string to a list, dict, tuple, etc.\n",
    "    try:\n",
    "        return ast.literal_eval(value)\n",
    "    except (ValueError, SyntaxError):\n",
    "        pass\n",
    "\n",
    "    # If all attempts fail, return the original string\n",
    "    return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb75c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_args(cfg: Config, args: argparse.Namespace) -> Config:\n",
    "    \"\"\"\n",
    "    This function merges the configuration file and command line arguments.\n",
    "\n",
    "    Args:\n",
    "        cfg (Config): The configuration object.\n",
    "        args (argparse.Namespace): The command line arguments.\n",
    "\n",
    "    Returns:\n",
    "        Config: The configuration object.\n",
    "    \"\"\"\n",
    "    for k, v in zip(args[::2], args[1::2]):\n",
    "        assert k.startswith(\"--\"), f\"Invalid argument: {k}\"\n",
    "        k = k[2:].replace(\"-\", \"_\")\n",
    "        k_split = k.split(\".\")\n",
    "        target = cfg\n",
    "        for key in k_split[:-1]:\n",
    "            assert key in cfg, f\"Key {key} not found in config\"\n",
    "            target = target[key]\n",
    "        if v.lower() == \"none\":\n",
    "            v = None\n",
    "        elif k in target:\n",
    "            v_type = type(target[k])\n",
    "            if v_type == bool:\n",
    "                v = auto_convert(v)\n",
    "            else:\n",
    "                v = type(target[k])(v)\n",
    "        else:\n",
    "            v = auto_convert(v)\n",
    "        target[k_split[-1]] = v\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6510015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_configs(args) -> Config:\n",
    "    \"\"\"\n",
    "    This function parses the configuration file and command line arguments.\n",
    "\n",
    "    Returns:\n",
    "        Config: The configuration object.\n",
    "    \"\"\"\n",
    "    config, args = parse_args(args)\n",
    "    cfg = read_config(config)\n",
    "    cfg = merge_args(cfg, args)\n",
    "    cfg.config_path = config\n",
    "\n",
    "    # hard-coded for spatial compression\n",
    "    if cfg.get(\"ae_spatial_compression\", None) is not None:\n",
    "        os.environ[\"AE_SPATIAL_COMPRESSION\"] = str(cfg.ae_spatial_compression)\n",
    "\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a28b05",
   "metadata": {},
   "source": [
    "### „Ç™„Éº„Éâ„Ç®„É≥„Ç≥„Éº„ÉÄ„ÅÆÊé®Ë´ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d338cab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchrun --nproc_per_node 1 --standalone scripts/vae/inference.py configs/vae/inference/video_dc_ae.py --save-dir samples/dcae\n",
    "\n",
    "ae_inference_cfg = parse_configs([\n",
    "    \"configs/vae/inference/video_dc_ae.py\",\n",
    "    \"--save-dir\",\n",
    "    \"samples/dcae\",\n",
    "])\n",
    "\n",
    "logger.info(f\"{ae_inference_cfg=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f044d52b",
   "metadata": {},
   "source": [
    "### DC-AEÁî®„ÅÆÂãïÁîªÁîüÊàê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f42ebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/high_compression.py --prompt \"The story of a robot's life in a cyberpunk setting.\" \n",
    "\n",
    "diffusion_inference_cfg = parse_configs([\n",
    "    \"configs/diffusion/inference/high_compression.py\",\n",
    "    \"--prompt\",\n",
    "    \"The story of a robot's life in a cyberpunk setting.\",\n",
    "])\n",
    "\n",
    "logger.info(f\"{diffusion_inference_cfg=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af47018",
   "metadata": {},
   "source": [
    "### hunyuanVideoAEÁî®„ÅÆÂãïÁîªÁîüÊàê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f16fd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/256px.py --prompt \"raining, sea\"\n",
    "\n",
    "diffusion_inference_256px_cfg = parse_configs([\n",
    "    \"configs/diffusion/inference/256px.py\",\n",
    "    \"--prompt\",\n",
    "    \"raining, sea\",\n",
    "])\n",
    "\n",
    "logger.info(f\"{diffusion_inference_256px_cfg=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8a2f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchrun --nproc_per_node 8 --standalone scripts/diffusion/inference.py configs/diffusion/inference/768px.py --prompt \"raining, sea\"\n",
    "\n",
    "diffusion_inference_768px_cfg = parse_configs([\n",
    "    \"configs/diffusion/inference/768px.py\",\n",
    "    \"--prompt\",\n",
    "    \"raining, sea\",\n",
    "])\n",
    "\n",
    "logger.info(f\"{diffusion_inference_768px_cfg=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b41885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text-to-Video 256x256\n",
    "\n",
    "# torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_256px.py --save-dir samples --prompt \"raining, sea\"\n",
    "\n",
    "t2i2v_256px_inference_cfg = parse_configs([\n",
    "    \"configs/diffusion/inference/t2i2v_256px.py\",\n",
    "    \"--save-dir\",\n",
    "    \"samples\",\n",
    "    \"--prompt\",\n",
    "    \"raining, sea\",\n",
    "])\n",
    "\n",
    "logger.info(f\"{t2i2v_256px_inference_cfg=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10dde6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text-to-Video 256x256 from CSV\n",
    "\n",
    "# torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_256px.py --save-dir samples --dataset.data-path assets/texts/example.csv\n",
    "\n",
    "t2i2v_256px_csv_inference_cfg = parse_configs([\n",
    "    \"configs/diffusion/inference/t2i2v_256px.py\",\n",
    "    \"--save-dir\",\n",
    "    \"samples\",\n",
    "    \"--dataset.data-path\",\n",
    "    \"assets/texts/example.csv\",\n",
    "])\n",
    "\n",
    "logger.info(f\"{t2i2v_256px_csv_inference_cfg=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67e71bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text-to-Video 768x768 \n",
    "\n",
    "# torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_768px.py --save-dir samples --prompt \"raining, sea\"\n",
    "\n",
    "t2i2v_768px_inference_cfg = parse_configs([\n",
    "    \"configs/diffusion/inference/t2i2v_768px.py\",\n",
    "    \"--save-dir\",\n",
    "    \"samples\",\n",
    "    \"--prompt\",\n",
    "    \"raining, sea\",\n",
    "])\n",
    "\n",
    "logger.info(f\"{t2i2v_768px_inference_cfg=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaf072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text-to-Video 768x768 multi-GPU\n",
    "\n",
    "# torchrun --nproc_per_node 8 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_768px.py --save-dir samples --prompt \"raining, sea\"\n",
    "\n",
    "t2i2v_768px_multi_gpu_inference_cfg = parse_configs([\n",
    "    \"configs/diffusion/inference/t2i2v_768px.py\",\n",
    "    \"--save-dir\",\n",
    "    \"samples\",\n",
    "    \"--prompt\",\n",
    "    \"raining, sea\",\n",
    "])\n",
    "\n",
    "logger.info(f\"{t2i2v_768px_multi_gpu_inference_cfg=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4596f327",
   "metadata": {},
   "source": [
    "### ÁîªÂÉè„Åã„ÇâÂãïÁîªÁîüÊàê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba7ede2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/256px.py --cond_type i2v_head --prompt \"A plump pig wallows in a muddy pond on a rustic farm, its pink snout poking out as it snorts contentedly. The camera captures the pig's playful splashes, sending ripples through the water under the midday sun. Wooden fences and a red barn stand in the background, framed by rolling green hills. The pig's muddy coat glistens in the sunlight, showcasing the simple pleasures of its carefree life.\" --ref assets/texts/i2v.png\n",
    "\n",
    "i2v_256px_inference_cfg = parse_configs([\n",
    "    \"configs/diffusion/inference/256px.py\",\n",
    "    \"--cond_type\",\n",
    "    \"i2v_head\",\n",
    "    \"--prompt\",\n",
    "    \"A plump pig wallows in a muddy pond on a rustic farm, its pink snout poking out as it snorts contentedly. The camera captures the pig's playful splashes, sending ripples through the water under the midday sun. Wooden fences and a red barn stand in the background, framed by rolling green hills. The pig's muddy coat glistens in the sunlight, showcasing the simple pleasures of its carefree life.\",\n",
    "    \"--ref\",\n",
    "    \"assets/texts/i2v.png\",\n",
    "])\n",
    "\n",
    "logger.info(f\"{i2v_256px_inference_cfg=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb0808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 256px with csv\n",
    "\n",
    "# torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/256px.py --cond_type i2v_head --dataset.data-path assets/texts/i2v.csv\n",
    "\n",
    "i2v_256px_csv_inference_cfg = parse_configs([\n",
    "    \"configs/diffusion/inference/256px.py\",\n",
    "    \"--cond_type\",\n",
    "    \"i2v_head\",\n",
    "    \"--dataset.data-path\",\n",
    "    \"assets/texts/i2v.csv\",\n",
    "])\n",
    "\n",
    "logger.info(f\"{i2v_256px_csv_inference_cfg=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1d1686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-GPU 768px\n",
    "\n",
    "# torchrun --nproc_per_node 8 --standalone scripts/diffusion/inference.py configs/diffusion/inference/768px.py --cond_type i2v_head --dataset.data-path assets/texts/i2v.csv\n",
    "\n",
    "i2v_768px_multi_gpu_inference_cfg = parse_configs([\n",
    "    \"configs/diffusion/inference/768px.py\",\n",
    "    \"--cond_type\",\n",
    "    \"i2v_head\",\n",
    "    \"--dataset.data-path\",\n",
    "    \"assets/texts/i2v.csv\",\n",
    "])\n",
    "\n",
    "logger.info(f\"{i2v_768px_multi_gpu_inference_cfg=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7a9d32",
   "metadata": {},
   "source": [
    "### „É¢„Éº„Ç∑„Éß„É≥„Çπ„Ç≥„Ç¢„ÇíÊåáÂÆö„Åó„ÅüÂãïÁîªÁîüÊàê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d96279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_256px.py --save-dir samples --prompt \"raining, sea\" --motion-score 4\n",
    "\n",
    "t2i2v_256px_motion_score_inference_cfg = parse_configs([\n",
    "    \"configs/diffusion/inference/t2i2v_256px.py\",\n",
    "    \"--save-dir\",\n",
    "    \"samples\",\n",
    "    \"--prompt\",\n",
    "    \"raining, sea\",\n",
    "    \"--motion-score\",\n",
    "    \"4\",\n",
    "])\n",
    "\n",
    "logger.info(f\"{t2i2v_256px_motion_score_inference_cfg=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb42ddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÂãïÁîª„ÅÆ„É¢„Éº„Ç∑„Éß„É≥„Çπ„Ç≥„Ç¢„ÇíË©ï‰æ°\n",
    "# OpenAI API Key„ÅåÂøÖË¶Å\n",
    "\n",
    "# torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_256px.py --save-dir samples --prompt \"raining, sea\" --motion-score dynamic\n",
    "\n",
    "t2i2v_256px_motion_score_dynamic_inference_cfg = parse_configs([\n",
    "    \"configs/diffusion/inference/t2i2v_256px.py\",\n",
    "    \"--save-dir\",\n",
    "    \"samples\",\n",
    "    \"--prompt\",\n",
    "    \"raining, sea\",\n",
    "    \"--motion-score\",\n",
    "    \"dynamic\",\n",
    "])\n",
    "\n",
    "logger.info(f\"{t2i2v_256px_motion_score_dynamic_inference_cfg=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e427a0dc",
   "metadata": {},
   "source": [
    "### „Éó„É≠„É≥„Éó„Éà„ÅÆÊ¥óÁ∑¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec44de4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export OPENAI_API_KEY=sk-xxxx\n",
    "# torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_256px.py --save-dir samples --prompt \"raining, sea\" --refine-prompt True\n",
    "\n",
    "t2i2v_256px_refine_prompt_inference_cfg = parse_configs([\n",
    "    \"configs/diffusion/inference/t2i2v_256px.py\",\n",
    "    \"--save-dir\",\n",
    "    \"samples\",\n",
    "    \"--prompt\",\n",
    "    \"raining, sea\",\n",
    "    \"--refine-prompt\",\n",
    "    \"True\",\n",
    "])\n",
    "\n",
    "logger.info(f\"{t2i2v_256px_refine_prompt_inference_cfg=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4173ec15",
   "metadata": {},
   "source": [
    "### „Ç∑„Éº„ÉâÂõ∫ÂÆö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1192a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_256px.py --save-dir samples --prompt \"raining, sea\" --sampling_option.seed 42 --seed 42\n",
    "\n",
    "t2i2v_256px_seed_inference_cfg = parse_configs([\n",
    "    \"configs/diffusion/inference/t2i2v_256px.py\",\n",
    "    \"--save-dir\",\n",
    "    \"samples\",\n",
    "    \"--prompt\",\n",
    "    \"raining, sea\",\n",
    "    \"--sampling_option.seed\",\n",
    "    \"42\",\n",
    "    \"--seed\",\n",
    "    \"42\",\n",
    "])\n",
    "\n",
    "logger.info(f\"{t2i2v_256px_seed_inference_cfg=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf2924f",
   "metadata": {},
   "source": [
    "### Ë®ìÁ∑¥Ë®≠ÂÆö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f665b022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DC-AEË®ìÁ∑¥\n",
    "\n",
    "# https://github.com/hpcaitech/Open-Sora/blob/main/docs/hcae.md\n",
    "\n",
    "# torchrun --nproc_per_node 8 scripts/vae/train.py configs/vae/train/video_dc_ae.py\n",
    "\n",
    "dc_ae_train_cfg = parse_configs([\n",
    "    \"configs/vae/train/video_dc_ae.py\",\n",
    "])\n",
    "\n",
    "logger.info(f\"{dc_ae_train_cfg=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebbccda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DC-AE-DISCË®ìÁ∑¥\n",
    "\n",
    "# torchrun --nproc_per_node 8 scripts/vae/train.py configs/vae/train/video_dc_ae_disc.py --model.from_pretrained <model_ckpt>\n",
    "\n",
    "# args_2 = [\n",
    "#     \"configs/vae/train/video_dc_ae_disc.py\",\n",
    "#     \"--model.from_pretrained\",\n",
    "#     \"<model_ckpt>\"\n",
    "# ]\n",
    "\n",
    "dc_ae_disc_train_cfg = parse_configs([\n",
    "    \"configs/vae/train/video_dc_ae_disc.py\",\n",
    "    \"--model.from_pretrained\",\n",
    "    \"<model_ckpt>\"\n",
    "])\n",
    "\n",
    "logger.info(f\"{dc_ae_disc_train_cfg=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873ddf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DiffusionË®ìÁ∑¥\n",
    "\n",
    "# torchrun --nproc_per_node 8 scripts/diffusion/train.py configs/diffusion/train/stage1.py --dataset.data-path datasets/pexels_45k_necessary.csv\n",
    "\n",
    "diffusion_train_cfg = parse_configs([\n",
    "    \"configs/diffusion/train/stage1.py\",\n",
    "    \"--dataset.data-path\",\n",
    "    \"datasets/pexels_45k_necessary.csv\"\n",
    "])\n",
    "\n",
    "logger.info(f\"{diffusion_train_cfg=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14a0a26",
   "metadata": {},
   "source": [
    "## DC-AE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c2122b",
   "metadata": {},
   "source": [
    "### „É¶„Éº„ÉÜ„Ç£„É™„ÉÜ„Ç£Èñ¢Êï∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54477e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ceil_to_divisible(n: int, dividend: int) -> int:\n",
    "    return math.ceil(dividend / (dividend // n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c352dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val2list(x: Union[list, tuple, Any], repeat_time=1) -> list:\n",
    "    \"\"\"\n",
    "    ÂÄ§„Çí„É™„Çπ„Éà„Å´Â§âÊèõ„Åô„Çã\n",
    "\n",
    "    Args:\n",
    "        x (Union[list, tuple, Any]): Â§âÊèõ„Åô„ÇãÂÄ§\n",
    "        repeat_time (int, optional): x„Åå„É™„Çπ„Éà„ÇÑ„Çø„Éó„É´„Åß„Å™„ÅÑÂ†¥Âêà„ÅÆÁπ∞„ÇäËøî„ÅóÂõûÊï∞\n",
    "    Returns:\n",
    "        list: Â§âÊèõÂæå„ÅÆ„É™„Çπ„Éà\n",
    "    \"\"\"\n",
    "\n",
    "    # „É™„Çπ„Éà„Åã„Çø„Éó„É´„ÅÆÂ†¥Âêà\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return list(x)\n",
    "\n",
    "    # „Åù„Çå‰ª•Â§ñ„ÅÆÂ†¥Âêà\n",
    "    return [x for _ in range(repeat_time)]\n",
    "\n",
    "# Ê§úË®º\n",
    "val2list([1,2,3]), val2list((4,5)), val2list(5, repeat_time=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ed0c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val2tuple(x: Union[list, tuple, Any], min_len: int = 1, idx_repeat: int = -1) -> tuple:\n",
    "    \"\"\"\n",
    "    ÂÄ§„Çí„Çø„Éó„É´„Å´Â§âÊèõ„Åô„Çã\n",
    "\n",
    "    Args:\n",
    "        x (Union[list, tuple, Any]): Â§âÊèõ„Åô„ÇãÂÄ§\n",
    "        min_len (int, optional): „Çø„Éó„É´„ÅÆÊúÄÂ∞èÈï∑„Åï\n",
    "        idx_repeat (int, optional): Áπ∞„ÇäËøî„ÅóÊåøÂÖ•„Åô„Çã„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ\n",
    "    Returns:\n",
    "        tuple: Â§âÊèõÂæå„ÅÆ„Çø„Éó„É´\n",
    "    \"\"\"\n",
    "\n",
    "    # ÂÄ§„Çí„É™„Çπ„Éà„Å´Â§âÊèõ\n",
    "    x = val2list(x)\n",
    "\n",
    "    # ÂøÖË¶Å„Å´Âøú„Åò„Å¶Ë¶ÅÁ¥†„ÇíÁπ∞„ÇäËøî„Åó\n",
    "    if len(x) > 0:\n",
    "        x[idx_repeat:idx_repeat] = [\n",
    "            x[idx_repeat] for _ in range(min_len - len(x))\n",
    "        ]\n",
    "\n",
    "    return tuple(x)\n",
    "\n",
    "# Ê§úË®º\n",
    "val2tuple(False, 2), val2tuple((None, \"bnb2d\"), 2), val2tuple((None, None), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fd9c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_kwargs_from_config(config: dict, target_func: Callable) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Ë®≠ÂÆö„Åã„ÇâÈñ¢Êï∞„ÅÆ„Ç≠„Éº„ÉØ„Éº„ÉâÂºïÊï∞„ÇíÊßãÁØâ„Åô„Çã\n",
    "\n",
    "    Args:\n",
    "        config (dict): Ë®≠ÂÆöËæûÊõ∏\n",
    "        target_func (Callable): ÂØæË±°„ÅÆÈñ¢Êï∞\n",
    "    Returns:\n",
    "        dict[str, Any]: Èñ¢Êï∞„Å´Ê∏°„Åô„Ç≠„Éº„ÉØ„Éº„ÉâÂºïÊï∞\n",
    "    \"\"\"\n",
    "    logger.info(f\"build_kwargs_from_config {config=} {target_func=}\")\n",
    "\n",
    "    valid_keys = list(signature(target_func).parameters)\n",
    "    kwargs = {}\n",
    "    for key in config:\n",
    "        if key in valid_keys:\n",
    "            kwargs[key] = config[key]\n",
    "    return kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8397c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_same_padding(kernel_size: Union[int, tuple[int, ...]]) -> Union[int, tuple[int, ...]]:\n",
    "    \"\"\"\n",
    "    „Ç´„Éº„Éç„É´„Çµ„Ç§„Ç∫„Å´Âü∫„Å•„ÅÑ„Å¶Âêå„Åò„Éë„Éá„Ç£„É≥„Ç∞„ÇíË®àÁÆó„Åô„Çã\n",
    "    ConvLayer, LiteMLA„Åß‰ΩøÁî®\n",
    "\n",
    "    Args:\n",
    "        kernel_size (Union[int, tuple[int, ...]]): „Ç´„Éº„Éç„É´„Çµ„Ç§„Ç∫\n",
    "    Returns:\n",
    "        Union[int, tuple[int, ...]]: Âêå„Åò„Éë„Éá„Ç£„É≥„Ç∞„Çµ„Ç§„Ç∫\n",
    "    \"\"\"\n",
    "    if isinstance(kernel_size, tuple):\n",
    "        return tuple([get_same_padding(ks) for ks in kernel_size])\n",
    "    else:\n",
    "        assert kernel_size % 2 > 0, \"kernel size should be odd number\"\n",
    "        return kernel_size // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68670c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_grad_checkpoint(module, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Ëá™ÂãïÂãæÈÖç„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„ÉàÈñ¢Êï∞\n",
    "    ÂãæÈÖç„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà„ÅåÊúâÂäπ„Å™Â†¥Âêà„ÄÅ„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà„Çí‰ΩøÁî®„Åó„Å¶„É¢„Ç∏„É•„Éº„É´„ÇíÂÆüË°å„Åô„Çã\n",
    "    „Åù„ÅÜ„Åß„Å™„ÅÑÂ†¥Âêà„ÄÅÈÄöÂ∏∏„ÅÆ„Éï„Ç©„ÉØ„Éº„Éâ„Éë„Çπ„ÇíÂÆüË°å„Åô„Çã\n",
    "\n",
    "    Args:\n",
    "        module: nn.Module„Åæ„Åü„ÅØnn.Module„ÅÆIterable\n",
    "        *args: „É¢„Ç∏„É•„Éº„É´„Å∏„ÅÆÂºïÊï∞\n",
    "        **kwargs: „É¢„Ç∏„É•„Éº„É´„Å∏„ÅÆ„Ç≠„Éº„ÉØ„Éº„ÉâÂºïÊï∞\n",
    "    Returns:\n",
    "        „É¢„Ç∏„É•„Éº„É´„ÅÆÂá∫Âäõ\n",
    "    \"\"\"\n",
    "    logger.info(f\"auto_grad_checkpoint {len(args)=} {args[0].shape=} {kwargs=}\")\n",
    "    if getattr(module, \"grad_checkpointing\", False):\n",
    "        if not isinstance(module, Iterable):\n",
    "            return checkpoint(module, *args, use_reentrant=True, **kwargs)\n",
    "        gc_step = module[0].grad_checkpointing_step\n",
    "        return checkpoint_sequential(module, gc_step, *args, use_reentrant=False, **kwargs)\n",
    "    return module(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fcbe88",
   "metadata": {},
   "source": [
    "### „É¢„Ç∏„É•„Éº„É´„É™„Çπ„Éà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaebc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpSequential(nn.Module):\n",
    "    \"\"\"\n",
    "    Ë§áÊï∞„ÅÆÊìç‰Ωú„ÇíÈ†ÜÊ¨°ÈÅ©Áî®„Åô„Çã„É¢„Ç∏„É•„Éº„É´\n",
    "    build_encoder_project_out_block, Encoder\n",
    "    build_decoder_project_out_block, Decoder\n",
    "    \"\"\"\n",
    "    def __init__(self, op_list: list[Optional[nn.Module]]):\n",
    "        super().__init__()\n",
    "        valid_op_list = []\n",
    "        for op in op_list:\n",
    "            if op is not None:\n",
    "                valid_op_list.append(op)\n",
    "        self.op_list = nn.ModuleList(valid_op_list)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for op in self.op_list:\n",
    "            x = op(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cd1bca",
   "metadata": {},
   "source": [
    "### ÊÅíÁ≠âÂ±§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f403bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    ÊÅíÁ≠âÂ±§\n",
    "    build_block, EfficientViTBlock„Åß‰ΩøÁî®\n",
    "    \"\"\"\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5208b6",
   "metadata": {},
   "source": [
    "### Ê≠£Ë¶èÂåñÂ±§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f3527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register normalization function here\n",
    "REGISTERED_NORM_DICT: dict[str, type] = {\n",
    "    \"bn2d\": nn.BatchNorm2d,\n",
    "    \"ln\": nn.LayerNorm,\n",
    "    # \"ln2d\": LayerNorm2d,\n",
    "    # \"rms2d\": RMSNorm2d,\n",
    "    # \"rms3d\": RMSNorm3d,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4289a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_norm(name=\"bn2d\", num_features=None, **kwargs) -> Optional[nn.Module]:\n",
    "    \"\"\"\n",
    "    Ê≠£Ë¶èÂåñÂ±§„ÇíÊßãÁØâ„Åô„Çã\n",
    "\n",
    "    Args:\n",
    "        name (str, optional): Ê≠£Ë¶èÂåñÂ±§„ÅÆÂêçÂâç\n",
    "        num_features (int, optional): Ê≠£Ë¶èÂåñÂ±§„ÅÆÁâπÂæ¥Êï∞\n",
    "        **kwargs: Ê≠£Ë¶èÂåñÂ±§„ÅÆËøΩÂä†ÂºïÊï∞\n",
    "    Returns:\n",
    "        Optional[nn.Module]: ÊßãÁØâ„Åï„Çå„ÅüÊ≠£Ë¶èÂåñÂ±§„ÄÅ„Åæ„Åü„ÅØNone\n",
    "    \"\"\"\n",
    "    logger.info(f\"build_norm {name=} {num_features=} {kwargs=}\")\n",
    "\n",
    "    if name in [\"ln\", \"ln2d\"]:\n",
    "        kwargs[\"normalized_shape\"] = num_features\n",
    "    else:\n",
    "        kwargs[\"num_features\"] = num_features\n",
    "    if name in REGISTERED_NORM_DICT:\n",
    "        norm_cls = REGISTERED_NORM_DICT[name]\n",
    "        args = build_kwargs_from_config(kwargs, norm_cls)\n",
    "        return norm_cls(**args)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc58644",
   "metadata": {},
   "source": [
    "### Ê¥ªÊÄßÂåñÈñ¢Êï∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e510b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGISTERED_ACT_DICT: dict[str, type] = {\n",
    "    \"relu\": nn.ReLU,\n",
    "    \"relu6\": nn.ReLU6,\n",
    "    \"hswish\": nn.Hardswish,\n",
    "    \"silu\": nn.SiLU,\n",
    "    \"gelu\": partial(nn.GELU, approximate=\"tanh\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e410569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_act(name: str, **kwargs) -> Optional[nn.Module]:\n",
    "    \"\"\"\n",
    "    Ê¥ªÊÄßÂåñÈñ¢Êï∞„ÇíÊßãÁØâ„Åô„Çã\n",
    "\n",
    "    Args:\n",
    "        name (str): Ê¥ªÊÄßÂåñÈñ¢Êï∞„ÅÆÂêçÂâç\n",
    "        **kwargs: Ê¥ªÊÄßÂåñÈñ¢Êï∞„ÅÆËøΩÂä†ÂºïÊï∞\n",
    "    Returns:\n",
    "        Optional[nn.Module]: ÊßãÁØâ„Åï„Çå„ÅüÊ¥ªÊÄßÂåñÈñ¢Êï∞„ÄÅ„Åæ„Åü„ÅØNone\n",
    "    \"\"\"\n",
    "    logger.info(f\"build_act {name=} {kwargs=}\")\n",
    "    if name in REGISTERED_ACT_DICT:\n",
    "        act_cls = REGISTERED_ACT_DICT[name]\n",
    "        args = build_kwargs_from_config(kwargs, act_cls)\n",
    "        return act_cls(**args)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7188dbd0",
   "metadata": {},
   "source": [
    "### Áï≥„ÅøËæº„ÅøÂ±§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca76b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelChunkConv3d(nn.Conv3d):\n",
    "    \"\"\"\n",
    "    „ÉÅ„É£„É≥„Éç„É´„Çí„ÉÅ„É£„É≥„ÇØ„Å´ÂàÜÂâ≤„Åó„Å¶Conv3d„ÇíÂÆüË°å„Åô„Çã„ÇØ„É©„Çπ\n",
    "    ConvLayer„Å®LiteMLA„Åß‰ΩøÁî®\n",
    "    \"\"\"\n",
    "\n",
    "    CONV3D_NUMEL_LIMIT = 2**31\n",
    "\n",
    "    def _get_output_numel(self, input_shape: torch.Size) -> int:\n",
    "        numel = self.out_channels\n",
    "        if len(input_shape) == 5:\n",
    "            numel *= input_shape[0]\n",
    "        for i, d in enumerate(input_shape[-3:]):\n",
    "            d_out = math.floor(\n",
    "                (d + 2 * self.padding[i] - self.dilation[i] * (self.kernel_size[i] - 1) - 1) / self.stride[i] + 1\n",
    "            )\n",
    "            numel *= d_out\n",
    "        return numel\n",
    "\n",
    "    def _get_n_chunks(self, numel: int, n_channels: int):\n",
    "        n_chunks = math.ceil(numel / ChannelChunkConv3d.CONV3D_NUMEL_LIMIT)\n",
    "        n_chunks = ceil_to_divisible(n_chunks, n_channels)\n",
    "        return n_chunks\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        ÂÖ•Âäõ„ÉÜ„É≥„ÇΩ„É´„Å´ÂØæ„Åó„Å¶„ÉÅ„É£„É≥„ÇØÂåñ„Åï„Çå„ÅüConv3d„ÇíÂÆüË°å„Åô„Çã\n",
    "        \"\"\"\n",
    "        logger.info(f\"ChannelChunkConv3d.forward {input.shape=}\")\n",
    "        if input.numel() // input.size(0) < ChannelChunkConv3d.CONV3D_NUMEL_LIMIT:\n",
    "            return super().forward(input)\n",
    "        n_in_chunks = self._get_n_chunks(input.numel(), self.in_channels)\n",
    "        n_out_chunks = self._get_n_chunks(self._get_output_numel(input.shape), self.out_channels)\n",
    "        if n_in_chunks == 1 and n_out_chunks == 1:\n",
    "            return super().forward(input)\n",
    "        outputs = []\n",
    "        input_shards = input.chunk(n_in_chunks, dim=1)\n",
    "        for weight, bias in zip(self.weight.chunk(n_out_chunks), self.bias.chunk(n_out_chunks)):\n",
    "            weight_shards = weight.chunk(n_in_chunks, dim=1)\n",
    "            o = None\n",
    "            for x, w in zip(input_shards, weight_shards):\n",
    "                if o is None:\n",
    "                    o = F.conv3d(x, w, bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "                else:\n",
    "                    o += F.conv3d(x, w, None, self.stride, self.padding, self.dilation, self.groups)\n",
    "            outputs.append(o)\n",
    "        return torch.cat(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1339b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    2D„Åæ„Åü„ÅØ3DÁï≥„ÅøËæº„ÅøÂ±§„ÄÅÊ≠£Ë¶èÂåñ„ÄÅÊ¥ªÊÄßÂåñÈñ¢Êï∞„ÇíÁµÑ„ÅøÂêà„Çè„Åõ„Åü„É¢„Ç∏„É•„Éº„É´\n",
    "    - build_downsample_block\n",
    "    - build_encoder_project_in_block\n",
    "    - GLUMBConv\n",
    "    - InterpolateConvUpSamplerLayer\n",
    "    - LiteMLA\n",
    "    - ResBlock\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size=3, stride=1, dilation=1, groups=1, use_bias=False, dropout=0, norm=\"bn2d\", act_func=\"relu\", is_video=False, pad_mode_3d=\"constant\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): ÂÖ•Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "            out_channels (int): Âá∫Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "            kernel_size (int, optional): Áï≥„ÅøËæº„Åø„Ç´„Éº„Éç„É´„ÅÆ„Çµ„Ç§„Ç∫\n",
    "            stride (int, optional): Áï≥„ÅøËæº„Åø„ÅÆ„Çπ„Éà„É©„Ç§„Éâ\n",
    "            dilation (int, optional): Áï≥„ÅøËæº„Åø„ÅÆ„ÉÄ„Ç§„É¨„Éº„Ç∑„Éß„É≥\n",
    "            groups (int, optional): Áï≥„ÅøËæº„Åø„ÅÆ„Ç∞„É´„Éº„ÉóÊï∞\n",
    "            use_bias (bool, optional): „Éê„Ç§„Ç¢„Çπ„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "            dropout (float, optional): „Éâ„É≠„ÉÉ„Éó„Ç¢„Ç¶„ÉàÁéá\n",
    "            norm (str, optional): Ê≠£Ë¶èÂåñÂ±§„ÅÆÁ®ÆÈ°û\n",
    "            act_func (str, optional): Ê¥ªÊÄßÂåñÈñ¢Êï∞„ÅÆÁ®ÆÈ°û\n",
    "            is_video (bool, optional): 3DÁï≥„ÅøËæº„Åø„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "            pad_mode_3d (str, optional): 3DÁï≥„ÅøËæº„Åø„ÅÆ„Éë„Éá„Ç£„É≥„Ç∞„É¢„Éº„Éâ\n",
    "        \"\"\"\n",
    "        logger.info(f\"ConvLayer.__init__ {in_channels=} {out_channels=} {kernel_size=} {stride=} {dilation=} {groups=} {use_bias=} {dropout=} {norm=} {act_func=} {is_video=} {pad_mode_3d=}\")\n",
    "\n",
    "        super().__init__()\n",
    "        self.is_video = is_video\n",
    "\n",
    "        if self.is_video:\n",
    "            assert dilation == 1, \"only support dilation=1 for 3d conv\"\n",
    "            assert kernel_size % 2 == 1, \"only support odd kernel size for 3d conv\"\n",
    "            self.pad_mode_3d = pad_mode_3d  # 3d padding follows CausalConv3d by Hunyuan\n",
    "            # padding = (\n",
    "            #     kernel_size // 2,\n",
    "            #     kernel_size // 2,\n",
    "            #     kernel_size // 2,\n",
    "            #     kernel_size // 2,\n",
    "            #     kernel_size - 1,\n",
    "            #     0,\n",
    "            # )  # W, H, T\n",
    "            # non-causal padding\n",
    "            padding = (\n",
    "                kernel_size // 2,\n",
    "                kernel_size // 2,\n",
    "                kernel_size // 2,\n",
    "                kernel_size // 2,\n",
    "                kernel_size // 2,\n",
    "                kernel_size // 2,\n",
    "            )\n",
    "            self.padding = padding\n",
    "            self.dropout = nn.Dropout3d(dropout, inplace=False) if dropout > 0 else None\n",
    "            assert isinstance(stride, (int, tuple)), \"stride must be an integer or 3-tuple for 3d conv\"\n",
    "            self.conv = ChannelChunkConv3d(  # padding is handled by F.pad() in forward()\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=(kernel_size, kernel_size, kernel_size),\n",
    "                stride=(stride, stride, stride) if isinstance(stride, int) else stride,\n",
    "                groups=groups,\n",
    "                bias=use_bias,\n",
    "            )\n",
    "        else:\n",
    "            padding = get_same_padding(kernel_size)\n",
    "            padding *= dilation\n",
    "            self.dropout = nn.Dropout2d(dropout, inplace=False) if dropout > 0 else None\n",
    "            self.conv = nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=(kernel_size, kernel_size),\n",
    "                stride=(stride, stride),\n",
    "                padding=padding,\n",
    "                dilation=(dilation, dilation),\n",
    "                groups=groups,\n",
    "                bias=use_bias,\n",
    "            )\n",
    "\n",
    "        self.norm = build_norm(norm, num_features=out_channels)\n",
    "        self.act = build_act(act_func)\n",
    "        self.pad = F.pad\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): ÂÖ•Âäõ„ÉÜ„É≥„ÇΩ„É´\n",
    "        Returns:\n",
    "            torch.Tensor: Âá∫Âäõ„ÉÜ„É≥„ÇΩ„É´\n",
    "        \"\"\"\n",
    "        logger.info(f\"ConvLayer.forward {x.shape=}\")\n",
    "        if self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "        if self.is_video:  # custom padding for 3d conv\n",
    "            x = self.pad(x, self.padding, mode=self.pad_mode_3d)  # \"constant\" padding defaults to 0\n",
    "        x = self.conv(x)\n",
    "        if self.norm:\n",
    "            x = self.norm(x)\n",
    "        if self.act:\n",
    "            x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8478f9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLUMBConv(nn.Module):\n",
    "    \"\"\"\n",
    "    GLU„ÇíÁî®„ÅÑ„ÅüMBConv„Éñ„É≠„ÉÉ„ÇØ\n",
    "    EfficientViTBlock„Åß‰ΩøÁî®\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size=3, stride=1, mid_channels=None, expand_ratio=6, use_bias=False, norm=(None, None, \"ln2d\"), act_func=(\"silu\", \"silu\", None), is_video=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): ÂÖ•Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "            out_channels (int): Âá∫Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "            kernel_size (int, optional): Áï≥„ÅøËæº„Åø„Ç´„Éº„Éç„É´„ÅÆ„Çµ„Ç§„Ç∫\n",
    "            stride (int, optional): Áï≥„ÅøËæº„Åø„ÅÆ„Çπ„Éà„É©„Ç§„Éâ\n",
    "            mid_channels (int, optional): ‰∏≠Èñì„ÉÅ„É£„Éç„É´Êï∞\n",
    "            expand_ratio (float, optional): Êã°ÂºµÊØîÁéá\n",
    "            use_bias (bool or tuple, optional): „Éê„Ç§„Ç¢„Çπ„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "            norm (str or tuple, optional): Ê≠£Ë¶èÂåñÂ±§„ÅÆÁ®ÆÈ°û\n",
    "            act_func (str or tuple, optional): Ê¥ªÊÄßÂåñÈñ¢Êï∞„ÅÆÁ®ÆÈ°û\n",
    "            is_video (bool, optional): 3DÁï≥„ÅøËæº„Åø„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "        \"\"\"\n",
    "        logger.info(f\"GLUMBConv.__init__ {in_channels=} {out_channels=} {kernel_size=} {stride=} {mid_channels=} {expand_ratio=} {use_bias=} {norm=} {act_func=} {is_video=}\")\n",
    "\n",
    "        super().__init__()\n",
    "        use_bias = val2tuple(use_bias, 3)\n",
    "        norm = val2tuple(norm, 3)\n",
    "        act_func = val2tuple(act_func, 3)\n",
    "\n",
    "        mid_channels = round(in_channels * expand_ratio) if mid_channels is None else mid_channels\n",
    "\n",
    "        self.glu_act = build_act(act_func[1], inplace=False)\n",
    "        self.inverted_conv = ConvLayer(\n",
    "            in_channels,\n",
    "            mid_channels * 2,\n",
    "            1,\n",
    "            use_bias=use_bias[0],\n",
    "            norm=norm[0],\n",
    "            act_func=act_func[0],\n",
    "            is_video=is_video,\n",
    "        )\n",
    "        self.depth_conv = ConvLayer(\n",
    "            mid_channels * 2,\n",
    "            mid_channels * 2,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            groups=mid_channels * 2,\n",
    "            use_bias=use_bias[1],\n",
    "            norm=norm[1],\n",
    "            act_func=None,\n",
    "            is_video=is_video,\n",
    "        )\n",
    "        self.point_conv = ConvLayer(\n",
    "            mid_channels,\n",
    "            out_channels,\n",
    "            1,\n",
    "            use_bias=use_bias[2],\n",
    "            norm=norm[2],\n",
    "            act_func=act_func[2],\n",
    "            is_video=is_video,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logger.info(f\"GLUMBConv.forward {x.shape=}\")\n",
    "        x = self.inverted_conv(x)\n",
    "        x = self.depth_conv(x)\n",
    "\n",
    "        x, gate = torch.chunk(x, 2, dim=1)\n",
    "        gate = self.glu_act(gate)\n",
    "        x = x * gate\n",
    "\n",
    "        x = self.point_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aecd9ea",
   "metadata": {},
   "source": [
    "### „Éî„ÇØ„Çª„É´„Ç∑„É£„ÉÉ„Éï„É™„É≥„Ç∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e2f06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_unshuffle_3d(x, downsample_factor):\n",
    "    \"\"\"\n",
    "    3D pixel unshuffle Êìç‰Ωú„ÄÇ\n",
    "    \"\"\"\n",
    "    B, C, T, H, W = x.shape\n",
    "\n",
    "    r = downsample_factor\n",
    "    assert T % r == 0, f\"Êó∂Èó¥Áª¥Â∫¶ÂøÖÈ°ªÊòØ‰∏ãÈááÊ†∑Âõ†Â≠êÁöÑÂÄçÊï∞, got shape {x.shape}\"\n",
    "    assert H % r == 0, f\"È´òÂ∫¶Áª¥Â∫¶ÂøÖÈ°ªÊòØ‰∏ãÈááÊ†∑Âõ†Â≠êÁöÑÂÄçÊï∞, got shape {x.shape}\"\n",
    "    assert W % r == 0, f\"ÂÆΩÂ∫¶Áª¥Â∫¶ÂøÖÈ°ªÊòØ‰∏ãÈááÊ†∑Âõ†Â≠êÁöÑÂÄçÊï∞, got shape {x.shape}\"\n",
    "    T_new = T // r\n",
    "    H_new = H // r\n",
    "    W_new = W // r\n",
    "    C_new = C * (r * r * r)\n",
    "\n",
    "    x = x.view(B, C, T_new, r, H_new, r, W_new, r)\n",
    "    x = x.permute(0, 1, 3, 5, 7, 2, 4, 6)\n",
    "    y = x.reshape(B, C_new, T_new, H_new, W_new)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02842da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelUnshuffleChannelAveragingDownSampleLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    „Éî„ÇØ„Çª„É´„Ç¢„É≥„Ç∑„É£„ÉÉ„Éï„É´„Å®„ÉÅ„É£„Éç„É´Âπ≥ÂùáÂåñ„Å´„Çà„Çã„ÉÄ„Ç¶„É≥„Çµ„É≥„Éó„É™„É≥„Ç∞Â±§\n",
    "    - build_downsample_block\n",
    "    - build_encoder_project_out_block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, factor: int, temporal_downsample: bool = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): ÂÖ•Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "            out_channels (int): Âá∫Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "            factor (int): „ÉÄ„Ç¶„É≥„Çµ„É≥„Éó„É™„É≥„Ç∞„Éï„Ç°„ÇØ„Çø„Éº\n",
    "            temporal_downsample (bool, optional):\n",
    "                5DÂÖ•Âäõ„ÉÜ„É≥„ÇΩ„É´„Å´ÂØæ„Åó„Å¶ÊôÇÈñìÊñπÂêë„ÅÆ„ÉÄ„Ç¶„É≥„Çµ„É≥„Éó„É™„É≥„Ç∞„ÇíË°å„ÅÜ„Åã„Å©„ÅÜ„Åã\n",
    "        \"\"\"\n",
    "        logger.info(f\"PixelUnshuffleChannelAveragingDownSampleLayer.__init__ {in_channels=} {out_channels=} {factor=} {temporal_downsample=}\")\n",
    "\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.factor = factor\n",
    "        self.temporal_downsample = temporal_downsample\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logger.info(f\"PixelUnshuffleChannelAveragingDownSampleLayer.forward {x.shape=}\")\n",
    "\n",
    "        if x.dim() == 4:\n",
    "            assert self.in_channels * self.factor**2 % self.out_channels == 0\n",
    "            group_size = self.in_channels * self.factor**2 // self.out_channels\n",
    "            x = F.pixel_unshuffle(x, self.factor)\n",
    "            B, C, H, W = x.shape\n",
    "            x = x.view(B, self.out_channels, group_size, H, W)\n",
    "            x = x.mean(dim=2)\n",
    "        elif x.dim() == 5:  # [B, C, T, H, W]\n",
    "            _, _, T, _, _ = x.shape\n",
    "            if self.temporal_downsample and T != 1:  # 3d pixel unshuffle\n",
    "                x = pixel_unshuffle_3d(x, self.factor)\n",
    "                assert self.in_channels * self.factor**3 % self.out_channels == 0\n",
    "                group_size = self.in_channels * self.factor**3 // self.out_channels\n",
    "            else:  # 2d pixel unshuffle\n",
    "                x = x.permute(0, 2, 1, 3, 4)  # [B, T, C, H, W]\n",
    "                x = F.pixel_unshuffle(x, self.factor)\n",
    "                x = x.permute(0, 2, 1, 3, 4)  # [B, C, T, H, W]\n",
    "                assert self.in_channels * self.factor**2 % self.out_channels == 0\n",
    "                group_size = self.in_channels * self.factor**2 // self.out_channels\n",
    "            B, C, T, H, W = x.shape\n",
    "            x = x.view(B, self.out_channels, group_size, T, H, W)\n",
    "            x = x.mean(dim=2)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported input dimension: {x.dim()}\")\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"PixelUnshuffleChannelAveragingDownSampleLayer(in_channels={self.in_channels}, out_channels={self.out_channels}, factor={self.factor}), temporal_downsample={self.temporal_downsample}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfa12d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelDuplicatingPixelShuffleUpSampleLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    „ÉÅ„É£„É≥„Éç„É´Ë§áË£Ω„Å®„Éî„ÇØ„Çª„É´„Ç∑„É£„ÉÉ„Éï„É´„Å´„Çà„Çã„Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É™„É≥„Ç∞Â±§\n",
    "    build_upsample_block, build_decoder_project_in_block„Åß‰ΩøÁî®\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, factor: int, temporal_upsample: bool = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): ÂÖ•Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "            out_channels (int): Âá∫Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "            factor (int): „Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É™„É≥„Ç∞„Éï„Ç°„ÇØ„Çø„Éº\n",
    "            temporal_upsample (bool, optional):\n",
    "                5DÂÖ•Âäõ„ÉÜ„É≥„ÇΩ„É´„Å´ÂØæ„Åó„Å¶ÊôÇÈñìÊñπÂêë„ÅÆ„Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É™„É≥„Ç∞„ÇíË°å„ÅÜ„Åã„Å©„ÅÜ„Åã\n",
    "        \"\"\"\n",
    "        logger.info(f\"ChannelDuplicatingPixelShuffleUpSampleLayer.__init__ {in_channels=} {out_channels=} {factor=} {temporal_upsample=}\")\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.factor = factor\n",
    "        assert out_channels * factor**2 % in_channels == 0\n",
    "        self.temporal_upsample = temporal_upsample\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logger.info(f\"ChannelDuplicatingPixelShuffleUpSampleLayer.forward {x.shape=}\")\n",
    "        if x.dim() == 5:\n",
    "            B, C, T, H, W = x.shape\n",
    "            assert C == self.in_channels\n",
    "\n",
    "        if self.temporal_upsample and T != 1:  # video input\n",
    "            repeats = self.out_channels * self.factor**3 // self.in_channels\n",
    "        else:\n",
    "            repeats = self.out_channels * self.factor**2 // self.in_channels\n",
    "\n",
    "        x = x.repeat_interleave(repeats, dim=1)\n",
    "\n",
    "        if x.dim() == 4:  # original image-only training\n",
    "            x = F.pixel_shuffle(x, self.factor)\n",
    "        elif x.dim() == 5:  # [B, C, T, H, W]\n",
    "            if self.temporal_upsample and T != 1:  # video input\n",
    "                x = pixel_shuffle_3d(x, self.factor)\n",
    "            else:\n",
    "                x = x.permute(0, 2, 1, 3, 4)  # [B, T, C, H, W]\n",
    "                x = F.pixel_shuffle(x, self.factor)  # on H and W only\n",
    "                x = x.permute(0, 2, 1, 3, 4)  # [B, C, T, H, W]\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"ChannelDuplicatingPixelShuffleUpSampleLayer(in_channels={self.in_channels}, out_channels={self.out_channels}, factor={self.factor}, temporal_upsample={self.temporal_upsample})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53173154",
   "metadata": {},
   "source": [
    "### InterpolateConvUpSamplerLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04050c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "VERBOSE = False\n",
    "\n",
    "def chunked_interpolate(x, scale_factor, mode=\"nearest\"):\n",
    "    \"\"\"\n",
    "    „ÉÅ„É£„É≥„Éç„É´„ÅÆÊ¨°ÂÖÉ„Å´Ê≤ø„Å£„Å¶„ÉÅ„É£„É≥„ÇØÂåñ„Åó„Å¶Â§ß„Åç„Å™„ÉÜ„É≥„ÇΩ„É´„ÇíË£úÈñì„Åô„Çã\n",
    "    ÁèæÂú®„ÅØ„Äånearest„ÄçË£úÈñì„É¢„Éº„Éâ„ÅÆ„Åø„Çµ„Éù„Éº„Éà\n",
    "\n",
    "    https://discuss.pytorch.org/t/error-using-f-interpolate-for-large-3d-input/207859\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): ÂÖ•Âäõ„ÉÜ„É≥„ÇΩ„É´ (B, C, D, H, W)\n",
    "        scale_factor: „Çπ„Ç±„Éº„É™„É≥„Ç∞„Éï„Ç°„ÇØ„Çø„Éº„ÅÆ„Çø„Éó„É´ (d, h, w)\n",
    "    Returns:\n",
    "        torch.Tensor: Ë£úÈñì„Åï„Çå„Åü„ÉÜ„É≥„ÇΩ„É´\n",
    "    \"\"\"\n",
    "    logger.info(f\"chunked_interpolate {x.shape=} {scale_factor=} {mode=}\")\n",
    "\n",
    "    assert (\n",
    "        mode == \"nearest\"\n",
    "    ), \"Only the nearest mode is supported\"  # actually other modes are theoretically supported but not tested\n",
    "    if len(x.shape) != 5:\n",
    "        raise ValueError(\"Expected 5D input tensor (B, C, D, H, W)\")\n",
    "\n",
    "    # Calculate max chunk size to avoid int32 overflow. num_elements < max_int32\n",
    "    # Max int32 is 2^31 - 1\n",
    "    max_elements_per_chunk = 2**31 - 1\n",
    "\n",
    "    # Calculate output spatial dimensions\n",
    "    out_d = math.ceil(x.shape[2] * scale_factor[0])\n",
    "    out_h = math.ceil(x.shape[3] * scale_factor[1])\n",
    "    out_w = math.ceil(x.shape[4] * scale_factor[2])\n",
    "\n",
    "    # Calculate max channels per chunk to stay under limit\n",
    "    elements_per_channel = out_d * out_h * out_w\n",
    "    max_channels = max_elements_per_chunk // (x.shape[0] * elements_per_channel)\n",
    "\n",
    "    # Use smaller of max channels or input channels\n",
    "    chunk_size = min(max_channels, x.shape[1])\n",
    "\n",
    "    # Ensure at least 1 channel per chunk\n",
    "    chunk_size = max(1, chunk_size)\n",
    "    if VERBOSE:\n",
    "        print(f\"Input channels: {x.shape[1]}\")\n",
    "        print(f\"Chunk size: {chunk_size}\")\n",
    "        print(f\"max_channels: {max_channels}\")\n",
    "        print(f\"num_chunks: {math.ceil(x.shape[1] / chunk_size)}\")\n",
    "\n",
    "    chunks = []\n",
    "    for i in range(0, x.shape[1], chunk_size):\n",
    "        start_idx = i\n",
    "        end_idx = min(i + chunk_size, x.shape[1])\n",
    "\n",
    "        chunk = x[:, start_idx:end_idx, :, :, :]\n",
    "\n",
    "        interpolated_chunk = F.interpolate(chunk, scale_factor=scale_factor, mode=\"nearest\")\n",
    "\n",
    "        chunks.append(interpolated_chunk)\n",
    "\n",
    "    if not chunks:\n",
    "        raise ValueError(f\"No chunks were generated. Input shape: {x.shape}\")\n",
    "\n",
    "    # Concatenate chunks along channel dimension\n",
    "    return torch.cat(chunks, dim=1)\n",
    "\n",
    "class InterpolateConvUpSampleLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Ë£úÈñì„Å®Áï≥„ÅøËæº„Åø„ÇíÁµÑ„ÅøÂêà„Çè„Åõ„Åü„Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É™„É≥„Ç∞Â±§\n",
    "    build_upsample_block„Åß‰ΩøÁî®\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, factor: int, mode: str = \"nearest\", is_video: bool = False, temporal_upsample: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): ÂÖ•Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "            out_channels (int): Âá∫Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "            kernel_size (int): Áï≥„ÅøËæº„Åø„Ç´„Éº„Éç„É´„ÅÆ„Çµ„Ç§„Ç∫\n",
    "            factor (int): „Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É™„É≥„Ç∞„Éï„Ç°„ÇØ„Çø„Éº\n",
    "            mode (str, optional): Ë£úÈñì„É¢„Éº„Éâ\n",
    "            is_video (bool, optional): 3DÁï≥„ÅøËæº„Åø„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "            temporal_upsample (bool, optional):\n",
    "                5DÂÖ•Âäõ„ÉÜ„É≥„ÇΩ„É´„Å´ÂØæ„Åó„Å¶ÊôÇÈñìÊñπÂêë„ÅÆ„Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É™„É≥„Ç∞„ÇíË°å„ÅÜ„Åã„Å©„ÅÜ„Åã\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.factor = factor\n",
    "        self.mode = mode\n",
    "        self.temporal_upsample = temporal_upsample\n",
    "        self.conv = ConvLayer(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            use_bias=True,\n",
    "            norm=None,\n",
    "            act_func=None,\n",
    "            is_video=is_video,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logger.info(f\"InterpolateConvUpSampleLayer.forward {x.shape=}\")\n",
    "        if x.dim() == 4:\n",
    "            x = F.interpolate(x, scale_factor=self.factor, mode=self.mode)\n",
    "        elif x.dim() == 5:\n",
    "            # [B, C, T, H, W] -> [B, C, T*factor, H*factor, W*factor]\n",
    "            if self.temporal_upsample and x.size(2) != 1:  # temporal upsample for video input\n",
    "                x = chunked_interpolate(x, scale_factor=[self.factor, self.factor, self.factor], mode=self.mode)\n",
    "            else:\n",
    "                x = chunked_interpolate(x, scale_factor=[1, self.factor, self.factor], mode=self.mode)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"InterpolateConvUpSampleLayer(factor={self.factor}, mode={self.mode}, temporal_upsample={self.temporal_upsample})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ebc7f5",
   "metadata": {},
   "source": [
    "### „Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥Ê©üÊßã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2013d84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiteMLA(nn.Module):\n",
    "    \"\"\"\n",
    "    ËªΩÈáè„Å™„Éû„É´„ÉÅ„Çπ„Ç±„Éº„É´Á∑öÂΩ¢„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥Ê©üÊßã\n",
    "    EfficientViTBlock„Åß‰ΩøÁî®\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, heads: Optional[int] = None, heads_ratio: float = 1.0, dim=8, use_bias=False, norm=(None, \"bn2d\"), act_func=(None, None), kernel_func=\"relu\", scales: tuple[int, ...] = (5,), eps=1.0e-15, is_video=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): ÂÖ•Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "            out_channels (int): Âá∫Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "            heads (Optional[int], optional): „Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥„Éò„ÉÉ„ÉâÊï∞\n",
    "            heads_ratio (float, optional): „Éò„ÉÉ„ÉâÊï∞„ÅÆÊØîÁéá\n",
    "            dim (int, optional): ÂêÑ„Éò„ÉÉ„Éâ„ÅÆÊ¨°ÂÖÉÊï∞\n",
    "            use_bias (bool or tuple, optional): „Éê„Ç§„Ç¢„Çπ„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "            norm (str or tuple, optional): Ê≠£Ë¶èÂåñÂ±§„ÅÆÁ®ÆÈ°û\n",
    "            act_func (str or tuple, optional): Ê¥ªÊÄßÂåñÈñ¢Êï∞„ÅÆÁ®ÆÈ°û\n",
    "            kernel_func (str, optional): „Ç´„Éº„Éç„É´Èñ¢Êï∞„ÅÆÁ®ÆÈ°û\n",
    "            scales (tuple[int, ...], optional): „Éû„É´„ÉÅ„Çπ„Ç±„Éº„É´Áï≥„ÅøËæº„Åø„ÅÆ„Çπ„Ç±„Éº„É´\n",
    "            eps (float, optional): Êï∞ÂÄ§ÂÆâÂÆöÊÄß„ÅÆ„Åü„ÇÅ„ÅÆÂ∞è„Åï„Å™ÂÄ§\n",
    "            is_video (bool, optional): 3DÁï≥„ÅøËæº„Åø„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "        \"\"\"\n",
    "        logger.info(f\"LiteMLA.__init__ {in_channels=} {out_channels=} {heads=} {heads_ratio=} {dim=} {use_bias=} {norm=} {act_func=} {kernel_func=} {scales=} {eps=} {is_video=}\")\n",
    "\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        heads = int(in_channels // dim * heads_ratio) if heads is None else heads\n",
    "\n",
    "        total_dim = heads * dim\n",
    "\n",
    "        use_bias = val2tuple(use_bias, 2)\n",
    "        norm = val2tuple(norm, 2)\n",
    "        act_func = val2tuple(act_func, 2)\n",
    "\n",
    "        self.dim = dim\n",
    "        self.qkv = ConvLayer(\n",
    "            in_channels,\n",
    "            3 * total_dim,\n",
    "            1,\n",
    "            use_bias=use_bias[0],\n",
    "            norm=norm[0],\n",
    "            act_func=act_func[0],\n",
    "            is_video=is_video,\n",
    "        )\n",
    "        conv_class = nn.Conv2d if not is_video else ChannelChunkConv3d\n",
    "        self.aggreg = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    conv_class(\n",
    "                        3 * total_dim,\n",
    "                        3 * total_dim,\n",
    "                        scale,\n",
    "                        padding=get_same_padding(scale),\n",
    "                        groups=3 * total_dim,\n",
    "                        bias=use_bias[0],\n",
    "                    ),\n",
    "                    conv_class(3 * total_dim, 3 * total_dim, 1, groups=3 * heads, bias=use_bias[0]),\n",
    "                )\n",
    "                for scale in scales\n",
    "            ]\n",
    "        )\n",
    "        self.kernel_func = build_act(kernel_func, inplace=False)\n",
    "\n",
    "        self.proj = ConvLayer(\n",
    "            total_dim * (1 + len(scales)),\n",
    "            out_channels,\n",
    "            1,\n",
    "            use_bias=use_bias[1],\n",
    "            norm=norm[1],\n",
    "            act_func=act_func[1],\n",
    "            is_video=is_video,\n",
    "        )\n",
    "\n",
    "    @torch.autocast(device_type=\"cuda\", enabled=False)\n",
    "    def relu_linear_att(self, qkv: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        ReLU„Ç´„Éº„Éç„É´„ÇíÁî®„ÅÑ„ÅüÁ∑öÂΩ¢„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥\n",
    "        \"\"\"\n",
    "        logger.info(f\"LiteMLA.relu_linear_att {qkv.shape=}\")\n",
    "\n",
    "        if qkv.ndim == 5:\n",
    "            B, _, T, H, W = list(qkv.size())\n",
    "            is_video = True\n",
    "        else:\n",
    "            B, _, H, W = list(qkv.size())\n",
    "            is_video = False\n",
    "\n",
    "        if qkv.dtype == torch.float16:\n",
    "            qkv = qkv.float()\n",
    "\n",
    "        if qkv.ndim == 4:\n",
    "            qkv = torch.reshape(\n",
    "                qkv,\n",
    "                (\n",
    "                    B,\n",
    "                    -1,\n",
    "                    3 * self.dim,\n",
    "                    H * W,\n",
    "                ),\n",
    "            )\n",
    "        elif qkv.ndim == 5:\n",
    "            qkv = torch.reshape(\n",
    "                qkv,\n",
    "                (\n",
    "                    B,\n",
    "                    -1,\n",
    "                    3 * self.dim,\n",
    "                    H * W * T,\n",
    "                ),\n",
    "            )\n",
    "        q, k, v = (\n",
    "            qkv[:, :, 0 : self.dim],\n",
    "            qkv[:, :, self.dim : 2 * self.dim],\n",
    "            qkv[:, :, 2 * self.dim :],\n",
    "        )\n",
    "\n",
    "        # lightweight linear attention\n",
    "        q = self.kernel_func(q)\n",
    "        k = self.kernel_func(k)\n",
    "\n",
    "        # linear matmul\n",
    "        trans_k = k.transpose(-1, -2)\n",
    "\n",
    "        v = F.pad(v, (0, 0, 0, 1), mode=\"constant\", value=1)\n",
    "        vk = torch.matmul(v, trans_k)\n",
    "        out = torch.matmul(vk, q)\n",
    "        if out.dtype == torch.bfloat16:\n",
    "            out = out.float()\n",
    "        out = out[:, :, :-1] / (out[:, :, -1:] + self.eps)\n",
    "\n",
    "        if not is_video:\n",
    "            out = torch.reshape(out, (B, -1, H, W))\n",
    "        else:\n",
    "            out = torch.reshape(out, (B, -1, T, H, W))\n",
    "        return out\n",
    "\n",
    "    @torch.autocast(device_type=\"cuda\", enabled=False)\n",
    "    def relu_quadratic_att(self, qkv: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Quadratic Attention with ReLU kernel\n",
    "        ‰ªäÂõû„ÅØ‰ΩøÁî®„Åó„Å™„ÅÑ\n",
    "        \"\"\"\n",
    "        B, _, H, W = list(qkv.size())\n",
    "\n",
    "        qkv = torch.reshape(\n",
    "            qkv,\n",
    "            (\n",
    "                B,\n",
    "                -1,\n",
    "                3 * self.dim,\n",
    "                H * W,\n",
    "            ),\n",
    "        )\n",
    "        q, k, v = (\n",
    "            qkv[:, :, 0 : self.dim],\n",
    "            qkv[:, :, self.dim : 2 * self.dim],\n",
    "            qkv[:, :, 2 * self.dim :],\n",
    "        )\n",
    "\n",
    "        q = self.kernel_func(q)\n",
    "        k = self.kernel_func(k)\n",
    "\n",
    "        att_map = torch.matmul(k.transpose(-1, -2), q)  # b h n n\n",
    "        original_dtype = att_map.dtype\n",
    "        if original_dtype in [torch.float16, torch.bfloat16]:\n",
    "            att_map = att_map.float()\n",
    "        att_map = att_map / (torch.sum(att_map, dim=2, keepdim=True) + self.eps)  # b h n n\n",
    "        att_map = att_map.to(original_dtype)\n",
    "        out = torch.matmul(v, att_map)  # b h d n\n",
    "\n",
    "        out = torch.reshape(out, (B, -1, H, W))\n",
    "        return out\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logger.info(f\"LiteMLA.forward {x.shape=}\")\n",
    "        # generate multi-scale q, k, v\n",
    "        qkv = self.qkv(x)\n",
    "        multi_scale_qkv = [qkv]\n",
    "        for op in self.aggreg:\n",
    "            multi_scale_qkv.append(op(qkv))\n",
    "        qkv = torch.cat(multi_scale_qkv, dim=1)\n",
    "\n",
    "        if qkv.ndim == 4:\n",
    "            H, W = list(qkv.size())[-2:]\n",
    "            # num_tokens = H * W\n",
    "        elif qkv.ndim == 5:\n",
    "            _, _, T, H, W = list(qkv.size())\n",
    "            # num_tokens = H * W * T\n",
    "\n",
    "        # if num_tokens > self.dim:\n",
    "        out = self.relu_linear_att(qkv).to(qkv.dtype)\n",
    "        # else:\n",
    "        #     if self.is_video:\n",
    "        #         raise NotImplementedError(\"Video is not supported for quadratic attention\")\n",
    "        #     out = self.relu_quadratic_att(qkv)\n",
    "        out = self.proj(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a570c05",
   "metadata": {},
   "source": [
    "### ResidualBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9395c0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ÊÆãÂ∑ÆÊé•Á∂ö„ÇíÊåÅ„Å§„Éñ„É≠„ÉÉ„ÇØ\n",
    "    EfficientViTBlock„Åß‰ΩøÁî®\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        main: Optional[nn.Module],\n",
    "        shortcut: Optional[nn.Module],\n",
    "        post_act=None,\n",
    "        pre_norm: Optional[nn.Module] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pre_norm = pre_norm\n",
    "        self.main = main\n",
    "        self.shortcut = shortcut\n",
    "        self.post_act = build_act(post_act)\n",
    "\n",
    "    def forward_main(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.pre_norm is None:\n",
    "            return self.main(x)\n",
    "        else:\n",
    "            return self.main(self.pre_norm(x))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.main is None:\n",
    "            res = x\n",
    "        elif self.shortcut is None:\n",
    "            res = self.forward_main(x)\n",
    "        else:\n",
    "            res = self.forward_main(x) + self.shortcut(x)\n",
    "            if self.post_act:\n",
    "                res = self.post_act(res)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca4f383",
   "metadata": {},
   "source": [
    "### EfficientViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53e3875",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientViTBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    EfficientViT„Éñ„É≠„ÉÉ„ÇØ\n",
    "    build_block„Åß‰ΩøÁî®\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, heads_ratio: float = 1.0, dim=32, expand_ratio: float = 4, scales: tuple[int, ...] = (5,), norm: str = \"bn2d\", act_func: str = \"hswish\", context_module: str = \"LiteMLA\", local_module: str = \"MBConv\", is_video: bool = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): ÂÖ•Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "            heads_ratio (float, optional): „Éò„ÉÉ„ÉâÊï∞„ÅÆÊØîÁéá\n",
    "            dim (int, optional): ÂêÑ„Éò„ÉÉ„Éâ„ÅÆÊ¨°ÂÖÉÊï∞\n",
    "            expand_ratio (float, optional): Êã°ÂºµÊØîÁéá\n",
    "            scales (tuple[int, ...], optional): „Éû„É´„ÉÅ„Çπ„Ç±„Éº„É´Áï≥„ÅøËæº„Åø„ÅÆ„Çπ„Ç±„Éº„É´\n",
    "            norm (str, optional): Ê≠£Ë¶èÂåñÂ±§„ÅÆÁ®ÆÈ°û\n",
    "            act_func (str, optional): Ê¥ªÊÄßÂåñÈñ¢Êï∞„ÅÆÁ®ÆÈ°û\n",
    "            context_module (str, optional): „Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„É¢„Ç∏„É•„Éº„É´„ÅÆÁ®ÆÈ°û\n",
    "            local_module (str, optional): „É≠„Éº„Ç´„É´„É¢„Ç∏„É•„Éº„É´„ÅÆÁ®ÆÈ°û\n",
    "            is_video (bool, optional): 3DÁï≥„ÅøËæº„Åø„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "        \"\"\"\n",
    "        logger.info(f\"EfficientViTBlock.__init__ {in_channels=} {heads_ratio=} {dim=} {expand_ratio=} {scales=} {norm=} {act_func=} {context_module=} {local_module=} {is_video=}\")\n",
    "\n",
    "        super().__init__()\n",
    "        if context_module == \"LiteMLA\":\n",
    "            self.context_module = ResidualBlock(\n",
    "                LiteMLA(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=in_channels,\n",
    "                    heads_ratio=heads_ratio,\n",
    "                    dim=dim,\n",
    "                    norm=(None, norm),\n",
    "                    scales=scales,\n",
    "                    is_video=is_video,\n",
    "                ),\n",
    "                IdentityLayer(),\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"context_module {context_module} is not supported\")\n",
    "        if local_module == \"MBConv\":\n",
    "            self.local_module = ResidualBlock(\n",
    "                MBConv(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=in_channels,\n",
    "                    expand_ratio=expand_ratio,\n",
    "                    use_bias=(True, True, False),\n",
    "                    norm=(None, None, norm),\n",
    "                    act_func=(act_func, act_func, None),\n",
    "                    is_video=is_video,\n",
    "                ),\n",
    "                IdentityLayer(),\n",
    "            )\n",
    "        elif local_module == \"GLUMBConv\":\n",
    "            self.local_module = ResidualBlock(\n",
    "                GLUMBConv(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=in_channels,\n",
    "                    expand_ratio=expand_ratio,\n",
    "                    use_bias=(True, True, False),\n",
    "                    norm=(None, None, norm),\n",
    "                    act_func=(act_func, act_func, None),\n",
    "                    is_video=is_video,\n",
    "                ),\n",
    "                IdentityLayer(),\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(f\"local_module {local_module} is not supported\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logger.info(f\"EfficientViTBlock.forward {x.shape=}\")\n",
    "        x = self.context_module(x)\n",
    "        x = self.local_module(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6717d723",
   "metadata": {},
   "source": [
    "### ResBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f023dc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ÊÆãÂ∑Æ„Éñ„É≠„ÉÉ„ÇØ\n",
    "    build_block„Åß‰ΩøÁî®\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        mid_channels=None,\n",
    "        expand_ratio=1,\n",
    "        use_bias=False,\n",
    "        norm=(\"bn2d\", \"bn2d\"),\n",
    "        act_func=(\"relu6\", None),\n",
    "        is_video=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): ÂÖ•Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "            out_channels (int): Âá∫Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "            kernel_size (int, optional): Áï≥„ÅøËæº„Åø„Ç´„Éº„Éç„É´„ÅÆ„Çµ„Ç§„Ç∫\n",
    "            stride (int, optional): Áï≥„ÅøËæº„Åø„ÅÆ„Çπ„Éà„É©„Ç§„Éâ\n",
    "            mid_channels (int, optional): ‰∏≠Èñì„ÉÅ„É£„Éç„É´Êï∞\n",
    "            expand_ratio (float, optional): Êã°ÂºµÊØîÁéá\n",
    "            use_bias (bool or tuple, optional): „Éê„Ç§„Ç¢„Çπ„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "            norm (str or tuple, optional): Ê≠£Ë¶èÂåñÂ±§„ÅÆÁ®ÆÈ°û\n",
    "            act_func (str or tuple, optional): Ê¥ªÊÄßÂåñÈñ¢Êï∞„ÅÆÁ®ÆÈ°û\n",
    "            is_video (bool, optional): 3DÁï≥„ÅøËæº„Åø„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "        \"\"\"\n",
    "        logger.info(f\"ResBlock.__init__ {in_channels=} {out_channels=} {kernel_size=} {stride=} {mid_channels=} {expand_ratio=} {use_bias=} {norm=} {act_func=} {is_video=}\")\n",
    "\n",
    "        super().__init__()\n",
    "        use_bias = val2tuple(use_bias, 2)\n",
    "        norm = val2tuple(norm, 2)\n",
    "        act_func = val2tuple(act_func, 2)\n",
    "\n",
    "        mid_channels = round(in_channels * expand_ratio) if mid_channels is None else mid_channels\n",
    "\n",
    "        self.conv1 = ConvLayer(\n",
    "            in_channels,\n",
    "            mid_channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            use_bias=use_bias[0],\n",
    "            norm=norm[0],\n",
    "            act_func=act_func[0],\n",
    "            is_video=is_video,\n",
    "        )\n",
    "        self.conv2 = ConvLayer(\n",
    "            mid_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            1,\n",
    "            use_bias=use_bias[1],\n",
    "            norm=norm[1],\n",
    "            act_func=act_func[1],\n",
    "            is_video=is_video,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logger.info(f\"ResBlock.forward {x.shape=}\")\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b516c0",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac36316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_downsample_block(\n",
    "    block_type: str,\n",
    "    in_channels: int,\n",
    "    out_channels: int,\n",
    "    shortcut: Optional[str],\n",
    "    is_video: bool,\n",
    "    temporal_downsample: bool = False,\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    „ÉÄ„Ç¶„É≥„Çµ„É≥„Éó„É™„É≥„Ç∞„Éñ„É≠„ÉÉ„ÇØ„ÇíÊßãÁØâ„Åô„Çã\n",
    "    Á©∫ÈñìÁöÑ„Å™„ÉÄ„Ç¶„É≥„Çµ„É≥„Éó„É™„É≥„Ç∞„ÅØÂ∏∏„Å´Ë°å„Çè„Çå„Çã\n",
    "    ÊôÇÈñìÁöÑ„Å™„ÉÄ„Ç¶„É≥„Çµ„É≥„Éó„É™„É≥„Ç∞„ÅØ„Ç™„Éó„Ç∑„Éß„É≥\n",
    "    build_encoder_project_in_block, Encoder„Åß‰ΩøÁî®\n",
    "\n",
    "    Args:\n",
    "        block_type (str): „Éñ„É≠„ÉÉ„ÇØ„ÅÆÁ®ÆÈ°û (\"Conv\" „Åæ„Åü„ÅØ \"ConvPixelUnshuffle\")\n",
    "        in_channels (int): ÂÖ•Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "        out_channels (int): Âá∫Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "        shortcut (Optional[str]): „Ç∑„Éß„Éº„Éà„Ç´„ÉÉ„Éà„ÅÆÁ®ÆÈ°û (None „Åæ„Åü„ÅØ \"averaging\")\n",
    "        is_video (bool): 3DÁï≥„ÅøËæº„Åø„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "        temporal_downsample (bool, optional):\n",
    "            ÊôÇÈñìÊñπÂêë„ÅÆ„ÉÄ„Ç¶„É≥„Çµ„É≥„Éó„É™„É≥„Ç∞„ÇíË°å„ÅÜ„Åã„Å©„ÅÜ„Åã\n",
    "    Returns:\n",
    "        nn.Module: „ÉÄ„Ç¶„É≥„Çµ„É≥„Éó„É™„É≥„Ç∞„Éñ„É≠„ÉÉ„ÇØ\n",
    "    \"\"\"\n",
    "    logger.info(f\"build_downsample_block {block_type=} {in_channels=} {out_channels=} {shortcut=} {is_video=} {temporal_downsample=}\")\n",
    "\n",
    "    if block_type == \"Conv\":\n",
    "        if is_video:\n",
    "            if temporal_downsample:\n",
    "                stride = (2, 2, 2)\n",
    "            else:\n",
    "                stride = (1, 2, 2)\n",
    "        else:\n",
    "            stride = 2\n",
    "        block = ConvLayer(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            use_bias=True,\n",
    "            norm=None,\n",
    "            act_func=None,\n",
    "            is_video=is_video,\n",
    "        )\n",
    "    elif block_type == \"ConvPixelUnshuffle\":\n",
    "        if is_video:\n",
    "            raise NotImplementedError(\"ConvPixelUnshuffle downsample is not supported for video\")\n",
    "        block = ConvPixelUnshuffleDownSampleLayer(\n",
    "            in_channels=in_channels, out_channels=out_channels, kernel_size=3, factor=2\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"block_type {block_type} is not supported for downsampling\")\n",
    "    if shortcut is None:\n",
    "        pass\n",
    "    elif shortcut == \"averaging\":\n",
    "        shortcut_block = PixelUnshuffleChannelAveragingDownSampleLayer(\n",
    "            in_channels=in_channels, out_channels=out_channels, factor=2, temporal_downsample=temporal_downsample\n",
    "        )\n",
    "        block = ResidualBlock(block, shortcut_block)\n",
    "    else:\n",
    "        raise ValueError(f\"shortcut {shortcut} is not supported for downsample\")\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771ff37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder_project_in_block(in_channels: int, out_channels: int, factor: int, downsample_block_type: str, is_video: bool):\n",
    "    \"\"\"\n",
    "    „Ç®„É≥„Ç≥„Éº„ÉÄ„ÅÆÂÖ•Âäõ„Éó„É≠„Ç∏„Çß„ÇØ„Ç∑„Éß„É≥„Éñ„É≠„ÉÉ„ÇØ„ÇíÊßãÁØâ„Åô„Çã\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): ÂÖ•Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "        out_channels (int): Âá∫Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "        factor (int): „ÉÄ„Ç¶„É≥„Çµ„É≥„Éó„É™„É≥„Ç∞„Éï„Ç°„ÇØ„Çø„Éº\n",
    "        downsample_block_type (str): „ÉÄ„Ç¶„É≥„Çµ„É≥„Éó„É™„É≥„Ç∞„Éñ„É≠„ÉÉ„ÇØ„ÅÆÁ®ÆÈ°û\n",
    "        is_video (bool): 3DÁï≥„ÅøËæº„Åø„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "    Returns:\n",
    "        nn.Module: „Ç®„É≥„Ç≥„Éº„ÉÄ„ÅÆÂÖ•Âäõ„Éó„É≠„Ç∏„Çß„ÇØ„Ç∑„Éß„É≥„Éñ„É≠„ÉÉ„ÇØ\n",
    "    \"\"\"\n",
    "    logger.info(f\"build_encoder_project_in_block {in_channels=} {out_channels=} {factor=} {downsample_block_type=} {is_video=}\")\n",
    "\n",
    "    if factor == 1:\n",
    "        block = ConvLayer(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            use_bias=True,\n",
    "            norm=None,\n",
    "            act_func=None,\n",
    "            is_video=is_video,\n",
    "        )\n",
    "    elif factor == 2:\n",
    "        if is_video:\n",
    "            raise NotImplementedError(\"Downsample during project_in is not supported for video\")\n",
    "        block = build_downsample_block(\n",
    "            block_type=downsample_block_type, in_channels=in_channels, out_channels=out_channels, shortcut=None\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"downsample factor {factor} is not supported for encoder project in\")\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44428bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder_project_out_block(\n",
    "    in_channels: int,\n",
    "    out_channels: int,\n",
    "    norm: Optional[str],\n",
    "    act: Optional[str],\n",
    "    shortcut: Optional[str],\n",
    "    is_video: bool,\n",
    "):\n",
    "    \"\"\"\n",
    "    „Ç®„É≥„Ç≥„Éº„ÉÄ„ÅÆÂá∫Âäõ„Éó„É≠„Ç∏„Çß„ÇØ„Ç∑„Éß„É≥„Éñ„É≠„ÉÉ„ÇØ„ÇíÊßãÁØâ„Åô„Çã\n",
    "    Encoder„Åß‰ΩøÁî®\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): ÂÖ•Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "        out_channels (int): Âá∫Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "        norm (Optional[str]): Ê≠£Ë¶èÂåñÂ±§„ÅÆÁ®ÆÈ°û\n",
    "        act (Optional[str]): Ê¥ªÊÄßÂåñÈñ¢Êï∞„ÅÆÁ®ÆÈ°û\n",
    "        shortcut (Optional[str]): „Ç∑„Éß„Éº„Éà„Ç´„ÉÉ„Éà„ÅÆÁ®ÆÈ°û (None „Åæ„Åü„ÅØ \"averaging\")\n",
    "        is_video (bool): 3DÁï≥„ÅøËæº„Åø„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "    Returns:\n",
    "        nn.Module: „Ç®„É≥„Ç≥„Éº„ÉÄ„ÅÆÂá∫Âäõ„Éó„É≠„Ç∏„Çß„ÇØ„Ç∑„Éß„É≥„Éñ„É≠„ÉÉ„ÇØ\n",
    "    \"\"\"\n",
    "    logger.info(f\"build_encoder_project_out_block {in_channels=} {out_channels=} {norm=} {act=} {shortcut=} {is_video=}\")\n",
    "\n",
    "    block = OpSequential(\n",
    "        [\n",
    "            build_norm(norm),\n",
    "            build_act(act),\n",
    "            ConvLayer(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                use_bias=True,\n",
    "                norm=None,\n",
    "                act_func=None,\n",
    "                is_video=is_video,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    if shortcut is None:\n",
    "        pass\n",
    "    elif shortcut == \"averaging\":\n",
    "        shortcut_block = PixelUnshuffleChannelAveragingDownSampleLayer(\n",
    "            in_channels=in_channels, out_channels=out_channels, factor=1\n",
    "        )\n",
    "        block = ResidualBlock(block, shortcut_block)\n",
    "    else:\n",
    "        raise ValueError(f\"shortcut {shortcut} is not supported for encoder project out\")\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3a77b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_block(\n",
    "    block_type: str, in_channels: int, out_channels: int, norm: Optional[str], act: Optional[str], is_video: bool\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    „Éñ„É≠„ÉÉ„ÇØ„ÇíÊßãÁØâ„Åô„Çã\n",
    "    build_stage_main„Åß‰ΩøÁî®\n",
    "    Args:\n",
    "        block_type (str): „Éñ„É≠„ÉÉ„ÇØ„ÅÆÁ®ÆÈ°û (\"ResBlock\", \"EViT_GLU\", \"EViTS5_GLU\")\n",
    "        in_channels (int): ÂÖ•Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "        out_channels (int): Âá∫Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "        norm (Optional[str]): Ê≠£Ë¶èÂåñÂ±§„ÅÆÁ®ÆÈ°û\n",
    "        act (Optional[str]): Ê¥ªÊÄßÂåñÈñ¢Êï∞„ÅÆÁ®ÆÈ°û\n",
    "        is_video (bool): 3DÁï≥„ÅøËæº„Åø„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "    Returns:\n",
    "        nn.Module: „Éñ„É≠„ÉÉ„ÇØ\n",
    "    \"\"\"\n",
    "    logger.info(f\"build_block {block_type=} {in_channels=} {out_channels=} {norm=} {act=} {is_video=}\")\n",
    "\n",
    "    if block_type == \"ResBlock\":\n",
    "        assert in_channels == out_channels\n",
    "        main_block = ResBlock(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            use_bias=(True, False),\n",
    "            norm=(None, norm),\n",
    "            act_func=(act, None),\n",
    "            is_video=is_video,\n",
    "        )\n",
    "        block = ResidualBlock(main_block, IdentityLayer())\n",
    "    elif block_type == \"EViT_GLU\":\n",
    "        assert in_channels == out_channels\n",
    "        block = EfficientViTBlock(\n",
    "            in_channels, norm=norm, act_func=act, local_module=\"GLUMBConv\", scales=(), is_video=is_video\n",
    "        )\n",
    "    elif block_type == \"EViTS5_GLU\":\n",
    "        assert in_channels == out_channels\n",
    "        block = EfficientViTBlock(\n",
    "            in_channels, norm=norm, act_func=act, local_module=\"GLUMBConv\", scales=(5,), is_video=is_video\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"block_type {block_type} is not supported\")\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7b4b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_stage_main(\n",
    "    width: int, depth: int, block_type: str | list[str], norm: str, act: str, input_width: int, is_video: bool\n",
    ") -> list[nn.Module]:\n",
    "    \"\"\"\n",
    "    „Çπ„ÉÜ„Éº„Ç∏„ÅÆ„É°„Ç§„É≥„Éñ„É≠„ÉÉ„ÇØ„ÇíÊßãÁØâ„Åô„Çã\n",
    "    Encoder, Decoder„Åß‰ΩøÁî®\n",
    "    Args:\n",
    "        width (int): „Éñ„É≠„ÉÉ„ÇØ„ÅÆ„ÉÅ„É£„Éç„É´Êï∞\n",
    "        depth (int): „Éñ„É≠„ÉÉ„ÇØ„ÅÆÊ∑±„Åï\n",
    "        block_type (str or list[str]): „Éñ„É≠„ÉÉ„ÇØ„ÅÆÁ®ÆÈ°û\n",
    "        norm (str): Ê≠£Ë¶èÂåñÂ±§„ÅÆÁ®ÆÈ°û\n",
    "        act (str): Ê¥ªÊÄßÂåñÈñ¢Êï∞„ÅÆÁ®ÆÈ°û\n",
    "        input_width (int): ÂÖ•Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "        is_video (bool): 3DÁï≥„ÅøËæº„Åø„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "    Returns:\n",
    "        list[nn.Module]: „Çπ„ÉÜ„Éº„Ç∏„ÅÆ„É°„Ç§„É≥„Éñ„É≠„ÉÉ„ÇØ„ÅÆ„É™„Çπ„Éà\n",
    "    \"\"\"\n",
    "    logger.info(f\"build_stage_main {width=} {depth=} {block_type=} {norm=} {act=} {input_width=} {is_video=}\")\n",
    "\n",
    "    assert isinstance(block_type, str) or (isinstance(block_type, list) and depth == len(block_type))\n",
    "    stage = []\n",
    "    for d in range(depth):\n",
    "        current_block_type = block_type[d] if isinstance(block_type, list) else block_type\n",
    "        block = build_block(\n",
    "            block_type=current_block_type,\n",
    "            in_channels=width if d > 0 else input_width,\n",
    "            out_channels=width,\n",
    "            norm=norm,\n",
    "            act=act,\n",
    "            is_video=is_video,\n",
    "        )\n",
    "        stage.append(block)\n",
    "    return stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45544988",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EncoderConfig:\n",
    "    in_channels: int = MISSING\n",
    "    latent_channels: int = MISSING\n",
    "    width_list: tuple[int, ...] = (128, 256, 512, 512, 1024, 1024)\n",
    "    depth_list: tuple[int, ...] = (2, 2, 2, 2, 2, 2)\n",
    "    block_type: Any = \"ResBlock\"\n",
    "    norm: str = \"rms2d\"\n",
    "    act: str = \"silu\"\n",
    "    downsample_block_type: str = \"ConvPixelUnshuffle\"\n",
    "    downsample_match_channel: bool = True\n",
    "    downsample_shortcut: Optional[str] = \"averaging\"\n",
    "    out_norm: Optional[str] = None\n",
    "    out_act: Optional[str] = None\n",
    "    out_shortcut: Optional[str] = \"averaging\"\n",
    "    double_latent: bool = False\n",
    "    is_video: bool = False\n",
    "    temporal_downsample: tuple[bool, ...] = ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8abf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    „Ç®„É≥„Ç≥„Éº„ÉÄ\n",
    "    DCAE„Åß‰ΩøÁî®\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: EncoderConfig):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cfg (EncoderConfig): „Ç®„É≥„Ç≥„Éº„ÉÄ„ÅÆË®≠ÂÆö\n",
    "        \"\"\"\n",
    "        logger.info(f\"Encoder.__init__ {cfg=}\")\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        num_stages = len(cfg.width_list)\n",
    "        self.num_stages = num_stages\n",
    "        assert len(cfg.depth_list) == num_stages\n",
    "        assert len(cfg.width_list) == num_stages\n",
    "        assert isinstance(cfg.block_type, str) or (\n",
    "            isinstance(cfg.block_type, list) and len(cfg.block_type) == num_stages\n",
    "        )\n",
    "\n",
    "        self.project_in = build_encoder_project_in_block(\n",
    "            in_channels=cfg.in_channels,\n",
    "            out_channels=cfg.width_list[0] if cfg.depth_list[0] > 0 else cfg.width_list[1],\n",
    "            factor=1 if cfg.depth_list[0] > 0 else 2,\n",
    "            downsample_block_type=cfg.downsample_block_type,\n",
    "            is_video=cfg.is_video,\n",
    "        )\n",
    "\n",
    "        self.stages: list[OpSequential] = []\n",
    "        for stage_id, (width, depth) in enumerate(zip(cfg.width_list, cfg.depth_list)):\n",
    "            block_type = cfg.block_type[stage_id] if isinstance(cfg.block_type, list) else cfg.block_type\n",
    "            stage = build_stage_main(\n",
    "                width=width,\n",
    "                depth=depth,\n",
    "                block_type=block_type,\n",
    "                norm=cfg.norm,\n",
    "                act=cfg.act,\n",
    "                input_width=width,\n",
    "                is_video=cfg.is_video,\n",
    "            )\n",
    "\n",
    "            if stage_id < num_stages - 1 and depth > 0:\n",
    "                downsample_block = build_downsample_block(\n",
    "                    block_type=cfg.downsample_block_type,\n",
    "                    in_channels=width,\n",
    "                    out_channels=cfg.width_list[stage_id + 1] if cfg.downsample_match_channel else width,\n",
    "                    shortcut=cfg.downsample_shortcut,\n",
    "                    is_video=cfg.is_video,\n",
    "                    temporal_downsample=cfg.temporal_downsample[stage_id] if cfg.temporal_downsample != [] else False,\n",
    "                )\n",
    "                stage.append(downsample_block)\n",
    "            self.stages.append(OpSequential(stage))\n",
    "        self.stages = nn.ModuleList(self.stages)\n",
    "\n",
    "        self.project_out = build_encoder_project_out_block(\n",
    "            in_channels=cfg.width_list[-1],\n",
    "            out_channels=2 * cfg.latent_channels if cfg.double_latent else cfg.latent_channels,\n",
    "            norm=cfg.out_norm,\n",
    "            act=cfg.out_act,\n",
    "            shortcut=cfg.out_shortcut,\n",
    "            is_video=cfg.is_video,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logger.info(f\"Encoder.forward {x.shape=}\")\n",
    "        x = self.project_in(x)\n",
    "        # x = auto_grad_checkpoint(self.project_in, x)\n",
    "        for stage in self.stages:\n",
    "            if len(stage.op_list) == 0:\n",
    "                continue\n",
    "            x = auto_grad_checkpoint(stage, x)\n",
    "        # x = self.project_out(x)\n",
    "        x = auto_grad_checkpoint(self.project_out, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e844bb",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366f5c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decoder_project_in_block(in_channels: int, out_channels: int, shortcut: Optional[str], is_video: bool):\n",
    "    \"\"\"\n",
    "    „Éá„Ç≥„Éº„ÉÄ„ÅÆÂÖ•Âäõ„Éó„É≠„Ç∏„Çß„ÇØ„Ç∑„Éß„É≥„Éñ„É≠„ÉÉ„ÇØ„ÇíÊßãÁØâ„Åô„Çã\n",
    "    Decoder„Åß‰ΩøÁî®\n",
    "    Args:\n",
    "        in_channels (int): ÂÖ•Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "        out_channels (int): Âá∫Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "        shortcut (Optional[str]): „Ç∑„Éß„Éº„Éà„Ç´„ÉÉ„Éà„ÅÆÁ®ÆÈ°û (None „Åæ„Åü„ÅØ \"duplicating\")\n",
    "        is_video (bool): 3DÁï≥„ÅøËæº„Åø„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "    Returns:\n",
    "        nn.Module: „Éá„Ç≥„Éº„ÉÄ„ÅÆÂÖ•Âäõ„Éó„É≠„Ç∏„Çß„ÇØ„Ç∑„Éß„É≥„Éñ„É≠„ÉÉ„ÇØ\n",
    "    \"\"\"\n",
    "    logger.info(f\"build_decoder_project_in_block {in_channels=} {out_channels=} {shortcut=} {is_video=}\")\n",
    "\n",
    "    block = ConvLayer(\n",
    "        in_channels=in_channels,\n",
    "        out_channels=out_channels,\n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        use_bias=True,\n",
    "        norm=None,\n",
    "        act_func=None,\n",
    "        is_video=is_video,\n",
    "    )\n",
    "    if shortcut is None:\n",
    "        pass\n",
    "    elif shortcut == \"duplicating\":\n",
    "        shortcut_block = ChannelDuplicatingPixelShuffleUpSampleLayer(\n",
    "            in_channels=in_channels, out_channels=out_channels, factor=1\n",
    "        )\n",
    "        block = ResidualBlock(block, shortcut_block)\n",
    "    else:\n",
    "        raise ValueError(f\"shortcut {shortcut} is not supported for decoder project in\")\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ec9105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_upsample_block(\n",
    "    block_type: str,\n",
    "    in_channels: int,\n",
    "    out_channels: int,\n",
    "    shortcut: Optional[str],\n",
    "    is_video: bool,\n",
    "    temporal_upsample: bool = False,\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    „Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É™„É≥„Ç∞„Éñ„É≠„ÉÉ„ÇØ„ÇíÊßãÁØâ„Åô„Çã\n",
    "    build_decoder_project_out_block, Decoder„Åß‰ΩøÁî®\n",
    "\n",
    "    Args:\n",
    "        block_type (str): „Éñ„É≠„ÉÉ„ÇØ„ÅÆÁ®ÆÈ°û (\"ConvPixelShuffle\" „Åæ„Åü„ÅØ \"InterpolateConv\")\n",
    "        in_channels (int): ÂÖ•Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "        out_channels (int): Âá∫Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "        shortcut (Optional[str]): „Ç∑„Éß„Éº„Éà„Ç´„ÉÉ„Éà„ÅÆÁ®ÆÈ°û (None „Åæ„Åü„ÅØ \"duplicating\")\n",
    "        is_video (bool): 3DÁï≥„ÅøËæº„Åø„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "        temporal_upsample (bool, optional):\n",
    "            ÊôÇÈñìÊñπÂêë„ÅÆ„Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É™„É≥„Ç∞„ÇíË°å„ÅÜ„Åã„Å©„ÅÜ„Åã\n",
    "    Returns:\n",
    "        nn.Module: „Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É™„É≥„Ç∞„Éñ„É≠„ÉÉ„ÇØ\n",
    "    \"\"\"\n",
    "    logger.info(f\"build_upsample_block {block_type=} {in_channels=} {out_channels=} {shortcut=} {is_video=} {temporal_upsample=}\")\n",
    "    if block_type == \"ConvPixelShuffle\":\n",
    "        if is_video:\n",
    "            raise NotImplementedError(\"ConvPixelShuffle upsample is not supported for video\")\n",
    "        block = ConvPixelShuffleUpSampleLayer(\n",
    "            in_channels=in_channels, out_channels=out_channels, kernel_size=3, factor=2\n",
    "        )\n",
    "    elif block_type == \"InterpolateConv\":\n",
    "        block = InterpolateConvUpSampleLayer(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            factor=2,\n",
    "            is_video=is_video,\n",
    "            temporal_upsample=temporal_upsample,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"block_type {block_type} is not supported for upsampling\")\n",
    "    if shortcut is None:\n",
    "        pass\n",
    "    elif shortcut == \"duplicating\":\n",
    "        shortcut_block = ChannelDuplicatingPixelShuffleUpSampleLayer(\n",
    "            in_channels=in_channels, out_channels=out_channels, factor=2, temporal_upsample=temporal_upsample\n",
    "        )\n",
    "        block = ResidualBlock(block, shortcut_block)\n",
    "    else:\n",
    "        raise ValueError(f\"shortcut {shortcut} is not supported for upsample\")\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2080afad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decoder_project_out_block(\n",
    "    in_channels: int,\n",
    "    out_channels: int,\n",
    "    factor: int,\n",
    "    upsample_block_type: str,\n",
    "    norm: Optional[str],\n",
    "    act: Optional[str],\n",
    "    is_video: bool,\n",
    "):\n",
    "    \"\"\"\n",
    "    „Éá„Ç≥„Éº„ÉÄ„ÅÆÂá∫Âäõ„Éó„É≠„Ç∏„Çß„ÇØ„Ç∑„Éß„É≥„Éñ„É≠„ÉÉ„ÇØ„ÇíÊßãÁØâ„Åô„Çã\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): ÂÖ•Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "        out_channels (int): Âá∫Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "        factor (int): „Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É™„É≥„Ç∞„Éï„Ç°„ÇØ„Çø„Éº\n",
    "        upsample_block_type (str): „Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É™„É≥„Ç∞„Éñ„É≠„ÉÉ„ÇØ„ÅÆÁ®ÆÈ°û\n",
    "        norm (Optional[str]): Ê≠£Ë¶èÂåñÂ±§„ÅÆÁ®ÆÈ°û\n",
    "        act (Optional[str]): Ê¥ªÊÄßÂåñÈñ¢Êï∞„ÅÆÁ®ÆÈ°û\n",
    "        is_video (bool): 3DÁï≥„ÅøËæº„Åø„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "    Returns:\n",
    "        nn.Module: „Éá„Ç≥„Éº„ÉÄ„ÅÆÂá∫Âäõ„Éó„É≠„Ç∏„Çß„ÇØ„Ç∑„Éß„É≥„Éñ„É≠„ÉÉ„ÇØ\n",
    "    \"\"\"\n",
    "    logger.info(f\"build_decoder_project_out_block {in_channels=} {out_channels=} {factor=} {upsample_block_type=} {norm=} {act=} {is_video=}\")\n",
    "\n",
    "    layers: list[nn.Module] = [\n",
    "        build_norm(norm, in_channels),\n",
    "        build_act(act),\n",
    "    ]\n",
    "    if factor == 1:\n",
    "        layers.append(\n",
    "            ConvLayer(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                use_bias=True,\n",
    "                norm=None,\n",
    "                act_func=None,\n",
    "                is_video=is_video,\n",
    "            )\n",
    "        )\n",
    "    elif factor == 2:\n",
    "        if is_video:\n",
    "            raise NotImplementedError(\"Upsample during project_out is not supported for video\")\n",
    "        layers.append(\n",
    "            build_upsample_block(\n",
    "                block_type=upsample_block_type, in_channels=in_channels, out_channels=out_channels, shortcut=None\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"upsample factor {factor} is not supported for decoder project out\")\n",
    "    return OpSequential(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf6a03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DecoderConfig:\n",
    "    in_channels: int = MISSING\n",
    "    latent_channels: int = MISSING\n",
    "    in_shortcut: Optional[str] = \"duplicating\"\n",
    "    width_list: tuple[int, ...] = (128, 256, 512, 512, 1024, 1024)\n",
    "    depth_list: tuple[int, ...] = (2, 2, 2, 2, 2, 2)\n",
    "    block_type: Any = \"ResBlock\"\n",
    "    norm: Any = \"rms2d\"\n",
    "    act: Any = \"silu\"\n",
    "    upsample_block_type: str = \"ConvPixelShuffle\"\n",
    "    upsample_match_channel: bool = True\n",
    "    upsample_shortcut: str = \"duplicating\"\n",
    "    out_norm: str = \"rms2d\"\n",
    "    out_act: str = \"relu\"\n",
    "    is_video: bool = False\n",
    "    temporal_upsample: tuple[bool, ...] = ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454056a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    „Éá„Ç≥„Éº„ÉÄ\n",
    "    DCAE„Åß‰ΩøÁî®\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: DecoderConfig):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cfg (DecoderConfig): „Éá„Ç≥„Éº„ÉÄ„ÅÆË®≠ÂÆö\n",
    "        \"\"\"\n",
    "        logger.info(f\"Decoder.__init__ {cfg=}\")\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        num_stages = len(cfg.width_list)\n",
    "        self.num_stages = num_stages\n",
    "        assert len(cfg.depth_list) == num_stages\n",
    "        assert len(cfg.width_list) == num_stages\n",
    "        assert isinstance(cfg.block_type, str) or (\n",
    "            isinstance(cfg.block_type, list) and len(cfg.block_type) == num_stages\n",
    "        )\n",
    "        assert isinstance(cfg.norm, str) or (isinstance(cfg.norm, list) and len(cfg.norm) == num_stages)\n",
    "        assert isinstance(cfg.act, str) or (isinstance(cfg.act, list) and len(cfg.act) == num_stages)\n",
    "\n",
    "        self.project_in = build_decoder_project_in_block(\n",
    "            in_channels=cfg.latent_channels,\n",
    "            out_channels=cfg.width_list[-1],\n",
    "            shortcut=cfg.in_shortcut,\n",
    "            is_video=cfg.is_video,\n",
    "        )\n",
    "\n",
    "        self.stages: list[OpSequential] = []\n",
    "        for stage_id, (width, depth) in reversed(list(enumerate(zip(cfg.width_list, cfg.depth_list)))):\n",
    "            stage = []\n",
    "            if stage_id < num_stages - 1 and depth > 0:\n",
    "                upsample_block = build_upsample_block(\n",
    "                    block_type=cfg.upsample_block_type,\n",
    "                    in_channels=cfg.width_list[stage_id + 1],\n",
    "                    out_channels=width if cfg.upsample_match_channel else cfg.width_list[stage_id + 1],\n",
    "                    shortcut=cfg.upsample_shortcut,\n",
    "                    is_video=cfg.is_video,\n",
    "                    temporal_upsample=cfg.temporal_upsample[stage_id] if cfg.temporal_upsample != [] else False,\n",
    "                )\n",
    "                stage.append(upsample_block)\n",
    "\n",
    "            block_type = cfg.block_type[stage_id] if isinstance(cfg.block_type, list) else cfg.block_type\n",
    "            norm = cfg.norm[stage_id] if isinstance(cfg.norm, list) else cfg.norm\n",
    "            act = cfg.act[stage_id] if isinstance(cfg.act, list) else cfg.act\n",
    "            stage.extend(\n",
    "                build_stage_main(\n",
    "                    width=width,\n",
    "                    depth=depth,\n",
    "                    block_type=block_type,\n",
    "                    norm=norm,\n",
    "                    act=act,\n",
    "                    input_width=(\n",
    "                        width if cfg.upsample_match_channel else cfg.width_list[min(stage_id + 1, num_stages - 1)]\n",
    "                    ),\n",
    "                    is_video=cfg.is_video,\n",
    "                )\n",
    "            )\n",
    "            self.stages.insert(0, OpSequential(stage))\n",
    "        self.stages = nn.ModuleList(self.stages)\n",
    "\n",
    "        self.project_out = build_decoder_project_out_block(\n",
    "            in_channels=cfg.width_list[0] if cfg.depth_list[0] > 0 else cfg.width_list[1],\n",
    "            out_channels=cfg.in_channels,\n",
    "            factor=1 if cfg.depth_list[0] > 0 else 2,\n",
    "            upsample_block_type=cfg.upsample_block_type,\n",
    "            norm=cfg.out_norm,\n",
    "            act=cfg.out_act,\n",
    "            is_video=cfg.is_video,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logger.info(f\"Decoder.forward {x.shape=}\")\n",
    "        x = auto_grad_checkpoint(self.project_in, x)\n",
    "        for stage in reversed(self.stages):\n",
    "            if len(stage.op_list) == 0:\n",
    "                continue\n",
    "            # x = stage(x)\n",
    "            x = auto_grad_checkpoint(stage, x)\n",
    "\n",
    "        if self.disc_off_grad_ckpt:\n",
    "            x = self.project_out(x)\n",
    "        else:\n",
    "            x = auto_grad_checkpoint(self.project_out, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e1fed3",
   "metadata": {},
   "source": [
    "### DC-AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cae01e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DCAEConfig:\n",
    "    in_channels: int = 3\n",
    "    latent_channels: int = 32\n",
    "    time_compression_ratio: int = 1\n",
    "    spatial_compression_ratio: int = 32\n",
    "    encoder: EncoderConfig = field(\n",
    "        default_factory=lambda: EncoderConfig(in_channels=\"${..in_channels}\", latent_channels=\"${..latent_channels}\")\n",
    "    )\n",
    "    decoder: DecoderConfig = field(\n",
    "        default_factory=lambda: DecoderConfig(in_channels=\"${..in_channels}\", latent_channels=\"${..latent_channels}\")\n",
    "    )\n",
    "    use_quant_conv: bool = False\n",
    "\n",
    "    pretrained_path: Optional[str] = None\n",
    "    pretrained_source: str = \"dc-ae\"\n",
    "\n",
    "    scaling_factor: Optional[float] = None\n",
    "    is_image_model: bool = False\n",
    "\n",
    "    is_training: bool = False  # NOTE: set to True in vae train config\n",
    "\n",
    "    use_spatial_tiling: bool = False\n",
    "    use_temporal_tiling: bool = False\n",
    "    spatial_tile_size: int = 256\n",
    "    temporal_tile_size: int = 32\n",
    "    tile_overlap_factor: float = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af7dd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_modules(model: Union[nn.Module, list[nn.Module]], init_type=\"trunc_normal\") -> None:\n",
    "    \"\"\"\n",
    "    „É¢„Éá„É´„ÅÆÈáç„Åø„ÇíÂàùÊúüÂåñ„Åô„Çã\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module or list[nn.Module]): ÂàùÊúüÂåñ„Åô„Çã„É¢„Éá„É´„Åæ„Åü„ÅØ„É¢„Éá„É´„ÅÆ„É™„Çπ„Éà\n",
    "        init_type (str, optional): ÂàùÊúüÂåñ„ÅÆÁ®ÆÈ°û (\"trunc_normal@std\" „Åæ„Åü„ÅØ \"normal@std\")\n",
    "    \"\"\"\n",
    "    logger.info(f\"init_modules {init_type=}\")\n",
    "\n",
    "    _DEFAULT_INIT_PARAM = {\"trunc_normal\": 0.02}\n",
    "\n",
    "    if isinstance(model, list):\n",
    "        for sub_module in model:\n",
    "            init_modules(sub_module, init_type)\n",
    "    else:\n",
    "        init_params = init_type.split(\"@\")\n",
    "        init_params = float(init_params[1]) if len(init_params) > 1 else None\n",
    "\n",
    "        if init_type.startswith(\"trunc_normal\"):\n",
    "            init_func = lambda param: nn.init.trunc_normal_(\n",
    "                param, std=(_DEFAULT_INIT_PARAM[\"trunc_normal\"] if init_params is None else init_params)\n",
    "            )\n",
    "        elif init_type.startswith(\"normal\"):\n",
    "            init_func = lambda param: nn.init.normal_(\n",
    "                param, std=(_DEFAULT_INIT_PARAM[\"trunc_normal\"] if init_params is None else init_params)\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        for m in model.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear, nn.ConvTranspose2d)):\n",
    "                init_func(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Embedding):\n",
    "                init_func(m.weight)\n",
    "            elif isinstance(m, (_BatchNorm, nn.GroupNorm, nn.LayerNorm)):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            else:\n",
    "                weight = getattr(m, \"weight\", None)\n",
    "                bias = getattr(m, \"bias\", None)\n",
    "                if isinstance(weight, torch.nn.Parameter):\n",
    "                    init_func(weight)\n",
    "                if isinstance(bias, torch.nn.Parameter):\n",
    "                    bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cea200",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Compressive Autoencoder (DCAE)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: DCAEConfig):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cfg (DCAEConfig): DCAE„ÅÆË®≠ÂÆö\n",
    "        \"\"\"\n",
    "        logger.info(f\"DCAE.__init__ {cfg=}\")\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.encoder = Encoder(cfg.encoder)\n",
    "        self.decoder = Decoder(cfg.decoder)\n",
    "        self.scaling_factor = cfg.scaling_factor\n",
    "        self.time_compression_ratio = cfg.time_compression_ratio\n",
    "        self.spatial_compression_ratio = cfg.spatial_compression_ratio\n",
    "        self.use_spatial_tiling = cfg.use_spatial_tiling\n",
    "        self.use_temporal_tiling = cfg.use_temporal_tiling\n",
    "        self.spatial_tile_size = cfg.spatial_tile_size\n",
    "        self.temporal_tile_size = cfg.temporal_tile_size\n",
    "        assert (\n",
    "            cfg.spatial_tile_size // cfg.spatial_compression_ratio\n",
    "        ), f\"spatial tile size {cfg.spatial_tile_size} must be divisible by spatial compression of {cfg.spatial_compression_ratio}\"\n",
    "        self.spatial_tile_latent_size = cfg.spatial_tile_size // cfg.spatial_compression_ratio\n",
    "        assert (\n",
    "            cfg.temporal_tile_size // cfg.time_compression_ratio\n",
    "        ), f\"temporal tile size {cfg.temporal_tile_size} must be divisible by temporal compression of {cfg.time_compression_ratio}\"\n",
    "        self.temporal_tile_latent_size = cfg.temporal_tile_size // cfg.time_compression_ratio\n",
    "        self.tile_overlap_factor = cfg.tile_overlap_factor\n",
    "        if self.cfg.pretrained_path is not None:\n",
    "            self.load_model()\n",
    "\n",
    "        self.to(torch.float32)\n",
    "        init_modules(self, init_type=\"trunc_normal\")\n",
    "\n",
    "    def load_model(self):\n",
    "        if self.cfg.pretrained_source == \"dc-ae\":\n",
    "            state_dict = torch.load(self.cfg.pretrained_path, map_location=\"cpu\", weights_only=True)[\"state_dict\"]\n",
    "            self.load_state_dict(state_dict)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def get_last_layer(self):\n",
    "        return self.decoder.project_out.op_list[2].conv.weight\n",
    "\n",
    "    # @property\n",
    "    # def spatial_compression_ratio(self) -> int:\n",
    "    #     return 2 ** (self.decoder.num_stages - 1)\n",
    "\n",
    "    def encode_single(self, x: torch.Tensor, is_video_encoder: bool = False) -> torch.Tensor:\n",
    "        assert x.shape[0] == 1\n",
    "        is_video = x.dim() == 5\n",
    "        if is_video and not is_video_encoder:\n",
    "            b, c, f, h, w = x.shape\n",
    "            x = x.permute(0, 2, 1, 3, 4).reshape(-1, c, h, w)\n",
    "        z = self.encoder(x)\n",
    "\n",
    "        if is_video and not is_video_encoder:\n",
    "            z = z.unsqueeze(dim=0).permute(0, 2, 1, 3, 4)\n",
    "\n",
    "        if self.scaling_factor is not None:\n",
    "            z = z / self.scaling_factor\n",
    "\n",
    "        return z\n",
    "\n",
    "    def _encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.cfg.is_training:\n",
    "            return self.encoder(x)\n",
    "        is_video_encoder = self.encoder.cfg.is_video if self.encoder.cfg.is_video is not None else False\n",
    "        x_ret = []\n",
    "        for i in range(x.shape[0]):\n",
    "            x_ret.append(self.encode_single(x[i : i + 1], is_video_encoder))\n",
    "        return torch.cat(x_ret, dim=0)\n",
    "\n",
    "    def blend_v(self, a: torch.Tensor, b: torch.Tensor, blend_extent: int) -> torch.Tensor:\n",
    "        blend_extent = min(a.shape[-2], b.shape[-2], blend_extent)\n",
    "        for y in range(blend_extent):\n",
    "            b[:, :, :, y, :] = a[:, :, :, -blend_extent + y, :] * (1 - y / blend_extent) + b[:, :, :, y, :] * (\n",
    "                y / blend_extent\n",
    "            )\n",
    "        return b\n",
    "\n",
    "    def blend_h(self, a: torch.Tensor, b: torch.Tensor, blend_extent: int) -> torch.Tensor:\n",
    "        blend_extent = min(a.shape[-1], b.shape[-1], blend_extent)\n",
    "        for x in range(blend_extent):\n",
    "            b[:, :, :, :, x] = a[:, :, :, :, -blend_extent + x] * (1 - x / blend_extent) + b[:, :, :, :, x] * (\n",
    "                x / blend_extent\n",
    "            )\n",
    "        return b\n",
    "\n",
    "    def blend_t(self, a: torch.Tensor, b: torch.Tensor, blend_extent: int) -> torch.Tensor:\n",
    "        blend_extent = min(a.shape[-3], b.shape[-3], blend_extent)\n",
    "        for x in range(blend_extent):\n",
    "            b[:, :, x, :, :] = a[:, :, -blend_extent + x, :, :] * (1 - x / blend_extent) + b[:, :, x, :, :] * (\n",
    "                x / blend_extent\n",
    "            )\n",
    "        return b\n",
    "\n",
    "    def spatial_tiled_encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        net_size = int(self.spatial_tile_size * (1 - self.tile_overlap_factor))\n",
    "        blend_extent = int(self.spatial_tile_latent_size * self.tile_overlap_factor)\n",
    "        row_limit = self.spatial_tile_latent_size - blend_extent\n",
    "\n",
    "        # Split video into tiles and encode them separately.\n",
    "        rows = []\n",
    "        for i in range(0, x.shape[-2], net_size):\n",
    "            row = []\n",
    "            for j in range(0, x.shape[-1], net_size):\n",
    "                tile = x[:, :, :, i : i + self.spatial_tile_size, j : j + self.spatial_tile_size]\n",
    "                tile = self._encode(tile)\n",
    "                row.append(tile)\n",
    "            rows.append(row)\n",
    "        result_rows = []\n",
    "        for i, row in enumerate(rows):\n",
    "            result_row = []\n",
    "            for j, tile in enumerate(row):\n",
    "                # blend the above tile and the left tile\n",
    "                # to the current tile and add the current tile to the result row\n",
    "                if i > 0:\n",
    "                    tile = self.blend_v(rows[i - 1][j], tile, blend_extent)\n",
    "                if j > 0:\n",
    "                    tile = self.blend_h(row[j - 1], tile, blend_extent)\n",
    "                result_row.append(tile[:, :, :, :row_limit, :row_limit])\n",
    "            result_rows.append(torch.cat(result_row, dim=-1))\n",
    "\n",
    "        return torch.cat(result_rows, dim=-2)\n",
    "\n",
    "    def temporal_tiled_encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        overlap_size = int(self.temporal_tile_size * (1 - self.tile_overlap_factor))\n",
    "        blend_extent = int(self.temporal_tile_latent_size * self.tile_overlap_factor)\n",
    "        t_limit = self.temporal_tile_latent_size - blend_extent\n",
    "\n",
    "        # Split the video into tiles and encode them separately.\n",
    "        row = []\n",
    "        for i in range(0, x.shape[2], overlap_size):\n",
    "            tile = x[:, :, i : i + self.temporal_tile_size, :, :]\n",
    "            if self.use_spatial_tiling and (\n",
    "                tile.shape[-1] > self.spatial_tile_size or tile.shape[-2] > self.spatial_tile_size\n",
    "            ):\n",
    "                tile = self.spatial_tiled_encode(tile)\n",
    "            else:\n",
    "                tile = self._encode(tile)\n",
    "            row.append(tile)\n",
    "        result_row = []\n",
    "        for i, tile in enumerate(row):\n",
    "            if i > 0:\n",
    "                tile = self.blend_t(row[i - 1], tile, blend_extent)\n",
    "            result_row.append(tile[:, :, :t_limit, :, :])\n",
    "\n",
    "        return torch.cat(result_row, dim=2)\n",
    "\n",
    "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.use_temporal_tiling and x.shape[2] > self.temporal_tile_size:\n",
    "            return self.temporal_tiled_encode(x)\n",
    "        elif self.use_spatial_tiling and (x.shape[-1] > self.spatial_tile_size or x.shape[-2] > self.spatial_tile_size):\n",
    "            return self.spatial_tiled_encode(x)\n",
    "        else:\n",
    "            return self._encode(x)\n",
    "\n",
    "    def spatial_tiled_decode(self, z: torch.FloatTensor) -> torch.Tensor:\n",
    "        net_size = int(self.spatial_tile_latent_size * (1 - self.tile_overlap_factor))\n",
    "        blend_extent = int(self.spatial_tile_size * self.tile_overlap_factor)\n",
    "        row_limit = self.spatial_tile_size - blend_extent\n",
    "\n",
    "        # Split z into overlapping tiles and decode them separately.\n",
    "        # The tiles have an overlap to avoid seams between tiles.\n",
    "        rows = []\n",
    "        for i in range(0, z.shape[-2], net_size):\n",
    "            row = []\n",
    "            for j in range(0, z.shape[-1], net_size):\n",
    "                tile = z[:, :, :, i : i + self.spatial_tile_latent_size, j : j + self.spatial_tile_latent_size]\n",
    "                decoded = self._decode(tile)\n",
    "                row.append(decoded)\n",
    "            rows.append(row)\n",
    "        result_rows = []\n",
    "        for i, row in enumerate(rows):\n",
    "            result_row = []\n",
    "            for j, tile in enumerate(row):\n",
    "                # blend the above tile and the left tile\n",
    "                # to the current tile and add the current tile to the result row\n",
    "                if i > 0:\n",
    "                    tile = self.blend_v(rows[i - 1][j], tile, blend_extent)\n",
    "                if j > 0:\n",
    "                    tile = self.blend_h(row[j - 1], tile, blend_extent)\n",
    "                result_row.append(tile[:, :, :, :row_limit, :row_limit])\n",
    "            result_rows.append(torch.cat(result_row, dim=-1))\n",
    "\n",
    "        return torch.cat(result_rows, dim=-2)\n",
    "\n",
    "    def temporal_tiled_decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        overlap_size = int(self.temporal_tile_latent_size * (1 - self.tile_overlap_factor))\n",
    "        blend_extent = int(self.temporal_tile_size * self.tile_overlap_factor)\n",
    "        t_limit = self.temporal_tile_size - blend_extent\n",
    "\n",
    "        row = []\n",
    "        for i in range(0, z.shape[2], overlap_size):\n",
    "            tile = z[:, :, i : i + self.temporal_tile_latent_size, :, :]\n",
    "            if self.use_spatial_tiling and (\n",
    "                tile.shape[-1] > self.spatial_tile_latent_size or tile.shape[-2] > self.spatial_tile_latent_size\n",
    "            ):\n",
    "                decoded = self.spatial_tiled_decode(tile)\n",
    "            else:\n",
    "                decoded = self._decode(tile)\n",
    "            row.append(decoded)\n",
    "        result_row = []\n",
    "        for i, tile in enumerate(row):\n",
    "            if i > 0:\n",
    "                tile = self.blend_t(row[i - 1], tile, blend_extent)\n",
    "            result_row.append(tile[:, :, :t_limit, :, :])\n",
    "\n",
    "        return torch.cat(result_row, dim=2)\n",
    "\n",
    "    def decode_single(self, z: torch.Tensor, is_video_decoder: bool = False) -> torch.Tensor:\n",
    "        assert z.shape[0] == 1\n",
    "        is_video = z.dim() == 5\n",
    "        if is_video and not is_video_decoder:\n",
    "            b, c, f, h, w = z.shape\n",
    "            z = z.permute(0, 2, 1, 3, 4).reshape(-1, c, h, w)\n",
    "        if self.scaling_factor is not None:\n",
    "            z = z * self.scaling_factor\n",
    "\n",
    "        x = self.decoder(z)\n",
    "\n",
    "        if is_video and not is_video_decoder:\n",
    "            x = x.unsqueeze(dim=0).permute(0, 2, 1, 3, 4)\n",
    "        return x\n",
    "\n",
    "    def _decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        if self.cfg.is_training:\n",
    "            return self.decoder(z)\n",
    "        is_video_decoder = self.decoder.cfg.is_video if self.decoder.cfg.is_video is not None else False\n",
    "        x_ret = []\n",
    "        for i in range(z.shape[0]):\n",
    "            x_ret.append(self.decode_single(z[i : i + 1], is_video_decoder))\n",
    "        return torch.cat(x_ret, dim=0)\n",
    "\n",
    "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        if self.use_temporal_tiling and z.shape[2] > self.temporal_tile_latent_size:\n",
    "            return self.temporal_tiled_decode(z)\n",
    "        elif self.use_spatial_tiling and (\n",
    "            z.shape[-1] > self.spatial_tile_latent_size or z.shape[-2] > self.spatial_tile_latent_size\n",
    "        ):\n",
    "            return self.spatial_tiled_decode(z)\n",
    "        else:\n",
    "            return self._decode(z)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[Any, Tensor, dict[Any, Any]]:\n",
    "        x_type = x.dtype\n",
    "        is_image_model = self.cfg.__dict__.get(\"is_image_model\", False)\n",
    "        x = x.to(self.encoder.project_in.conv.weight.dtype)\n",
    "\n",
    "        if is_image_model:\n",
    "            b, c, _, h, w = x.shape\n",
    "            x = x.permute(0, 2, 1, 3, 4).reshape(-1, c, h, w)\n",
    "\n",
    "        z = self.encode(x)\n",
    "        dec = self.decode(z)\n",
    "\n",
    "        if is_image_model:\n",
    "            dec = dec.reshape(b, 1, c, h, w).permute(0, 2, 1, 3, 4)\n",
    "            z = z.unsqueeze(dim=0).permute(0, 2, 1, 3, 4)\n",
    "\n",
    "        dec = dec.to(x_type)\n",
    "        return dec, None, z\n",
    "\n",
    "    def get_latent_size(self, input_size: list[int]) -> list[int]:\n",
    "        latent_size = []\n",
    "        # T\n",
    "        latent_size.append((input_size[0] - 1) // self.time_compression_ratio + 1)\n",
    "        # H, w\n",
    "        for i in range(1, 3):\n",
    "            latent_size.append((input_size[i] - 1) // self.spatial_compression_ratio + 1)\n",
    "        return latent_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63711488",
   "metadata": {},
   "source": [
    "### HuggingFace HubÂØæÂøúDC-AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08c3d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dc_ae_f32(name: str, pretrained_path: str) -> DCAEConfig:\n",
    "    if name in [\"dc-ae-f32t4c128\"]:\n",
    "        cfg_str = (\n",
    "            \"time_compression_ratio=4 \"\n",
    "            \"spatial_compression_ratio=32 \"\n",
    "            \"encoder.block_type=[ResBlock,ResBlock,ResBlock,EViTS5_GLU,EViTS5_GLU,EViTS5_GLU] \"\n",
    "            \"encoder.width_list=[128,256,512,512,1024,1024] encoder.depth_list=[2,2,2,3,3,3] \"\n",
    "            \"encoder.downsample_block_type=Conv \"\n",
    "            \"encoder.norm=rms3d \"\n",
    "            \"encoder.is_video=True \"\n",
    "            \"decoder.block_type=[ResBlock,ResBlock,ResBlock,EViTS5_GLU,EViTS5_GLU,EViTS5_GLU] \"\n",
    "            \"decoder.width_list=[128,256,512,512,1024,1024] decoder.depth_list=[3,3,3,3,3,3] \"\n",
    "            \"decoder.upsample_block_type=InterpolateConv \"\n",
    "            \"decoder.norm=rms3d decoder.act=silu decoder.out_norm=rms3d \"\n",
    "            \"decoder.is_video=True \"\n",
    "            \"encoder.temporal_downsample=[False,False,False,True,True,False] \"\n",
    "            \"decoder.temporal_upsample=[False,False,False,True,True,False] \"\n",
    "            \"latent_channels=128\"\n",
    "        )  # make sure there is no trailing blankspace in the last line\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    cfg = OmegaConf.from_dotlist(cfg_str.split(\" \"))\n",
    "    cfg: DCAEConfig = OmegaConf.to_object(OmegaConf.merge(OmegaConf.structured(DCAEConfig), cfg))\n",
    "    cfg.pretrained_path = pretrained_path\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847c9fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGISTERED_DCAE_MODEL: dict[str, tuple[Callable, Optional[str]]] = {\n",
    "    \"dc-ae-f32t4c128\": (dc_ae_f32, None),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267e5839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dc_ae_model_cfg(name: str, pretrained_path: Optional[str] = None) -> DCAEConfig:\n",
    "    \"\"\"\n",
    "    ÁôªÈå≤„Åï„Çå„ÅüDCAE„É¢„Éá„É´Ë®≠ÂÆö„Çí‰ΩúÊàê„Åô„Çã\n",
    "\n",
    "    Args:\n",
    "        name (str): ÁôªÈå≤„Åï„Çå„ÅüDCAE„É¢„Éá„É´„ÅÆÂêçÂâç\n",
    "        pretrained_path (Optional[str], optional): ‰∫ãÂâçÂ≠¶ÁøíÊ∏à„Åø„É¢„Éá„É´„ÅÆ„Éë„Çπ\n",
    "    Returns:\n",
    "        DCAEConfig: DCAE„É¢„Éá„É´„ÅÆË®≠ÂÆö\n",
    "    \"\"\"\n",
    "    assert name in REGISTERED_DCAE_MODEL, f\"{name} is not supported\"\n",
    "    dc_ae_cls, default_pt_path = REGISTERED_DCAE_MODEL[name]\n",
    "    pretrained_path = default_pt_path if pretrained_path is None else pretrained_path\n",
    "    model_cfg = dc_ae_cls(name, pretrained_path)\n",
    "    return model_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8d3234",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCAE_HF(DCAE, PyTorchModelHubMixin):\n",
    "    \"\"\"\n",
    "    HuggingFace HubÂØæÂøúDCAE„É¢„Éá„É´\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str):\n",
    "        logger.info(f\"DCAE_HF.__init__ {model_name=}\")\n",
    "        cfg = create_dc_ae_model_cfg(model_name)\n",
    "        DCAE.__init__(self, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8679561a",
   "metadata": {},
   "source": [
    "### Ê§úË®º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85f81ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @MODELS.register_module(\"dc_ae\")\n",
    "def DC_AE(\n",
    "    model_name: str,\n",
    "    device_map: str | torch.device = \"cuda\",\n",
    "    torch_dtype: torch.dtype = torch.bfloat16,\n",
    "    from_scratch: bool = False,\n",
    "    from_pretrained: str | None = None,\n",
    "    is_training: bool = False,\n",
    "    use_spatial_tiling: bool = False,\n",
    "    use_temporal_tiling: bool = False,\n",
    "    spatial_tile_size: int = 256,\n",
    "    temporal_tile_size: int = 32,\n",
    "    tile_overlap_factor: float = 0.25,\n",
    "    scaling_factor: float = None,\n",
    "    disc_off_grad_ckpt: bool = False,\n",
    ") -> DCAE_HF:\n",
    "    \"\"\"\n",
    "    Deep Compressive Autoencoder (DCAE)„É¢„Éá„É´„Çí‰ΩúÊàê„Åô„Çã\n",
    "\n",
    "    Args:\n",
    "        model_name (str): „É¢„Éá„É´„ÅÆÂêçÂâç\n",
    "        device_map (str or torch.device, optional): „É¢„Éá„É´„ÇíÈÖçÁΩÆ„Åô„Çã„Éá„Éê„Ç§„Çπ\n",
    "        torch_dtype (torch.dtype, optional): „É¢„Éá„É´„ÅÆ„Éá„Éº„ÇøÂûã\n",
    "        from_scratch (bool, optional): „É©„É≥„ÉÄ„É†ÂàùÊúüÂåñ„Åã„Çâ„É¢„Éá„É´„Çí‰ΩúÊàê„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "        from_pretrained (str or None, optional): ‰∫ãÂâçÂ≠¶ÁøíÊ∏à„Åø„É¢„Éá„É´„ÅÆ„Éë„Çπ\n",
    "        is_training (bool, optional): „É¢„Éá„É´„Çí„Éà„É¨„Éº„Éã„É≥„Ç∞„É¢„Éº„Éâ„Å´Ë®≠ÂÆö„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "        use_spatial_tiling (bool, optional): Á©∫Èñì„Çø„Ç§„É´Âá¶ÁêÜ„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "        use_temporal_tiling (bool, optional): ÊôÇÈñì„Çø„Ç§„É´Âá¶ÁêÜ„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "        spatial_tile_size (int, optional): Á©∫Èñì„Çø„Ç§„É´„ÅÆ„Çµ„Ç§„Ç∫\n",
    "        temporal_tile_size (int, optional): ÊôÇÈñì„Çø„Ç§„É´„ÅÆ„Çµ„Ç§„Ç∫\n",
    "        tile_overlap_factor (float, optional): „Çø„Ç§„É´„ÅÆÈáç„Å™„Çä‰øÇÊï∞\n",
    "        scaling_factor (float, optional): „Çπ„Ç±„Éº„É™„É≥„Ç∞„Éï„Ç°„ÇØ„Çø„Éº\n",
    "        disc_off_grad_ckpt (bool, optional): ÂãæÈÖç„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà„ÇíÁÑ°Âäπ„Å´„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "    Returns:\n",
    "        DCAE_HF: DCAE„É¢„Éá„É´\n",
    "    \"\"\"\n",
    "    logger.info(f\"DC_AE {model_name=} {device_map=} {torch_dtype=} {from_scratch=} {from_pretrained=} {is_training=} {use_spatial_tiling=} {use_temporal_tiling=} {spatial_tile_size=} {temporal_tile_size=} {tile_overlap_factor=} {scaling_factor=} {disc_off_grad_ckpt=}\")\n",
    "\n",
    "    if not from_scratch:\n",
    "        model = DCAE_HF.from_pretrained(model_name).to(device_map, torch_dtype)\n",
    "    else:\n",
    "        model = DCAE_HF(model_name).to(device_map, torch_dtype)\n",
    "\n",
    "    if from_pretrained is not None:\n",
    "        model = load_checkpoint(model, from_pretrained, device_map=device_map)\n",
    "        print(f\"loaded dc_ae from ckpt path: {from_pretrained}\")\n",
    "\n",
    "    model.cfg.is_training = is_training\n",
    "    model.use_spatial_tiling = use_spatial_tiling\n",
    "    model.use_temporal_tiling = use_temporal_tiling\n",
    "    model.spatial_tile_size = spatial_tile_size\n",
    "    model.temporal_tile_size = temporal_tile_size\n",
    "    model.tile_overlap_factor = tile_overlap_factor\n",
    "    if scaling_factor is not None:\n",
    "        model.scaling_factor = scaling_factor\n",
    "    model.decoder.disc_off_grad_ckpt = disc_off_grad_ckpt\n",
    "    return model\n",
    "\n",
    "# {'type': 'dc_ae',\n",
    "#  'model_name': 'dc-ae-f32t4c128',\n",
    "#  'from_scratch': True,\n",
    "#  'from_pretrained': None}\n",
    "\n",
    "if False:\n",
    "    DC_AE(\n",
    "        model_name=\"dc-ae-f32t4c128\",\n",
    "        from_scratch=True,\n",
    "        from_pretrained=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fabd6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True: \n",
    "    # (batch_size, channels, frames, height, width)\n",
    "    # (1, 3, 96, 512, 512)\n",
    "    sample_input = torch.randn(1, 3, 2, 512, 512).to(\"cuda\")\n",
    "\n",
    "    model = DC_AE(\n",
    "        model_name=\"dc-ae-f32t4c128\",\n",
    "        from_scratch=True,\n",
    "        from_pretrained=None,\n",
    "        is_training=False,\n",
    "        # use_spatial_tiling=True,\n",
    "        # use_temporal_tiling=True,\n",
    "        # spatial_tile_size=256,\n",
    "        # temporal_tile_size=16,\n",
    "        # tile_overlap_factor=0.25,\n",
    "    )\n",
    "    output = model(sample_input)\n",
    "    print(output[0].shape)  # Decoded output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471d61ec",
   "metadata": {},
   "source": [
    "### Êé®Ë´ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368fd739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/hpcai-tech/Open-Sora-v2-Video-DC-AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412f49a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '12345'\n",
    "os.environ['LOCAL_RANK'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f185397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pformat\n",
    "\n",
    "import colossalai\n",
    "import torch\n",
    "from colossalai.utils import get_current_device, set_seed\n",
    "from tqdm import tqdm\n",
    "\n",
    "from opensora.acceleration.parallel_states import get_data_parallel_group\n",
    "from opensora.datasets import save_sample\n",
    "from opensora.datasets.dataloader import prepare_dataloader\n",
    "from opensora.registry import DATASETS, MODELS, build_module\n",
    "from opensora.utils.logger import create_logger, is_distributed, is_main_process\n",
    "from opensora.utils.misc import log_cuda_max_memory, log_model_params, to_torch_dtype\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def main(args):\n",
    "    torch.set_grad_enabled(False)\n",
    "    # ======================================================\n",
    "    # configs & runtime variables\n",
    "    # ======================================================\n",
    "    # == parse configs ==\n",
    "    cfg = parse_configs(args)\n",
    "\n",
    "    # == get dtype & device ==\n",
    "    dtype = to_torch_dtype(cfg.get(\"dtype\", \"fp32\"))\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if is_distributed():\n",
    "        colossalai.launch_from_torch({})\n",
    "        device = get_current_device()\n",
    "    set_seed(cfg.get(\"seed\", 1024))\n",
    "\n",
    "    # == init logger ==\n",
    "    # logger = create_logger()\n",
    "    # logger.info(\"Inference configuration:\\n %s\", pformat(cfg.to_dict()))\n",
    "    verbose = cfg.get(\"verbose\", 1)\n",
    "\n",
    "    # ======================================================\n",
    "    # build model & loss\n",
    "    # ======================================================\n",
    "    if cfg.get(\"ckpt_path\", None) is not None:\n",
    "        cfg.model.from_pretrained = cfg.ckpt_path\n",
    "    logger.info(\"Building models...\")\n",
    "    model = build_module(cfg.model, MODELS, device_map=device, torch_dtype=dtype).eval()\n",
    "    log_model_params(model)\n",
    "\n",
    "    # ======================================================\n",
    "    # build dataset and dataloader\n",
    "    # ======================================================\n",
    "    logger.info(\"Building dataset...\")\n",
    "    # == build dataset ==\n",
    "    dataset = build_module(cfg.dataset, DATASETS)\n",
    "    logger.info(\"Dataset contains %s samples.\", len(dataset))\n",
    "    # == build dataloader ==\n",
    "    dataloader_args = dict(\n",
    "        dataset=dataset,\n",
    "        batch_size=cfg.get(\"batch_size\", None),\n",
    "        num_workers=cfg.get(\"num_workers\", 4),\n",
    "        seed=cfg.get(\"seed\", 1024),\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "        process_group=get_data_parallel_group(),\n",
    "        prefetch_factor=cfg.get(\"prefetch_factor\", None),\n",
    "    )\n",
    "\n",
    "    if cfg.get(\"eval_setting\", None) is not None:\n",
    "        # e.g. 32x256, 1x1024\n",
    "        num_frames = int(cfg.eval_setting.split(\"x\")[0])\n",
    "        resolution = str(cfg.eval_setting.split(\"x\")[-1])\n",
    "        bucket_config = {\n",
    "            resolution + \"px\" + \"_ar1:1\": {num_frames: (1.0, 1)},\n",
    "        }\n",
    "        print(\"eval setting:\\n\", bucket_config)\n",
    "    else:\n",
    "        bucket_config = cfg.get(\"bucket_config\", None)\n",
    "\n",
    "    dataloader, sampler = prepare_dataloader(\n",
    "        bucket_config=bucket_config,\n",
    "        num_bucket_build_workers=cfg.get(\"num_bucket_build_workers\", 1),\n",
    "        **dataloader_args,\n",
    "    )\n",
    "    dataiter = iter(dataloader)\n",
    "    num_steps_per_epoch = len(dataloader)\n",
    "\n",
    "    # ======================================================\n",
    "    # inference\n",
    "    # ======================================================\n",
    "    # prepare arguments\n",
    "    save_fps = cfg.get(\"fps\", 16) // cfg.get(\"frame_interval\", 1)\n",
    "    save_dir = cfg.get(\"save_dir\", None)\n",
    "    save_dir_orig = os.path.join(save_dir, \"orig\")\n",
    "    save_dir_recn = os.path.join(save_dir, \"recn\")\n",
    "    os.makedirs(save_dir_orig, exist_ok=True)\n",
    "    os.makedirs(save_dir_recn, exist_ok=True)\n",
    "\n",
    "    running_sum = running_var = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    # Iter over the dataset\n",
    "    with tqdm(\n",
    "        enumerate(dataiter),\n",
    "        disable=not is_main_process() or verbose < 1,\n",
    "        total=num_steps_per_epoch,\n",
    "        initial=0,\n",
    "    ) as pbar:\n",
    "        for _, batch in pbar:\n",
    "            # == load data ==\n",
    "            x = batch[\"video\"].to(device, dtype)  # [B, C, T, H, W]\n",
    "            path = batch[\"path\"]\n",
    "\n",
    "            # == vae encoding & decoding ===\n",
    "            x_rec, posterior, z = model(x)\n",
    "\n",
    "            num_samples += 1\n",
    "            running_sum += z.mean()\n",
    "            running_var += (z - running_sum / num_samples).pow(2).mean()\n",
    "            if num_samples % 10 == 0:\n",
    "                logger.info(\n",
    "                    \"VAE feature per channel stats: mean %s, var %s\",\n",
    "                    (running_sum / num_samples).item(),\n",
    "                    (running_var / num_samples).sqrt().item(),\n",
    "                )\n",
    "\n",
    "            # == save samples ==\n",
    "            if is_main_process() and save_dir is not None:\n",
    "                for idx, x_orig in enumerate(x):\n",
    "                    fname = os.path.splitext(os.path.basename(path[idx]))[0]\n",
    "                    save_path_orig = os.path.join(save_dir_orig, f\"{fname}_orig\")\n",
    "                    save_sample(x_orig, save_path=save_path_orig, fps=save_fps)\n",
    "\n",
    "                    save_path_rec = os.path.join(save_dir_recn, f\"{fname}_recn\")\n",
    "                    save_sample(x_rec[idx], save_path=save_path_rec, fps=save_fps)\n",
    "\n",
    "    logger.info(\"Inference finished.\")\n",
    "    log_cuda_max_memory(\"inference\")\n",
    "\n",
    "# main([\n",
    "#     \"configs/vae/inference/video_dc_ae.py\",\n",
    "#     \"--save-dir\",\n",
    "#     \"samples/dcae\",\n",
    "#     \"--dataset.data_path\",\n",
    "#     \"../pexels_45k/pexels_45k_necessary.csv\",\n",
    "#     \"--num_workers\",\n",
    "#     \"0\",\n",
    "#     \"--prefetch_factor\",\n",
    "#     \"None\",\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41770ec",
   "metadata": {},
   "source": [
    "## MMDiT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08bcb7f",
   "metadata": {},
   "source": [
    "### LigerEmbedND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49838b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def liger_rope(pos: Tensor, dim: int, theta: int) -> Tuple:\n",
    "    \"\"\"\n",
    "    Liger RoPE\n",
    "    LigerEmbedND„Åß‰ΩøÁî®\n",
    "\n",
    "    Args:\n",
    "        pos (Tensor): ‰ΩçÁΩÆ„Ç®„É≥„Ç≥„Éº„Éá„Ç£„É≥„Ç∞„ÅÆ‰ΩçÁΩÆ„ÉÜ„É≥„ÇΩ„É´ (..., n)\n",
    "        dim (int): Âüã„ÇÅËæº„ÅøÊ¨°ÂÖÉÊï∞\n",
    "        theta (int): RoPE„ÅÆ„Çπ„Ç±„Éº„É™„É≥„Ç∞„Éë„É©„É°„Éº„Çø\n",
    "    Returns:\n",
    "        Tuple: „Ç≥„Çµ„Ç§„É≥„Å®„Çµ„Ç§„É≥„ÅÆ„ÉÜ„É≥„ÇΩ„É´ (..., n, dim//2)\n",
    "    \"\"\"\n",
    "    logger.info(f\"liger_rope {pos.shape=} {dim=} {theta=}\")\n",
    "\n",
    "    assert dim % 2 == 0\n",
    "    scale = torch.arange(0, dim, 2, dtype=torch.float32, device=pos.device) / dim\n",
    "    omega = 1.0 / (theta**scale)\n",
    "    out = torch.einsum(\"...n,d->...nd\", pos, omega)  # (b, seq, dim//2)\n",
    "    cos = out.cos()\n",
    "    sin = out.sin()\n",
    "\n",
    "    return (cos, sin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36150305",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LigerEmbedND(nn.Module):\n",
    "    \"\"\"\n",
    "    Liger Multi-dimensional RoPE Embedding\n",
    "    MMDiTModel„Åß‰ΩøÁî®\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, theta: int, axes_dim: list[int]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): Âüã„ÇÅËæº„ÅøÊ¨°ÂÖÉÊï∞\n",
    "            theta (int): RoPE„ÅÆ„Çπ„Ç±„Éº„É™„É≥„Ç∞„Éë„É©„É°„Éº„Çø\n",
    "            axes_dim (list[int]): ÂêÑËª∏„ÅÆÊ¨°ÂÖÉÊï∞\n",
    "        \"\"\"\n",
    "        logger.info(f\"LigerEmbedND.__init__ {dim=} {theta=} {axes_dim=}\")\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.theta = theta\n",
    "        self.axes_dim = axes_dim\n",
    "\n",
    "    def forward(self, ids: Tensor) -> Tensor:\n",
    "        logger.info(f\"LigerEmbedND.forward {ids.shape=}\")\n",
    "        n_axes = ids.shape[-1]\n",
    "        cos_list = []\n",
    "        sin_list = []\n",
    "        for i in range(n_axes):\n",
    "            cos, sin = liger_rope(ids[..., i], self.axes_dim[i], self.theta)\n",
    "            cos_list.append(cos)\n",
    "            sin_list.append(sin)\n",
    "        cos_emb = torch.cat(cos_list, dim=-1).repeat(1, 1, 2).contiguous()\n",
    "        sin_emb = torch.cat(sin_list, dim=-1).repeat(1, 1, 2).contiguous()\n",
    "\n",
    "        return (cos_emb, sin_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e418d284",
   "metadata": {},
   "source": [
    "### MLPEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bc8a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPEmbedder(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP Embedder\n",
    "    MMDiTModel„Åß‰ΩøÁî®\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int, hidden_dim: int):\n",
    "        logger.info(f\"MLPEmbedder.__init__ {in_dim=} {hidden_dim=}\")\n",
    "        super().__init__()\n",
    "        self.in_layer = nn.Linear(in_dim, hidden_dim, bias=True)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.out_layer = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        logger.info(f\"MLPEmbedder.forward {x.shape=}\")\n",
    "        return self.out_layer(self.silu(self.in_layer(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68deecd",
   "metadata": {},
   "source": [
    "### QKNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf50457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    RMS Normalization\n",
    "    FusedRMSNorm„ÅßÁ∂ôÊâø\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): Âüã„ÇÅËæº„ÅøÊ¨°ÂÖÉÊï∞\n",
    "        \"\"\"\n",
    "        logger.info(f\"RMSNorm.__init__ {dim=}\")\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        logger.info(f\"RMSNorm.forward {x.shape=}\")\n",
    "        x_dtype = x.dtype\n",
    "        x = x.float()\n",
    "        rrms = torch.rsqrt(torch.mean(x**2, dim=-1, keepdim=True) + 1e-6)\n",
    "        return (x * rrms).to(dtype=x_dtype) * self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3175d11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusedRMSNorm(RMSNorm):\n",
    "    \"\"\"\n",
    "    Fused RMS Normalization\n",
    "    QKNorm„Åß‰ΩøÁî®\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        logger.info(f\"FusedRMSNorm.forward {x.shape=}\")\n",
    "        return LigerRMSNormFunction.apply(\n",
    "            x,\n",
    "            self.scale,\n",
    "            1e-6,\n",
    "            0.0,\n",
    "            \"llama\",\n",
    "            False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8675753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QKNorm(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Query-Key Normalization\n",
    "    Self-Attention„Åß‰ΩøÁî®\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): Âüã„ÇÅËæº„ÅøÊ¨°ÂÖÉÊï∞\n",
    "        \"\"\"\n",
    "        logger.info(f\"QKNorm.__init__ {dim=}\")\n",
    "        super().__init__()\n",
    "        self.query_norm = FusedRMSNorm(dim)\n",
    "        self.key_norm = FusedRMSNorm(dim)\n",
    "\n",
    "    def forward(self, q: Tensor, k: Tensor, v: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            q (Tensor): „ÇØ„Ç®„É™„ÉÜ„É≥„ÇΩ„É´ (..., dim)\n",
    "            k (Tensor): „Ç≠„Éº„ÉÜ„É≥„ÇΩ„É´ (..., dim)\n",
    "            v (Tensor): „Éê„É™„É•„Éº„ÉÜ„É≥„ÇΩ„É´ (..., dim)\n",
    "        Returns:\n",
    "            tuple[Tensor, Tensor]: Ê≠£Ë¶èÂåñ„Åï„Çå„Åü„ÇØ„Ç®„É™„Å®„Ç≠„Éº„ÅÆ„ÉÜ„É≥„ÇΩ„É´\n",
    "        \"\"\"\n",
    "        logger.info(f\"QKNorm.forward {q.shape=} {k.shape=} {v.shape=}\")\n",
    "        q = self.query_norm(q)\n",
    "        k = self.key_norm(k)\n",
    "        return q.to(v), k.to(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ad6464",
   "metadata": {},
   "source": [
    "### „Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥Ê©üÊßã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6511eb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flash_attn_func(q: Tensor, k: Tensor, v: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Flash Attention„Çí‰ΩøÁî®„Åó„Åü„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥Ë®àÁÆó\n",
    "    Args:\n",
    "        q (Tensor): „ÇØ„Ç®„É™„ÉÜ„É≥„ÇΩ„É´ (B, H, L, D)\n",
    "        k (Tensor): „Ç≠„Éº„ÉÜ„É≥„ÇΩ„É´ (B, H, L, D)\n",
    "        v (Tensor): „Éê„É™„É•„Éº„ÉÜ„É≥„ÇΩ„É´ (B, H, L, D)\n",
    "    Returns:\n",
    "        Tensor: „Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥Âá∫Âäõ„ÉÜ„É≥„ÇΩ„É´ (B, L, H, D)\n",
    "    \"\"\"\n",
    "    logger.info(f\"flash_attn_func {q.shape=} {k.shape=} {v.shape=}\")\n",
    "\n",
    "    if SUPPORT_FA3:\n",
    "        return flash_attn_func_v3(q, k, v)[0]\n",
    "    return flash_attn_func_v2(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8654fb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(q: Tensor, k: Tensor, v: Tensor, pe) -> Tensor:\n",
    "    \"\"\"\n",
    "    „Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥Ë®àÁÆó\n",
    "    SingleStreamBlockProcessor, DoubleSreamBlockProcessor,\n",
    "    SelfAttention„Åß‰ΩøÁî®\n",
    "    Args:\n",
    "        q (Tensor): „ÇØ„Ç®„É™„ÉÜ„É≥„ÇΩ„É´ (B, L, H, D)\n",
    "        k (Tensor): „Ç≠„Éº„ÉÜ„É≥„ÇΩ„É´ (B, L, H, D)\n",
    "        v (Tensor): „Éê„É™„É•„Éº„ÉÜ„É≥„ÇΩ„É´ (B, L, H, D)\n",
    "        pe: ‰ΩçÁΩÆ„Ç®„É≥„Ç≥„Éº„Éá„Ç£„É≥„Ç∞„ÉÜ„É≥„ÇΩ„É´„Åæ„Åü„ÅØ„Ç≥„Çµ„Ç§„É≥„Éª„Çµ„Ç§„É≥„ÉÜ„É≥„ÇΩ„É´„ÅÆ„Çø„Éó„É´\n",
    "    Returns:\n",
    "        Tensor: „Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥Âá∫Âäõ„ÉÜ„É≥„ÇΩ„É´ (B, L, H*D)\n",
    "    \"\"\"\n",
    "    if isinstance(pe, torch.Tensor):\n",
    "        q, k = apply_rope(q, k, pe)\n",
    "    else:\n",
    "        cos, sin = pe\n",
    "        q, k = LigerRopeFunction.apply(q, k, cos, sin)\n",
    "        # to compare with the original implementation\n",
    "        # k = reverse_rearrange_tensor(k)\n",
    "    q = rearrange(q, \"B H L D -> B L H D\")\n",
    "    k = rearrange(k, \"B H L D -> B L H D\")\n",
    "    v = rearrange(v, \"B H L D -> B L H D\")\n",
    "    x = flash_attn_func(q, k, v)\n",
    "    x = rearrange(x, \"B L H D -> B L (H D)\")\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3045aa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    „Çª„É´„Éï„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥\n",
    "    DoubleStreamBlock„Åß‰ΩøÁî®\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, num_heads: int = 8, qkv_bias: bool = False, fused_qkv: bool = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): Âüã„ÇÅËæº„ÅøÊ¨°ÂÖÉÊï∞\n",
    "            num_heads (int, optional): „Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥„Éò„ÉÉ„ÉâÊï∞. Defaults to 8.\n",
    "            qkv_bias (bool, optional): QKV„ÅÆ„Éê„Ç§„Ç¢„Çπ„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã. Defaults to False.\n",
    "            fused_qkv (bool, optional): QKV„ÇíËûçÂêà„Åó„ÅüÁ∑öÂΩ¢Â±§„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã. Defaults to True.\n",
    "        \"\"\"\n",
    "        logger.info(f\"SelfAttention.__init__ {dim=} {num_heads=} {qkv_bias=} {fused_qkv=}\")\n",
    "\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.fused_qkv = fused_qkv\n",
    "        head_dim = dim // num_heads\n",
    "\n",
    "        if fused_qkv:\n",
    "            self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        else:\n",
    "            self.q_proj = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "            self.k_proj = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "            self.v_proj = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.norm = QKNorm(head_dim)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x: Tensor, pe: Tensor) -> Tensor:\n",
    "        logger.info(f\"SelfAttention.forward {x.shape=} {pe.shape=}\")\n",
    "        if self.fused_qkv:\n",
    "            qkv = self.qkv(x)\n",
    "            q, k, v = rearrange(qkv, \"B L (K H D) -> K B H L D\", K=3, H=self.num_heads)\n",
    "        else:\n",
    "            q = rearrange(self.q_proj(x), \"B L (H D) -> B L H D\", H=self.num_heads)\n",
    "            k = rearrange(self.k_proj(x), \"B L (H D) -> B L H D\", H=self.num_heads)\n",
    "            v = rearrange(self.v_proj(x), \"B L (H D) -> B L H D\", H=self.num_heads)\n",
    "        q, k = self.norm(q, k, v)\n",
    "        if not self.fused_qkv:\n",
    "            q = rearrange(q, \"B L H D -> B H L D\")\n",
    "            k = rearrange(k, \"B L H D -> B H L D\")\n",
    "            v = rearrange(v, \"B L H D -> B H L D\")\n",
    "        x = attention(q, k, v, pe=pe)\n",
    "        x = self.proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e9da89",
   "metadata": {},
   "source": [
    "### Modulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee5fbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModulationOut:\n",
    "    shift: Tensor\n",
    "    scale: Tensor\n",
    "    gate: Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c045c4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modulation(nn.Module):\n",
    "    \"\"\"\n",
    "    „É¢„Ç∏„É•„É¨„Éº„Ç∑„Éß„É≥Â±§\n",
    "    DoubleStreamBlock„Åß‰ΩøÁî®\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, double: bool):\n",
    "        logger.info(f\"Modulation.__init__ {dim=} {double=}\")\n",
    "        super().__init__()\n",
    "        self.is_double = double\n",
    "        self.multiplier = 6 if double else 3\n",
    "        self.lin = nn.Linear(dim, self.multiplier * dim, bias=True)\n",
    "\n",
    "    def forward(self, vec: Tensor) -> tuple[ModulationOut, ModulationOut | None]:\n",
    "        logger.info(f\"Modulation.forward {vec.shape=}\")\n",
    "        out = self.lin(nn.functional.silu(vec))[:, None, :].chunk(self.multiplier, dim=-1)\n",
    "\n",
    "        return (\n",
    "            ModulationOut(*out[:3]),\n",
    "            ModulationOut(*out[3:]) if self.is_double else None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd63fafd",
   "metadata": {},
   "source": [
    "### DoubleStreamBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3eafe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleStreamBlockProcessor:\n",
    "    \"\"\"\n",
    "    „ÉÄ„Éñ„É´„Çπ„Éà„É™„Éº„É†„Éñ„É≠„ÉÉ„ÇØ„Éó„É≠„Çª„ÉÉ„Çµ\n",
    "    DoubleStreamBlock„Åß‰ΩøÁî®\n",
    "    ÁîªÂÉè„Å®„ÉÜ„Ç≠„Çπ„Éà„ÅÆ‰∏°Êñπ„ÅÆ„Çπ„Éà„É™„Éº„É†„ÇíÂá¶ÁêÜ„Åô„Çã\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, attn: nn.Module, img: Tensor, txt: Tensor, vec: Tensor, pe: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        logger.info(f\"DoubleStreamBlockProcessor.__call__ {img.shape=} {txt.shape=} {vec.shape=} {pe[0].shape=} {pe[1].shape=}\")\n",
    "\n",
    "        # attn is the DoubleStreamBlock;\n",
    "        # process img and txt separately while both is influenced by text vec\n",
    "\n",
    "        # vec will interact with image latent and text context\n",
    "        img_mod1, img_mod2 = attn.img_mod(vec)  # get shift, scale, gate for each mod\n",
    "        txt_mod1, txt_mod2 = attn.txt_mod(vec)\n",
    "\n",
    "        # prepare image for attention\n",
    "        img_modulated = attn.img_norm1(img)\n",
    "        img_modulated = (1 + img_mod1.scale) * img_modulated + img_mod1.shift\n",
    "\n",
    "        if attn.img_attn.fused_qkv:\n",
    "            img_qkv = attn.img_attn.qkv(img_modulated)\n",
    "            img_q, img_k, img_v = rearrange(img_qkv, \"B L (K H D) -> K B H L D\", K=3, H=attn.num_heads, D=attn.head_dim)\n",
    "        else:\n",
    "            img_q = rearrange(attn.img_attn.q_proj(img_modulated), \"B L (H D) -> B L H D\", H=attn.num_heads)\n",
    "            img_k = rearrange(attn.img_attn.k_proj(img_modulated), \"B L (H D) -> B L H D\", H=attn.num_heads)\n",
    "            img_v = rearrange(attn.img_attn.v_proj(img_modulated), \"B L (H D) -> B L H D\", H=attn.num_heads)\n",
    "\n",
    "        img_q, img_k = attn.img_attn.norm(img_q, img_k, img_v)  # RMSNorm for QK Norm as in SD3 paper\n",
    "        if not attn.img_attn.fused_qkv:\n",
    "            img_q = rearrange(img_q, \"B L H D -> B H L D\")\n",
    "            img_k = rearrange(img_k, \"B L H D -> B H L D\")\n",
    "            img_v = rearrange(img_v, \"B L H D -> B H L D\")\n",
    "\n",
    "        # prepare txt for attention\n",
    "        txt_modulated = attn.txt_norm1(txt)\n",
    "        txt_modulated = (1 + txt_mod1.scale) * txt_modulated + txt_mod1.shift\n",
    "        if attn.txt_attn.fused_qkv:\n",
    "            txt_qkv = attn.txt_attn.qkv(txt_modulated)\n",
    "            txt_q, txt_k, txt_v = rearrange(txt_qkv, \"B L (K H D) -> K B H L D\", K=3, H=attn.num_heads, D=attn.head_dim)\n",
    "        else:\n",
    "            txt_q = rearrange(attn.txt_attn.q_proj(txt_modulated), \"B L (H D) -> B L H D\", H=attn.num_heads)\n",
    "            txt_k = rearrange(attn.txt_attn.k_proj(txt_modulated), \"B L (H D) -> B L H D\", H=attn.num_heads)\n",
    "            txt_v = rearrange(attn.txt_attn.v_proj(txt_modulated), \"B L (H D) -> B L H D\", H=attn.num_heads)\n",
    "        txt_q, txt_k = attn.txt_attn.norm(txt_q, txt_k, txt_v)\n",
    "        if not attn.txt_attn.fused_qkv:\n",
    "            txt_q = rearrange(txt_q, \"B L H D -> B H L D\")\n",
    "            txt_k = rearrange(txt_k, \"B L H D -> B H L D\")\n",
    "            txt_v = rearrange(txt_v, \"B L H D -> B H L D\")\n",
    "\n",
    "        # run actual attention, image and text attention are calculated together by concat different attn heads\n",
    "        q = torch.cat((txt_q, img_q), dim=2)\n",
    "        k = torch.cat((txt_k, img_k), dim=2)\n",
    "        v = torch.cat((txt_v, img_v), dim=2)\n",
    "\n",
    "        attn1 = attention(q, k, v, pe=pe)\n",
    "        txt_attn, img_attn = attn1[:, : txt_q.shape[2]], attn1[:, txt_q.shape[2] :]\n",
    "\n",
    "        # calculate the img bloks\n",
    "        img = img + img_mod1.gate * attn.img_attn.proj(img_attn)\n",
    "        img = img + img_mod2.gate * attn.img_mlp((1 + img_mod2.scale) * attn.img_norm2(img) + img_mod2.shift)\n",
    "\n",
    "        # calculate the txt bloks\n",
    "        txt = txt + txt_mod1.gate * attn.txt_attn.proj(txt_attn)\n",
    "        txt = txt + txt_mod2.gate * attn.txt_mlp((1 + txt_mod2.scale) * attn.txt_norm2(txt) + txt_mod2.shift)\n",
    "        return img, txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82098a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleStreamBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    „ÉÄ„Éñ„É´„Çπ„Éà„É™„Éº„É†„Éñ„É≠„ÉÉ„ÇØ\n",
    "    MMDiTModel„Åß‰ΩøÁî®\n",
    "    ÁîªÂÉè„Å®„ÉÜ„Ç≠„Çπ„Éà„ÅÆ‰∏°Êñπ„ÅÆ„Çπ„Éà„É™„Éº„É†„ÇíÂá¶ÁêÜ„Åô„Çã\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size: int, num_heads: int, mlp_ratio: float, qkv_bias: bool = False, fused_qkv: bool = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_size (int): Âüã„ÇÅËæº„ÅøÊ¨°ÂÖÉÊï∞\n",
    "            num_heads (int): „Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥„Éò„ÉÉ„ÉâÊï∞\n",
    "            mlp_ratio (float): MLP„ÅÆÈö†„ÇåÂ±§„ÅÆÊ¨°ÂÖÉÊï∞„ÅÆÊØîÁéá\n",
    "            qkv_bias (bool, optional): QKV„ÅÆ„Éê„Ç§„Ç¢„Çπ„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "            fused_qkv (bool, optional): QKV„ÇíËûçÂêà„Åó„ÅüÁ∑öÂΩ¢Â±§„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "        \"\"\"\n",
    "        logger.info(f\"DoubleStreamBlock.__init__ {hidden_size=} {num_heads=} {mlp_ratio=} {qkv_bias=} {fused_qkv=}\")\n",
    "\n",
    "        super().__init__()\n",
    "        mlp_hidden_dim = int(hidden_size * mlp_ratio)\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "\n",
    "        # image stream\n",
    "        self.img_mod = Modulation(hidden_size, double=True)\n",
    "        self.img_norm1 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
    "        self.img_attn = SelfAttention(dim=hidden_size, num_heads=num_heads, qkv_bias=qkv_bias, fused_qkv=fused_qkv)\n",
    "\n",
    "        self.img_norm2 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
    "        self.img_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, mlp_hidden_dim, bias=True),\n",
    "            nn.GELU(approximate=\"tanh\"),\n",
    "            nn.Linear(mlp_hidden_dim, hidden_size, bias=True),\n",
    "        )\n",
    "\n",
    "        # text stream\n",
    "        self.txt_mod = Modulation(hidden_size, double=True)\n",
    "        self.txt_norm1 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
    "        self.txt_attn = SelfAttention(dim=hidden_size, num_heads=num_heads, qkv_bias=qkv_bias, fused_qkv=fused_qkv)\n",
    "\n",
    "        self.txt_norm2 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
    "        self.txt_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, mlp_hidden_dim, bias=True),\n",
    "            nn.GELU(approximate=\"tanh\"),\n",
    "            nn.Linear(mlp_hidden_dim, hidden_size, bias=True),\n",
    "        )\n",
    "\n",
    "        # processor\n",
    "        processor = DoubleStreamBlockProcessor()\n",
    "        self.set_processor(processor)\n",
    "\n",
    "    def set_processor(self, processor) -> None:\n",
    "        self.processor = processor\n",
    "\n",
    "    def get_processor(self):\n",
    "        return self.processor\n",
    "\n",
    "    def forward(self, img: Tensor, txt: Tensor, vec: Tensor, pe: Tensor, **kwargs) -> tuple[Tensor, Tensor]:\n",
    "        logger.info(f\"DoubleStreamBlock.forward {img.shape=} {txt.shape=} {vec.shape=} {pe[0].shape=} {pe[1].shape=}\")\n",
    "        return self.processor(self, img, txt, vec, pe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43837fc",
   "metadata": {},
   "source": [
    "### SingleStreamBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc340020",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleStreamBlockProcessor:\n",
    "    \"\"\"\n",
    "    „Ç∑„É≥„Ç∞„É´„Çπ„Éà„É™„Éº„É†„Éñ„É≠„ÉÉ„ÇØ„Éó„É≠„Çª„ÉÉ„Çµ\n",
    "    SingleStreamBlock„Åß‰ΩøÁî®\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, attn: nn.Module, x: Tensor, vec: Tensor, pe: Tensor) -> Tensor:\n",
    "        logger.info(f\"SingleStreamBlockProcessor.__call__ {x.shape=} {vec.shape=} {pe[0].shape=} {pe[1].shape=}\")\n",
    "\n",
    "        mod, _ = attn.modulation(vec)\n",
    "        x_mod = (1 + mod.scale) * attn.pre_norm(x) + mod.shift\n",
    "        if attn.fused_qkv:\n",
    "            qkv, mlp = torch.split(attn.linear1(x_mod), [3 * attn.hidden_size, attn.mlp_hidden_dim], dim=-1)\n",
    "            q, k, v = rearrange(qkv, \"B L (K H D) -> K B H L D\", K=3, H=attn.num_heads)\n",
    "        else:\n",
    "            q = rearrange(attn.q_proj(x_mod), \"B L (H D) -> B L H D\", H=attn.num_heads)\n",
    "            k = rearrange(attn.k_proj(x_mod), \"B L (H D) -> B L H D\", H=attn.num_heads)\n",
    "            v, mlp = torch.split(attn.v_mlp(x_mod), [attn.hidden_size, attn.mlp_hidden_dim], dim=-1)\n",
    "            v = rearrange(v, \"B L (H D) -> B L H D\", H=attn.num_heads)\n",
    "\n",
    "        q, k = attn.norm(q, k, v)\n",
    "        if not attn.fused_qkv:\n",
    "            q = rearrange(q, \"B L H D -> B H L D\")\n",
    "            k = rearrange(k, \"B L H D -> B H L D\")\n",
    "            v = rearrange(v, \"B L H D -> B H L D\")\n",
    "\n",
    "        # compute attention\n",
    "        attn_1 = attention(q, k, v, pe=pe)\n",
    "\n",
    "        # compute activation in mlp stream, cat again and run second linear layer\n",
    "        output = attn.linear2(torch.cat((attn_1, attn.mlp_act(mlp)), 2))\n",
    "        output = x + mod.gate * output\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d3021b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleStreamBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    „Ç∑„É≥„Ç∞„É´„Çπ„Éà„É™„Éº„É†„Éñ„É≠„ÉÉ„ÇØ\n",
    "    MMDiTModel„Åß‰ΩøÁî®\n",
    "\n",
    "    ‰∏¶ÂàóÁ∑öÂΩ¢Â±§„ÇíÊåÅ„Å§DiT„Éñ„É≠„ÉÉ„ÇØ„Åß„ÄÅÂ§âË™ø„Ç§„É≥„Çø„Éº„Éï„Çß„Éº„Çπ„ÅåÈÅ©Âøú\n",
    "    https://arxiv.org/abs/2302.05442\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        num_heads: int,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        qk_scale: float | None = None,\n",
    "        fused_qkv: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_size (int): Âüã„ÇÅËæº„ÅøÊ¨°ÂÖÉÊï∞\n",
    "            num_heads (int): „Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥„Éò„ÉÉ„ÉâÊï∞\n",
    "            mlp_ratio (float, optional): MLP„ÅÆÈö†„ÇåÂ±§„ÅÆÊ¨°ÂÖÉÊï∞„ÅÆÊØîÁéá\n",
    "            qk_scale (float | None, optional): QK„Çπ„Ç±„Éº„É™„É≥„Ç∞„Éï„Ç°„ÇØ„Çø„Éº\n",
    "            fused_qkv (bool, optional): QKV„ÇíËûçÂêà„Åó„ÅüÁ∑öÂΩ¢Â±§„Çí‰ΩøÁî®„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
    "        \"\"\"\n",
    "        logger.info(f\"SingleStreamBlock.__init__ {hidden_size=} {num_heads=} {mlp_ratio=} {qk_scale=} {fused_qkv=}\")\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        self.scale = qk_scale or self.head_dim**-0.5\n",
    "        self.fused_qkv = fused_qkv\n",
    "\n",
    "        self.mlp_hidden_dim = int(hidden_size * mlp_ratio)\n",
    "        if fused_qkv:\n",
    "            # qkv and mlp_in\n",
    "            self.linear1 = nn.Linear(hidden_size, hidden_size * 3 + self.mlp_hidden_dim)\n",
    "        else:\n",
    "            self.q_proj = nn.Linear(hidden_size, hidden_size)\n",
    "            self.k_proj = nn.Linear(hidden_size, hidden_size)\n",
    "            self.v_mlp = nn.Linear(hidden_size, hidden_size + self.mlp_hidden_dim)\n",
    "\n",
    "        # proj and mlp_out\n",
    "        self.linear2 = nn.Linear(hidden_size + self.mlp_hidden_dim, hidden_size)\n",
    "\n",
    "        self.norm = QKNorm(self.head_dim)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.pre_norm = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
    "\n",
    "        self.mlp_act = nn.GELU(approximate=\"tanh\")\n",
    "        self.modulation = Modulation(hidden_size, double=False)\n",
    "\n",
    "        processor = SingleStreamBlockProcessor()\n",
    "        self.set_processor(processor)\n",
    "\n",
    "    def set_processor(self, processor) -> None:\n",
    "        self.processor = processor\n",
    "\n",
    "    def get_processor(self):\n",
    "        return self.processor\n",
    "\n",
    "    def forward(self, x: Tensor, vec: Tensor, pe: Tensor, **kwargs) -> Tensor:\n",
    "        logger.info(f\"SingleStreamBlock.forward {x.shape=} {vec.shape=} {pe[0].shape=} {pe[1].shape=}\")\n",
    "        return self.processor(self, x, vec, pe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1b6d7a",
   "metadata": {},
   "source": [
    "### MMDiT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7a53e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile(mode=\"max-autotune-no-cudagraphs\", dynamic=True)\n",
    "def timestep_embedding(t: Tensor, dim, max_period=10000, time_factor: float = 1000.0):\n",
    "    \"\"\"\n",
    "    „Çµ„Ç§„É≥Ê≥¢„Çø„Ç§„É†„Çπ„ÉÜ„ÉÉ„ÉóÂüã„ÇÅËæº„Åø„Çí‰ΩúÊàê„Åô„Çã\n",
    "\n",
    "    Args:\n",
    "        t (Tensor): „Éê„ÉÉ„ÉÅË¶ÅÁ¥†„Åî„Å®„Å´1„Å§„ÅÆN„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„ÇíÊåÅ„Å§1-D„ÉÜ„É≥„ÇΩ„É´„ÄÇ„Åì„Çå„Çâ„ÅØÂàÜÊï∞ÂÄ§„Åß„ÅÇ„ÇãÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ\n",
    "        dim (int): Âá∫Âäõ„ÅÆÊ¨°ÂÖÉ„ÄÇ\n",
    "        max_period (int, optional): Âüã„ÇÅËæº„Åø„ÅÆÊúÄÂ∞èÂë®Ê≥¢Êï∞„ÇíÂà∂Âæ°„Åó„Åæ„Åô„ÄÇ„Éá„Éï„Ç©„É´„Éà„ÅØ10000„ÄÇ\n",
    "        time_factor (float, optional): ÊôÇÈñì„Çπ„Ç±„Éº„É™„É≥„Ç∞„Éï„Ç°„ÇØ„Çø„Éº„ÄÇ„Éá„Éï„Ç©„É´„Éà„ÅØ1000.0„ÄÇ\n",
    "    Returns:\n",
    "        Tensor: Âüã„ÇÅËæº„Åø„ÉÜ„É≥„ÇΩ„É´„ÅÆÂΩ¢Áä∂„ÅØ(t.shape[0], dim)\n",
    "    \"\"\"\n",
    "    logger.info(f\"timestep_embedding {t.shape=} {dim=} {max_period=} {time_factor=}\")\n",
    "\n",
    "    t = time_factor * t\n",
    "    half = dim // 2\n",
    "    freqs = torch.exp(-math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half).to(t.device)\n",
    "\n",
    "    args = t[:, None].float() * freqs[None]\n",
    "    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "    if dim % 2:\n",
    "        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
    "    if torch.is_floating_point(t):\n",
    "        embedding = embedding.to(t)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74746fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    DCAE„Éá„Ç≥„Éº„ÉÄ„ÅÆÊúÄÂæå„ÅÆÂ±§\n",
    "    ÁîªÂÉè„Éë„ÉÉ„ÉÅ„ÇíÁîüÊàê„Åô„Çã„Åü„ÇÅ„Å´‰ΩøÁî®„Åï„Çå„Çã\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size: int, patch_size: int, out_channels: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_size (int): Èö†„ÇåÂ±§„ÅÆÊ¨°ÂÖÉÊï∞\n",
    "            patch_size (int): „Éë„ÉÉ„ÉÅ„ÅÆ„Çµ„Ç§„Ç∫\n",
    "            out_channels (int): Âá∫Âäõ„ÉÅ„É£„Éç„É´Êï∞\n",
    "        \"\"\"\n",
    "        logger.info(f\"LastLayer.__init__ {hidden_size=} {patch_size=} {out_channels=}\")\n",
    "\n",
    "        super().__init__()\n",
    "        self.norm_final = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
    "        self.linear = nn.Linear(hidden_size, patch_size * patch_size * out_channels, bias=True)\n",
    "        self.adaLN_modulation = nn.Sequential(nn.SiLU(), nn.Linear(hidden_size, 2 * hidden_size, bias=True))\n",
    "\n",
    "    def forward(self, x: Tensor, vec: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): ÂÖ•Âäõ„ÉÜ„É≥„ÇΩ„É´ (B, N, hidden_size)\n",
    "            vec (Tensor): „É¢„Ç∏„É•„É¨„Éº„Ç∑„Éß„É≥„Éô„ÇØ„Éà„É´ (B, hidden_size)\n",
    "        Returns:\n",
    "            Tensor: Âá∫Âäõ„ÉÜ„É≥„ÇΩ„É´ (B, N, patch_size * patch_size * out_channels)\n",
    "        \"\"\"\n",
    "        logger.info(f\"LastLayer.forward {x.shape=} {vec.shape=}\")\n",
    "        shift, scale = self.adaLN_modulation(vec).chunk(2, dim=1)\n",
    "        x = (1 + scale[:, None, :]) * self.norm_final(x) + shift[:, None, :]\n",
    "        x = self.linear(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8179a144",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MMDiTConfig:\n",
    "    model_type = \"MMDiT\"\n",
    "    from_pretrained: str\n",
    "    cache_dir: str\n",
    "    in_channels: int\n",
    "    vec_in_dim: int\n",
    "    context_in_dim: int\n",
    "    hidden_size: int\n",
    "    mlp_ratio: float\n",
    "    num_heads: int\n",
    "    depth: int\n",
    "    depth_single_blocks: int\n",
    "    axes_dim: list[int]\n",
    "    theta: int\n",
    "    qkv_bias: bool\n",
    "    guidance_embed: bool\n",
    "    cond_embed: bool = False\n",
    "    fused_qkv: bool = True\n",
    "    grad_ckpt_settings: tuple[int, int] | None = None\n",
    "    use_liger_rope: bool = False\n",
    "    patch_size: int = 2\n",
    "\n",
    "    def get(self, attribute_name, default=None):\n",
    "        return getattr(self, attribute_name, default)\n",
    "\n",
    "    def __contains__(self, attribute_name):\n",
    "        return hasattr(self, attribute_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d538c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMDiTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    „Éû„É´„ÉÅ„É¢„Éº„ÉÄ„É´„Éá„Ç£„Éï„É•„Éº„Ç∏„Éß„É≥„Éà„É©„É≥„Çπ„Éï„Ç©„Éº„Éû„Éº„É¢„Éá„É´\n",
    "    ÁîªÂÉè„Å®„ÉÜ„Ç≠„Çπ„Éà„ÅÆ‰∏°Êñπ„ÅÆ„Çπ„Éà„É™„Éº„É†„ÇíÂá¶ÁêÜ„Åô„Çã\n",
    "    ÁîªÂÉèÁîüÊàê„Çø„Çπ„ÇØ„Å´‰ΩøÁî®„Åï„Çå„Çã\n",
    "    \"\"\"\n",
    "    config_class = MMDiTConfig\n",
    "\n",
    "    def __init__(self, config: MMDiTConfig):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            config (MMDiTConfig): „É¢„Éá„É´„ÅÆË®≠ÂÆö\n",
    "        \"\"\"\n",
    "        logger.info(f\"MMDiTModel.__init__ {config=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.in_channels = config.in_channels\n",
    "        self.out_channels = self.in_channels\n",
    "        self.patch_size = config.patch_size\n",
    "\n",
    "        if config.hidden_size % config.num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"Hidden size {config.hidden_size} must be divisible by num_heads {config.num_heads}\"\n",
    "            )\n",
    "\n",
    "        pe_dim = config.hidden_size // config.num_heads\n",
    "        if sum(config.axes_dim) != pe_dim:\n",
    "            raise ValueError(\n",
    "                f\"Got {config.axes_dim} but expected positional dim {pe_dim}\"\n",
    "            )\n",
    "\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_heads\n",
    "        pe_embedder_cls = LigerEmbedND if config.use_liger_rope else EmbedND\n",
    "        self.pe_embedder = pe_embedder_cls(\n",
    "            dim=pe_dim, theta=config.theta, axes_dim=config.axes_dim\n",
    "        )\n",
    "\n",
    "        self.img_in = nn.Linear(self.in_channels, self.hidden_size, bias=True)\n",
    "        self.time_in = MLPEmbedder(in_dim=256, hidden_dim=self.hidden_size)\n",
    "        self.vector_in = MLPEmbedder(config.vec_in_dim, self.hidden_size)\n",
    "        self.guidance_in = (\n",
    "            MLPEmbedder(in_dim=256, hidden_dim=self.hidden_size)\n",
    "            if config.guidance_embed\n",
    "            else nn.Identity()\n",
    "        )\n",
    "        self.cond_in = (\n",
    "            nn.Linear(\n",
    "                self.in_channels + self.patch_size**2, self.hidden_size, bias=True\n",
    "            )\n",
    "            if config.cond_embed\n",
    "            else nn.Identity()\n",
    "        )\n",
    "        self.txt_in = nn.Linear(config.context_in_dim, self.hidden_size)\n",
    "\n",
    "        self.double_blocks = nn.ModuleList(\n",
    "            [\n",
    "                DoubleStreamBlock(\n",
    "                    self.hidden_size,\n",
    "                    self.num_heads,\n",
    "                    mlp_ratio=config.mlp_ratio,\n",
    "                    qkv_bias=config.qkv_bias,\n",
    "                    fused_qkv=config.fused_qkv,\n",
    "                )\n",
    "                for _ in range(config.depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.single_blocks = nn.ModuleList(\n",
    "            [\n",
    "                SingleStreamBlock(\n",
    "                    self.hidden_size,\n",
    "                    self.num_heads,\n",
    "                    mlp_ratio=config.mlp_ratio,\n",
    "                    fused_qkv=config.fused_qkv,\n",
    "                )\n",
    "                for _ in range(config.depth_single_blocks)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.final_layer = LastLayer(self.hidden_size, 1, self.out_channels)\n",
    "        self.initialize_weights()\n",
    "\n",
    "        if self.config.grad_ckpt_settings:\n",
    "            self.forward = self.forward_selective_ckpt\n",
    "        else:\n",
    "            self.forward = self.forward_ckpt\n",
    "        self._input_requires_grad = False\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        if self.config.cond_embed:\n",
    "            nn.init.zeros_(self.cond_in.weight)\n",
    "            nn.init.zeros_(self.cond_in.bias)\n",
    "\n",
    "    def prepare_block_inputs(\n",
    "        self,\n",
    "        img: Tensor,\n",
    "        img_ids: Tensor,\n",
    "        txt: Tensor,  # t5 encoded vec\n",
    "        txt_ids: Tensor,\n",
    "        timesteps: Tensor,\n",
    "        y_vec: Tensor,  # clip encoded vec\n",
    "        cond: Tensor = None,\n",
    "        guidance: Tensor | None = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        obtain the processed:\n",
    "            img: projected noisy img latent,\n",
    "            txt: text context (from t5),\n",
    "            vec: clip encoded vector,\n",
    "            pe: the positional embeddings for concatenated img and txt\n",
    "        \"\"\"\n",
    "        if img.ndim != 3 or txt.ndim != 3:\n",
    "            raise ValueError(\"Input img and txt tensors must have 3 dimensions.\")\n",
    "\n",
    "        # running on sequences img\n",
    "        print(\"img shape before img_in:\", img.shape)\n",
    "        img = self.img_in(img)\n",
    "        if self.config.cond_embed:\n",
    "            if cond is None:\n",
    "                raise ValueError(\"Didn't get conditional input for conditional model.\")\n",
    "            img = img + self.cond_in(cond)\n",
    "\n",
    "        vec = self.time_in(timestep_embedding(timesteps, 256))\n",
    "        if self.config.guidance_embed:\n",
    "            if guidance is None:\n",
    "                raise ValueError(\n",
    "                    \"Didn't get guidance strength for guidance distilled model.\"\n",
    "                )\n",
    "            vec = vec + self.guidance_in(timestep_embedding(guidance, 256))\n",
    "        vec = vec + self.vector_in(y_vec)\n",
    "\n",
    "        txt = self.txt_in(txt)\n",
    "\n",
    "        # concat: 4096 + t*h*2/4\n",
    "        ids = torch.cat((txt_ids, img_ids), dim=1)\n",
    "        pe = self.pe_embedder(ids)\n",
    "\n",
    "        if self._input_requires_grad:\n",
    "            # we only apply lora to double/single blocks, thus we only need to enable grad for these inputs\n",
    "            img.requires_grad_()\n",
    "            txt.requires_grad_()\n",
    "\n",
    "        return img, txt, vec, pe\n",
    "\n",
    "    def enable_input_require_grads(self):\n",
    "        \"\"\"Fit peft lora. This method should not be called manually.\"\"\"\n",
    "        self._input_requires_grad = True\n",
    "\n",
    "    def forward_ckpt(\n",
    "        self,\n",
    "        img: Tensor,\n",
    "        img_ids: Tensor,\n",
    "        txt: Tensor,\n",
    "        txt_ids: Tensor,\n",
    "        timesteps: Tensor,\n",
    "        y_vec: Tensor,\n",
    "        cond: Tensor = None,\n",
    "        guidance: Tensor | None = None,\n",
    "        **kwargs,\n",
    "    ) -> Tensor:\n",
    "        img, txt, vec, pe = self.prepare_block_inputs(\n",
    "            img, img_ids, txt, txt_ids, timesteps, y_vec, cond, guidance\n",
    "        )\n",
    "\n",
    "        for block in self.double_blocks:\n",
    "            img, txt = auto_grad_checkpoint(block, img, txt, vec, pe)\n",
    "\n",
    "        img = torch.cat((txt, img), 1)\n",
    "        for block in self.single_blocks:\n",
    "            img = auto_grad_checkpoint(block, img, vec, pe)\n",
    "        img = img[:, txt.shape[1] :, ...]\n",
    "\n",
    "        img = self.final_layer(img, vec)  # (N, T, patch_size ** 2 * out_channels)\n",
    "        return img\n",
    "\n",
    "    def forward_selective_ckpt(\n",
    "        self,\n",
    "        img: Tensor,\n",
    "        img_ids: Tensor,\n",
    "        txt: Tensor,\n",
    "        txt_ids: Tensor,\n",
    "        timesteps: Tensor,\n",
    "        y_vec: Tensor,\n",
    "        cond: Tensor = None,\n",
    "        guidance: Tensor | None = None,\n",
    "        **kwargs,\n",
    "    ) -> Tensor:\n",
    "        img, txt, vec, pe = self.prepare_block_inputs(\n",
    "            img, img_ids, txt, txt_ids, timesteps, y_vec, cond, guidance\n",
    "        )\n",
    "\n",
    "        ckpt_depth_double = self.config.grad_ckpt_settings[0]\n",
    "        for block in self.double_blocks[:ckpt_depth_double]:\n",
    "            img, txt = auto_grad_checkpoint(block, img, txt, vec, pe)\n",
    "\n",
    "        for block in self.double_blocks[ckpt_depth_double:]:\n",
    "            img, txt = block(img, txt, vec, pe)\n",
    "\n",
    "        ckpt_depth_single = self.config.grad_ckpt_settings[1]\n",
    "        img = torch.cat((txt, img), 1)\n",
    "        for block in self.single_blocks[:ckpt_depth_single]:\n",
    "            img = auto_grad_checkpoint(block, img, vec, pe)\n",
    "        for block in self.single_blocks[ckpt_depth_single:]:\n",
    "            img = block(img, vec, pe)\n",
    "\n",
    "        img = img[:, txt.shape[1] :, ...]\n",
    "\n",
    "        img = self.final_layer(img, vec)  # (N, T, patch_size ** 2 * out_channels)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e309fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @MODELS.register_module(\"flux\")\n",
    "def Flux(\n",
    "    cache_dir: str = None,\n",
    "    from_pretrained: str = None,\n",
    "    device_map: str | torch.device = \"cuda\",\n",
    "    torch_dtype: torch.dtype = torch.bfloat16,\n",
    "    strict_load: bool = False,\n",
    "    **kwargs,\n",
    ") -> MMDiTModel:\n",
    "    \"\"\"\n",
    "    MMDiT„É¢„Éá„É´„Çí‰ΩúÊàê„Åô„Çã„Éï„Ç°„ÇØ„Éà„É™Èñ¢Êï∞\n",
    "    Args:\n",
    "        cache_dir (str, optional): ‰∫ãÂâçÂ≠¶ÁøíÊ∏à„Åø„É¢„Éá„É´„ÅÆ„Ç≠„É£„ÉÉ„Ç∑„É•„Éá„Ç£„É¨„ÇØ„Éà„É™. Defaults to None.\n",
    "        from_pretrained (str, optional): ‰∫ãÂâçÂ≠¶ÁøíÊ∏à„Åø„É¢„Éá„É´„ÅÆ„Éë„Çπ„Åæ„Åü„ÅØÂêçÂâç. Defaults to None.\n",
    "        device_map (str | torch.device, optional): „É¢„Éá„É´„ÇíÈÖçÁΩÆ„Åô„Çã„Éá„Éê„Ç§„Çπ. Defaults to \"cuda\".\n",
    "        torch_dtype (torch.dtype, optional): „É¢„Éá„É´„ÅÆ„Éá„Éï„Ç©„É´„Éà„Éá„Éº„ÇøÂûã. Defaults to torch.bfloat16.\n",
    "        strict_load (bool, optional): „ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà„ÅÆÂé≥ÂØÜ„Å™Ë™≠„ÅøËæº„Åø„ÇíË°å„ÅÜ„Åã„Å©„ÅÜ„Åã. Defaults to False.\n",
    "    Returns:\n",
    "        MMDiTModel: ÂàùÊúüÂåñ„Åï„Çå„ÅüMMDiT„É¢„Éá„É´\n",
    "    \"\"\"\n",
    "    logger.info(f\"Flux {from_pretrained=} {device_map=} {torch_dtype=} {strict_load=} {kwargs=}\")\n",
    "\n",
    "    config = MMDiTConfig(\n",
    "        from_pretrained=from_pretrained,\n",
    "        cache_dir=cache_dir,\n",
    "        **kwargs,\n",
    "    )\n",
    "    low_precision_init = from_pretrained is not None and len(from_pretrained) > 0\n",
    "    if low_precision_init:\n",
    "        default_dtype = torch.get_default_dtype()\n",
    "        torch.set_default_dtype(torch_dtype)\n",
    "    with torch.device(device_map):\n",
    "        model = MMDiTModel(config)\n",
    "    if low_precision_init:\n",
    "        torch.set_default_dtype(default_dtype)\n",
    "    else:\n",
    "        model = model.to(torch_dtype)\n",
    "    if from_pretrained:\n",
    "        model = load_checkpoint(\n",
    "            model,\n",
    "            from_pretrained,\n",
    "            cache_dir=cache_dir,\n",
    "            device_map=device_map,\n",
    "            strict=strict_load,\n",
    "        )\n",
    "    return model\n",
    "\n",
    "if False:\n",
    "    # {'type': 'flux',\n",
    "    #  'from_pretrained': None,\n",
    "    #  'strict_load': False,\n",
    "    #  'guidance_embed': False,\n",
    "    #  'fused_qkv': False,\n",
    "    #  'use_liger_rope': True,\n",
    "    #  'grad_ckpt_settings': (8, 100),\n",
    "    #  'in_channels': 64,\n",
    "    #  'vec_in_dim': 768,\n",
    "    #  'context_in_dim': 4096,\n",
    "    #  'hidden_size': 384,\n",
    "    #  'mlp_ratio': 4.0,\n",
    "    #  'num_heads': 3,\n",
    "    #  'depth': 1,\n",
    "    #  'depth_single_blocks': 38,\n",
    "    #  'axes_dim': [16, 56, 56],\n",
    "    #  'theta': 10000,\n",
    "    #  'qkv_bias': True}\n",
    "\n",
    "    model = Flux(\n",
    "        from_pretrained=None,\n",
    "        strict_load=False,\n",
    "        guidance_embed=False,\n",
    "        fused_qkv=False,\n",
    "        use_liger_rope=True,\n",
    "        grad_ckpt_settings=(8, 100),\n",
    "        in_channels=64,\n",
    "        vec_in_dim=768,\n",
    "        context_in_dim=4096,\n",
    "        hidden_size=384,\n",
    "        mlp_ratio=4.0,\n",
    "        num_heads=3,\n",
    "        depth=1,\n",
    "        depth_single_blocks=38,\n",
    "        axes_dim=[16, 56, 56],\n",
    "        theta=10000,\n",
    "        qkv_bias=True,\n",
    "    )\n",
    "\n",
    "    # (batch_size, seq_len, in_channels)\n",
    "    sample_input = torch.randn(1, 16, 64).to(\"cuda\").to(torch.bfloat16)\n",
    "\n",
    "    # (batch_size, seq_len)\n",
    "    sample_img_ids = torch.randint(0, 10000, (1, 16, 3)).to(\"cuda\")\n",
    "\n",
    "    # (batch_size, seq_len, context_in_dim)\n",
    "    sample_txt = torch.randn(1, 16, 4096).to(\"cuda\").to(torch.bfloat16)\n",
    "\n",
    "    # (batch_size, seq_len)\n",
    "    sample_txt_ids = torch.randint(0, 10000, (1, 16, 3)).to(\"cuda\")\n",
    "\n",
    "    # (batch_size,)\n",
    "    sample_timesteps = torch.randint(0, 1000, (1,)).to(\"cuda\").to(torch.bfloat16)\n",
    "\n",
    "    # (batch_size, vec_in_dim)\n",
    "    sample_y_vec = torch.randn(1, 768).to(\"cuda\").to(torch.bfloat16)\n",
    "\n",
    "    output = model(\n",
    "        img=sample_input,\n",
    "        img_ids=sample_img_ids,\n",
    "        txt=sample_txt,\n",
    "        txt_ids=sample_txt_ids,\n",
    "        timesteps=sample_timesteps,\n",
    "        y_vec=sample_y_vec,\n",
    "    )\n",
    "\n",
    "    # (1, 16, patch_size ** 2 * out_channels)\n",
    "    print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec5ee08",
   "metadata": {},
   "source": [
    "### Êé®Ë´ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc10e2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from pprint import pformat\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from colossalai.utils import set_seed\n",
    "from tqdm import tqdm\n",
    "\n",
    "from opensora.acceleration.parallel_states import get_data_parallel_group\n",
    "from opensora.datasets.dataloader import prepare_dataloader\n",
    "from opensora.registry import DATASETS, build_module\n",
    "from opensora.utils.cai import (\n",
    "    get_booster,\n",
    "    get_is_saving_process,\n",
    "    init_inference_environment,\n",
    ")\n",
    "# from opensora.utils.config import parse_alias, parse_configs\n",
    "from opensora.utils.config import parse_alias\n",
    "from opensora.utils.inference import (\n",
    "    add_fps_info_to_text,\n",
    "    add_motion_score_to_text,\n",
    "    create_tmp_csv,\n",
    "    modify_option_to_t2i,\n",
    "    process_and_save,\n",
    ")\n",
    "from opensora.utils.logger import create_logger, is_main_process\n",
    "from opensora.utils.misc import log_cuda_max_memory, to_torch_dtype\n",
    "from opensora.utils.prompt_refine import refine_prompts\n",
    "from opensora.utils.sampling import (\n",
    "    SamplingOption,\n",
    "    prepare_api,\n",
    "    prepare_models,\n",
    "    sanitize_sampling_option,\n",
    ")\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def main(args):\n",
    "    # ======================================================\n",
    "    # 1. configs & runtime variables\n",
    "    # ======================================================\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    # == parse configs ==\n",
    "    cfg = parse_configs(args)\n",
    "    cfg = parse_alias(cfg)\n",
    "\n",
    "    # == device and dtype ==\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dtype = to_torch_dtype(cfg.get(\"dtype\", \"bf16\"))\n",
    "    seed = cfg.get(\"seed\", 1024)\n",
    "    if seed is not None:\n",
    "        set_seed(seed)\n",
    "\n",
    "    # == init distributed env ==\n",
    "    init_inference_environment()\n",
    "    logger = create_logger()\n",
    "    logger.info(\"Inference configuration:\\n %s\", pformat(cfg.to_dict()))\n",
    "    is_saving_process = get_is_saving_process(cfg)\n",
    "    booster = get_booster(cfg)\n",
    "    booster_ae = get_booster(cfg, ae=True)\n",
    "\n",
    "    # ======================================================\n",
    "    # 2. build dataset and dataloader\n",
    "    # ======================================================\n",
    "    logger.info(\"Building dataset...\")\n",
    "\n",
    "    # save directory\n",
    "    save_dir = cfg.save_dir\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # == build dataset ==\n",
    "    if cfg.get(\"prompt\"):\n",
    "        cfg.dataset.data_path = create_tmp_csv(save_dir, cfg.prompt, cfg.get(\"ref\", None), create=is_main_process())\n",
    "    dist.barrier()\n",
    "    dataset = build_module(cfg.dataset, DATASETS)\n",
    "\n",
    "    # range selection\n",
    "    start_index = cfg.get(\"start_index\", 0)\n",
    "    end_index = cfg.get(\"end_index\", None)\n",
    "    if end_index is None:\n",
    "        end_index = start_index + cfg.get(\"num_samples\", len(dataset.data) + 1)\n",
    "    dataset.data = dataset.data[start_index:end_index]\n",
    "    logger.info(\"Dataset contains %s samples.\", len(dataset))\n",
    "\n",
    "    # == build dataloader ==\n",
    "    dataloader_args = dict(\n",
    "        dataset=dataset,\n",
    "        batch_size=cfg.get(\"batch_size\", 1),\n",
    "        num_workers=cfg.get(\"num_workers\", 4),\n",
    "        seed=cfg.get(\"seed\", 1024),\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "        process_group=get_data_parallel_group(),\n",
    "        prefetch_factor=cfg.get(\"prefetch_factor\", None),\n",
    "    )\n",
    "    dataloader, _ = prepare_dataloader(**dataloader_args)\n",
    "\n",
    "    # == prepare default params ==\n",
    "    sampling_option = SamplingOption(**cfg.sampling_option)\n",
    "    sampling_option = sanitize_sampling_option(sampling_option)\n",
    "\n",
    "    cond_type = cfg.get(\"cond_type\", \"t2v\")\n",
    "    prompt_refine = cfg.get(\"prompt_refine\", False)\n",
    "    fps_save = cfg.get(\"fps_save\", 16)\n",
    "    num_sample = cfg.get(\"num_sample\", 1)\n",
    "\n",
    "    type_name = \"image\" if cfg.sampling_option.num_frames == 1 else \"video\"\n",
    "    sub_dir = f\"{type_name}_{cfg.sampling_option.resolution}\"\n",
    "    os.makedirs(os.path.join(save_dir, sub_dir), exist_ok=True)\n",
    "    use_t2i2v = cfg.get(\"use_t2i2v\", False)\n",
    "    img_sub_dir = os.path.join(sub_dir, \"generated_condition\")\n",
    "    if use_t2i2v:\n",
    "        os.makedirs(os.path.join(save_dir, sub_dir, \"generated_condition\"), exist_ok=True)\n",
    "\n",
    "    # ======================================================\n",
    "    # 3. build model\n",
    "    # ======================================================\n",
    "    logger.info(\"Building models...\")\n",
    "\n",
    "    # == build flux model ==\n",
    "    model, model_ae, model_t5, model_clip, optional_models = prepare_models(\n",
    "        cfg, device, dtype, offload_model=cfg.get(\"offload_model\", False)\n",
    "    )\n",
    "    log_cuda_max_memory(\"build model\")\n",
    "\n",
    "    if booster:\n",
    "        model, _, _, _, _ = booster.boost(model=model)\n",
    "        model = model.unwrap()\n",
    "    if booster_ae:\n",
    "        model_ae, _, _, _, _ = booster_ae.boost(model=model_ae)\n",
    "        model_ae = model_ae.unwrap()\n",
    "\n",
    "    api_fn = prepare_api(model, model_ae, model_t5, model_clip, optional_models)\n",
    "\n",
    "    # prepare image flux model if t2i2v\n",
    "    if use_t2i2v:\n",
    "        api_fn_img = prepare_api(\n",
    "            optional_models[\"img_flux\"], optional_models[\"img_flux_ae\"], model_t5, model_clip, optional_models\n",
    "        )\n",
    "\n",
    "    # ======================================================\n",
    "    # 4. inference\n",
    "    # ======================================================\n",
    "    for epoch in range(num_sample):  # generate multiple samples with different seeds\n",
    "        dataloader_iter = iter(dataloader)\n",
    "        with tqdm(\n",
    "            enumerate(dataloader_iter, start=0),\n",
    "            desc=\"Inference progress\",\n",
    "            disable=not is_main_process(),\n",
    "            initial=0,\n",
    "            total=len(dataloader),\n",
    "        ) as pbar:\n",
    "            for _, batch in pbar:\n",
    "                original_text = batch.pop(\"text\")\n",
    "                if use_t2i2v:\n",
    "                    batch[\"text\"] = original_text if not prompt_refine else refine_prompts(original_text, type=\"t2i\")\n",
    "                    sampling_option_t2i = modify_option_to_t2i(\n",
    "                        sampling_option,\n",
    "                        distilled=True,\n",
    "                        img_resolution=cfg.get(\"img_resolution\", \"768px\"),\n",
    "                    )\n",
    "                    if cfg.get(\"offload_model\", False):\n",
    "                        model_move_start = time.time()\n",
    "                        model = model.to(\"cpu\", dtype)\n",
    "                        model_ae = model_ae.to(\"cpu\", dtype)\n",
    "                        optional_models[\"img_flux\"].to(device, dtype)\n",
    "                        optional_models[\"img_flux_ae\"].to(device, dtype)\n",
    "                        logger.info(\n",
    "                            \"offload video diffusion model to cpu, load image flux model to gpu: %s s\",\n",
    "                            time.time() - model_move_start,\n",
    "                        )\n",
    "\n",
    "                    logger.info(\"Generating image condition by flux...\")\n",
    "                    x_cond = api_fn_img(\n",
    "                        sampling_option_t2i,\n",
    "                        \"t2v\",\n",
    "                        seed=sampling_option.seed + epoch if sampling_option.seed else None,\n",
    "                        channel=cfg[\"img_flux\"][\"in_channels\"],\n",
    "                        **batch,\n",
    "                    ).cpu()\n",
    "\n",
    "                    # save image to disk\n",
    "                    batch[\"name\"] = process_and_save(\n",
    "                        x_cond,\n",
    "                        batch,\n",
    "                        cfg,\n",
    "                        img_sub_dir,\n",
    "                        sampling_option_t2i,\n",
    "                        epoch,\n",
    "                        start_index,\n",
    "                        saving=is_saving_process,\n",
    "                    )\n",
    "                    dist.barrier()\n",
    "\n",
    "                    if cfg.get(\"offload_model\", False):\n",
    "                        model_move_start = time.time()\n",
    "                        model = model.to(device, dtype)\n",
    "                        model_ae = model_ae.to(device, dtype)\n",
    "                        optional_models[\"img_flux\"].to(\"cpu\", dtype)\n",
    "                        optional_models[\"img_flux_ae\"].to(\"cpu\", dtype)\n",
    "                        logger.info(\n",
    "                            \"load video diffusion model to gpu, offload image flux model to cpu: %s s\",\n",
    "                            time.time() - model_move_start,\n",
    "                        )\n",
    "\n",
    "                    ref_dir = os.path.join(save_dir, os.path.join(sub_dir, \"generated_condition\"))\n",
    "                    batch[\"ref\"] = [os.path.join(ref_dir, f\"{x}.png\") for x in batch[\"name\"]]\n",
    "                    cond_type = \"i2v_head\"\n",
    "\n",
    "                batch[\"text\"] = original_text\n",
    "                if prompt_refine:\n",
    "                    batch[\"text\"] = refine_prompts(\n",
    "                        original_text, type=\"t2v\" if cond_type == \"t2v\" else \"t2i\", image_paths=batch.get(\"ref\", None)\n",
    "                    )\n",
    "                batch[\"text\"] = add_fps_info_to_text(batch.pop(\"text\"), fps=fps_save)\n",
    "                if \"motion_score\" in cfg:\n",
    "                    batch[\"text\"] = add_motion_score_to_text(batch.pop(\"text\"), cfg.get(\"motion_score\", 5))\n",
    "\n",
    "                logger.info(\"Generating video...\")\n",
    "                x = api_fn(\n",
    "                    sampling_option,\n",
    "                    cond_type,\n",
    "                    seed=sampling_option.seed + epoch if sampling_option.seed else None,\n",
    "                    patch_size=cfg.get(\"patch_size\", 2),\n",
    "                    save_prefix=cfg.get(\"save_prefix\", \"\"),\n",
    "                    channel=cfg[\"model\"][\"in_channels\"],\n",
    "                    **batch,\n",
    "                ).cpu()\n",
    "\n",
    "                if is_saving_process:\n",
    "                    process_and_save(x, batch, cfg, sub_dir, sampling_option, epoch, start_index)\n",
    "                dist.barrier()\n",
    "\n",
    "    logger.info(\"Inference finished.\")\n",
    "    log_cuda_max_memory(\"inference\")\n",
    "\n",
    "if False:\n",
    "    # OOM\n",
    "    main(\n",
    "        [\n",
    "            \"configs/diffusion/inference/high_compression.py\",\n",
    "            \"--prompt\",\n",
    "            \"The story of a robot's life in a cyberpunk setting.\",\n",
    "        ]\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
