{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e54053b",
   "metadata": {},
   "source": [
    "## æ¦‚è¦"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53236702",
   "metadata": {},
   "source": [
    "Open-Sora 2.0ã¯ã€3000ä¸‡å††ã§å®Ÿç¾å¯èƒ½ãªå•†ç”¨ãƒ¬ãƒ™ãƒ«ã®å‹•ç”»ç”Ÿæˆãƒ¢ãƒ‡ãƒ«\n",
    "\n",
    "è¨“ç·´ã‚³ã‚¹ãƒˆã¯ã€åŒç­‰ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆMovieGenã‚„Step-Video-T2Vï¼‰ã‚ˆã‚Šã‚‚5~10å€ä½Žã„\n",
    "\n",
    "äººã®è©•ä¾¡ã¨VBenchã®ã‚¹ã‚³ã‚¢ã§ã¯ã€Huyyuan Videoã‚„Runway Gen-3 Alphaã«åŒ¹æ•µ:\n",
    "\n",
    "![](image/fig1.png)\n",
    "\n",
    "- Visual Quality: è¦–è¦šå“è³ª\n",
    "- Prompt Following: æŒ‡ç¤ºè¿½å¾“æ€§\n",
    "- Motion Quality: å‹•ãã®å“è³ª"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12386bd",
   "metadata": {},
   "source": [
    "## ãƒ‡ãƒ¼ã‚¿"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1780041",
   "metadata": {},
   "source": [
    "ç›®çš„ã¯ã€å­¦ç¿’ã®é€²æ—ã«åˆã‚ã›ãŸãƒ‡ãƒ¼ã‚¿ãƒ”ãƒ©ãƒŸãƒƒãƒ‰ï¼ˆhierarchical data pyramidï¼‰ã®æ§‹ç¯‰\n",
    "\n",
    "æ§˜ã€…ãªç¨®é¡žã®ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œå‡ºå¯èƒ½ãªãªãƒ•ã‚£ãƒ«ã‚¿ã‚’é–‹ç™º\n",
    "\n",
    "å­¦ç¿’ã®é€²æ—ã«å¿œã˜ã¦ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®å¼·åº¦ã‚’é«˜ã‚ã€ç´”åº¦ã¨å“è³ªã®é«˜ã„å°ã•ã„ã‚µãƒ–ã‚»ãƒƒãƒˆã§è¨“ç·´\n",
    "\n",
    "ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®å…¨ä½“åƒ:\n",
    "\n",
    "![](image/fig2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7093517c",
   "metadata": {},
   "source": [
    "- ç´«: ç”Ÿã®å‹•ç”»ã®å‰å‡¦ç†\n",
    "    1. ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "        - ç ´æã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®é™¤åŽ»\n",
    "        - æ¥µç«¯ãªå‹•ç”»ã®é™¤åŽ»\n",
    "            - å†ç”Ÿæ™‚é–“ãŒ2ç§’æœªæº€\n",
    "            - 1ç”»åƒã‚ãŸã‚Šã®ãƒ‡ãƒ¼ã‚¿é‡ï¼ˆBit per pixelï¼‰ãŒ0.02æœªæº€\n",
    "            - ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ¼ãƒˆï¼ˆfpsï¼‰ãŒ16æœªæº€\n",
    "            - ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ãŒç¯„å›²å¤–ï¼ˆ1/3, 3ï¼‰\n",
    "            - ç‰¹å®šã®ä½Žå“è³ªãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰è¨­å®šï¼ˆConstrained Baseline profileï¼‰\n",
    "    2. é€£ç¶šã—ãŸæ˜ åƒã‚’æ¤œå‡ºã—ã€çŸ­ã„ã‚¯ãƒªãƒƒãƒ—ã«åˆ†å‰²\n",
    "        - FFmpegã®libavfilterã‚’ä½¿ç”¨ã—ã€ã‚·ãƒ¼ãƒ³ã‚¹ã‚³ã‚¢ï¼ˆãƒ•ãƒ¬ãƒ¼ãƒ é–“ã®è¦–è¦šå·®åˆ†ï¼‰ã‚’è¨ˆç®—\n",
    "    3. å‹•ç”»ã®ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆ\n",
    "        - ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ¼ãƒˆã¯30fpsä»¥ä¸‹\n",
    "        - é•·è¾ºã¯1080pxä»¥ä¸‹\n",
    "        - ã‚³ãƒ¼ãƒ‡ãƒƒã‚¯ï¼ˆåœ§ç¸®å½¢å¼ï¼‰ã¯H.264\n",
    "        - å‹•ç”»ã®é»’å¸¯ã‚’å‰Šé™¤\n",
    "        - 8ç§’ã‚’è¶…ãˆã‚‹ã‚·ãƒ§ãƒƒãƒˆã¯ã€8ç§’ã®ã‚¯ãƒªãƒƒãƒ—ã«åˆ†å‰²ã—ã€2ç§’æœªæº€ã¯ç ´æ£„\n",
    "- é’: ã‚¯ãƒªãƒƒãƒ—å‹•ç”»ã®ã‚¹ã‚³ã‚¢ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "    - ç¾Žçš„ã‚¹ã‚³ã‚¢ã§ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "        - [CLIPã¨MLP][1]ã§ç¾Žçš„ã‚¹ã‚³ã‚¢ã‚’äºˆæ¸¬\n",
    "            - æœ€åˆãƒ»ä¸­é–“ãƒ»æœ€çµ‚ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æŠ½å‡ºã—ã€ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã—ã€å¹³å‡\n",
    "    - é®®æ˜Žã•ã®ä½Žã„å‹•ç”»ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "        - OpenCVã®ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³æ¼”ç®—å­ã§ç”»åƒã®åˆ†æ•£ãŒä½Žã„ï¼ˆã¼ã‚„ã‘ã¦ã„ã‚‹ï¼‰å‹•ç”»ã‚’é™¤åŽ»\n",
    "    - æ‰‹ãƒ–ãƒ¬ã®å¤šã„å‹•ç”»ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "        - PySceneDetectã‚’ä½¿ç”¨ã—ã¦ã€ãƒ•ãƒ¬ãƒ¼ãƒ é–“ã®å¤‰åŒ–ãŒå¤§ãã„å‹•ç”»ã‚’é™¤åŽ»\n",
    "    - é‡è¤‡ã‚¯ãƒªãƒƒãƒ—ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "- ç·‘: 256pxã®ä½Žè§£åƒåº¦å‹•ç”»\n",
    "    - å¤šãã®æ–‡ç« ãŒå«ã¾ã‚Œã‚‹å‹•ç”»ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "        - PaddleOCRã‚’ä½¿ç”¨ã—ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆã®ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’æ¤œå‡º\n",
    "        - ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ãŒ0.7ã‚’è¶…ãˆã‚‹ãƒœãƒƒã‚¯ã‚¹ã®ç·é¢ç©ãŒå¤šã„å‹•ç”»ã‚’é™¤åŽ»\n",
    "    - ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã§ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "        - libavfilterã®VMAFã‚’ä½¿ç”¨ã—ã¦å‹•ç”»ã®å‹•ãã®æ¿€ã—ã•ã‚’æ¸¬å®š\n",
    "        - ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ãŒæ¥µç«¯ã«ä½Žã„ãƒ»é«˜ã„å‹•ç”»ã‚’é™¤åŽ»\n",
    "- é»„è‰²: 768pxã®é«˜è§£åƒåº¦å‹•ç”»\n",
    "\n",
    "[1]: https://github.com/christophschuhmann/improved-aesthetic-predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa995eea",
   "metadata": {},
   "source": [
    "å‹•ç”»ã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½œæˆã™ã‚‹ãŸã‚ã«ã€è¦–è¦šè¨€èªžãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨:\n",
    "\n",
    "- 256pxå‹•ç”»ã«ã¯ã€LLaVA-Video\n",
    "- 778pxå‹•ç”»ã«ã¯ã€Qwen 2.5 Maxï¼ˆãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ãŒå°‘ãªã„ãƒ—ãƒ­ãƒ—ãƒ©ã‚¤ã‚¨ã‚¿ãƒªãƒ¢ãƒ‡ãƒ«ï¼‰\n",
    "\n",
    "ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹æˆ:\n",
    "\n",
    "- ä¸»ãªè¢«å†™ä½“\n",
    "- è¢«å†™ä½“ã®å‹•ã\n",
    "- èƒŒæ™¯ã‚„ç’°å¢ƒ\n",
    "- è¨¼æ˜Žæ¡ä»¶ã‚„é›°å›²æ°—\n",
    "- ã‚«ãƒ¡ãƒ©ãƒ¯ãƒ¼ã‚¯\n",
    "- ãƒªã‚¢ãƒ«ãƒ»ã‚·ãƒãƒžãƒ†ã‚£ãƒƒã‚¯ãƒ»3Dãƒ»ã‚¢ãƒ‹ãƒ¡ãªã©ã®å‹•ç”»ã®ã‚¹ã‚¿ã‚¤ãƒ«\n",
    "\n",
    "ç”Ÿæˆæ™‚ã«å‹•ãã®å¼·åº¦ã‚’èª¿æ•´å¯èƒ½ã«ã™ã‚‹ãŸã‚ã€ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã®æœ€å¾Œã«ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’è¿½è¨˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7587fc55",
   "metadata": {},
   "source": [
    "ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆ:\n",
    "\n",
    "![](image/fig3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff7e82d",
   "metadata": {},
   "source": [
    "- ç¾Žçš„ã‚¹ã‚³ã‚¢ã¯4.5~5.5ã§ä¸­ç¨‹åº¦\n",
    "- å‹•ç”»ã®é•·ã•ã¯2~8ç§’ã§ã€åŠåˆ†è¿‘ããŒ6~8ç§’\n",
    "- ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã¯ã€å¤§éƒ¨åˆ†ãŒ0.5~0.75ï¼ˆ16:9ã®æ¨ªé•·å‹•ç”»ï¼‰\n",
    "- ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã®70%ã¯75å˜èªžã‚’è¶…ãˆã¦ã„ã¦æƒ…å ±é‡ãŒå¤šã„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1277b7",
   "metadata": {},
   "source": [
    "ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã®ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab60e0b",
   "metadata": {},
   "source": [
    "![](image/fig4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1d547b",
   "metadata": {},
   "source": [
    "- èƒŒæ™¯ã‚„ç…§æ˜Žæ¡ä»¶ã‚‚å«ã¾ã‚Œã¦ã„ã¦ã€è¢«å†™ä½“ã¯äººç‰©ãŒå¤šã„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c25823",
   "metadata": {},
   "source": [
    "## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346c05aa",
   "metadata": {},
   "source": [
    "### 3æ¬¡å…ƒã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a577fd",
   "metadata": {},
   "source": [
    "[Hunyuan Video VAE][1]ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’åŠ¹çŽ‡åŒ–ã—ãŸVideo DC-AEã‚’é–‹ç™º\n",
    "\n",
    "DC-AEã¯ã€[Deep Compression Autoencoder][2]ã®ç•¥\n",
    "\n",
    "åœ§ç¸®çŽ‡ã¯ã€$4\\times 32\\times 32$ï¼ˆæ™‚é–“ã¯ $\\frac{1}{4}$ã€ç¸¦ã¨æ¨ªã¯$\\frac{1}{32}$ã«åœ§ç¸®ï¼‰\n",
    "\n",
    "ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¯32ãƒ•ãƒ¬ãƒ¼ãƒ ã€256pxã®ãŸã‚ã€æ½œåœ¨è¡¨ç¾ã¯$8\\times 8\\times 8$\n",
    "\n",
    "[1]: https://arxiv.org/abs/2412.03603\n",
    "[2]: https://arxiv.org/abs/2410.10733"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef5fb66",
   "metadata": {},
   "source": [
    "Video DC-AEã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£:\n",
    "\n",
    "![](image/fig5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346d9939",
   "metadata": {},
   "source": [
    "- ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€\n",
    "    - 3å±¤ã®ResBlockã¨3å±¤ã®EfficientViT Blockã§æ§‹æˆã•ã‚Œã‚‹\n",
    "    - æœ€åˆã®5ã¤ã®ãƒ–ãƒ­ãƒƒã‚¯ã¯ã€ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç”¨\n",
    "    - å­¦ç¿’ã‚’å¯èƒ½ã«ã™ã‚‹ãŸã‚ã€ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒ«ãƒ–ãƒ­ãƒƒã‚¯ã«ã¯ã€æ®‹å·®æŽ¥ç¶šãŒå°Žå…¥\n",
    "    - æ®‹å·®æŽ¥ç¶šã¯ãƒ”ã‚¯ã‚»ãƒ«ã‚¢ãƒ³ã‚·ãƒ£ãƒƒãƒ•ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨ï¼ˆSpace&Time->Channelï¼‰\n",
    "- ãƒ‡ã‚³ãƒ¼ãƒ€\n",
    "    - 3å±¤ã®EfficientViT Blockã¨3å±¤ã®ResBlockã§æ§‹æˆã•ã‚Œã‚‹\n",
    "    - æœ€å¾Œã®5ã¤ã®ãƒ–ãƒ­ãƒƒã‚¯ã¯ã€ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç”¨\n",
    "    - å­¦ç¿’ã‚’å¯èƒ½ã«ã™ã‚‹ãŸã‚ã€ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒ«ãƒ–ãƒ­ãƒƒã‚¯ã«ã¯ã€æ®‹å·®æŽ¥ç¶šãŒå°Žå…¥\n",
    "    - æ®‹å·®æŽ¥ç¶šã¯ãƒ”ã‚¯ã‚»ãƒ«ã‚·ãƒ£ãƒƒãƒ•ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨ï¼ˆChannel->Space&Timeï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb61f01",
   "metadata": {},
   "source": [
    "Video DC-AEã‚’ã‚¹ã‚¯ãƒ©ãƒƒãƒã‹ã‚‰å­¦ç¿’ã—ã€å†æ§‹æˆå“è³ªã‚’è©•ä¾¡:\n",
    "\n",
    "![](image/table1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df750de",
   "metadata": {},
   "source": [
    "- LPIPS: äººé–“ã®çŸ¥è¦šã«è¿‘ã„ç”»è³ªè©•ä¾¡æŒ‡æ¨™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adeda3cd",
   "metadata": {},
   "source": [
    "## DiTã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759c3421",
   "metadata": {},
   "source": [
    "é›¢ã‚ŒãŸãƒ•ãƒ¬ãƒ¼ãƒ ã‚„ç”»ç´ åŒå£«ã®é–¢ä¿‚ã‚’åŠ¹æžœçš„ã«æ‰ãˆã‚‹ãƒ•ãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’æŽ¡ç”¨\n",
    "\n",
    "å‹•ç”»ã¯Video DC-AEã§åœ§ç¸®å¾Œã€ãƒ‘ãƒƒãƒã‚µã‚¤ã‚º1ï¼ˆ=ãƒ‘ãƒƒãƒåŒ–ç„¡ã—ï¼‰ã§ãƒ•ãƒ©ãƒƒãƒˆåŒ–\n",
    "\n",
    "Hunyuan Videoã®ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€ãƒ‘ãƒƒãƒã‚µã‚¤ã‚º2ãŒå¿…è¦"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df79314e",
   "metadata": {},
   "source": [
    "FLUXã®[MMDiT][1]ã‚’å‚è€ƒã«ã€ãƒ‡ãƒ¥ã‚¢ãƒ«ã‚¹ãƒˆãƒªãƒ¼ãƒ ã¨ã‚·ãƒ³ã‚°ãƒ«ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‹ã‚‰ãªã‚‹æ§‹é€ ã‚’æŽ¡ç”¨:\n",
    "\n",
    "![](image/fig6.png)\n",
    "\n",
    "[1]: https://github.com/black-forest-labs/flux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968e2d5c",
   "metadata": {},
   "source": [
    "- ãƒ‡ãƒ¥ã‚¢ãƒ«ã‚¹ãƒˆãƒªãƒ¼ãƒ ãƒ–ãƒ­ãƒƒã‚¯ã§ã€å‹•ç”»ã¨ãƒ†ã‚­ã‚¹ãƒˆãŒåˆ¥ã€…ã«ç‰¹å¾´æŠ½å‡ºã•ã‚Œã‚‹\n",
    "- ã‚·ãƒ³ã‚°ãƒ«ã‚¹ãƒˆãƒªãƒ¼ãƒ ãƒ–ãƒ­ãƒƒã‚¯ã§ã€ç‰¹å¾´ã‚’çµ±åˆã™ã‚‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f3e053",
   "metadata": {},
   "source": [
    "ç©ºé–“ã¨æ™‚é–“æƒ…å ±ã‚’æ‰ãˆã‚‹ãŸã‚ã«ã€3D RoPEã‚’æŽ¡ç”¨\n",
    "\n",
    "ãƒ†ã‚­ã‚¹ãƒˆãƒŽã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã¯ã€2ã¤ã®äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’æŽ¡ç”¨:\n",
    "\n",
    "- T5-XXL: è¤‡é›‘ãªãƒ†ã‚­ã‚¹ãƒˆã®æ„å‘³ã‚’æ‰ãˆã‚‹\n",
    "- CLIP-Large: ãƒ†ã‚­ã‚¹ãƒˆã¨è¦–è¦šæ¦‚å¿µã®æ•´åˆæ€§ã‚’æ‰ãˆã‚‹ï¼ˆ=æŒ‡ç¤ºè¿½å¾“æ€§ã‚’é«˜ã‚ã‚‹ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6548ce9c",
   "metadata": {},
   "source": [
    "![](image/table2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baaba03",
   "metadata": {},
   "source": [
    "## å®Ÿè£…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf13bf8",
   "metadata": {},
   "source": [
    "- image.py: ç”»åƒã®ã¿ã§å­¦ç¿’ã€‚\n",
    "- stage1.py: 256pxè§£åƒåº¦ã®å‹•ç”»ã§å­¦ç¿’ã€‚\n",
    "- stage2.py: 768pxè§£åƒåº¦ã®å‹•ç”»ã§å­¦ç¿’ï¼ˆã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä¸¦åˆ—åŒ–ã‚’ä½¿ç”¨ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯4ï¼‰ã€‚\n",
    "- stage1_i2v.py: 256pxè§£åƒåº¦ã§T2Vï¼ˆãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å‹•ç”»ï¼‰ã¨I2Vï¼ˆç”»åƒã‹ã‚‰å‹•ç”»ï¼‰ã‚’å­¦ç¿’ã€‚\n",
    "- stage2_i2v.py: 768pxè§£åƒåº¦ã§T2Vã¨I2Vã‚’å­¦ç¿’ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e895226e",
   "metadata": {},
   "source": [
    "## ç’°å¢ƒæ§‹ç¯‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9f1318",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd /workspaces/open-sora/Open-Sora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52ec244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import platform\n",
    "import subprocess\n",
    "\n",
    "if os.path.exists(\"debug.log\"):\n",
    "    os.remove(\"debug.log\")\n",
    "\n",
    "def custom_format(record):\n",
    "    match record.levelno:\n",
    "        case logging.DEBUG:\n",
    "            level = \"ðŸŸ¦\"\n",
    "        case logging.INFO:\n",
    "            level = \"ðŸŸ©\"\n",
    "        case logging.WARNING:\n",
    "            level = \"ðŸŸ¨\"\n",
    "        case logging.ERROR:\n",
    "            level = \"ðŸŸ¥\"\n",
    "        case logging.CRITICAL:\n",
    "            level = \"ðŸ›‘\"\n",
    "    return f\"{level} {record.getMessage()}\"\n",
    "\n",
    "logger_ = logging.getLogger()\n",
    "\n",
    "for handler in logger_.handlers:\n",
    "    logger_.removeHandler(handler)\n",
    "\n",
    "formatter = logging.Formatter()\n",
    "formatter.format = custom_format\n",
    "\n",
    "file_handler = logging.FileHandler(\"debug.log\")\n",
    "file_handler.setFormatter(formatter)\n",
    "logger_.addHandler(file_handler)\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger_.addHandler(stream_handler)\n",
    "logger_.setLevel(logging.DEBUG)\n",
    "\n",
    "PYTHON_VERSION = platform.python_version()\n",
    "logger_.info(f\"Python {PYTHON_VERSION}\")\n",
    "\n",
    "NVIDIA_SMI = subprocess.run(\"nvidia-smi\", capture_output=True, text=True).stdout\n",
    "logger_.info(f\"{NVIDIA_SMI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57815940",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    %pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "    %pip install \\\n",
    "        accelerate \\\n",
    "        av \\\n",
    "        colossalai \\\n",
    "        ftfy \\\n",
    "        liger-kernel \\\n",
    "        omegaconf \\\n",
    "        mmengine \\\n",
    "        openai \\\n",
    "        pandas \\\n",
    "        pandarallel \\\n",
    "        pyarrow \\\n",
    "        tensorboard \\\n",
    "        wandb \\\n",
    "        --extra-index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "    %pip install flash-attn --no-build-isolation\n",
    "\n",
    "    %pip install -e . --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754fc1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# import math\n",
    "# import os\n",
    "# import subprocess\n",
    "# import warnings\n",
    "# from contextlib import nullcontext\n",
    "# from copy import deepcopy\n",
    "# from pprint import pformat\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "# gc.disable()\n",
    "\n",
    "\n",
    "# import torch\n",
    "# import torch.distributed as dist\n",
    "# import torch.nn.functional as F\n",
    "# import wandb\n",
    "# from colossalai.booster import Booster\n",
    "# from colossalai.utils import set_seed\n",
    "# from peft import LoraConfig\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# from opensora.acceleration.checkpoint import (\n",
    "#     GLOBAL_ACTIVATION_MANAGER,\n",
    "#     set_grad_checkpoint,\n",
    "# )\n",
    "# from opensora.acceleration.parallel_states import get_data_parallel_group\n",
    "# from opensora.datasets.aspect import bucket_to_shapes\n",
    "# from opensora.datasets.dataloader import prepare_dataloader\n",
    "# from opensora.datasets.pin_memory_cache import PinMemoryCache\n",
    "# from opensora.models.mmdit.distributed import MMDiTPolicy\n",
    "# from opensora.registry import DATASETS, MODELS, build_module\n",
    "# from opensora.utils.ckpt import (\n",
    "#     CheckpointIO,\n",
    "#     model_sharding,\n",
    "#     record_model_param_shape,\n",
    "#     rm_checkpoints,\n",
    "# )\n",
    "# from opensora.utils.config import (\n",
    "#     config_to_name,\n",
    "#     create_experiment_workspace,\n",
    "#     parse_configs,\n",
    "# )\n",
    "# from opensora.utils.logger import create_logger\n",
    "# from opensora.utils.misc import (\n",
    "#     NsysProfiler,\n",
    "#     Timers,\n",
    "#     all_reduce_mean,\n",
    "#     create_tensorboard_writer,\n",
    "#     is_log_process,\n",
    "#     is_pipeline_enabled,\n",
    "#     log_cuda_max_memory,\n",
    "#     log_cuda_memory,\n",
    "#     log_model_params,\n",
    "#     print_mem,\n",
    "#     to_torch_dtype,\n",
    "# )\n",
    "# from opensora.utils.optimizer import create_lr_scheduler, create_optimizer\n",
    "# from opensora.utils.sampling import (\n",
    "#     get_res_lin_function,\n",
    "#     pack,\n",
    "#     prepare,\n",
    "#     prepare_ids,\n",
    "#     time_shift,\n",
    "# )\n",
    "# from opensora.utils.train import (\n",
    "#     create_colossalai_plugin,\n",
    "#     dropout_condition,\n",
    "#     get_batch_loss,\n",
    "#     prepare_visual_condition_causal,\n",
    "#     prepare_visual_condition_uncausal,\n",
    "#     set_eps,\n",
    "#     set_lr,\n",
    "#     setup_device,\n",
    "#     update_ema,\n",
    "#     warmup_ae,\n",
    "# )\n",
    "\n",
    "# torch.backends.cudnn.benchmark = False  # True leads to slow down in conv3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14a0a26",
   "metadata": {},
   "source": [
    "## DC-AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a745e96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from omegaconf import MISSING, OmegaConf\n",
    "from torch import Tensor\n",
    "from functools import partial\n",
    "from typing import Optional\n",
    "\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.batchnorm import _BatchNorm\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "from inspect import signature\n",
    "from typing import Any, Callable, Optional, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e195e6e",
   "metadata": {},
   "source": [
    "### ãƒ¢ãƒ‡ãƒ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13a6fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataclasses import dataclass, field\n",
    "# from typing import Any, Optional\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from omegaconf import MISSING, OmegaConf\n",
    "# from torch import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cab2ff",
   "metadata": {},
   "source": [
    "#### ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61014d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class EncoderConfig:\n",
    "    in_channels: int = MISSING\n",
    "    latent_channels: int = MISSING\n",
    "    width_list: tuple[int, ...] = (128, 256, 512, 512, 1024, 1024)\n",
    "    depth_list: tuple[int, ...] = (2, 2, 2, 2, 2, 2)\n",
    "    block_type: Any = \"ResBlock\"\n",
    "    norm: str = \"rms2d\"\n",
    "    act: str = \"silu\"\n",
    "    downsample_block_type: str = \"ConvPixelUnshuffle\"\n",
    "    downsample_match_channel: bool = True\n",
    "    downsample_shortcut: Optional[str] = \"averaging\"\n",
    "    out_norm: Optional[str] = None\n",
    "    out_act: Optional[str] = None\n",
    "    out_shortcut: Optional[str] = \"averaging\"\n",
    "    double_latent: bool = False\n",
    "    is_video: bool = False\n",
    "    temporal_downsample: tuple[bool, ...] = ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a315d020",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DecoderConfig:\n",
    "    in_channels: int = MISSING\n",
    "    latent_channels: int = MISSING\n",
    "    in_shortcut: Optional[str] = \"duplicating\"\n",
    "    width_list: tuple[int, ...] = (128, 256, 512, 512, 1024, 1024)\n",
    "    depth_list: tuple[int, ...] = (2, 2, 2, 2, 2, 2)\n",
    "    block_type: Any = \"ResBlock\"\n",
    "    norm: Any = \"rms2d\"\n",
    "    act: Any = \"silu\"\n",
    "    upsample_block_type: str = \"ConvPixelShuffle\"\n",
    "    upsample_match_channel: bool = True\n",
    "    upsample_shortcut: str = \"duplicating\"\n",
    "    out_norm: str = \"rms2d\"\n",
    "    out_act: str = \"relu\"\n",
    "    is_video: bool = False\n",
    "    temporal_upsample: tuple[bool, ...] = ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a50506",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DCAEConfig:\n",
    "    in_channels: int = 3\n",
    "    latent_channels: int = 32\n",
    "    time_compression_ratio: int = 1\n",
    "    spatial_compression_ratio: int = 32\n",
    "    encoder: EncoderConfig = field(\n",
    "        default_factory=lambda: EncoderConfig(in_channels=\"${..in_channels}\", latent_channels=\"${..latent_channels}\")\n",
    "    )\n",
    "    decoder: DecoderConfig = field(\n",
    "        default_factory=lambda: DecoderConfig(in_channels=\"${..in_channels}\", latent_channels=\"${..latent_channels}\")\n",
    "    )\n",
    "    use_quant_conv: bool = False\n",
    "\n",
    "    pretrained_path: Optional[str] = None\n",
    "    pretrained_source: str = \"dc-ae\"\n",
    "\n",
    "    scaling_factor: Optional[float] = None\n",
    "    is_image_model: bool = False\n",
    "\n",
    "    is_training: bool = False  # NOTE: set to True in vae train config\n",
    "\n",
    "    use_spatial_tiling: bool = False\n",
    "    use_temporal_tiling: bool = False\n",
    "    spatial_tile_size: int = 256\n",
    "    temporal_tile_size: int = 32\n",
    "    tile_overlap_factor: float = 0.25\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c2122b",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ed0c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val2tuple(x: Union[list, tuple, Any], min_len: int = 1, idx_repeat: int = -1) -> tuple:\n",
    "    x = val2list(x)\n",
    "\n",
    "    # repeat elements if necessary\n",
    "    if len(x) > 0:\n",
    "        x[idx_repeat:idx_repeat] = [x[idx_repeat] for _ in range(min_len - len(x))]\n",
    "\n",
    "    return tuple(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c352dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val2list(x: Union[list, tuple, Any], repeat_time=1) -> list:\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return list(x)\n",
    "    return [x for _ in range(repeat_time)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5208b6",
   "metadata": {},
   "source": [
    "#### Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f3527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register normalization function here\n",
    "REGISTERED_NORM_DICT: dict[str, type] = {\n",
    "    \"bn2d\": nn.BatchNorm2d,\n",
    "    \"ln\": nn.LayerNorm,\n",
    "    # \"ln2d\": LayerNorm2d,\n",
    "    # \"rms2d\": RMSNorm2d,\n",
    "    # \"rms3d\": RMSNorm3d,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fd9c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_kwargs_from_config(config: dict, target_func: Callable) -> dict[str, Any]:\n",
    "    valid_keys = list(signature(target_func).parameters)\n",
    "    kwargs = {}\n",
    "    for key in config:\n",
    "        if key in valid_keys:\n",
    "            kwargs[key] = config[key]\n",
    "    return kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4289a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_norm(name=\"bn2d\", num_features=None, **kwargs) -> Optional[nn.Module]:\n",
    "    if name in [\"ln\", \"ln2d\"]:\n",
    "        kwargs[\"normalized_shape\"] = num_features\n",
    "    else:\n",
    "        kwargs[\"num_features\"] = num_features\n",
    "    if name in REGISTERED_NORM_DICT:\n",
    "        norm_cls = REGISTERED_NORM_DICT[name]\n",
    "        args = build_kwargs_from_config(kwargs, norm_cls)\n",
    "        return norm_cls(**args)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc58644",
   "metadata": {},
   "source": [
    "#### ACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e410569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register activation function here\n",
    "REGISTERED_ACT_DICT: dict[str, type] = {\n",
    "    \"relu\": nn.ReLU,\n",
    "    \"relu6\": nn.ReLU6,\n",
    "    \"hswish\": nn.Hardswish,\n",
    "    \"silu\": nn.SiLU,\n",
    "    \"gelu\": partial(nn.GELU, approximate=\"tanh\"),\n",
    "}\n",
    "\n",
    "\n",
    "def build_act(name: str, **kwargs) -> Optional[nn.Module]:\n",
    "    if name in REGISTERED_ACT_DICT:\n",
    "        act_cls = REGISTERED_ACT_DICT[name]\n",
    "        args = build_kwargs_from_config(kwargs, act_cls)\n",
    "        return act_cls(**args)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dcb2b1",
   "metadata": {},
   "source": [
    "#### stage main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a64ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_block(\n",
    "    block_type: str, in_channels: int, out_channels: int, norm: Optional[str], act: Optional[str], is_video: bool\n",
    ") -> nn.Module:\n",
    "    if block_type == \"ResBlock\":\n",
    "        assert in_channels == out_channels\n",
    "        main_block = ResBlock(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            use_bias=(True, False),\n",
    "            norm=(None, norm),\n",
    "            act_func=(act, None),\n",
    "            is_video=is_video,\n",
    "        )\n",
    "        block = ResidualBlock(main_block, IdentityLayer())\n",
    "    elif block_type == \"EViT_GLU\":\n",
    "        assert in_channels == out_channels\n",
    "        block = EfficientViTBlock(\n",
    "            in_channels, norm=norm, act_func=act, local_module=\"GLUMBConv\", scales=(), is_video=is_video\n",
    "        )\n",
    "    elif block_type == \"EViTS5_GLU\":\n",
    "        assert in_channels == out_channels\n",
    "        block = EfficientViTBlock(\n",
    "            in_channels, norm=norm, act_func=act, local_module=\"GLUMBConv\", scales=(5,), is_video=is_video\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"block_type {block_type} is not supported\")\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25401409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_stage_main(\n",
    "    width: int, depth: int, block_type: str | list[str], norm: str, act: str, input_width: int, is_video: bool\n",
    ") -> list[nn.Module]:\n",
    "    assert isinstance(block_type, str) or (isinstance(block_type, list) and depth == len(block_type))\n",
    "    stage = []\n",
    "    for d in range(depth):\n",
    "        current_block_type = block_type[d] if isinstance(block_type, list) else block_type\n",
    "        block = build_block(\n",
    "            block_type=current_block_type,\n",
    "            in_channels=width if d > 0 else input_width,\n",
    "            out_channels=width,\n",
    "            norm=norm,\n",
    "            act=act,\n",
    "            is_video=is_video,\n",
    "        )\n",
    "        stage.append(block)\n",
    "    return stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7188dbd0",
   "metadata": {},
   "source": [
    "#### BasicLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca76b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ChannelChunkConv3d(nn.Conv3d):\n",
    "    CONV3D_NUMEL_LIMIT = 2**31\n",
    "\n",
    "    def _get_output_numel(self, input_shape: torch.Size) -> int:\n",
    "        numel = self.out_channels\n",
    "        if len(input_shape) == 5:\n",
    "            numel *= input_shape[0]\n",
    "        for i, d in enumerate(input_shape[-3:]):\n",
    "            d_out = math.floor(\n",
    "                (d + 2 * self.padding[i] - self.dilation[i] * (self.kernel_size[i] - 1) - 1) / self.stride[i] + 1\n",
    "            )\n",
    "            numel *= d_out\n",
    "        return numel\n",
    "\n",
    "    def _get_n_chunks(self, numel: int, n_channels: int):\n",
    "        n_chunks = math.ceil(numel / ChannelChunkConv3d.CONV3D_NUMEL_LIMIT)\n",
    "        n_chunks = ceil_to_divisible(n_chunks, n_channels)\n",
    "        return n_chunks\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        if input.numel() // input.size(0) < ChannelChunkConv3d.CONV3D_NUMEL_LIMIT:\n",
    "            return super().forward(input)\n",
    "        n_in_chunks = self._get_n_chunks(input.numel(), self.in_channels)\n",
    "        n_out_chunks = self._get_n_chunks(self._get_output_numel(input.shape), self.out_channels)\n",
    "        if n_in_chunks == 1 and n_out_chunks == 1:\n",
    "            return super().forward(input)\n",
    "        outputs = []\n",
    "        input_shards = input.chunk(n_in_chunks, dim=1)\n",
    "        for weight, bias in zip(self.weight.chunk(n_out_chunks), self.bias.chunk(n_out_chunks)):\n",
    "            weight_shards = weight.chunk(n_in_chunks, dim=1)\n",
    "            o = None\n",
    "            for x, w in zip(input_shards, weight_shards):\n",
    "                if o is None:\n",
    "                    o = F.conv3d(x, w, bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "                else:\n",
    "                    o += F.conv3d(x, w, None, self.stride, self.padding, self.dilation, self.groups)\n",
    "            outputs.append(o)\n",
    "        return torch.cat(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1339b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        dilation=1,\n",
    "        groups=1,\n",
    "        use_bias=False,\n",
    "        dropout=0,\n",
    "        norm=\"bn2d\",\n",
    "        act_func=\"relu\",\n",
    "        is_video=False,\n",
    "        pad_mode_3d=\"constant\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.is_video = is_video\n",
    "\n",
    "        if self.is_video:\n",
    "            assert dilation == 1, \"only support dilation=1 for 3d conv\"\n",
    "            assert kernel_size % 2 == 1, \"only support odd kernel size for 3d conv\"\n",
    "            self.pad_mode_3d = pad_mode_3d  # 3d padding follows CausalConv3d by Hunyuan\n",
    "            # padding = (\n",
    "            #     kernel_size // 2,\n",
    "            #     kernel_size // 2,\n",
    "            #     kernel_size // 2,\n",
    "            #     kernel_size // 2,\n",
    "            #     kernel_size - 1,\n",
    "            #     0,\n",
    "            # )  # W, H, T\n",
    "            # non-causal padding\n",
    "            padding = (\n",
    "                kernel_size // 2,\n",
    "                kernel_size // 2,\n",
    "                kernel_size // 2,\n",
    "                kernel_size // 2,\n",
    "                kernel_size // 2,\n",
    "                kernel_size // 2,\n",
    "            )\n",
    "            self.padding = padding\n",
    "            self.dropout = nn.Dropout3d(dropout, inplace=False) if dropout > 0 else None\n",
    "            assert isinstance(stride, (int, tuple)), \"stride must be an integer or 3-tuple for 3d conv\"\n",
    "            self.conv = ChannelChunkConv3d(  # padding is handled by F.pad() in forward()\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=(kernel_size, kernel_size, kernel_size),\n",
    "                stride=(stride, stride, stride) if isinstance(stride, int) else stride,\n",
    "                groups=groups,\n",
    "                bias=use_bias,\n",
    "            )\n",
    "        else:\n",
    "            padding = get_same_padding(kernel_size)\n",
    "            padding *= dilation\n",
    "            self.dropout = nn.Dropout2d(dropout, inplace=False) if dropout > 0 else None\n",
    "            self.conv = nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=(kernel_size, kernel_size),\n",
    "                stride=(stride, stride),\n",
    "                padding=padding,\n",
    "                dilation=(dilation, dilation),\n",
    "                groups=groups,\n",
    "                bias=use_bias,\n",
    "            )\n",
    "\n",
    "        self.norm = build_norm(norm, num_features=out_channels)\n",
    "        self.act = build_act(act_func)\n",
    "        self.pad = F.pad\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "        if self.is_video:  # custom padding for 3d conv\n",
    "            x = self.pad(x, self.padding, mode=self.pad_mode_3d)  # \"constant\" padding defaults to 0\n",
    "        x = self.conv(x)\n",
    "        if self.norm:\n",
    "            x = self.norm(x)\n",
    "        if self.act:\n",
    "            x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02842da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelUnshuffleChannelAveragingDownSampleLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        factor: int,\n",
    "        temporal_downsample: bool = False,  # temporal downsample for 5d input tensor\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.factor = factor\n",
    "        self.temporal_downsample = temporal_downsample\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if x.dim() == 4:\n",
    "            assert self.in_channels * self.factor**2 % self.out_channels == 0\n",
    "            group_size = self.in_channels * self.factor**2 // self.out_channels\n",
    "            x = F.pixel_unshuffle(x, self.factor)\n",
    "            B, C, H, W = x.shape\n",
    "            x = x.view(B, self.out_channels, group_size, H, W)\n",
    "            x = x.mean(dim=2)\n",
    "        elif x.dim() == 5:  # [B, C, T, H, W]\n",
    "            _, _, T, _, _ = x.shape\n",
    "            if self.temporal_downsample and T != 1:  # 3d pixel unshuffle\n",
    "                x = pixel_unshuffle_3d(x, self.factor)\n",
    "                assert self.in_channels * self.factor**3 % self.out_channels == 0\n",
    "                group_size = self.in_channels * self.factor**3 // self.out_channels\n",
    "            else:  # 2d pixel unshuffle\n",
    "                x = x.permute(0, 2, 1, 3, 4)  # [B, T, C, H, W]\n",
    "                x = F.pixel_unshuffle(x, self.factor)\n",
    "                x = x.permute(0, 2, 1, 3, 4)  # [B, C, T, H, W]\n",
    "                assert self.in_channels * self.factor**2 % self.out_channels == 0\n",
    "                group_size = self.in_channels * self.factor**2 // self.out_channels\n",
    "            B, C, T, H, W = x.shape\n",
    "            x = x.view(B, self.out_channels, group_size, T, H, W)\n",
    "            x = x.mean(dim=2)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported input dimension: {x.dim()}\")\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"PixelUnshuffleChannelAveragingDownSampleLayer(in_channels={self.in_channels}, out_channels={self.out_channels}, factor={self.factor}), temporal_downsample={self.temporal_downsample}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774011cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_same_padding(kernel_size: Union[int, tuple[int, ...]]) -> Union[int, tuple[int, ...]]:\n",
    "    if isinstance(kernel_size, tuple):\n",
    "        return tuple([get_same_padding(ks) for ks in kernel_size])\n",
    "    else:\n",
    "        assert kernel_size % 2 > 0, \"kernel size should be odd number\"\n",
    "        return kernel_size // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2013d84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiteMLA(nn.Module):\n",
    "    r\"\"\"Lightweight multi-scale linear attention\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        heads: Optional[int] = None,\n",
    "        heads_ratio: float = 1.0,\n",
    "        dim=8,\n",
    "        use_bias=False,\n",
    "        norm=(None, \"bn2d\"),\n",
    "        act_func=(None, None),\n",
    "        kernel_func=\"relu\",\n",
    "        scales: tuple[int, ...] = (5,),\n",
    "        eps=1.0e-15,\n",
    "        is_video=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        heads = int(in_channels // dim * heads_ratio) if heads is None else heads\n",
    "\n",
    "        total_dim = heads * dim\n",
    "\n",
    "        use_bias = val2tuple(use_bias, 2)\n",
    "        norm = val2tuple(norm, 2)\n",
    "        act_func = val2tuple(act_func, 2)\n",
    "\n",
    "        self.dim = dim\n",
    "        self.qkv = ConvLayer(\n",
    "            in_channels,\n",
    "            3 * total_dim,\n",
    "            1,\n",
    "            use_bias=use_bias[0],\n",
    "            norm=norm[0],\n",
    "            act_func=act_func[0],\n",
    "            is_video=is_video,\n",
    "        )\n",
    "        conv_class = nn.Conv2d if not is_video else ChannelChunkConv3d\n",
    "        self.aggreg = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    conv_class(\n",
    "                        3 * total_dim,\n",
    "                        3 * total_dim,\n",
    "                        scale,\n",
    "                        padding=get_same_padding(scale),\n",
    "                        groups=3 * total_dim,\n",
    "                        bias=use_bias[0],\n",
    "                    ),\n",
    "                    conv_class(3 * total_dim, 3 * total_dim, 1, groups=3 * heads, bias=use_bias[0]),\n",
    "                )\n",
    "                for scale in scales\n",
    "            ]\n",
    "        )\n",
    "        self.kernel_func = build_act(kernel_func, inplace=False)\n",
    "\n",
    "        self.proj = ConvLayer(\n",
    "            total_dim * (1 + len(scales)),\n",
    "            out_channels,\n",
    "            1,\n",
    "            use_bias=use_bias[1],\n",
    "            norm=norm[1],\n",
    "            act_func=act_func[1],\n",
    "            is_video=is_video,\n",
    "        )\n",
    "\n",
    "    @torch.autocast(device_type=\"cuda\", enabled=False)\n",
    "    def relu_linear_att(self, qkv: torch.Tensor) -> torch.Tensor:\n",
    "        if qkv.ndim == 5:\n",
    "            B, _, T, H, W = list(qkv.size())\n",
    "            is_video = True\n",
    "        else:\n",
    "            B, _, H, W = list(qkv.size())\n",
    "            is_video = False\n",
    "\n",
    "        if qkv.dtype == torch.float16:\n",
    "            qkv = qkv.float()\n",
    "\n",
    "        if qkv.ndim == 4:\n",
    "            qkv = torch.reshape(\n",
    "                qkv,\n",
    "                (\n",
    "                    B,\n",
    "                    -1,\n",
    "                    3 * self.dim,\n",
    "                    H * W,\n",
    "                ),\n",
    "            )\n",
    "        elif qkv.ndim == 5:\n",
    "            qkv = torch.reshape(\n",
    "                qkv,\n",
    "                (\n",
    "                    B,\n",
    "                    -1,\n",
    "                    3 * self.dim,\n",
    "                    H * W * T,\n",
    "                ),\n",
    "            )\n",
    "        q, k, v = (\n",
    "            qkv[:, :, 0 : self.dim],\n",
    "            qkv[:, :, self.dim : 2 * self.dim],\n",
    "            qkv[:, :, 2 * self.dim :],\n",
    "        )\n",
    "\n",
    "        # lightweight linear attention\n",
    "        q = self.kernel_func(q)\n",
    "        k = self.kernel_func(k)\n",
    "\n",
    "        # linear matmul\n",
    "        trans_k = k.transpose(-1, -2)\n",
    "\n",
    "        v = F.pad(v, (0, 0, 0, 1), mode=\"constant\", value=1)\n",
    "        vk = torch.matmul(v, trans_k)\n",
    "        out = torch.matmul(vk, q)\n",
    "        if out.dtype == torch.bfloat16:\n",
    "            out = out.float()\n",
    "        out = out[:, :, :-1] / (out[:, :, -1:] + self.eps)\n",
    "\n",
    "        if not is_video:\n",
    "            out = torch.reshape(out, (B, -1, H, W))\n",
    "        else:\n",
    "            out = torch.reshape(out, (B, -1, T, H, W))\n",
    "        return out\n",
    "\n",
    "    @torch.autocast(device_type=\"cuda\", enabled=False)\n",
    "    def relu_quadratic_att(self, qkv: torch.Tensor) -> torch.Tensor:\n",
    "        B, _, H, W = list(qkv.size())\n",
    "\n",
    "        qkv = torch.reshape(\n",
    "            qkv,\n",
    "            (\n",
    "                B,\n",
    "                -1,\n",
    "                3 * self.dim,\n",
    "                H * W,\n",
    "            ),\n",
    "        )\n",
    "        q, k, v = (\n",
    "            qkv[:, :, 0 : self.dim],\n",
    "            qkv[:, :, self.dim : 2 * self.dim],\n",
    "            qkv[:, :, 2 * self.dim :],\n",
    "        )\n",
    "\n",
    "        q = self.kernel_func(q)\n",
    "        k = self.kernel_func(k)\n",
    "\n",
    "        att_map = torch.matmul(k.transpose(-1, -2), q)  # b h n n\n",
    "        original_dtype = att_map.dtype\n",
    "        if original_dtype in [torch.float16, torch.bfloat16]:\n",
    "            att_map = att_map.float()\n",
    "        att_map = att_map / (torch.sum(att_map, dim=2, keepdim=True) + self.eps)  # b h n n\n",
    "        att_map = att_map.to(original_dtype)\n",
    "        out = torch.matmul(v, att_map)  # b h d n\n",
    "\n",
    "        out = torch.reshape(out, (B, -1, H, W))\n",
    "        return out\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # generate multi-scale q, k, v\n",
    "        qkv = self.qkv(x)\n",
    "        multi_scale_qkv = [qkv]\n",
    "        for op in self.aggreg:\n",
    "            multi_scale_qkv.append(op(qkv))\n",
    "        qkv = torch.cat(multi_scale_qkv, dim=1)\n",
    "\n",
    "        if qkv.ndim == 4:\n",
    "            H, W = list(qkv.size())[-2:]\n",
    "            # num_tokens = H * W\n",
    "        elif qkv.ndim == 5:\n",
    "            _, _, T, H, W = list(qkv.size())\n",
    "            # num_tokens = H * W * T\n",
    "\n",
    "        # if num_tokens > self.dim:\n",
    "        out = self.relu_linear_att(qkv).to(qkv.dtype)\n",
    "        # else:\n",
    "        #     if self.is_video:\n",
    "        #         raise NotImplementedError(\"Video is not supported for quadratic attention\")\n",
    "        #     out = self.relu_quadratic_att(qkv)\n",
    "        out = self.proj(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53e3875",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientViTBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        heads_ratio: float = 1.0,\n",
    "        dim=32,\n",
    "        expand_ratio: float = 4,\n",
    "        scales: tuple[int, ...] = (5,),\n",
    "        norm: str = \"bn2d\",\n",
    "        act_func: str = \"hswish\",\n",
    "        context_module: str = \"LiteMLA\",\n",
    "        local_module: str = \"MBConv\",\n",
    "        is_video: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if context_module == \"LiteMLA\":\n",
    "            self.context_module = ResidualBlock(\n",
    "                LiteMLA(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=in_channels,\n",
    "                    heads_ratio=heads_ratio,\n",
    "                    dim=dim,\n",
    "                    norm=(None, norm),\n",
    "                    scales=scales,\n",
    "                    is_video=is_video,\n",
    "                ),\n",
    "                IdentityLayer(),\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"context_module {context_module} is not supported\")\n",
    "        if local_module == \"MBConv\":\n",
    "            self.local_module = ResidualBlock(\n",
    "                MBConv(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=in_channels,\n",
    "                    expand_ratio=expand_ratio,\n",
    "                    use_bias=(True, True, False),\n",
    "                    norm=(None, None, norm),\n",
    "                    act_func=(act_func, act_func, None),\n",
    "                    is_video=is_video,\n",
    "                ),\n",
    "                IdentityLayer(),\n",
    "            )\n",
    "        elif local_module == \"GLUMBConv\":\n",
    "            self.local_module = ResidualBlock(\n",
    "                GLUMBConv(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=in_channels,\n",
    "                    expand_ratio=expand_ratio,\n",
    "                    use_bias=(True, True, False),\n",
    "                    norm=(None, None, norm),\n",
    "                    act_func=(act_func, act_func, None),\n",
    "                    is_video=is_video,\n",
    "                ),\n",
    "                IdentityLayer(),\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(f\"local_module {local_module} is not supported\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.context_module(x)\n",
    "        x = self.local_module(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baf4dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelDuplicatingPixelShuffleUpSampleLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        factor: int,\n",
    "        temporal_upsample: bool = False,  # upsample on the temporal dimension as well\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.factor = factor\n",
    "        assert out_channels * factor**2 % in_channels == 0\n",
    "        self.temporal_upsample = temporal_upsample\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if x.dim() == 5:\n",
    "            B, C, T, H, W = x.shape\n",
    "            assert C == self.in_channels\n",
    "\n",
    "        if self.temporal_upsample and T != 1:  # video input\n",
    "            repeats = self.out_channels * self.factor**3 // self.in_channels\n",
    "        else:\n",
    "            repeats = self.out_channels * self.factor**2 // self.in_channels\n",
    "\n",
    "        x = x.repeat_interleave(repeats, dim=1)\n",
    "\n",
    "        if x.dim() == 4:  # original image-only training\n",
    "            x = F.pixel_shuffle(x, self.factor)\n",
    "        elif x.dim() == 5:  # [B, C, T, H, W]\n",
    "            if self.temporal_upsample and T != 1:  # video input\n",
    "                x = pixel_shuffle_3d(x, self.factor)\n",
    "            else:\n",
    "                x = x.permute(0, 2, 1, 3, 4)  # [B, T, C, H, W]\n",
    "                x = F.pixel_shuffle(x, self.factor)  # on H and W only\n",
    "                x = x.permute(0, 2, 1, 3, 4)  # [B, C, T, H, W]\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"ChannelDuplicatingPixelShuffleUpSampleLayer(in_channels={self.in_channels}, out_channels={self.out_channels}, factor={self.factor}, temporal_upsample={self.temporal_upsample})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad296cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39251b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunked_interpolate(x, scale_factor, mode=\"nearest\"):\n",
    "    \"\"\"\n",
    "    Interpolate large tensors by chunking along the channel dimension. https://discuss.pytorch.org/t/error-using-f-interpolate-for-large-3d-input/207859\n",
    "    Only supports 'nearest' interpolation mode.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Input tensor (B, C, D, H, W)\n",
    "        scale_factor: Tuple of scaling factors (d, h, w)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Interpolated tensor\n",
    "    \"\"\"\n",
    "    assert (\n",
    "        mode == \"nearest\"\n",
    "    ), \"Only the nearest mode is supported\"  # actually other modes are theoretically supported but not tested\n",
    "    if len(x.shape) != 5:\n",
    "        raise ValueError(\"Expected 5D input tensor (B, C, D, H, W)\")\n",
    "\n",
    "    # Calculate max chunk size to avoid int32 overflow. num_elements < max_int32\n",
    "    # Max int32 is 2^31 - 1\n",
    "    max_elements_per_chunk = 2**31 - 1\n",
    "\n",
    "    # Calculate output spatial dimensions\n",
    "    out_d = math.ceil(x.shape[2] * scale_factor[0])\n",
    "    out_h = math.ceil(x.shape[3] * scale_factor[1])\n",
    "    out_w = math.ceil(x.shape[4] * scale_factor[2])\n",
    "\n",
    "    # Calculate max channels per chunk to stay under limit\n",
    "    elements_per_channel = out_d * out_h * out_w\n",
    "    max_channels = max_elements_per_chunk // (x.shape[0] * elements_per_channel)\n",
    "\n",
    "    # Use smaller of max channels or input channels\n",
    "    chunk_size = min(max_channels, x.shape[1])\n",
    "\n",
    "    # Ensure at least 1 channel per chunk\n",
    "    chunk_size = max(1, chunk_size)\n",
    "    if VERBOSE:\n",
    "        print(f\"Input channels: {x.shape[1]}\")\n",
    "        print(f\"Chunk size: {chunk_size}\")\n",
    "        print(f\"max_channels: {max_channels}\")\n",
    "        print(f\"num_chunks: {math.ceil(x.shape[1] / chunk_size)}\")\n",
    "\n",
    "    chunks = []\n",
    "    for i in range(0, x.shape[1], chunk_size):\n",
    "        start_idx = i\n",
    "        end_idx = min(i + chunk_size, x.shape[1])\n",
    "\n",
    "        chunk = x[:, start_idx:end_idx, :, :, :]\n",
    "\n",
    "        interpolated_chunk = F.interpolate(chunk, scale_factor=scale_factor, mode=\"nearest\")\n",
    "\n",
    "        chunks.append(interpolated_chunk)\n",
    "\n",
    "    if not chunks:\n",
    "        raise ValueError(f\"No chunks were generated. Input shape: {x.shape}\")\n",
    "\n",
    "    # Concatenate chunks along channel dimension\n",
    "    return torch.cat(chunks, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f96866d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpolateConvUpSampleLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int,\n",
    "        factor: int,\n",
    "        mode: str = \"nearest\",\n",
    "        is_video: bool = False,\n",
    "        temporal_upsample: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.factor = factor\n",
    "        self.mode = mode\n",
    "        self.temporal_upsample = temporal_upsample\n",
    "        self.conv = ConvLayer(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            use_bias=True,\n",
    "            norm=None,\n",
    "            act_func=None,\n",
    "            is_video=is_video,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if x.dim() == 4:\n",
    "            x = F.interpolate(x, scale_factor=self.factor, mode=self.mode)\n",
    "        elif x.dim() == 5:\n",
    "            # [B, C, T, H, W] -> [B, C, T*factor, H*factor, W*factor]\n",
    "            if self.temporal_upsample and x.size(2) != 1:  # temporal upsample for video input\n",
    "                x = chunked_interpolate(x, scale_factor=[self.factor, self.factor, self.factor], mode=self.mode)\n",
    "            else:\n",
    "                x = chunked_interpolate(x, scale_factor=[1, self.factor, self.factor], mode=self.mode)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"InterpolateConvUpSampleLayer(factor={self.factor}, mode={self.mode}, temporal_upsample={self.temporal_upsample})\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6717d723",
   "metadata": {},
   "source": [
    "#### Basic Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8788c4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityLayer(nn.Module):\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f023dc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        mid_channels=None,\n",
    "        expand_ratio=1,\n",
    "        use_bias=False,\n",
    "        norm=(\"bn2d\", \"bn2d\"),\n",
    "        act_func=(\"relu6\", None),\n",
    "        is_video=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        use_bias = val2tuple(use_bias, 2)\n",
    "        norm = val2tuple(norm, 2)\n",
    "        act_func = val2tuple(act_func, 2)\n",
    "\n",
    "        mid_channels = round(in_channels * expand_ratio) if mid_channels is None else mid_channels\n",
    "\n",
    "        self.conv1 = ConvLayer(\n",
    "            in_channels,\n",
    "            mid_channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            use_bias=use_bias[0],\n",
    "            norm=norm[0],\n",
    "            act_func=act_func[0],\n",
    "            is_video=is_video,\n",
    "        )\n",
    "        self.conv2 = ConvLayer(\n",
    "            mid_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            1,\n",
    "            use_bias=use_bias[1],\n",
    "            norm=norm[1],\n",
    "            act_func=act_func[1],\n",
    "            is_video=is_video,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95c9b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        main: Optional[nn.Module],\n",
    "        shortcut: Optional[nn.Module],\n",
    "        post_act=None,\n",
    "        pre_norm: Optional[nn.Module] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pre_norm = pre_norm\n",
    "        self.main = main\n",
    "        self.shortcut = shortcut\n",
    "        self.post_act = build_act(post_act)\n",
    "\n",
    "    def forward_main(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.pre_norm is None:\n",
    "            return self.main(x)\n",
    "        else:\n",
    "            return self.main(self.pre_norm(x))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.main is None:\n",
    "            res = x\n",
    "        elif self.shortcut is None:\n",
    "            res = self.forward_main(x)\n",
    "        else:\n",
    "            res = self.forward_main(x) + self.shortcut(x)\n",
    "            if self.post_act:\n",
    "                res = self.post_act(res)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cac3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpSequential(nn.Module):\n",
    "    def __init__(self, op_list: list[Optional[nn.Module]]):\n",
    "        super().__init__()\n",
    "        valid_op_list = []\n",
    "        for op in op_list:\n",
    "            if op is not None:\n",
    "                valid_op_list.append(op)\n",
    "        self.op_list = nn.ModuleList(valid_op_list)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for op in self.op_list:\n",
    "            x = op(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4e0313",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GLUMBConv(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        mid_channels=None,\n",
    "        expand_ratio=6,\n",
    "        use_bias=False,\n",
    "        norm=(None, None, \"ln2d\"),\n",
    "        act_func=(\"silu\", \"silu\", None),\n",
    "        is_video=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        use_bias = val2tuple(use_bias, 3)\n",
    "        norm = val2tuple(norm, 3)\n",
    "        act_func = val2tuple(act_func, 3)\n",
    "\n",
    "        mid_channels = round(in_channels * expand_ratio) if mid_channels is None else mid_channels\n",
    "\n",
    "        self.glu_act = build_act(act_func[1], inplace=False)\n",
    "        self.inverted_conv = ConvLayer(\n",
    "            in_channels,\n",
    "            mid_channels * 2,\n",
    "            1,\n",
    "            use_bias=use_bias[0],\n",
    "            norm=norm[0],\n",
    "            act_func=act_func[0],\n",
    "            is_video=is_video,\n",
    "        )\n",
    "        self.depth_conv = ConvLayer(\n",
    "            mid_channels * 2,\n",
    "            mid_channels * 2,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            groups=mid_channels * 2,\n",
    "            use_bias=use_bias[1],\n",
    "            norm=norm[1],\n",
    "            act_func=None,\n",
    "            is_video=is_video,\n",
    "        )\n",
    "        self.point_conv = ConvLayer(\n",
    "            mid_channels,\n",
    "            out_channels,\n",
    "            1,\n",
    "            use_bias=use_bias[2],\n",
    "            norm=norm[2],\n",
    "            act_func=act_func[2],\n",
    "            is_video=is_video,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.inverted_conv(x)\n",
    "        x = self.depth_conv(x)\n",
    "\n",
    "        x, gate = torch.chunk(x, 2, dim=1)\n",
    "        gate = self.glu_act(gate)\n",
    "        x = x * gate\n",
    "\n",
    "        x = self.point_conv(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b516c0",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac36316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_downsample_block(\n",
    "    block_type: str,\n",
    "    in_channels: int,\n",
    "    out_channels: int,\n",
    "    shortcut: Optional[str],\n",
    "    is_video: bool,\n",
    "    temporal_downsample: bool = False,\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Spatial downsample is always performed. Temporal downsample is optional.\n",
    "    \"\"\"\n",
    "\n",
    "    if block_type == \"Conv\":\n",
    "        if is_video:\n",
    "            if temporal_downsample:\n",
    "                stride = (2, 2, 2)\n",
    "            else:\n",
    "                stride = (1, 2, 2)\n",
    "        else:\n",
    "            stride = 2\n",
    "        block = ConvLayer(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            use_bias=True,\n",
    "            norm=None,\n",
    "            act_func=None,\n",
    "            is_video=is_video,\n",
    "        )\n",
    "    elif block_type == \"ConvPixelUnshuffle\":\n",
    "        if is_video:\n",
    "            raise NotImplementedError(\"ConvPixelUnshuffle downsample is not supported for video\")\n",
    "        block = ConvPixelUnshuffleDownSampleLayer(\n",
    "            in_channels=in_channels, out_channels=out_channels, kernel_size=3, factor=2\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"block_type {block_type} is not supported for downsampling\")\n",
    "    if shortcut is None:\n",
    "        pass\n",
    "    elif shortcut == \"averaging\":\n",
    "        shortcut_block = PixelUnshuffleChannelAveragingDownSampleLayer(\n",
    "            in_channels=in_channels, out_channels=out_channels, factor=2, temporal_downsample=temporal_downsample\n",
    "        )\n",
    "        block = ResidualBlock(block, shortcut_block)\n",
    "    else:\n",
    "        raise ValueError(f\"shortcut {shortcut} is not supported for downsample\")\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771ff37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder_project_in_block(\n",
    "    in_channels: int, out_channels: int, factor: int, downsample_block_type: str, is_video: bool\n",
    "):\n",
    "    if factor == 1:\n",
    "        block = ConvLayer(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            use_bias=True,\n",
    "            norm=None,\n",
    "            act_func=None,\n",
    "            is_video=is_video,\n",
    "        )\n",
    "    elif factor == 2:\n",
    "        if is_video:\n",
    "            raise NotImplementedError(\"Downsample during project_in is not supported for video\")\n",
    "        block = build_downsample_block(\n",
    "            block_type=downsample_block_type, in_channels=in_channels, out_channels=out_channels, shortcut=None\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"downsample factor {factor} is not supported for encoder project in\")\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44428bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder_project_out_block(\n",
    "    in_channels: int,\n",
    "    out_channels: int,\n",
    "    norm: Optional[str],\n",
    "    act: Optional[str],\n",
    "    shortcut: Optional[str],\n",
    "    is_video: bool,\n",
    "):\n",
    "    block = OpSequential(\n",
    "        [\n",
    "            build_norm(norm),\n",
    "            build_act(act),\n",
    "            ConvLayer(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                use_bias=True,\n",
    "                norm=None,\n",
    "                act_func=None,\n",
    "                is_video=is_video,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    if shortcut is None:\n",
    "        pass\n",
    "    elif shortcut == \"averaging\":\n",
    "        shortcut_block = PixelUnshuffleChannelAveragingDownSampleLayer(\n",
    "            in_channels=in_channels, out_channels=out_channels, factor=1\n",
    "        )\n",
    "        block = ResidualBlock(block, shortcut_block)\n",
    "    else:\n",
    "        raise ValueError(f\"shortcut {shortcut} is not supported for encoder project out\")\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d38cd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_grad_checkpoint(module, *args, **kwargs):\n",
    "    if getattr(module, \"grad_checkpointing\", False):\n",
    "        if not isinstance(module, Iterable):\n",
    "            return checkpoint(module, *args, use_reentrant=True, **kwargs)\n",
    "        gc_step = module[0].grad_checkpointing_step\n",
    "        return checkpoint_sequential(module, gc_step, *args, use_reentrant=False, **kwargs)\n",
    "    return module(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8abf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, cfg: EncoderConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        num_stages = len(cfg.width_list)\n",
    "        self.num_stages = num_stages\n",
    "        assert len(cfg.depth_list) == num_stages\n",
    "        assert len(cfg.width_list) == num_stages\n",
    "        assert isinstance(cfg.block_type, str) or (\n",
    "            isinstance(cfg.block_type, list) and len(cfg.block_type) == num_stages\n",
    "        )\n",
    "\n",
    "        self.project_in = build_encoder_project_in_block(\n",
    "            in_channels=cfg.in_channels,\n",
    "            out_channels=cfg.width_list[0] if cfg.depth_list[0] > 0 else cfg.width_list[1],\n",
    "            factor=1 if cfg.depth_list[0] > 0 else 2,\n",
    "            downsample_block_type=cfg.downsample_block_type,\n",
    "            is_video=cfg.is_video,\n",
    "        )\n",
    "\n",
    "        self.stages: list[OpSequential] = []\n",
    "        for stage_id, (width, depth) in enumerate(zip(cfg.width_list, cfg.depth_list)):\n",
    "            block_type = cfg.block_type[stage_id] if isinstance(cfg.block_type, list) else cfg.block_type\n",
    "            stage = build_stage_main(\n",
    "                width=width,\n",
    "                depth=depth,\n",
    "                block_type=block_type,\n",
    "                norm=cfg.norm,\n",
    "                act=cfg.act,\n",
    "                input_width=width,\n",
    "                is_video=cfg.is_video,\n",
    "            )\n",
    "\n",
    "            if stage_id < num_stages - 1 and depth > 0:\n",
    "                downsample_block = build_downsample_block(\n",
    "                    block_type=cfg.downsample_block_type,\n",
    "                    in_channels=width,\n",
    "                    out_channels=cfg.width_list[stage_id + 1] if cfg.downsample_match_channel else width,\n",
    "                    shortcut=cfg.downsample_shortcut,\n",
    "                    is_video=cfg.is_video,\n",
    "                    temporal_downsample=cfg.temporal_downsample[stage_id] if cfg.temporal_downsample != [] else False,\n",
    "                )\n",
    "                stage.append(downsample_block)\n",
    "            self.stages.append(OpSequential(stage))\n",
    "        self.stages = nn.ModuleList(self.stages)\n",
    "\n",
    "        self.project_out = build_encoder_project_out_block(\n",
    "            in_channels=cfg.width_list[-1],\n",
    "            out_channels=2 * cfg.latent_channels if cfg.double_latent else cfg.latent_channels,\n",
    "            norm=cfg.out_norm,\n",
    "            act=cfg.out_act,\n",
    "            shortcut=cfg.out_shortcut,\n",
    "            is_video=cfg.is_video,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.project_in(x)\n",
    "        # x = auto_grad_checkpoint(self.project_in, x)\n",
    "        for stage in self.stages:\n",
    "            if len(stage.op_list) == 0:\n",
    "                continue\n",
    "            x = auto_grad_checkpoint(stage, x)\n",
    "        # x = self.project_out(x)\n",
    "        x = auto_grad_checkpoint(self.project_out, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e844bb",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366f5c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decoder_project_in_block(in_channels: int, out_channels: int, shortcut: Optional[str], is_video: bool):\n",
    "    block = ConvLayer(\n",
    "        in_channels=in_channels,\n",
    "        out_channels=out_channels,\n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        use_bias=True,\n",
    "        norm=None,\n",
    "        act_func=None,\n",
    "        is_video=is_video,\n",
    "    )\n",
    "    if shortcut is None:\n",
    "        pass\n",
    "    elif shortcut == \"duplicating\":\n",
    "        shortcut_block = ChannelDuplicatingPixelShuffleUpSampleLayer(\n",
    "            in_channels=in_channels, out_channels=out_channels, factor=1\n",
    "        )\n",
    "        block = ResidualBlock(block, shortcut_block)\n",
    "    else:\n",
    "        raise ValueError(f\"shortcut {shortcut} is not supported for decoder project in\")\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ec9105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_upsample_block(\n",
    "    block_type: str,\n",
    "    in_channels: int,\n",
    "    out_channels: int,\n",
    "    shortcut: Optional[str],\n",
    "    is_video: bool,\n",
    "    temporal_upsample: bool = False,\n",
    ") -> nn.Module:\n",
    "    if block_type == \"ConvPixelShuffle\":\n",
    "        if is_video:\n",
    "            raise NotImplementedError(\"ConvPixelShuffle upsample is not supported for video\")\n",
    "        block = ConvPixelShuffleUpSampleLayer(\n",
    "            in_channels=in_channels, out_channels=out_channels, kernel_size=3, factor=2\n",
    "        )\n",
    "    elif block_type == \"InterpolateConv\":\n",
    "        block = InterpolateConvUpSampleLayer(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            factor=2,\n",
    "            is_video=is_video,\n",
    "            temporal_upsample=temporal_upsample,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"block_type {block_type} is not supported for upsampling\")\n",
    "    if shortcut is None:\n",
    "        pass\n",
    "    elif shortcut == \"duplicating\":\n",
    "        shortcut_block = ChannelDuplicatingPixelShuffleUpSampleLayer(\n",
    "            in_channels=in_channels, out_channels=out_channels, factor=2, temporal_upsample=temporal_upsample\n",
    "        )\n",
    "        block = ResidualBlock(block, shortcut_block)\n",
    "    else:\n",
    "        raise ValueError(f\"shortcut {shortcut} is not supported for upsample\")\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2080afad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decoder_project_out_block(\n",
    "    in_channels: int,\n",
    "    out_channels: int,\n",
    "    factor: int,\n",
    "    upsample_block_type: str,\n",
    "    norm: Optional[str],\n",
    "    act: Optional[str],\n",
    "    is_video: bool,\n",
    "):\n",
    "    layers: list[nn.Module] = [\n",
    "        build_norm(norm, in_channels),\n",
    "        build_act(act),\n",
    "    ]\n",
    "    if factor == 1:\n",
    "        layers.append(\n",
    "            ConvLayer(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                use_bias=True,\n",
    "                norm=None,\n",
    "                act_func=None,\n",
    "                is_video=is_video,\n",
    "            )\n",
    "        )\n",
    "    elif factor == 2:\n",
    "        if is_video:\n",
    "            raise NotImplementedError(\"Upsample during project_out is not supported for video\")\n",
    "        layers.append(\n",
    "            build_upsample_block(\n",
    "                block_type=upsample_block_type, in_channels=in_channels, out_channels=out_channels, shortcut=None\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"upsample factor {factor} is not supported for decoder project out\")\n",
    "    return OpSequential(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454056a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, cfg: DecoderConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        num_stages = len(cfg.width_list)\n",
    "        self.num_stages = num_stages\n",
    "        assert len(cfg.depth_list) == num_stages\n",
    "        assert len(cfg.width_list) == num_stages\n",
    "        assert isinstance(cfg.block_type, str) or (\n",
    "            isinstance(cfg.block_type, list) and len(cfg.block_type) == num_stages\n",
    "        )\n",
    "        assert isinstance(cfg.norm, str) or (isinstance(cfg.norm, list) and len(cfg.norm) == num_stages)\n",
    "        assert isinstance(cfg.act, str) or (isinstance(cfg.act, list) and len(cfg.act) == num_stages)\n",
    "\n",
    "        self.project_in = build_decoder_project_in_block(\n",
    "            in_channels=cfg.latent_channels,\n",
    "            out_channels=cfg.width_list[-1],\n",
    "            shortcut=cfg.in_shortcut,\n",
    "            is_video=cfg.is_video,\n",
    "        )\n",
    "\n",
    "        self.stages: list[OpSequential] = []\n",
    "        for stage_id, (width, depth) in reversed(list(enumerate(zip(cfg.width_list, cfg.depth_list)))):\n",
    "            stage = []\n",
    "            if stage_id < num_stages - 1 and depth > 0:\n",
    "                upsample_block = build_upsample_block(\n",
    "                    block_type=cfg.upsample_block_type,\n",
    "                    in_channels=cfg.width_list[stage_id + 1],\n",
    "                    out_channels=width if cfg.upsample_match_channel else cfg.width_list[stage_id + 1],\n",
    "                    shortcut=cfg.upsample_shortcut,\n",
    "                    is_video=cfg.is_video,\n",
    "                    temporal_upsample=cfg.temporal_upsample[stage_id] if cfg.temporal_upsample != [] else False,\n",
    "                )\n",
    "                stage.append(upsample_block)\n",
    "\n",
    "            block_type = cfg.block_type[stage_id] if isinstance(cfg.block_type, list) else cfg.block_type\n",
    "            norm = cfg.norm[stage_id] if isinstance(cfg.norm, list) else cfg.norm\n",
    "            act = cfg.act[stage_id] if isinstance(cfg.act, list) else cfg.act\n",
    "            stage.extend(\n",
    "                build_stage_main(\n",
    "                    width=width,\n",
    "                    depth=depth,\n",
    "                    block_type=block_type,\n",
    "                    norm=norm,\n",
    "                    act=act,\n",
    "                    input_width=(\n",
    "                        width if cfg.upsample_match_channel else cfg.width_list[min(stage_id + 1, num_stages - 1)]\n",
    "                    ),\n",
    "                    is_video=cfg.is_video,\n",
    "                )\n",
    "            )\n",
    "            self.stages.insert(0, OpSequential(stage))\n",
    "        self.stages = nn.ModuleList(self.stages)\n",
    "\n",
    "        self.project_out = build_decoder_project_out_block(\n",
    "            in_channels=cfg.width_list[0] if cfg.depth_list[0] > 0 else cfg.width_list[1],\n",
    "            out_channels=cfg.in_channels,\n",
    "            factor=1 if cfg.depth_list[0] > 0 else 2,\n",
    "            upsample_block_type=cfg.upsample_block_type,\n",
    "            norm=cfg.out_norm,\n",
    "            act=cfg.out_act,\n",
    "            is_video=cfg.is_video,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = auto_grad_checkpoint(self.project_in, x)\n",
    "        for stage in reversed(self.stages):\n",
    "            if len(stage.op_list) == 0:\n",
    "                continue\n",
    "            # x = stage(x)\n",
    "            x = auto_grad_checkpoint(stage, x)\n",
    "\n",
    "        if self.disc_off_grad_ckpt:\n",
    "            x = self.project_out(x)\n",
    "        else:\n",
    "            x = auto_grad_checkpoint(self.project_out, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e1fed3",
   "metadata": {},
   "source": [
    "#### DCAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d39f3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.batchnorm import _BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af7dd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_modules(model: Union[nn.Module, list[nn.Module]], init_type=\"trunc_normal\") -> None:\n",
    "    _DEFAULT_INIT_PARAM = {\"trunc_normal\": 0.02}\n",
    "\n",
    "    if isinstance(model, list):\n",
    "        for sub_module in model:\n",
    "            init_modules(sub_module, init_type)\n",
    "    else:\n",
    "        init_params = init_type.split(\"@\")\n",
    "        init_params = float(init_params[1]) if len(init_params) > 1 else None\n",
    "\n",
    "        if init_type.startswith(\"trunc_normal\"):\n",
    "            init_func = lambda param: nn.init.trunc_normal_(\n",
    "                param, std=(_DEFAULT_INIT_PARAM[\"trunc_normal\"] if init_params is None else init_params)\n",
    "            )\n",
    "        elif init_type.startswith(\"normal\"):\n",
    "            init_func = lambda param: nn.init.normal_(\n",
    "                param, std=(_DEFAULT_INIT_PARAM[\"trunc_normal\"] if init_params is None else init_params)\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        for m in model.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear, nn.ConvTranspose2d)):\n",
    "                init_func(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Embedding):\n",
    "                init_func(m.weight)\n",
    "            elif isinstance(m, (_BatchNorm, nn.GroupNorm, nn.LayerNorm)):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            else:\n",
    "                weight = getattr(m, \"weight\", None)\n",
    "                bias = getattr(m, \"bias\", None)\n",
    "                if isinstance(weight, torch.nn.Parameter):\n",
    "                    init_func(weight)\n",
    "                if isinstance(bias, torch.nn.Parameter):\n",
    "                    bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cea200",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCAE(nn.Module):\n",
    "    def __init__(self, cfg: DCAEConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.encoder = Encoder(cfg.encoder)\n",
    "        self.decoder = Decoder(cfg.decoder)\n",
    "        self.scaling_factor = cfg.scaling_factor\n",
    "        self.time_compression_ratio = cfg.time_compression_ratio\n",
    "        self.spatial_compression_ratio = cfg.spatial_compression_ratio\n",
    "        self.use_spatial_tiling = cfg.use_spatial_tiling\n",
    "        self.use_temporal_tiling = cfg.use_temporal_tiling\n",
    "        self.spatial_tile_size = cfg.spatial_tile_size\n",
    "        self.temporal_tile_size = cfg.temporal_tile_size\n",
    "        assert (\n",
    "            cfg.spatial_tile_size // cfg.spatial_compression_ratio\n",
    "        ), f\"spatial tile size {cfg.spatial_tile_size} must be divisible by spatial compression of {cfg.spatial_compression_ratio}\"\n",
    "        self.spatial_tile_latent_size = cfg.spatial_tile_size // cfg.spatial_compression_ratio\n",
    "        assert (\n",
    "            cfg.temporal_tile_size // cfg.time_compression_ratio\n",
    "        ), f\"temporal tile size {cfg.temporal_tile_size} must be divisible by temporal compression of {cfg.time_compression_ratio}\"\n",
    "        self.temporal_tile_latent_size = cfg.temporal_tile_size // cfg.time_compression_ratio\n",
    "        self.tile_overlap_factor = cfg.tile_overlap_factor\n",
    "        if self.cfg.pretrained_path is not None:\n",
    "            self.load_model()\n",
    "\n",
    "        self.to(torch.float32)\n",
    "        init_modules(self, init_type=\"trunc_normal\")\n",
    "\n",
    "    def load_model(self):\n",
    "        if self.cfg.pretrained_source == \"dc-ae\":\n",
    "            state_dict = torch.load(self.cfg.pretrained_path, map_location=\"cpu\", weights_only=True)[\"state_dict\"]\n",
    "            self.load_state_dict(state_dict)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def get_last_layer(self):\n",
    "        return self.decoder.project_out.op_list[2].conv.weight\n",
    "\n",
    "    # @property\n",
    "    # def spatial_compression_ratio(self) -> int:\n",
    "    #     return 2 ** (self.decoder.num_stages - 1)\n",
    "\n",
    "    def encode_single(self, x: torch.Tensor, is_video_encoder: bool = False) -> torch.Tensor:\n",
    "        assert x.shape[0] == 1\n",
    "        is_video = x.dim() == 5\n",
    "        if is_video and not is_video_encoder:\n",
    "            b, c, f, h, w = x.shape\n",
    "            x = x.permute(0, 2, 1, 3, 4).reshape(-1, c, h, w)\n",
    "        z = self.encoder(x)\n",
    "\n",
    "        if is_video and not is_video_encoder:\n",
    "            z = z.unsqueeze(dim=0).permute(0, 2, 1, 3, 4)\n",
    "\n",
    "        if self.scaling_factor is not None:\n",
    "            z = z / self.scaling_factor\n",
    "\n",
    "        return z\n",
    "\n",
    "    def _encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.cfg.is_training:\n",
    "            return self.encoder(x)\n",
    "        is_video_encoder = self.encoder.cfg.is_video if self.encoder.cfg.is_video is not None else False\n",
    "        x_ret = []\n",
    "        for i in range(x.shape[0]):\n",
    "            x_ret.append(self.encode_single(x[i : i + 1], is_video_encoder))\n",
    "        return torch.cat(x_ret, dim=0)\n",
    "\n",
    "    def blend_v(self, a: torch.Tensor, b: torch.Tensor, blend_extent: int) -> torch.Tensor:\n",
    "        blend_extent = min(a.shape[-2], b.shape[-2], blend_extent)\n",
    "        for y in range(blend_extent):\n",
    "            b[:, :, :, y, :] = a[:, :, :, -blend_extent + y, :] * (1 - y / blend_extent) + b[:, :, :, y, :] * (\n",
    "                y / blend_extent\n",
    "            )\n",
    "        return b\n",
    "\n",
    "    def blend_h(self, a: torch.Tensor, b: torch.Tensor, blend_extent: int) -> torch.Tensor:\n",
    "        blend_extent = min(a.shape[-1], b.shape[-1], blend_extent)\n",
    "        for x in range(blend_extent):\n",
    "            b[:, :, :, :, x] = a[:, :, :, :, -blend_extent + x] * (1 - x / blend_extent) + b[:, :, :, :, x] * (\n",
    "                x / blend_extent\n",
    "            )\n",
    "        return b\n",
    "\n",
    "    def blend_t(self, a: torch.Tensor, b: torch.Tensor, blend_extent: int) -> torch.Tensor:\n",
    "        blend_extent = min(a.shape[-3], b.shape[-3], blend_extent)\n",
    "        for x in range(blend_extent):\n",
    "            b[:, :, x, :, :] = a[:, :, -blend_extent + x, :, :] * (1 - x / blend_extent) + b[:, :, x, :, :] * (\n",
    "                x / blend_extent\n",
    "            )\n",
    "        return b\n",
    "\n",
    "    def spatial_tiled_encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        net_size = int(self.spatial_tile_size * (1 - self.tile_overlap_factor))\n",
    "        blend_extent = int(self.spatial_tile_latent_size * self.tile_overlap_factor)\n",
    "        row_limit = self.spatial_tile_latent_size - blend_extent\n",
    "\n",
    "        # Split video into tiles and encode them separately.\n",
    "        rows = []\n",
    "        for i in range(0, x.shape[-2], net_size):\n",
    "            row = []\n",
    "            for j in range(0, x.shape[-1], net_size):\n",
    "                tile = x[:, :, :, i : i + self.spatial_tile_size, j : j + self.spatial_tile_size]\n",
    "                tile = self._encode(tile)\n",
    "                row.append(tile)\n",
    "            rows.append(row)\n",
    "        result_rows = []\n",
    "        for i, row in enumerate(rows):\n",
    "            result_row = []\n",
    "            for j, tile in enumerate(row):\n",
    "                # blend the above tile and the left tile\n",
    "                # to the current tile and add the current tile to the result row\n",
    "                if i > 0:\n",
    "                    tile = self.blend_v(rows[i - 1][j], tile, blend_extent)\n",
    "                if j > 0:\n",
    "                    tile = self.blend_h(row[j - 1], tile, blend_extent)\n",
    "                result_row.append(tile[:, :, :, :row_limit, :row_limit])\n",
    "            result_rows.append(torch.cat(result_row, dim=-1))\n",
    "\n",
    "        return torch.cat(result_rows, dim=-2)\n",
    "\n",
    "    def temporal_tiled_encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        overlap_size = int(self.temporal_tile_size * (1 - self.tile_overlap_factor))\n",
    "        blend_extent = int(self.temporal_tile_latent_size * self.tile_overlap_factor)\n",
    "        t_limit = self.temporal_tile_latent_size - blend_extent\n",
    "\n",
    "        # Split the video into tiles and encode them separately.\n",
    "        row = []\n",
    "        for i in range(0, x.shape[2], overlap_size):\n",
    "            tile = x[:, :, i : i + self.temporal_tile_size, :, :]\n",
    "            if self.use_spatial_tiling and (\n",
    "                tile.shape[-1] > self.spatial_tile_size or tile.shape[-2] > self.spatial_tile_size\n",
    "            ):\n",
    "                tile = self.spatial_tiled_encode(tile)\n",
    "            else:\n",
    "                tile = self._encode(tile)\n",
    "            row.append(tile)\n",
    "        result_row = []\n",
    "        for i, tile in enumerate(row):\n",
    "            if i > 0:\n",
    "                tile = self.blend_t(row[i - 1], tile, blend_extent)\n",
    "            result_row.append(tile[:, :, :t_limit, :, :])\n",
    "\n",
    "        return torch.cat(result_row, dim=2)\n",
    "\n",
    "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.use_temporal_tiling and x.shape[2] > self.temporal_tile_size:\n",
    "            return self.temporal_tiled_encode(x)\n",
    "        elif self.use_spatial_tiling and (x.shape[-1] > self.spatial_tile_size or x.shape[-2] > self.spatial_tile_size):\n",
    "            return self.spatial_tiled_encode(x)\n",
    "        else:\n",
    "            return self._encode(x)\n",
    "\n",
    "    def spatial_tiled_decode(self, z: torch.FloatTensor) -> torch.Tensor:\n",
    "        net_size = int(self.spatial_tile_latent_size * (1 - self.tile_overlap_factor))\n",
    "        blend_extent = int(self.spatial_tile_size * self.tile_overlap_factor)\n",
    "        row_limit = self.spatial_tile_size - blend_extent\n",
    "\n",
    "        # Split z into overlapping tiles and decode them separately.\n",
    "        # The tiles have an overlap to avoid seams between tiles.\n",
    "        rows = []\n",
    "        for i in range(0, z.shape[-2], net_size):\n",
    "            row = []\n",
    "            for j in range(0, z.shape[-1], net_size):\n",
    "                tile = z[:, :, :, i : i + self.spatial_tile_latent_size, j : j + self.spatial_tile_latent_size]\n",
    "                decoded = self._decode(tile)\n",
    "                row.append(decoded)\n",
    "            rows.append(row)\n",
    "        result_rows = []\n",
    "        for i, row in enumerate(rows):\n",
    "            result_row = []\n",
    "            for j, tile in enumerate(row):\n",
    "                # blend the above tile and the left tile\n",
    "                # to the current tile and add the current tile to the result row\n",
    "                if i > 0:\n",
    "                    tile = self.blend_v(rows[i - 1][j], tile, blend_extent)\n",
    "                if j > 0:\n",
    "                    tile = self.blend_h(row[j - 1], tile, blend_extent)\n",
    "                result_row.append(tile[:, :, :, :row_limit, :row_limit])\n",
    "            result_rows.append(torch.cat(result_row, dim=-1))\n",
    "\n",
    "        return torch.cat(result_rows, dim=-2)\n",
    "\n",
    "    def temporal_tiled_decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        overlap_size = int(self.temporal_tile_latent_size * (1 - self.tile_overlap_factor))\n",
    "        blend_extent = int(self.temporal_tile_size * self.tile_overlap_factor)\n",
    "        t_limit = self.temporal_tile_size - blend_extent\n",
    "\n",
    "        row = []\n",
    "        for i in range(0, z.shape[2], overlap_size):\n",
    "            tile = z[:, :, i : i + self.temporal_tile_latent_size, :, :]\n",
    "            if self.use_spatial_tiling and (\n",
    "                tile.shape[-1] > self.spatial_tile_latent_size or tile.shape[-2] > self.spatial_tile_latent_size\n",
    "            ):\n",
    "                decoded = self.spatial_tiled_decode(tile)\n",
    "            else:\n",
    "                decoded = self._decode(tile)\n",
    "            row.append(decoded)\n",
    "        result_row = []\n",
    "        for i, tile in enumerate(row):\n",
    "            if i > 0:\n",
    "                tile = self.blend_t(row[i - 1], tile, blend_extent)\n",
    "            result_row.append(tile[:, :, :t_limit, :, :])\n",
    "\n",
    "        return torch.cat(result_row, dim=2)\n",
    "\n",
    "    def decode_single(self, z: torch.Tensor, is_video_decoder: bool = False) -> torch.Tensor:\n",
    "        assert z.shape[0] == 1\n",
    "        is_video = z.dim() == 5\n",
    "        if is_video and not is_video_decoder:\n",
    "            b, c, f, h, w = z.shape\n",
    "            z = z.permute(0, 2, 1, 3, 4).reshape(-1, c, h, w)\n",
    "        if self.scaling_factor is not None:\n",
    "            z = z * self.scaling_factor\n",
    "\n",
    "        x = self.decoder(z)\n",
    "\n",
    "        if is_video and not is_video_decoder:\n",
    "            x = x.unsqueeze(dim=0).permute(0, 2, 1, 3, 4)\n",
    "        return x\n",
    "\n",
    "    def _decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        if self.cfg.is_training:\n",
    "            return self.decoder(z)\n",
    "        is_video_decoder = self.decoder.cfg.is_video if self.decoder.cfg.is_video is not None else False\n",
    "        x_ret = []\n",
    "        for i in range(z.shape[0]):\n",
    "            x_ret.append(self.decode_single(z[i : i + 1], is_video_decoder))\n",
    "        return torch.cat(x_ret, dim=0)\n",
    "\n",
    "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        if self.use_temporal_tiling and z.shape[2] > self.temporal_tile_latent_size:\n",
    "            return self.temporal_tiled_decode(z)\n",
    "        elif self.use_spatial_tiling and (\n",
    "            z.shape[-1] > self.spatial_tile_latent_size or z.shape[-2] > self.spatial_tile_latent_size\n",
    "        ):\n",
    "            return self.spatial_tiled_decode(z)\n",
    "        else:\n",
    "            return self._decode(z)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[Any, Tensor, dict[Any, Any]]:\n",
    "        x_type = x.dtype\n",
    "        is_image_model = self.cfg.__dict__.get(\"is_image_model\", False)\n",
    "        x = x.to(self.encoder.project_in.conv.weight.dtype)\n",
    "\n",
    "        if is_image_model:\n",
    "            b, c, _, h, w = x.shape\n",
    "            x = x.permute(0, 2, 1, 3, 4).reshape(-1, c, h, w)\n",
    "\n",
    "        z = self.encode(x)\n",
    "        dec = self.decode(z)\n",
    "\n",
    "        if is_image_model:\n",
    "            dec = dec.reshape(b, 1, c, h, w).permute(0, 2, 1, 3, 4)\n",
    "            z = z.unsqueeze(dim=0).permute(0, 2, 1, 3, 4)\n",
    "\n",
    "        dec = dec.to(x_type)\n",
    "        return dec, None, z\n",
    "\n",
    "    def get_latent_size(self, input_size: list[int]) -> list[int]:\n",
    "        latent_size = []\n",
    "        # T\n",
    "        latent_size.append((input_size[0] - 1) // self.time_compression_ratio + 1)\n",
    "        # H, w\n",
    "        for i in range(1, 3):\n",
    "            latent_size.append((input_size[i] - 1) // self.spatial_compression_ratio + 1)\n",
    "        return latent_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63711488",
   "metadata": {},
   "source": [
    "### ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21018af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional\n",
    "\n",
    "import diffusers\n",
    "import torch\n",
    "from huggingface_hub import PyTorchModelHubMixin\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08c3d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dc_ae_f32(name: str, pretrained_path: str) -> DCAEConfig:\n",
    "    if name in [\"dc-ae-f32t4c128\"]:\n",
    "        cfg_str = (\n",
    "            \"time_compression_ratio=4 \"\n",
    "            \"spatial_compression_ratio=32 \"\n",
    "            \"encoder.block_type=[ResBlock,ResBlock,ResBlock,EViTS5_GLU,EViTS5_GLU,EViTS5_GLU] \"\n",
    "            \"encoder.width_list=[128,256,512,512,1024,1024] encoder.depth_list=[2,2,2,3,3,3] \"\n",
    "            \"encoder.downsample_block_type=Conv \"\n",
    "            \"encoder.norm=rms3d \"\n",
    "            \"encoder.is_video=True \"\n",
    "            \"decoder.block_type=[ResBlock,ResBlock,ResBlock,EViTS5_GLU,EViTS5_GLU,EViTS5_GLU] \"\n",
    "            \"decoder.width_list=[128,256,512,512,1024,1024] decoder.depth_list=[3,3,3,3,3,3] \"\n",
    "            \"decoder.upsample_block_type=InterpolateConv \"\n",
    "            \"decoder.norm=rms3d decoder.act=silu decoder.out_norm=rms3d \"\n",
    "            \"decoder.is_video=True \"\n",
    "            \"encoder.temporal_downsample=[False,False,False,True,True,False] \"\n",
    "            \"decoder.temporal_upsample=[False,False,False,True,True,False] \"\n",
    "            \"latent_channels=128\"\n",
    "        )  # make sure there is no trailing blankspace in the last line\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    cfg = OmegaConf.from_dotlist(cfg_str.split(\" \"))\n",
    "    cfg: DCAEConfig = OmegaConf.to_object(OmegaConf.merge(OmegaConf.structured(DCAEConfig), cfg))\n",
    "    cfg.pretrained_path = pretrained_path\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847c9fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGISTERED_DCAE_MODEL: dict[str, tuple[Callable, Optional[str]]] = {\n",
    "    \"dc-ae-f32t4c128\": (dc_ae_f32, None),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267e5839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dc_ae_model_cfg(name: str, pretrained_path: Optional[str] = None) -> DCAEConfig:\n",
    "    assert name in REGISTERED_DCAE_MODEL, f\"{name} is not supported\"\n",
    "    dc_ae_cls, default_pt_path = REGISTERED_DCAE_MODEL[name]\n",
    "    pretrained_path = default_pt_path if pretrained_path is None else pretrained_path\n",
    "    model_cfg = dc_ae_cls(name, pretrained_path)\n",
    "    return model_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8d3234",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCAE_HF(DCAE, PyTorchModelHubMixin):\n",
    "    def __init__(self, model_name: str):\n",
    "        cfg = create_dc_ae_model_cfg(model_name)\n",
    "        DCAE.__init__(self, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85f81ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @MODELS.register_module(\"dc_ae\")\n",
    "def DC_AE(\n",
    "    model_name: str,\n",
    "    device_map: str | torch.device = \"cuda\",\n",
    "    torch_dtype: torch.dtype = torch.bfloat16,\n",
    "    from_scratch: bool = False,\n",
    "    from_pretrained: str | None = None,\n",
    "    is_training: bool = False,\n",
    "    use_spatial_tiling: bool = False,\n",
    "    use_temporal_tiling: bool = False,\n",
    "    spatial_tile_size: int = 256,\n",
    "    temporal_tile_size: int = 32,\n",
    "    tile_overlap_factor: float = 0.25,\n",
    "    scaling_factor: float = None,\n",
    "    disc_off_grad_ckpt: bool = False,\n",
    ") -> DCAE_HF:\n",
    "    if not from_scratch:\n",
    "        model = DCAE_HF.from_pretrained(model_name).to(device_map, torch_dtype)\n",
    "    else:\n",
    "        model = DCAE_HF(model_name).to(device_map, torch_dtype)\n",
    "\n",
    "    if from_pretrained is not None:\n",
    "        model = load_checkpoint(model, from_pretrained, device_map=device_map)\n",
    "        print(f\"loaded dc_ae from ckpt path: {from_pretrained}\")\n",
    "\n",
    "    model.cfg.is_training = is_training\n",
    "    model.use_spatial_tiling = use_spatial_tiling\n",
    "    model.use_temporal_tiling = use_temporal_tiling\n",
    "    model.spatial_tile_size = spatial_tile_size\n",
    "    model.temporal_tile_size = temporal_tile_size\n",
    "    model.tile_overlap_factor = tile_overlap_factor\n",
    "    if scaling_factor is not None:\n",
    "        model.scaling_factor = scaling_factor\n",
    "    model.decoder.disc_off_grad_ckpt = disc_off_grad_ckpt\n",
    "    return model\n",
    "\n",
    "# {'type': 'dc_ae',\n",
    "#  'model_name': 'dc-ae-f32t4c128',\n",
    "#  'from_scratch': True,\n",
    "#  'from_pretrained': None}\n",
    "\n",
    "DC_AE(\n",
    "    model_name=\"dc-ae-f32t4c128\",\n",
    "    from_scratch=True,\n",
    "    from_pretrained=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fabd6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (batch_size, channels, frames, height, width)\n",
    "sample_input = torch.randn(1, 3, 1, 256, 256).to(\"cuda\")\n",
    "\n",
    "model = DC_AE(\n",
    "    model_name=\"dc-ae-f32t4c128\",\n",
    "    from_scratch=True,\n",
    "    from_pretrained=None,\n",
    "    is_training=False,\n",
    "    # use_spatial_tiling=True,\n",
    "    # use_temporal_tiling=True,\n",
    "    # spatial_tile_size=256,\n",
    "    # temporal_tile_size=16,\n",
    "    # tile_overlap_factor=0.25,\n",
    ")\n",
    "output = model(sample_input)\n",
    "print(output[0].shape)  # Decoded output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41770ec",
   "metadata": {},
   "source": [
    "## MMDiT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fe560f",
   "metadata": {},
   "source": [
    "### è¨“ç·´è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260212ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensora.utils.config import read_config, merge_args\n",
    "import argparse\n",
    "from mmengine.config import Config\n",
    "\n",
    "def parse_args(args) -> tuple[str, argparse.Namespace]:\n",
    "    \"\"\"\n",
    "    This function parses the command line arguments.\n",
    "\n",
    "    Returns:\n",
    "        tuple[str, argparse.Namespace]: The path to the configuration file and the command line arguments.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"config\", type=str, help=\"model config file path\")\n",
    "    args, unknown_args = parser.parse_known_args(args)\n",
    "    return args.config, unknown_args\n",
    "\n",
    "def parse_configs(args) -> Config:\n",
    "    \"\"\"\n",
    "    This function parses the configuration file and command line arguments.\n",
    "\n",
    "    Returns:\n",
    "        Config: The configuration object.\n",
    "    \"\"\"\n",
    "    config, args = parse_args(args)\n",
    "    cfg = read_config(config)\n",
    "    cfg = merge_args(cfg, args)\n",
    "    cfg.config_path = config\n",
    "\n",
    "    # hard-coded for spatial compression\n",
    "    if cfg.get(\"ae_spatial_compression\", None) is not None:\n",
    "        os.environ[\"AE_SPATIAL_COMPRESSION\"] = str(cfg.ae_spatial_compression)\n",
    "\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1047d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DC-AEã®è¨“ç·´\n",
    "\n",
    "# https://github.com/hpcaitech/Open-Sora/blob/main/docs/hcae.md\n",
    "\n",
    "# torchrun --nproc_per_node 8 scripts/vae/train.py configs/vae/train/video_dc_ae.py\n",
    "\n",
    "args_1 = [\n",
    "    \"configs/vae/train/video_dc_ae.py\",\n",
    "]\n",
    "\n",
    "# torchrun --nproc_per_node 8 scripts/vae/train.py configs/vae/train/video_dc_ae_disc.py --model.from_pretrained <model_ckpt>\n",
    "\n",
    "args_2 = [\n",
    "    \"configs/vae/train/video_dc_ae_disc.py\",\n",
    "    \"--model.from_pretrained\",\n",
    "    \"<model_ckpt>\"\n",
    "]\n",
    "\n",
    "# torchrun --nproc_per_node 8 scripts/diffusion/train.py configs/diffusion/train/stage1.py --dataset.data-path datasets/pexels_45k_necessary.csv\n",
    "\n",
    "args_3 = [\n",
    "    \"configs/diffusion/train/stage1.py\",\n",
    "    \"--dataset.data-path\",\n",
    "    \"datasets/pexels_45k_necessary.csv\"\n",
    "]\n",
    "\n",
    "cfg = parse_configs(args_3)\n",
    "cfg.model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
