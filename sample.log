ðŸŸ© Python 3.12.11
ðŸŸ© Fri Dec 19 17:55:48 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 4090        Off |   00000000:01:00.0 Off |                  Off |
|  0%   34C    P0             45W /  450W |     138MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

ðŸŸ© DCAEã‚’æ¤œè¨¼
ðŸŸ© DC_AE model_name='dc-ae-f32t4c128' device_map='cuda' torch_dtype=torch.bfloat16 from_scratch=True from_pretrained=None is_training=False use_spatial_tiling=True use_temporal_tiling=True spatial_tile_size=256 temporal_tile_size=16 tile_overlap_factor=0.25 scaling_factor=None disc_off_grad_ckpt=False
ðŸŸ© DCAE_HF.__init__ model_name='dc-ae-f32t4c128'
ðŸŸ© create_dc_ae_model_cfg name='dc-ae-f32t4c128' pretrained_path=None
ðŸŸ© DCAE.__init__ cfg=DCAEConfig(in_channels=3, latent_channels=128, time_compression_ratio=4, spatial_compression_ratio=32, encoder=EncoderConfig(in_channels=3, latent_channels=128, width_list=[128, 256, 512, 512, 1024, 1024], depth_list=[2, 2, 2, 3, 3, 3], block_type=['ResBlock', 'ResBlock', 'ResBlock', 'EViTS5_GLU', 'EViTS5_GLU', 'EViTS5_GLU'], norm='rms3d', act='silu', downsample_block_type='Conv', downsample_match_channel=True, downsample_shortcut='averaging', out_norm=None, out_act=None, out_shortcut='averaging', double_latent=False, is_video=True, temporal_downsample=[False, False, False, True, True, False]), decoder=DecoderConfig(in_channels=3, latent_channels=128, in_shortcut='duplicating', width_list=[128, 256, 512, 512, 1024, 1024], depth_list=[3, 3, 3, 3, 3, 3], block_type=['ResBlock', 'ResBlock', 'ResBlock', 'EViTS5_GLU', 'EViTS5_GLU', 'EViTS5_GLU'], norm='rms3d', act='silu', upsample_block_type='InterpolateConv', upsample_match_channel=True, upsample_shortcut='duplicating', out_norm='rms3d', out_act='relu', is_video=True, temporal_upsample=[False, False, False, True, True, False]), use_quant_conv=False, pretrained_path=None, pretrained_source='dc-ae', scaling_factor=None, is_image_model=False, is_training=False, use_spatial_tiling=False, use_temporal_tiling=False, spatial_tile_size=256, temporal_tile_size=32, tile_overlap_factor=0.25)
ðŸŸ© Encoder.__init__ cfg=EncoderConfig(in_channels=3, latent_channels=128, width_list=[128, 256, 512, 512, 1024, 1024], depth_list=[2, 2, 2, 3, 3, 3], block_type=['ResBlock', 'ResBlock', 'ResBlock', 'EViTS5_GLU', 'EViTS5_GLU', 'EViTS5_GLU'], norm='rms3d', act='silu', downsample_block_type='Conv', downsample_match_channel=True, downsample_shortcut='averaging', out_norm=None, out_act=None, out_shortcut='averaging', double_latent=False, is_video=True, temporal_downsample=[False, False, False, True, True, False])
ðŸŸ© build_encoder_project_in_block in_channels=3 out_channels=128 factor=1 downsample_block_type='Conv' is_video=True
ðŸŸ© ConvLayer.__init__ in_channels=3 out_channels=128 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=128 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© build_stage_main width=128 depth=2 block_type='ResBlock' norm='rms3d' act='silu' input_width=128 is_video=True
ðŸŸ© build_block block_type='ResBlock' in_channels=128 out_channels=128 norm='rms3d' act='silu' is_video=True
ðŸŸ© ResBlock.__init__ in_channels=128 out_channels=128 kernel_size=3 stride=1 mid_channels=None expand_ratio=1 use_bias=(True, False) norm=(None, 'rms3d') act_func=('silu', None) is_video=True
ðŸŸ© ConvLayer.__init__ in_channels=128 out_channels=128 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func='silu' is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=128 kwargs={}
ðŸŸ© build_act name='silu' kwargs={}
ðŸŸ© build_kwargs_from_config config={} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=128 out_channels=128 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=128 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 128} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=128, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=ResBlock(
  (conv1): ConvLayer(
    (conv): ChannelChunkConv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1))
    (act): SiLU()
  )
  (conv2): ConvLayer(
    (conv): ChannelChunkConv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© build_block block_type='ResBlock' in_channels=128 out_channels=128 norm='rms3d' act='silu' is_video=True
ðŸŸ© ResBlock.__init__ in_channels=128 out_channels=128 kernel_size=3 stride=1 mid_channels=None expand_ratio=1 use_bias=(True, False) norm=(None, 'rms3d') act_func=('silu', None) is_video=True
ðŸŸ© ConvLayer.__init__ in_channels=128 out_channels=128 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func='silu' is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=128 kwargs={}
ðŸŸ© build_act name='silu' kwargs={}
ðŸŸ© build_kwargs_from_config config={} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=128 out_channels=128 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=128 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 128} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=128, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=ResBlock(
  (conv1): ConvLayer(
    (conv): ChannelChunkConv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1))
    (act): SiLU()
  )
  (conv2): ConvLayer(
    (conv): ChannelChunkConv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© build_downsample_block block_type='Conv' in_channels=128 out_channels=256 shortcut='averaging' is_video=True temporal_downsample=False
ðŸŸ© ConvLayer.__init__ in_channels=128 out_channels=256 kernel_size=3 stride=(1, 2, 2) dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=256 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© PixelUnshuffleChannelAveragingDownSampleLayer.__init__ in_channels=128 out_channels=256 factor=2 temporal_downsample=False
ðŸŸ© ResidualBlock.__init__ main=ConvLayer(
  (conv): ChannelChunkConv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 2, 2))
) shortcut=PixelUnshuffleChannelAveragingDownSampleLayer(in_channels=128, out_channels=256, factor=2), temporal_downsample=False post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© OpSequential init with op_list=[ResidualBlock(
  (main): ResBlock(
    (conv1): ConvLayer(
      (conv): ChannelChunkConv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      (act): SiLU()
    )
    (conv2): ConvLayer(
      (conv): ChannelChunkConv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)
      (norm): RMSNorm3d()
    )
  )
  (shortcut): IdentityLayer()
), ResidualBlock(
  (main): ResBlock(
    (conv1): ConvLayer(
      (conv): ChannelChunkConv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      (act): SiLU()
    )
    (conv2): ConvLayer(
      (conv): ChannelChunkConv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)
      (norm): RMSNorm3d()
    )
  )
  (shortcut): IdentityLayer()
), ResidualBlock(
  (main): ConvLayer(
    (conv): ChannelChunkConv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 2, 2))
  )
  (shortcut): PixelUnshuffleChannelAveragingDownSampleLayer(in_channels=128, out_channels=256, factor=2), temporal_downsample=False
)]
ðŸŸ© build_stage_main width=256 depth=2 block_type='ResBlock' norm='rms3d' act='silu' input_width=256 is_video=True
ðŸŸ© build_block block_type='ResBlock' in_channels=256 out_channels=256 norm='rms3d' act='silu' is_video=True
ðŸŸ© ResBlock.__init__ in_channels=256 out_channels=256 kernel_size=3 stride=1 mid_channels=None expand_ratio=1 use_bias=(True, False) norm=(None, 'rms3d') act_func=('silu', None) is_video=True
ðŸŸ© ConvLayer.__init__ in_channels=256 out_channels=256 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func='silu' is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=256 kwargs={}
ðŸŸ© build_act name='silu' kwargs={}
ðŸŸ© build_kwargs_from_config config={} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=256 out_channels=256 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=256 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 256} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=256, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=ResBlock(
  (conv1): ConvLayer(
    (conv): ChannelChunkConv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1))
    (act): SiLU()
  )
  (conv2): ConvLayer(
    (conv): ChannelChunkConv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© build_block block_type='ResBlock' in_channels=256 out_channels=256 norm='rms3d' act='silu' is_video=True
ðŸŸ© ResBlock.__init__ in_channels=256 out_channels=256 kernel_size=3 stride=1 mid_channels=None expand_ratio=1 use_bias=(True, False) norm=(None, 'rms3d') act_func=('silu', None) is_video=True
ðŸŸ© ConvLayer.__init__ in_channels=256 out_channels=256 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func='silu' is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=256 kwargs={}
ðŸŸ© build_act name='silu' kwargs={}
ðŸŸ© build_kwargs_from_config config={} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=256 out_channels=256 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=256 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 256} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=256, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=ResBlock(
  (conv1): ConvLayer(
    (conv): ChannelChunkConv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1))
    (act): SiLU()
  )
  (conv2): ConvLayer(
    (conv): ChannelChunkConv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© build_downsample_block block_type='Conv' in_channels=256 out_channels=512 shortcut='averaging' is_video=True temporal_downsample=False
ðŸŸ© ConvLayer.__init__ in_channels=256 out_channels=512 kernel_size=3 stride=(1, 2, 2) dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=512 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© PixelUnshuffleChannelAveragingDownSampleLayer.__init__ in_channels=256 out_channels=512 factor=2 temporal_downsample=False
ðŸŸ© ResidualBlock.__init__ main=ConvLayer(
  (conv): ChannelChunkConv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 2, 2))
) shortcut=PixelUnshuffleChannelAveragingDownSampleLayer(in_channels=256, out_channels=512, factor=2), temporal_downsample=False post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© OpSequential init with op_list=[ResidualBlock(
  (main): ResBlock(
    (conv1): ConvLayer(
      (conv): ChannelChunkConv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      (act): SiLU()
    )
    (conv2): ConvLayer(
      (conv): ChannelChunkConv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)
      (norm): RMSNorm3d()
    )
  )
  (shortcut): IdentityLayer()
), ResidualBlock(
  (main): ResBlock(
    (conv1): ConvLayer(
      (conv): ChannelChunkConv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      (act): SiLU()
    )
    (conv2): ConvLayer(
      (conv): ChannelChunkConv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)
      (norm): RMSNorm3d()
    )
  )
  (shortcut): IdentityLayer()
), ResidualBlock(
  (main): ConvLayer(
    (conv): ChannelChunkConv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 2, 2))
  )
  (shortcut): PixelUnshuffleChannelAveragingDownSampleLayer(in_channels=256, out_channels=512, factor=2), temporal_downsample=False
)]
ðŸŸ© build_stage_main width=512 depth=2 block_type='ResBlock' norm='rms3d' act='silu' input_width=512 is_video=True
ðŸŸ© build_block block_type='ResBlock' in_channels=512 out_channels=512 norm='rms3d' act='silu' is_video=True
ðŸŸ© ResBlock.__init__ in_channels=512 out_channels=512 kernel_size=3 stride=1 mid_channels=None expand_ratio=1 use_bias=(True, False) norm=(None, 'rms3d') act_func=('silu', None) is_video=True
ðŸŸ© ConvLayer.__init__ in_channels=512 out_channels=512 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func='silu' is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=512 kwargs={}
ðŸŸ© build_act name='silu' kwargs={}
ðŸŸ© build_kwargs_from_config config={} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=512 out_channels=512 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=512 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 512} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=512, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=ResBlock(
  (conv1): ConvLayer(
    (conv): ChannelChunkConv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1))
    (act): SiLU()
  )
  (conv2): ConvLayer(
    (conv): ChannelChunkConv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© build_block block_type='ResBlock' in_channels=512 out_channels=512 norm='rms3d' act='silu' is_video=True
ðŸŸ© ResBlock.__init__ in_channels=512 out_channels=512 kernel_size=3 stride=1 mid_channels=None expand_ratio=1 use_bias=(True, False) norm=(None, 'rms3d') act_func=('silu', None) is_video=True
ðŸŸ© ConvLayer.__init__ in_channels=512 out_channels=512 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func='silu' is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=512 kwargs={}
ðŸŸ© build_act name='silu' kwargs={}
ðŸŸ© build_kwargs_from_config config={} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=512 out_channels=512 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=512 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 512} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=512, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=ResBlock(
  (conv1): ConvLayer(
    (conv): ChannelChunkConv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1))
    (act): SiLU()
  )
  (conv2): ConvLayer(
    (conv): ChannelChunkConv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© build_downsample_block block_type='Conv' in_channels=512 out_channels=512 shortcut='averaging' is_video=True temporal_downsample=False
ðŸŸ© ConvLayer.__init__ in_channels=512 out_channels=512 kernel_size=3 stride=(1, 2, 2) dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=512 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© PixelUnshuffleChannelAveragingDownSampleLayer.__init__ in_channels=512 out_channels=512 factor=2 temporal_downsample=False
ðŸŸ© ResidualBlock.__init__ main=ConvLayer(
  (conv): ChannelChunkConv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 2, 2))
) shortcut=PixelUnshuffleChannelAveragingDownSampleLayer(in_channels=512, out_channels=512, factor=2), temporal_downsample=False post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© OpSequential init with op_list=[ResidualBlock(
  (main): ResBlock(
    (conv1): ConvLayer(
      (conv): ChannelChunkConv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      (act): SiLU()
    )
    (conv2): ConvLayer(
      (conv): ChannelChunkConv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)
      (norm): RMSNorm3d()
    )
  )
  (shortcut): IdentityLayer()
), ResidualBlock(
  (main): ResBlock(
    (conv1): ConvLayer(
      (conv): ChannelChunkConv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      (act): SiLU()
    )
    (conv2): ConvLayer(
      (conv): ChannelChunkConv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)
      (norm): RMSNorm3d()
    )
  )
  (shortcut): IdentityLayer()
), ResidualBlock(
  (main): ConvLayer(
    (conv): ChannelChunkConv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 2, 2))
  )
  (shortcut): PixelUnshuffleChannelAveragingDownSampleLayer(in_channels=512, out_channels=512, factor=2), temporal_downsample=False
)]
ðŸŸ© build_stage_main width=512 depth=3 block_type='EViTS5_GLU' norm='rms3d' act='silu' input_width=512 is_video=True
ðŸŸ© build_block block_type='EViTS5_GLU' in_channels=512 out_channels=512 norm='rms3d' act='silu' is_video=True
ðŸŸ© EfficientViTBlock.__init__ in_channels=512 heads_ratio=1.0 dim=32 expand_ratio=4 scales=(5,) norm='rms3d' act_func='silu' context_module='LiteMLA' local_module='GLUMBConv' is_video=True
ðŸŸ© LiteMLA.__init__ in_channels=512 out_channels=512 heads=None heads_ratio=1.0 dim=32 use_bias=False norm=(None, 'rms3d') act_func=(None, None) kernel_func='relu' scales=(5,) eps=1e-15 is_video=True
ðŸŸ© ConvLayer.__init__ in_channels=512 out_channels=1536 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=1536 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© get_same_padding kernel_size=5
ðŸŸ¦ get_same_padding result: 2
ðŸŸ© build_act name='relu' kwargs={'inplace': False}
ðŸŸ© build_kwargs_from_config config={'inplace': False} target_func=<class 'torch.nn.modules.activation.ReLU'>
ðŸŸ© ConvLayer.__init__ in_channels=1024 out_channels=512 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=512 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 512} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=512, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=LiteMLA(
  (qkv): ConvLayer(
    (conv): ChannelChunkConv3d(512, 1536, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
  (aggreg): ModuleList(
    (0): Sequential(
      (0): ChannelChunkConv3d(1536, 1536, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=1536, bias=False)
      (1): ChannelChunkConv3d(1536, 1536, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=48, bias=False)
    )
  )
  (kernel_func): ReLU()
  (proj): ConvLayer(
    (conv): ChannelChunkConv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© GLUMBConv.__init__ in_channels=512 out_channels=512 kernel_size=3 stride=1 mid_channels=None expand_ratio=4 use_bias=(True, True, False) norm=(None, None, 'rms3d') act_func=('silu', 'silu', None) is_video=True
ðŸŸ© build_act name='silu' kwargs={'inplace': False}
ðŸŸ© build_kwargs_from_config config={'inplace': False} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=512 out_channels=4096 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func='silu' is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=4096 kwargs={}
ðŸŸ© build_act name='silu' kwargs={}
ðŸŸ© build_kwargs_from_config config={} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=4096 out_channels=4096 kernel_size=3 stride=1 dilation=1 groups=4096 use_bias=True dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=4096 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ConvLayer.__init__ in_channels=2048 out_channels=512 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=512 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 512} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=512, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=GLUMBConv(
  (glu_act): SiLU()
  (inverted_conv): ConvLayer(
    (conv): ChannelChunkConv3d(512, 4096, kernel_size=(1, 1, 1), stride=(1, 1, 1))
    (act): SiLU()
  )
  (depth_conv): ConvLayer(
    (conv): ChannelChunkConv3d(4096, 4096, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=4096)
  )
  (point_conv): ConvLayer(
    (conv): ChannelChunkConv3d(2048, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© build_block block_type='EViTS5_GLU' in_channels=512 out_channels=512 norm='rms3d' act='silu' is_video=True
ðŸŸ© EfficientViTBlock.__init__ in_channels=512 heads_ratio=1.0 dim=32 expand_ratio=4 scales=(5,) norm='rms3d' act_func='silu' context_module='LiteMLA' local_module='GLUMBConv' is_video=True
ðŸŸ© LiteMLA.__init__ in_channels=512 out_channels=512 heads=None heads_ratio=1.0 dim=32 use_bias=False norm=(None, 'rms3d') act_func=(None, None) kernel_func='relu' scales=(5,) eps=1e-15 is_video=True
ðŸŸ© ConvLayer.__init__ in_channels=512 out_channels=1536 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=1536 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© get_same_padding kernel_size=5
ðŸŸ¦ get_same_padding result: 2
ðŸŸ© build_act name='relu' kwargs={'inplace': False}
ðŸŸ© build_kwargs_from_config config={'inplace': False} target_func=<class 'torch.nn.modules.activation.ReLU'>
ðŸŸ© ConvLayer.__init__ in_channels=1024 out_channels=512 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=512 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 512} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=512, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=LiteMLA(
  (qkv): ConvLayer(
    (conv): ChannelChunkConv3d(512, 1536, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
  (aggreg): ModuleList(
    (0): Sequential(
      (0): ChannelChunkConv3d(1536, 1536, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=1536, bias=False)
      (1): ChannelChunkConv3d(1536, 1536, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=48, bias=False)
    )
  )
  (kernel_func): ReLU()
  (proj): ConvLayer(
    (conv): ChannelChunkConv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© GLUMBConv.__init__ in_channels=512 out_channels=512 kernel_size=3 stride=1 mid_channels=None expand_ratio=4 use_bias=(True, True, False) norm=(None, None, 'rms3d') act_func=('silu', 'silu', None) is_video=True
ðŸŸ© build_act name='silu' kwargs={'inplace': False}
ðŸŸ© build_kwargs_from_config config={'inplace': False} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=512 out_channels=4096 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func='silu' is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=4096 kwargs={}
ðŸŸ© build_act name='silu' kwargs={}
ðŸŸ© build_kwargs_from_config config={} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=4096 out_channels=4096 kernel_size=3 stride=1 dilation=1 groups=4096 use_bias=True dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=4096 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ConvLayer.__init__ in_channels=2048 out_channels=512 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=512 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 512} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=512, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=GLUMBConv(
  (glu_act): SiLU()
  (inverted_conv): ConvLayer(
    (conv): ChannelChunkConv3d(512, 4096, kernel_size=(1, 1, 1), stride=(1, 1, 1))
    (act): SiLU()
  )
  (depth_conv): ConvLayer(
    (conv): ChannelChunkConv3d(4096, 4096, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=4096)
  )
  (point_conv): ConvLayer(
    (conv): ChannelChunkConv3d(2048, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© build_block block_type='EViTS5_GLU' in_channels=512 out_channels=512 norm='rms3d' act='silu' is_video=True
ðŸŸ© EfficientViTBlock.__init__ in_channels=512 heads_ratio=1.0 dim=32 expand_ratio=4 scales=(5,) norm='rms3d' act_func='silu' context_module='LiteMLA' local_module='GLUMBConv' is_video=True
ðŸŸ© LiteMLA.__init__ in_channels=512 out_channels=512 heads=None heads_ratio=1.0 dim=32 use_bias=False norm=(None, 'rms3d') act_func=(None, None) kernel_func='relu' scales=(5,) eps=1e-15 is_video=True
ðŸŸ© ConvLayer.__init__ in_channels=512 out_channels=1536 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=1536 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© get_same_padding kernel_size=5
ðŸŸ¦ get_same_padding result: 2
ðŸŸ© build_act name='relu' kwargs={'inplace': False}
ðŸŸ© build_kwargs_from_config config={'inplace': False} target_func=<class 'torch.nn.modules.activation.ReLU'>
ðŸŸ© ConvLayer.__init__ in_channels=1024 out_channels=512 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=512 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 512} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=512, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=LiteMLA(
  (qkv): ConvLayer(
    (conv): ChannelChunkConv3d(512, 1536, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
  (aggreg): ModuleList(
    (0): Sequential(
      (0): ChannelChunkConv3d(1536, 1536, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=1536, bias=False)
      (1): ChannelChunkConv3d(1536, 1536, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=48, bias=False)
    )
  )
  (kernel_func): ReLU()
  (proj): ConvLayer(
    (conv): ChannelChunkConv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© GLUMBConv.__init__ in_channels=512 out_channels=512 kernel_size=3 stride=1 mid_channels=None expand_ratio=4 use_bias=(True, True, False) norm=(None, None, 'rms3d') act_func=('silu', 'silu', None) is_video=True
ðŸŸ© build_act name='silu' kwargs={'inplace': False}
ðŸŸ© build_kwargs_from_config config={'inplace': False} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=512 out_channels=4096 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func='silu' is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=4096 kwargs={}
ðŸŸ© build_act name='silu' kwargs={}
ðŸŸ© build_kwargs_from_config config={} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=4096 out_channels=4096 kernel_size=3 stride=1 dilation=1 groups=4096 use_bias=True dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=4096 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ConvLayer.__init__ in_channels=2048 out_channels=512 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=512 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 512} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=512, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=GLUMBConv(
  (glu_act): SiLU()
  (inverted_conv): ConvLayer(
    (conv): ChannelChunkConv3d(512, 4096, kernel_size=(1, 1, 1), stride=(1, 1, 1))
    (act): SiLU()
  )
  (depth_conv): ConvLayer(
    (conv): ChannelChunkConv3d(4096, 4096, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=4096)
  )
  (point_conv): ConvLayer(
    (conv): ChannelChunkConv3d(2048, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© build_downsample_block block_type='Conv' in_channels=512 out_channels=1024 shortcut='averaging' is_video=True temporal_downsample=True
ðŸŸ© ConvLayer.__init__ in_channels=512 out_channels=1024 kernel_size=3 stride=(2, 2, 2) dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=1024 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© PixelUnshuffleChannelAveragingDownSampleLayer.__init__ in_channels=512 out_channels=1024 factor=2 temporal_downsample=True
ðŸŸ© ResidualBlock.__init__ main=ConvLayer(
  (conv): ChannelChunkConv3d(512, 1024, kernel_size=(3, 3, 3), stride=(2, 2, 2))
) shortcut=PixelUnshuffleChannelAveragingDownSampleLayer(in_channels=512, out_channels=1024, factor=2), temporal_downsample=True post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© OpSequential init with op_list=[EfficientViTBlock(
  (context_module): ResidualBlock(
    (main): LiteMLA(
      (qkv): ConvLayer(
        (conv): ChannelChunkConv3d(512, 1536, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
      )
      (aggreg): ModuleList(
        (0): Sequential(
          (0): ChannelChunkConv3d(1536, 1536, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=1536, bias=False)
          (1): ChannelChunkConv3d(1536, 1536, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=48, bias=False)
        )
      )
      (kernel_func): ReLU()
      (proj): ConvLayer(
        (conv): ChannelChunkConv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (norm): RMSNorm3d()
      )
    )
    (shortcut): IdentityLayer()
  )
  (local_module): ResidualBlock(
    (main): GLUMBConv(
      (glu_act): SiLU()
      (inverted_conv): ConvLayer(
        (conv): ChannelChunkConv3d(512, 4096, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): SiLU()
      )
      (depth_conv): ConvLayer(
        (conv): ChannelChunkConv3d(4096, 4096, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=4096)
      )
      (point_conv): ConvLayer(
        (conv): ChannelChunkConv3d(2048, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (norm): RMSNorm3d()
      )
    )
    (shortcut): IdentityLayer()
  )
), EfficientViTBlock(
  (context_module): ResidualBlock(
    (main): LiteMLA(
      (qkv): ConvLayer(
        (conv): ChannelChunkConv3d(512, 1536, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
      )
      (aggreg): ModuleList(
        (0): Sequential(
          (0): ChannelChunkConv3d(1536, 1536, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=1536, bias=False)
          (1): ChannelChunkConv3d(1536, 1536, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=48, bias=False)
        )
      )
      (kernel_func): ReLU()
      (proj): ConvLayer(
        (conv): ChannelChunkConv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (norm): RMSNorm3d()
      )
    )
    (shortcut): IdentityLayer()
  )
  (local_module): ResidualBlock(
    (main): GLUMBConv(
      (glu_act): SiLU()
      (inverted_conv): ConvLayer(
        (conv): ChannelChunkConv3d(512, 4096, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): SiLU()
      )
      (depth_conv): ConvLayer(
        (conv): ChannelChunkConv3d(4096, 4096, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=4096)
      )
      (point_conv): ConvLayer(
        (conv): ChannelChunkConv3d(2048, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (norm): RMSNorm3d()
      )
    )
    (shortcut): IdentityLayer()
  )
), EfficientViTBlock(
  (context_module): ResidualBlock(
    (main): LiteMLA(
      (qkv): ConvLayer(
        (conv): ChannelChunkConv3d(512, 1536, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
      )
      (aggreg): ModuleList(
        (0): Sequential(
          (0): ChannelChunkConv3d(1536, 1536, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=1536, bias=False)
          (1): ChannelChunkConv3d(1536, 1536, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=48, bias=False)
        )
      )
      (kernel_func): ReLU()
      (proj): ConvLayer(
        (conv): ChannelChunkConv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (norm): RMSNorm3d()
      )
    )
    (shortcut): IdentityLayer()
  )
  (local_module): ResidualBlock(
    (main): GLUMBConv(
      (glu_act): SiLU()
      (inverted_conv): ConvLayer(
        (conv): ChannelChunkConv3d(512, 4096, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): SiLU()
      )
      (depth_conv): ConvLayer(
        (conv): ChannelChunkConv3d(4096, 4096, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=4096)
      )
      (point_conv): ConvLayer(
        (conv): ChannelChunkConv3d(2048, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (norm): RMSNorm3d()
      )
    )
    (shortcut): IdentityLayer()
  )
), ResidualBlock(
  (main): ConvLayer(
    (conv): ChannelChunkConv3d(512, 1024, kernel_size=(3, 3, 3), stride=(2, 2, 2))
  )
  (shortcut): PixelUnshuffleChannelAveragingDownSampleLayer(in_channels=512, out_channels=1024, factor=2), temporal_downsample=True
)]
ðŸŸ© build_stage_main width=1024 depth=3 block_type='EViTS5_GLU' norm='rms3d' act='silu' input_width=1024 is_video=True
ðŸŸ© build_block block_type='EViTS5_GLU' in_channels=1024 out_channels=1024 norm='rms3d' act='silu' is_video=True
ðŸŸ© EfficientViTBlock.__init__ in_channels=1024 heads_ratio=1.0 dim=32 expand_ratio=4 scales=(5,) norm='rms3d' act_func='silu' context_module='LiteMLA' local_module='GLUMBConv' is_video=True
ðŸŸ© LiteMLA.__init__ in_channels=1024 out_channels=1024 heads=None heads_ratio=1.0 dim=32 use_bias=False norm=(None, 'rms3d') act_func=(None, None) kernel_func='relu' scales=(5,) eps=1e-15 is_video=True
ðŸŸ© ConvLayer.__init__ in_channels=1024 out_channels=3072 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=3072 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© get_same_padding kernel_size=5
ðŸŸ¦ get_same_padding result: 2
ðŸŸ© build_act name='relu' kwargs={'inplace': False}
ðŸŸ© build_kwargs_from_config config={'inplace': False} target_func=<class 'torch.nn.modules.activation.ReLU'>
ðŸŸ© ConvLayer.__init__ in_channels=2048 out_channels=1024 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=1024 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 1024} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=1024, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=LiteMLA(
  (qkv): ConvLayer(
    (conv): ChannelChunkConv3d(1024, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
  (aggreg): ModuleList(
    (0): Sequential(
      (0): ChannelChunkConv3d(3072, 3072, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=3072, bias=False)
      (1): ChannelChunkConv3d(3072, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=96, bias=False)
    )
  )
  (kernel_func): ReLU()
  (proj): ConvLayer(
    (conv): ChannelChunkConv3d(2048, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© GLUMBConv.__init__ in_channels=1024 out_channels=1024 kernel_size=3 stride=1 mid_channels=None expand_ratio=4 use_bias=(True, True, False) norm=(None, None, 'rms3d') act_func=('silu', 'silu', None) is_video=True
ðŸŸ© build_act name='silu' kwargs={'inplace': False}
ðŸŸ© build_kwargs_from_config config={'inplace': False} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=1024 out_channels=8192 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func='silu' is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=8192 kwargs={}
ðŸŸ© build_act name='silu' kwargs={}
ðŸŸ© build_kwargs_from_config config={} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=8192 out_channels=8192 kernel_size=3 stride=1 dilation=1 groups=8192 use_bias=True dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=8192 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ConvLayer.__init__ in_channels=4096 out_channels=1024 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=1024 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 1024} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=1024, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=GLUMBConv(
  (glu_act): SiLU()
  (inverted_conv): ConvLayer(
    (conv): ChannelChunkConv3d(1024, 8192, kernel_size=(1, 1, 1), stride=(1, 1, 1))
    (act): SiLU()
  )
  (depth_conv): ConvLayer(
    (conv): ChannelChunkConv3d(8192, 8192, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=8192)
  )
  (point_conv): ConvLayer(
    (conv): ChannelChunkConv3d(4096, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© build_block block_type='EViTS5_GLU' in_channels=1024 out_channels=1024 norm='rms3d' act='silu' is_video=True
ðŸŸ© EfficientViTBlock.__init__ in_channels=1024 heads_ratio=1.0 dim=32 expand_ratio=4 scales=(5,) norm='rms3d' act_func='silu' context_module='LiteMLA' local_module='GLUMBConv' is_video=True
ðŸŸ© LiteMLA.__init__ in_channels=1024 out_channels=1024 heads=None heads_ratio=1.0 dim=32 use_bias=False norm=(None, 'rms3d') act_func=(None, None) kernel_func='relu' scales=(5,) eps=1e-15 is_video=True
ðŸŸ© ConvLayer.__init__ in_channels=1024 out_channels=3072 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=3072 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© get_same_padding kernel_size=5
ðŸŸ¦ get_same_padding result: 2
ðŸŸ© build_act name='relu' kwargs={'inplace': False}
ðŸŸ© build_kwargs_from_config config={'inplace': False} target_func=<class 'torch.nn.modules.activation.ReLU'>
ðŸŸ© ConvLayer.__init__ in_channels=2048 out_channels=1024 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=1024 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 1024} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=1024, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=LiteMLA(
  (qkv): ConvLayer(
    (conv): ChannelChunkConv3d(1024, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
  (aggreg): ModuleList(
    (0): Sequential(
      (0): ChannelChunkConv3d(3072, 3072, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=3072, bias=False)
      (1): ChannelChunkConv3d(3072, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=96, bias=False)
    )
  )
  (kernel_func): ReLU()
  (proj): ConvLayer(
    (conv): ChannelChunkConv3d(2048, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© GLUMBConv.__init__ in_channels=1024 out_channels=1024 kernel_size=3 stride=1 mid_channels=None expand_ratio=4 use_bias=(True, True, False) norm=(None, None, 'rms3d') act_func=('silu', 'silu', None) is_video=True
ðŸŸ© build_act name='silu' kwargs={'inplace': False}
ðŸŸ© build_kwargs_from_config config={'inplace': False} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=1024 out_channels=8192 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func='silu' is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=8192 kwargs={}
ðŸŸ© build_act name='silu' kwargs={}
ðŸŸ© build_kwargs_from_config config={} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=8192 out_channels=8192 kernel_size=3 stride=1 dilation=1 groups=8192 use_bias=True dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=8192 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ConvLayer.__init__ in_channels=4096 out_channels=1024 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=1024 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 1024} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=1024, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=GLUMBConv(
  (glu_act): SiLU()
  (inverted_conv): ConvLayer(
    (conv): ChannelChunkConv3d(1024, 8192, kernel_size=(1, 1, 1), stride=(1, 1, 1))
    (act): SiLU()
  )
  (depth_conv): ConvLayer(
    (conv): ChannelChunkConv3d(8192, 8192, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=8192)
  )
  (point_conv): ConvLayer(
    (conv): ChannelChunkConv3d(4096, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© build_block block_type='EViTS5_GLU' in_channels=1024 out_channels=1024 norm='rms3d' act='silu' is_video=True
ðŸŸ© EfficientViTBlock.__init__ in_channels=1024 heads_ratio=1.0 dim=32 expand_ratio=4 scales=(5,) norm='rms3d' act_func='silu' context_module='LiteMLA' local_module='GLUMBConv' is_video=True
ðŸŸ© LiteMLA.__init__ in_channels=1024 out_channels=1024 heads=None heads_ratio=1.0 dim=32 use_bias=False norm=(None, 'rms3d') act_func=(None, None) kernel_func='relu' scales=(5,) eps=1e-15 is_video=True
ðŸŸ© ConvLayer.__init__ in_channels=1024 out_channels=3072 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=3072 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© get_same_padding kernel_size=5
ðŸŸ¦ get_same_padding result: 2
ðŸŸ© build_act name='relu' kwargs={'inplace': False}
ðŸŸ© build_kwargs_from_config config={'inplace': False} target_func=<class 'torch.nn.modules.activation.ReLU'>
ðŸŸ© ConvLayer.__init__ in_channels=2048 out_channels=1024 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=1024 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 1024} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=1024, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=LiteMLA(
  (qkv): ConvLayer(
    (conv): ChannelChunkConv3d(1024, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
  (aggreg): ModuleList(
    (0): Sequential(
      (0): ChannelChunkConv3d(3072, 3072, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=3072, bias=False)
      (1): ChannelChunkConv3d(3072, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=96, bias=False)
    )
  )
  (kernel_func): ReLU()
  (proj): ConvLayer(
    (conv): ChannelChunkConv3d(2048, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© GLUMBConv.__init__ in_channels=1024 out_channels=1024 kernel_size=3 stride=1 mid_channels=None expand_ratio=4 use_bias=(True, True, False) norm=(None, None, 'rms3d') act_func=('silu', 'silu', None) is_video=True
ðŸŸ© build_act name='silu' kwargs={'inplace': False}
ðŸŸ© build_kwargs_from_config config={'inplace': False} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=1024 out_channels=8192 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func='silu' is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=8192 kwargs={}
ðŸŸ© build_act name='silu' kwargs={}
ðŸŸ© build_kwargs_from_config config={} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=8192 out_channels=8192 kernel_size=3 stride=1 dilation=1 groups=8192 use_bias=True dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=8192 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ConvLayer.__init__ in_channels=4096 out_channels=1024 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=1024 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 1024} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=1024, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=GLUMBConv(
  (glu_act): SiLU()
  (inverted_conv): ConvLayer(
    (conv): ChannelChunkConv3d(1024, 8192, kernel_size=(1, 1, 1), stride=(1, 1, 1))
    (act): SiLU()
  )
  (depth_conv): ConvLayer(
    (conv): ChannelChunkConv3d(8192, 8192, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=8192)
  )
  (point_conv): ConvLayer(
    (conv): ChannelChunkConv3d(4096, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© build_downsample_block block_type='Conv' in_channels=1024 out_channels=1024 shortcut='averaging' is_video=True temporal_downsample=True
ðŸŸ© ConvLayer.__init__ in_channels=1024 out_channels=1024 kernel_size=3 stride=(2, 2, 2) dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=1024 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© PixelUnshuffleChannelAveragingDownSampleLayer.__init__ in_channels=1024 out_channels=1024 factor=2 temporal_downsample=True
ðŸŸ© ResidualBlock.__init__ main=ConvLayer(
  (conv): ChannelChunkConv3d(1024, 1024, kernel_size=(3, 3, 3), stride=(2, 2, 2))
) shortcut=PixelUnshuffleChannelAveragingDownSampleLayer(in_channels=1024, out_channels=1024, factor=2), temporal_downsample=True post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© OpSequential init with op_list=[EfficientViTBlock(
  (context_module): ResidualBlock(
    (main): LiteMLA(
      (qkv): ConvLayer(
        (conv): ChannelChunkConv3d(1024, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
      )
      (aggreg): ModuleList(
        (0): Sequential(
          (0): ChannelChunkConv3d(3072, 3072, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=3072, bias=False)
          (1): ChannelChunkConv3d(3072, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=96, bias=False)
        )
      )
      (kernel_func): ReLU()
      (proj): ConvLayer(
        (conv): ChannelChunkConv3d(2048, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (norm): RMSNorm3d()
      )
    )
    (shortcut): IdentityLayer()
  )
  (local_module): ResidualBlock(
    (main): GLUMBConv(
      (glu_act): SiLU()
      (inverted_conv): ConvLayer(
        (conv): ChannelChunkConv3d(1024, 8192, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): SiLU()
      )
      (depth_conv): ConvLayer(
        (conv): ChannelChunkConv3d(8192, 8192, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=8192)
      )
      (point_conv): ConvLayer(
        (conv): ChannelChunkConv3d(4096, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (norm): RMSNorm3d()
      )
    )
    (shortcut): IdentityLayer()
  )
), EfficientViTBlock(
  (context_module): ResidualBlock(
    (main): LiteMLA(
      (qkv): ConvLayer(
        (conv): ChannelChunkConv3d(1024, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
      )
      (aggreg): ModuleList(
        (0): Sequential(
          (0): ChannelChunkConv3d(3072, 3072, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=3072, bias=False)
          (1): ChannelChunkConv3d(3072, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=96, bias=False)
        )
      )
      (kernel_func): ReLU()
      (proj): ConvLayer(
        (conv): ChannelChunkConv3d(2048, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (norm): RMSNorm3d()
      )
    )
    (shortcut): IdentityLayer()
  )
  (local_module): ResidualBlock(
    (main): GLUMBConv(
      (glu_act): SiLU()
      (inverted_conv): ConvLayer(
        (conv): ChannelChunkConv3d(1024, 8192, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): SiLU()
      )
      (depth_conv): ConvLayer(
        (conv): ChannelChunkConv3d(8192, 8192, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=8192)
      )
      (point_conv): ConvLayer(
        (conv): ChannelChunkConv3d(4096, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (norm): RMSNorm3d()
      )
    )
    (shortcut): IdentityLayer()
  )
), EfficientViTBlock(
  (context_module): ResidualBlock(
    (main): LiteMLA(
      (qkv): ConvLayer(
        (conv): ChannelChunkConv3d(1024, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
      )
      (aggreg): ModuleList(
        (0): Sequential(
          (0): ChannelChunkConv3d(3072, 3072, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=3072, bias=False)
          (1): ChannelChunkConv3d(3072, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=96, bias=False)
        )
      )
      (kernel_func): ReLU()
      (proj): ConvLayer(
        (conv): ChannelChunkConv3d(2048, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (norm): RMSNorm3d()
      )
    )
    (shortcut): IdentityLayer()
  )
  (local_module): ResidualBlock(
    (main): GLUMBConv(
      (glu_act): SiLU()
      (inverted_conv): ConvLayer(
        (conv): ChannelChunkConv3d(1024, 8192, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): SiLU()
      )
      (depth_conv): ConvLayer(
        (conv): ChannelChunkConv3d(8192, 8192, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=8192)
      )
      (point_conv): ConvLayer(
        (conv): ChannelChunkConv3d(4096, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (norm): RMSNorm3d()
      )
    )
    (shortcut): IdentityLayer()
  )
), ResidualBlock(
  (main): ConvLayer(
    (conv): ChannelChunkConv3d(1024, 1024, kernel_size=(3, 3, 3), stride=(2, 2, 2))
  )
  (shortcut): PixelUnshuffleChannelAveragingDownSampleLayer(in_channels=1024, out_channels=1024, factor=2), temporal_downsample=True
)]
ðŸŸ© build_stage_main width=1024 depth=3 block_type='EViTS5_GLU' norm='rms3d' act='silu' input_width=1024 is_video=True
ðŸŸ© build_block block_type='EViTS5_GLU' in_channels=1024 out_channels=1024 norm='rms3d' act='silu' is_video=True
ðŸŸ© EfficientViTBlock.__init__ in_channels=1024 heads_ratio=1.0 dim=32 expand_ratio=4 scales=(5,) norm='rms3d' act_func='silu' context_module='LiteMLA' local_module='GLUMBConv' is_video=True
ðŸŸ© LiteMLA.__init__ in_channels=1024 out_channels=1024 heads=None heads_ratio=1.0 dim=32 use_bias=False norm=(None, 'rms3d') act_func=(None, None) kernel_func='relu' scales=(5,) eps=1e-15 is_video=True
ðŸŸ© ConvLayer.__init__ in_channels=1024 out_channels=3072 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=3072 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© get_same_padding kernel_size=5
ðŸŸ¦ get_same_padding result: 2
ðŸŸ© build_act name='relu' kwargs={'inplace': False}
ðŸŸ© build_kwargs_from_config config={'inplace': False} target_func=<class 'torch.nn.modules.activation.ReLU'>
ðŸŸ© ConvLayer.__init__ in_channels=2048 out_channels=1024 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=1024 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 1024} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=1024, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=LiteMLA(
  (qkv): ConvLayer(
    (conv): ChannelChunkConv3d(1024, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
  (aggreg): ModuleList(
    (0): Sequential(
      (0): ChannelChunkConv3d(3072, 3072, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=3072, bias=False)
      (1): ChannelChunkConv3d(3072, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=96, bias=False)
    )
  )
  (kernel_func): ReLU()
  (proj): ConvLayer(
    (conv): ChannelChunkConv3d(2048, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© GLUMBConv.__init__ in_channels=1024 out_channels=1024 kernel_size=3 stride=1 mid_channels=None expand_ratio=4 use_bias=(True, True, False) norm=(None, None, 'rms3d') act_func=('silu', 'silu', None) is_video=True
ðŸŸ© build_act name='silu' kwargs={'inplace': False}
ðŸŸ© build_kwargs_from_config config={'inplace': False} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=1024 out_channels=8192 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func='silu' is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=8192 kwargs={}
ðŸŸ© build_act name='silu' kwargs={}
ðŸŸ© build_kwargs_from_config config={} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=8192 out_channels=8192 kernel_size=3 stride=1 dilation=1 groups=8192 use_bias=True dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=8192 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ConvLayer.__init__ in_channels=4096 out_channels=1024 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=1024 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 1024} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=1024, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=GLUMBConv(
  (glu_act): SiLU()
  (inverted_conv): ConvLayer(
    (conv): ChannelChunkConv3d(1024, 8192, kernel_size=(1, 1, 1), stride=(1, 1, 1))
    (act): SiLU()
  )
  (depth_conv): ConvLayer(
    (conv): ChannelChunkConv3d(8192, 8192, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=8192)
  )
  (point_conv): ConvLayer(
    (conv): ChannelChunkConv3d(4096, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© build_block block_type='EViTS5_GLU' in_channels=1024 out_channels=1024 norm='rms3d' act='silu' is_video=True
ðŸŸ© EfficientViTBlock.__init__ in_channels=1024 heads_ratio=1.0 dim=32 expand_ratio=4 scales=(5,) norm='rms3d' act_func='silu' context_module='LiteMLA' local_module='GLUMBConv' is_video=True
ðŸŸ© LiteMLA.__init__ in_channels=1024 out_channels=1024 heads=None heads_ratio=1.0 dim=32 use_bias=False norm=(None, 'rms3d') act_func=(None, None) kernel_func='relu' scales=(5,) eps=1e-15 is_video=True
ðŸŸ© ConvLayer.__init__ in_channels=1024 out_channels=3072 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=3072 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© get_same_padding kernel_size=5
ðŸŸ¦ get_same_padding result: 2
ðŸŸ© build_act name='relu' kwargs={'inplace': False}
ðŸŸ© build_kwargs_from_config config={'inplace': False} target_func=<class 'torch.nn.modules.activation.ReLU'>
ðŸŸ© ConvLayer.__init__ in_channels=2048 out_channels=1024 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=1024 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 1024} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=1024, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=LiteMLA(
  (qkv): ConvLayer(
    (conv): ChannelChunkConv3d(1024, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
  (aggreg): ModuleList(
    (0): Sequential(
      (0): ChannelChunkConv3d(3072, 3072, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=3072, bias=False)
      (1): ChannelChunkConv3d(3072, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=96, bias=False)
    )
  )
  (kernel_func): ReLU()
  (proj): ConvLayer(
    (conv): ChannelChunkConv3d(2048, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© GLUMBConv.__init__ in_channels=1024 out_channels=1024 kernel_size=3 stride=1 mid_channels=None expand_ratio=4 use_bias=(True, True, False) norm=(None, None, 'rms3d') act_func=('silu', 'silu', None) is_video=True
ðŸŸ© build_act name='silu' kwargs={'inplace': False}
ðŸŸ© build_kwargs_from_config config={'inplace': False} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=1024 out_channels=8192 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func='silu' is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=8192 kwargs={}
ðŸŸ© build_act name='silu' kwargs={}
ðŸŸ© build_kwargs_from_config config={} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=8192 out_channels=8192 kernel_size=3 stride=1 dilation=1 groups=8192 use_bias=True dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=8192 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ConvLayer.__init__ in_channels=4096 out_channels=1024 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=1024 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 1024} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=1024, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=GLUMBConv(
  (glu_act): SiLU()
  (inverted_conv): ConvLayer(
    (conv): ChannelChunkConv3d(1024, 8192, kernel_size=(1, 1, 1), stride=(1, 1, 1))
    (act): SiLU()
  )
  (depth_conv): ConvLayer(
    (conv): ChannelChunkConv3d(8192, 8192, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=8192)
  )
  (point_conv): ConvLayer(
    (conv): ChannelChunkConv3d(4096, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© build_block block_type='EViTS5_GLU' in_channels=1024 out_channels=1024 norm='rms3d' act='silu' is_video=True
ðŸŸ© EfficientViTBlock.__init__ in_channels=1024 heads_ratio=1.0 dim=32 expand_ratio=4 scales=(5,) norm='rms3d' act_func='silu' context_module='LiteMLA' local_module='GLUMBConv' is_video=True
ðŸŸ© LiteMLA.__init__ in_channels=1024 out_channels=1024 heads=None heads_ratio=1.0 dim=32 use_bias=False norm=(None, 'rms3d') act_func=(None, None) kernel_func='relu' scales=(5,) eps=1e-15 is_video=True
ðŸŸ© ConvLayer.__init__ in_channels=1024 out_channels=3072 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=3072 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© get_same_padding kernel_size=5
ðŸŸ¦ get_same_padding result: 2
ðŸŸ© build_act name='relu' kwargs={'inplace': False}
ðŸŸ© build_kwargs_from_config config={'inplace': False} target_func=<class 'torch.nn.modules.activation.ReLU'>
ðŸŸ© ConvLayer.__init__ in_channels=2048 out_channels=1024 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=1024 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 1024} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=1024, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=LiteMLA(
  (qkv): ConvLayer(
    (conv): ChannelChunkConv3d(1024, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
  (aggreg): ModuleList(
    (0): Sequential(
      (0): ChannelChunkConv3d(3072, 3072, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=3072, bias=False)
      (1): ChannelChunkConv3d(3072, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=96, bias=False)
    )
  )
  (kernel_func): ReLU()
  (proj): ConvLayer(
    (conv): ChannelChunkConv3d(2048, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© GLUMBConv.__init__ in_channels=1024 out_channels=1024 kernel_size=3 stride=1 mid_channels=None expand_ratio=4 use_bias=(True, True, False) norm=(None, None, 'rms3d') act_func=('silu', 'silu', None) is_video=True
ðŸŸ© build_act name='silu' kwargs={'inplace': False}
ðŸŸ© build_kwargs_from_config config={'inplace': False} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=1024 out_channels=8192 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func='silu' is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=8192 kwargs={}
ðŸŸ© build_act name='silu' kwargs={}
ðŸŸ© build_kwargs_from_config config={} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=8192 out_channels=8192 kernel_size=3 stride=1 dilation=1 groups=8192 use_bias=True dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=8192 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ConvLayer.__init__ in_channels=4096 out_channels=1024 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=1024 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 1024} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=1024, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=GLUMBConv(
  (glu_act): SiLU()
  (inverted_conv): ConvLayer(
    (conv): ChannelChunkConv3d(1024, 8192, kernel_size=(1, 1, 1), stride=(1, 1, 1))
    (act): SiLU()
  )
  (depth_conv): ConvLayer(
    (conv): ChannelChunkConv3d(8192, 8192, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=8192)
  )
  (point_conv): ConvLayer(
    (conv): ChannelChunkConv3d(4096, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© OpSequential init with op_list=[EfficientViTBlock(
  (context_module): ResidualBlock(
    (main): LiteMLA(
      (qkv): ConvLayer(
        (conv): ChannelChunkConv3d(1024, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
      )
      (aggreg): ModuleList(
        (0): Sequential(
          (0): ChannelChunkConv3d(3072, 3072, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=3072, bias=False)
          (1): ChannelChunkConv3d(3072, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=96, bias=False)
        )
      )
      (kernel_func): ReLU()
      (proj): ConvLayer(
        (conv): ChannelChunkConv3d(2048, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (norm): RMSNorm3d()
      )
    )
    (shortcut): IdentityLayer()
  )
  (local_module): ResidualBlock(
    (main): GLUMBConv(
      (glu_act): SiLU()
      (inverted_conv): ConvLayer(
        (conv): ChannelChunkConv3d(1024, 8192, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): SiLU()
      )
      (depth_conv): ConvLayer(
        (conv): ChannelChunkConv3d(8192, 8192, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=8192)
      )
      (point_conv): ConvLayer(
        (conv): ChannelChunkConv3d(4096, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (norm): RMSNorm3d()
      )
    )
    (shortcut): IdentityLayer()
  )
), EfficientViTBlock(
  (context_module): ResidualBlock(
    (main): LiteMLA(
      (qkv): ConvLayer(
        (conv): ChannelChunkConv3d(1024, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
      )
      (aggreg): ModuleList(
        (0): Sequential(
          (0): ChannelChunkConv3d(3072, 3072, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=3072, bias=False)
          (1): ChannelChunkConv3d(3072, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=96, bias=False)
        )
      )
      (kernel_func): ReLU()
      (proj): ConvLayer(
        (conv): ChannelChunkConv3d(2048, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (norm): RMSNorm3d()
      )
    )
    (shortcut): IdentityLayer()
  )
  (local_module): ResidualBlock(
    (main): GLUMBConv(
      (glu_act): SiLU()
      (inverted_conv): ConvLayer(
        (conv): ChannelChunkConv3d(1024, 8192, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): SiLU()
      )
      (depth_conv): ConvLayer(
        (conv): ChannelChunkConv3d(8192, 8192, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=8192)
      )
      (point_conv): ConvLayer(
        (conv): ChannelChunkConv3d(4096, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (norm): RMSNorm3d()
      )
    )
    (shortcut): IdentityLayer()
  )
), EfficientViTBlock(
  (context_module): ResidualBlock(
    (main): LiteMLA(
      (qkv): ConvLayer(
        (conv): ChannelChunkConv3d(1024, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
      )
      (aggreg): ModuleList(
        (0): Sequential(
          (0): ChannelChunkConv3d(3072, 3072, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=3072, bias=False)
          (1): ChannelChunkConv3d(3072, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=96, bias=False)
        )
      )
      (kernel_func): ReLU()
      (proj): ConvLayer(
        (conv): ChannelChunkConv3d(2048, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (norm): RMSNorm3d()
      )
    )
    (shortcut): IdentityLayer()
  )
  (local_module): ResidualBlock(
    (main): GLUMBConv(
      (glu_act): SiLU()
      (inverted_conv): ConvLayer(
        (conv): ChannelChunkConv3d(1024, 8192, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): SiLU()
      )
      (depth_conv): ConvLayer(
        (conv): ChannelChunkConv3d(8192, 8192, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=8192)
      )
      (point_conv): ConvLayer(
        (conv): ChannelChunkConv3d(4096, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (norm): RMSNorm3d()
      )
    )
    (shortcut): IdentityLayer()
  )
)]
ðŸŸ© build_encoder_project_out_block in_channels=1024 out_channels=128 norm=None act=None shortcut='averaging' is_video=True
ðŸŸ© build_norm name=None num_features=None kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ConvLayer.__init__ in_channels=1024 out_channels=128 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=128 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© OpSequential init with op_list=[None, None, ConvLayer(
  (conv): ChannelChunkConv3d(1024, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1))
)]
ðŸŸ© PixelUnshuffleChannelAveragingDownSampleLayer.__init__ in_channels=1024 out_channels=128 factor=1 temporal_downsample=False
ðŸŸ© ResidualBlock.__init__ main=OpSequential(
  (op_list): ModuleList(
    (0): ConvLayer(
      (conv): ChannelChunkConv3d(1024, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1))
    )
  )
) shortcut=PixelUnshuffleChannelAveragingDownSampleLayer(in_channels=1024, out_channels=128, factor=1), temporal_downsample=False post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© Decoder.__init__ cfg=DecoderConfig(in_channels=3, latent_channels=128, in_shortcut='duplicating', width_list=[128, 256, 512, 512, 1024, 1024], depth_list=[3, 3, 3, 3, 3, 3], block_type=['ResBlock', 'ResBlock', 'ResBlock', 'EViTS5_GLU', 'EViTS5_GLU', 'EViTS5_GLU'], norm='rms3d', act='silu', upsample_block_type='InterpolateConv', upsample_match_channel=True, upsample_shortcut='duplicating', out_norm='rms3d', out_act='relu', is_video=True, temporal_upsample=[False, False, False, True, True, False])
ðŸŸ© build_decoder_project_in_block in_channels=128 out_channels=1024 shortcut='duplicating' is_video=True
ðŸŸ© ConvLayer.__init__ in_channels=128 out_channels=1024 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=1024 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ChannelDuplicatingPixelShuffleUpSampleLayer.__init__ in_channels=128 out_channels=1024 factor=1 temporal_upsample=False
ðŸŸ© ResidualBlock.__init__ main=ConvLayer(
  (conv): ChannelChunkConv3d(128, 1024, kernel_size=(3, 3, 3), stride=(1, 1, 1))
) shortcut=ChannelDuplicatingPixelShuffleUpSampleLayer(in_channels=128, out_channels=1024, factor=1, temporal_upsample=False) post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© build_stage_main width=1024 depth=3 block_type='EViTS5_GLU' norm='rms3d' act='silu' input_width=1024 is_video=True
ðŸŸ© build_block block_type='EViTS5_GLU' in_channels=1024 out_channels=1024 norm='rms3d' act='silu' is_video=True
ðŸŸ© EfficientViTBlock.__init__ in_channels=1024 heads_ratio=1.0 dim=32 expand_ratio=4 scales=(5,) norm='rms3d' act_func='silu' context_module='LiteMLA' local_module='GLUMBConv' is_video=True
ðŸŸ© LiteMLA.__init__ in_channels=1024 out_channels=1024 heads=None heads_ratio=1.0 dim=32 use_bias=False norm=(None, 'rms3d') act_func=(None, None) kernel_func='relu' scales=(5,) eps=1e-15 is_video=True
ðŸŸ© ConvLayer.__init__ in_channels=1024 out_channels=3072 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=3072 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© get_same_padding kernel_size=5
ðŸŸ¦ get_same_padding result: 2
ðŸŸ© build_act name='relu' kwargs={'inplace': False}
ðŸŸ© build_kwargs_from_config config={'inplace': False} target_func=<class 'torch.nn.modules.activation.ReLU'>
ðŸŸ© ConvLayer.__init__ in_channels=2048 out_channels=1024 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=1024 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 1024} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=1024, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=LiteMLA(
  (qkv): ConvLayer(
    (conv): ChannelChunkConv3d(1024, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
  (aggreg): ModuleList(
    (0): Sequential(
      (0): ChannelChunkConv3d(3072, 3072, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=3072, bias=False)
      (1): ChannelChunkConv3d(3072, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=96, bias=False)
    )
  )
  (kernel_func): ReLU()
  (proj): ConvLayer(
    (conv): ChannelChunkConv3d(2048, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© GLUMBConv.__init__ in_channels=1024 out_channels=1024 kernel_size=3 stride=1 mid_channels=None expand_ratio=4 use_bias=(True, True, False) norm=(None, None, 'rms3d') act_func=('silu', 'silu', None) is_video=True
ðŸŸ© build_act name='silu' kwargs={'inplace': False}
ðŸŸ© build_kwargs_from_config config={'inplace': False} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=1024 out_channels=8192 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func='silu' is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=8192 kwargs={}
ðŸŸ© build_act name='silu' kwargs={}
ðŸŸ© build_kwargs_from_config config={} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=8192 out_channels=8192 kernel_size=3 stride=1 dilation=1 groups=8192 use_bias=True dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=8192 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ConvLayer.__init__ in_channels=4096 out_channels=1024 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=1024 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 1024} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=1024, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=GLUMBConv(
  (glu_act): SiLU()
  (inverted_conv): ConvLayer(
    (conv): ChannelChunkConv3d(1024, 8192, kernel_size=(1, 1, 1), stride=(1, 1, 1))
    (act): SiLU()
  )
  (depth_conv): ConvLayer(
    (conv): ChannelChunkConv3d(8192, 8192, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=8192)
  )
  (point_conv): ConvLayer(
    (conv): ChannelChunkConv3d(4096, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© build_block block_type='EViTS5_GLU' in_channels=1024 out_channels=1024 norm='rms3d' act='silu' is_video=True
ðŸŸ© EfficientViTBlock.__init__ in_channels=1024 heads_ratio=1.0 dim=32 expand_ratio=4 scales=(5,) norm='rms3d' act_func='silu' context_module='LiteMLA' local_module='GLUMBConv' is_video=True
ðŸŸ© LiteMLA.__init__ in_channels=1024 out_channels=1024 heads=None heads_ratio=1.0 dim=32 use_bias=False norm=(None, 'rms3d') act_func=(None, None) kernel_func='relu' scales=(5,) eps=1e-15 is_video=True
ðŸŸ© ConvLayer.__init__ in_channels=1024 out_channels=3072 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=3072 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© get_same_padding kernel_size=5
ðŸŸ¦ get_same_padding result: 2
ðŸŸ© build_act name='relu' kwargs={'inplace': False}
ðŸŸ© build_kwargs_from_config config={'inplace': False} target_func=<class 'torch.nn.modules.activation.ReLU'>
ðŸŸ© ConvLayer.__init__ in_channels=2048 out_channels=1024 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=1024 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 1024} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=1024, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=LiteMLA(
  (qkv): ConvLayer(
    (conv): ChannelChunkConv3d(1024, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
  (aggreg): ModuleList(
    (0): Sequential(
      (0): ChannelChunkConv3d(3072, 3072, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=3072, bias=False)
      (1): ChannelChunkConv3d(3072, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=96, bias=False)
    )
  )
  (kernel_func): ReLU()
  (proj): ConvLayer(
    (conv): ChannelChunkConv3d(2048, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© GLUMBConv.__init__ in_channels=1024 out_channels=1024 kernel_size=3 stride=1 mid_channels=None expand_ratio=4 use_bias=(True, True, False) norm=(None, None, 'rms3d') act_func=('silu', 'silu', None) is_video=True
ðŸŸ© build_act name='silu' kwargs={'inplace': False}
ðŸŸ© build_kwargs_from_config config={'inplace': False} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=1024 out_channels=8192 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func='silu' is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=8192 kwargs={}
ðŸŸ© build_act name='silu' kwargs={}
ðŸŸ© build_kwargs_from_config config={} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=8192 out_channels=8192 kernel_size=3 stride=1 dilation=1 groups=8192 use_bias=True dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=8192 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ConvLayer.__init__ in_channels=4096 out_channels=1024 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=1024 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 1024} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=1024, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=GLUMBConv(
  (glu_act): SiLU()
  (inverted_conv): ConvLayer(
    (conv): ChannelChunkConv3d(1024, 8192, kernel_size=(1, 1, 1), stride=(1, 1, 1))
    (act): SiLU()
  )
  (depth_conv): ConvLayer(
    (conv): ChannelChunkConv3d(8192, 8192, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=8192)
  )
  (point_conv): ConvLayer(
    (conv): ChannelChunkConv3d(4096, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© build_block block_type='EViTS5_GLU' in_channels=1024 out_channels=1024 norm='rms3d' act='silu' is_video=True
ðŸŸ© EfficientViTBlock.__init__ in_channels=1024 heads_ratio=1.0 dim=32 expand_ratio=4 scales=(5,) norm='rms3d' act_func='silu' context_module='LiteMLA' local_module='GLUMBConv' is_video=True
ðŸŸ© LiteMLA.__init__ in_channels=1024 out_channels=1024 heads=None heads_ratio=1.0 dim=32 use_bias=False norm=(None, 'rms3d') act_func=(None, None) kernel_func='relu' scales=(5,) eps=1e-15 is_video=True
ðŸŸ© ConvLayer.__init__ in_channels=1024 out_channels=3072 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=3072 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© get_same_padding kernel_size=5
ðŸŸ¦ get_same_padding result: 2
ðŸŸ© build_act name='relu' kwargs={'inplace': False}
ðŸŸ© build_kwargs_from_config config={'inplace': False} target_func=<class 'torch.nn.modules.activation.ReLU'>
ðŸŸ© ConvLayer.__init__ in_channels=2048 out_channels=1024 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=1024 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 1024} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=1024, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=LiteMLA(
  (qkv): ConvLayer(
    (conv): ChannelChunkConv3d(1024, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
  (aggreg): ModuleList(
    (0): Sequential(
      (0): ChannelChunkConv3d(3072, 3072, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=3072, bias=False)
      (1): ChannelChunkConv3d(3072, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=96, bias=False)
    )
  )
  (kernel_func): ReLU()
  (proj): ConvLayer(
    (conv): ChannelChunkConv3d(2048, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© GLUMBConv.__init__ in_channels=1024 out_channels=1024 kernel_size=3 stride=1 mid_channels=None expand_ratio=4 use_bias=(True, True, False) norm=(None, None, 'rms3d') act_func=('silu', 'silu', None) is_video=True
ðŸŸ© build_act name='silu' kwargs={'inplace': False}
ðŸŸ© build_kwargs_from_config config={'inplace': False} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=1024 out_channels=8192 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func='silu' is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=8192 kwargs={}
ðŸŸ© build_act name='silu' kwargs={}
ðŸŸ© build_kwargs_from_config config={} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=8192 out_channels=8192 kernel_size=3 stride=1 dilation=1 groups=8192 use_bias=True dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=8192 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ConvLayer.__init__ in_channels=4096 out_channels=1024 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=1024 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 1024} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=1024, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=GLUMBConv(
  (glu_act): SiLU()
  (inverted_conv): ConvLayer(
    (conv): ChannelChunkConv3d(1024, 8192, kernel_size=(1, 1, 1), stride=(1, 1, 1))
    (act): SiLU()
  )
  (depth_conv): ConvLayer(
    (conv): ChannelChunkConv3d(8192, 8192, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=8192)
  )
  (point_conv): ConvLayer(
    (conv): ChannelChunkConv3d(4096, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© OpSequential init with op_list=[EfficientViTBlock(
  (context_module): ResidualBlock(
    (main): LiteMLA(
      (qkv): ConvLayer(
        (conv): ChannelChunkConv3d(1024, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
      )
      (aggreg): ModuleList(
        (0): Sequential(
          (0): ChannelChunkConv3d(3072, 3072, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=3072, bias=False)
          (1): ChannelChunkConv3d(3072, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=96, bias=False)
        )
      )
      (kernel_func): ReLU()
      (proj): ConvLayer(
        (conv): ChannelChunkConv3d(2048, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (norm): RMSNorm3d()
      )
    )
    (shortcut): IdentityLayer()
  )
  (local_module): ResidualBlock(
    (main): GLUMBConv(
      (glu_act): SiLU()
      (inverted_conv): ConvLayer(
        (conv): ChannelChunkConv3d(1024, 8192, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): SiLU()
      )
      (depth_conv): ConvLayer(
        (conv): ChannelChunkConv3d(8192, 8192, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=8192)
      )
      (point_conv): ConvLayer(
        (conv): ChannelChunkConv3d(4096, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (norm): RMSNorm3d()
      )
    )
    (shortcut): IdentityLayer()
  )
), EfficientViTBlock(
  (context_module): ResidualBlock(
    (main): LiteMLA(
      (qkv): ConvLayer(
        (conv): ChannelChunkConv3d(1024, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
      )
      (aggreg): ModuleList(
        (0): Sequential(
          (0): ChannelChunkConv3d(3072, 3072, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=3072, bias=False)
          (1): ChannelChunkConv3d(3072, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=96, bias=False)
        )
      )
      (kernel_func): ReLU()
      (proj): ConvLayer(
        (conv): ChannelChunkConv3d(2048, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (norm): RMSNorm3d()
      )
    )
    (shortcut): IdentityLayer()
  )
  (local_module): ResidualBlock(
    (main): GLUMBConv(
      (glu_act): SiLU()
      (inverted_conv): ConvLayer(
        (conv): ChannelChunkConv3d(1024, 8192, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): SiLU()
      )
      (depth_conv): ConvLayer(
        (conv): ChannelChunkConv3d(8192, 8192, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=8192)
      )
      (point_conv): ConvLayer(
        (conv): ChannelChunkConv3d(4096, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (norm): RMSNorm3d()
      )
    )
    (shortcut): IdentityLayer()
  )
), EfficientViTBlock(
  (context_module): ResidualBlock(
    (main): LiteMLA(
      (qkv): ConvLayer(
        (conv): ChannelChunkConv3d(1024, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
      )
      (aggreg): ModuleList(
        (0): Sequential(
          (0): ChannelChunkConv3d(3072, 3072, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=3072, bias=False)
          (1): ChannelChunkConv3d(3072, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=96, bias=False)
        )
      )
      (kernel_func): ReLU()
      (proj): ConvLayer(
        (conv): ChannelChunkConv3d(2048, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (norm): RMSNorm3d()
      )
    )
    (shortcut): IdentityLayer()
  )
  (local_module): ResidualBlock(
    (main): GLUMBConv(
      (glu_act): SiLU()
      (inverted_conv): ConvLayer(
        (conv): ChannelChunkConv3d(1024, 8192, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): SiLU()
      )
      (depth_conv): ConvLayer(
        (conv): ChannelChunkConv3d(8192, 8192, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=8192)
      )
      (point_conv): ConvLayer(
        (conv): ChannelChunkConv3d(4096, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (norm): RMSNorm3d()
      )
    )
    (shortcut): IdentityLayer()
  )
)]
ðŸŸ© build_upsample_block block_type='InterpolateConv' in_channels=1024 out_channels=1024 shortcut='duplicating' is_video=True temporal_upsample=True
ðŸŸ© InterpolateConvUpSampleLayer.__init__ in_channels=1024 out_channels=1024 kernel_size=3 factor=2 mode='nearest' is_video=True temporal_upsample=True
ðŸŸ© ConvLayer.__init__ in_channels=1024 out_channels=1024 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=1024 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ChannelDuplicatingPixelShuffleUpSampleLayer.__init__ in_channels=1024 out_channels=1024 factor=2 temporal_upsample=True
ðŸŸ© ResidualBlock.__init__ main=InterpolateConvUpSampleLayer(factor=2, mode=nearest, temporal_upsample=True) shortcut=ChannelDuplicatingPixelShuffleUpSampleLayer(in_channels=1024, out_channels=1024, factor=2, temporal_upsample=True) post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© build_stage_main width=1024 depth=3 block_type='EViTS5_GLU' norm='rms3d' act='silu' input_width=1024 is_video=True
ðŸŸ© build_block block_type='EViTS5_GLU' in_channels=1024 out_channels=1024 norm='rms3d' act='silu' is_video=True
ðŸŸ© EfficientViTBlock.__init__ in_channels=1024 heads_ratio=1.0 dim=32 expand_ratio=4 scales=(5,) norm='rms3d' act_func='silu' context_module='LiteMLA' local_module='GLUMBConv' is_video=True
ðŸŸ© LiteMLA.__init__ in_channels=1024 out_channels=1024 heads=None heads_ratio=1.0 dim=32 use_bias=False norm=(None, 'rms3d') act_func=(None, None) kernel_func='relu' scales=(5,) eps=1e-15 is_video=True
ðŸŸ© ConvLayer.__init__ in_channels=1024 out_channels=3072 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=3072 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© get_same_padding kernel_size=5
ðŸŸ¦ get_same_padding result: 2
ðŸŸ© build_act name='relu' kwargs={'inplace': False}
ðŸŸ© build_kwargs_from_config config={'inplace': False} target_func=<class 'torch.nn.modules.activation.ReLU'>
ðŸŸ© ConvLayer.__init__ in_channels=2048 out_channels=1024 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=1024 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 1024} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=1024, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=LiteMLA(
  (qkv): ConvLayer(
    (conv): ChannelChunkConv3d(1024, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
  (aggreg): ModuleList(
    (0): Sequential(
      (0): ChannelChunkConv3d(3072, 3072, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=3072, bias=False)
      (1): ChannelChunkConv3d(3072, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=96, bias=False)
    )
  )
  (kernel_func): ReLU()
  (proj): ConvLayer(
    (conv): ChannelChunkConv3d(2048, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© GLUMBConv.__init__ in_channels=1024 out_channels=1024 kernel_size=3 stride=1 mid_channels=None expand_ratio=4 use_bias=(True, True, False) norm=(None, None, 'rms3d') act_func=('silu', 'silu', None) is_video=True
ðŸŸ© build_act name='silu' kwargs={'inplace': False}
ðŸŸ© build_kwargs_from_config config={'inplace': False} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=1024 out_channels=8192 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func='silu' is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=8192 kwargs={}
ðŸŸ© build_act name='silu' kwargs={}
ðŸŸ© build_kwargs_from_config config={} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=8192 out_channels=8192 kernel_size=3 stride=1 dilation=1 groups=8192 use_bias=True dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=8192 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ConvLayer.__init__ in_channels=4096 out_channels=1024 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=1024 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 1024} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=1024, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=GLUMBConv(
  (glu_act): SiLU()
  (inverted_conv): ConvLayer(
    (conv): ChannelChunkConv3d(1024, 8192, kernel_size=(1, 1, 1), stride=(1, 1, 1))
    (act): SiLU()
  )
  (depth_conv): ConvLayer(
    (conv): ChannelChunkConv3d(8192, 8192, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=8192)
  )
  (point_conv): ConvLayer(
    (conv): ChannelChunkConv3d(4096, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© build_block block_type='EViTS5_GLU' in_channels=1024 out_channels=1024 norm='rms3d' act='silu' is_video=True
ðŸŸ© EfficientViTBlock.__init__ in_channels=1024 heads_ratio=1.0 dim=32 expand_ratio=4 scales=(5,) norm='rms3d' act_func='silu' context_module='LiteMLA' local_module='GLUMBConv' is_video=True
ðŸŸ© LiteMLA.__init__ in_channels=1024 out_channels=1024 heads=None heads_ratio=1.0 dim=32 use_bias=False norm=(None, 'rms3d') act_func=(None, None) kernel_func='relu' scales=(5,) eps=1e-15 is_video=True
ðŸŸ© ConvLayer.__init__ in_channels=1024 out_channels=3072 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=3072 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© get_same_padding kernel_size=5
ðŸŸ¦ get_same_padding result: 2
ðŸŸ© build_act name='relu' kwargs={'inplace': False}
ðŸŸ© build_kwargs_from_config config={'inplace': False} target_func=<class 'torch.nn.modules.activation.ReLU'>
ðŸŸ© ConvLayer.__init__ in_channels=2048 out_channels=1024 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=1024 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 1024} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=1024, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=LiteMLA(
  (qkv): ConvLayer(
    (conv): ChannelChunkConv3d(1024, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
  (aggreg): ModuleList(
    (0): Sequential(
      (0): ChannelChunkConv3d(3072, 3072, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=3072, bias=False)
      (1): ChannelChunkConv3d(3072, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=96, bias=False)
    )
  )
  (kernel_func): ReLU()
  (proj): ConvLayer(
    (conv): ChannelChunkConv3d(2048, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© GLUMBConv.__init__ in_channels=1024 out_channels=1024 kernel_size=3 stride=1 mid_channels=None expand_ratio=4 use_bias=(True, True, False) norm=(None, None, 'rms3d') act_func=('silu', 'silu', None) is_video=True
ðŸŸ© build_act name='silu' kwargs={'inplace': False}
ðŸŸ© build_kwargs_from_config config={'inplace': False} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=1024 out_channels=8192 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func='silu' is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=8192 kwargs={}
ðŸŸ© build_act name='silu' kwargs={}
ðŸŸ© build_kwargs_from_config config={} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=8192 out_channels=8192 kernel_size=3 stride=1 dilation=1 groups=8192 use_bias=True dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=8192 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ConvLayer.__init__ in_channels=4096 out_channels=1024 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=1024 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 1024} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=1024, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=GLUMBConv(
  (glu_act): SiLU()
  (inverted_conv): ConvLayer(
    (conv): ChannelChunkConv3d(1024, 8192, kernel_size=(1, 1, 1), stride=(1, 1, 1))
    (act): SiLU()
  )
  (depth_conv): ConvLayer(
    (conv): ChannelChunkConv3d(8192, 8192, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=8192)
  )
  (point_conv): ConvLayer(
    (conv): ChannelChunkConv3d(4096, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© build_block block_type='EViTS5_GLU' in_channels=1024 out_channels=1024 norm='rms3d' act='silu' is_video=True
ðŸŸ© EfficientViTBlock.__init__ in_channels=1024 heads_ratio=1.0 dim=32 expand_ratio=4 scales=(5,) norm='rms3d' act_func='silu' context_module='LiteMLA' local_module='GLUMBConv' is_video=True
ðŸŸ© LiteMLA.__init__ in_channels=1024 out_channels=1024 heads=None heads_ratio=1.0 dim=32 use_bias=False norm=(None, 'rms3d') act_func=(None, None) kernel_func='relu' scales=(5,) eps=1e-15 is_video=True
ðŸŸ© ConvLayer.__init__ in_channels=1024 out_channels=3072 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=3072 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© get_same_padding kernel_size=5
ðŸŸ¦ get_same_padding result: 2
ðŸŸ© build_act name='relu' kwargs={'inplace': False}
ðŸŸ© build_kwargs_from_config config={'inplace': False} target_func=<class 'torch.nn.modules.activation.ReLU'>
ðŸŸ© ConvLayer.__init__ in_channels=2048 out_channels=1024 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=1024 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 1024} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=1024, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=LiteMLA(
  (qkv): ConvLayer(
    (conv): ChannelChunkConv3d(1024, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
  (aggreg): ModuleList(
    (0): Sequential(
      (0): ChannelChunkConv3d(3072, 3072, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=3072, bias=False)
      (1): ChannelChunkConv3d(3072, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=96, bias=False)
    )
  )
  (kernel_func): ReLU()
  (proj): ConvLayer(
    (conv): ChannelChunkConv3d(2048, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© GLUMBConv.__init__ in_channels=1024 out_channels=1024 kernel_size=3 stride=1 mid_channels=None expand_ratio=4 use_bias=(True, True, False) norm=(None, None, 'rms3d') act_func=('silu', 'silu', None) is_video=True
ðŸŸ© build_act name='silu' kwargs={'inplace': False}
ðŸŸ© build_kwargs_from_config config={'inplace': False} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=1024 out_channels=8192 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func='silu' is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=8192 kwargs={}
ðŸŸ© build_act name='silu' kwargs={}
ðŸŸ© build_kwargs_from_config config={} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=8192 out_channels=8192 kernel_size=3 stride=1 dilation=1 groups=8192 use_bias=True dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=8192 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ConvLayer.__init__ in_channels=4096 out_channels=1024 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=1024 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 1024} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=1024, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=GLUMBConv(
  (glu_act): SiLU()
  (inverted_conv): ConvLayer(
    (conv): ChannelChunkConv3d(1024, 8192, kernel_size=(1, 1, 1), stride=(1, 1, 1))
    (act): SiLU()
  )
  (depth_conv): ConvLayer(
    (conv): ChannelChunkConv3d(8192, 8192, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=8192)
  )
  (point_conv): ConvLayer(
    (conv): ChannelChunkConv3d(4096, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© OpSequential init with op_list=[ResidualBlock(
  (main): InterpolateConvUpSampleLayer(factor=2, mode=nearest, temporal_upsample=True)
  (shortcut): ChannelDuplicatingPixelShuffleUpSampleLayer(in_channels=1024, out_channels=1024, factor=2, temporal_upsample=True)
), EfficientViTBlock(
  (context_module): ResidualBlock(
    (main): LiteMLA(
      (qkv): ConvLayer(
        (conv): ChannelChunkConv3d(1024, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
      )
      (aggreg): ModuleList(
        (0): Sequential(
          (0): ChannelChunkConv3d(3072, 3072, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=3072, bias=False)
          (1): ChannelChunkConv3d(3072, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=96, bias=False)
        )
      )
      (kernel_func): ReLU()
      (proj): ConvLayer(
        (conv): ChannelChunkConv3d(2048, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (norm): RMSNorm3d()
      )
    )
    (shortcut): IdentityLayer()
  )
  (local_module): ResidualBlock(
    (main): GLUMBConv(
      (glu_act): SiLU()
      (inverted_conv): ConvLayer(
        (conv): ChannelChunkConv3d(1024, 8192, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): SiLU()
      )
      (depth_conv): ConvLayer(
        (conv): ChannelChunkConv3d(8192, 8192, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=8192)
      )
      (point_conv): ConvLayer(
        (conv): ChannelChunkConv3d(4096, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (norm): RMSNorm3d()
      )
    )
    (shortcut): IdentityLayer()
  )
), EfficientViTBlock(
  (context_module): ResidualBlock(
    (main): LiteMLA(
      (qkv): ConvLayer(
        (conv): ChannelChunkConv3d(1024, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
      )
      (aggreg): ModuleList(
        (0): Sequential(
          (0): ChannelChunkConv3d(3072, 3072, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=3072, bias=False)
          (1): ChannelChunkConv3d(3072, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=96, bias=False)
        )
      )
      (kernel_func): ReLU()
      (proj): ConvLayer(
        (conv): ChannelChunkConv3d(2048, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (norm): RMSNorm3d()
      )
    )
    (shortcut): IdentityLayer()
  )
  (local_module): ResidualBlock(
    (main): GLUMBConv(
      (glu_act): SiLU()
      (inverted_conv): ConvLayer(
        (conv): ChannelChunkConv3d(1024, 8192, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): SiLU()
      )
      (depth_conv): ConvLayer(
        (conv): ChannelChunkConv3d(8192, 8192, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=8192)
      )
      (point_conv): ConvLayer(
        (conv): ChannelChunkConv3d(4096, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (norm): RMSNorm3d()
      )
    )
    (shortcut): IdentityLayer()
  )
), EfficientViTBlock(
  (context_module): ResidualBlock(
    (main): LiteMLA(
      (qkv): ConvLayer(
        (conv): ChannelChunkConv3d(1024, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
      )
      (aggreg): ModuleList(
        (0): Sequential(
          (0): ChannelChunkConv3d(3072, 3072, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=3072, bias=False)
          (1): ChannelChunkConv3d(3072, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=96, bias=False)
        )
      )
      (kernel_func): ReLU()
      (proj): ConvLayer(
        (conv): ChannelChunkConv3d(2048, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (norm): RMSNorm3d()
      )
    )
    (shortcut): IdentityLayer()
  )
  (local_module): ResidualBlock(
    (main): GLUMBConv(
      (glu_act): SiLU()
      (inverted_conv): ConvLayer(
        (conv): ChannelChunkConv3d(1024, 8192, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): SiLU()
      )
      (depth_conv): ConvLayer(
        (conv): ChannelChunkConv3d(8192, 8192, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=8192)
      )
      (point_conv): ConvLayer(
        (conv): ChannelChunkConv3d(4096, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (norm): RMSNorm3d()
      )
    )
    (shortcut): IdentityLayer()
  )
)]
ðŸŸ© build_upsample_block block_type='InterpolateConv' in_channels=1024 out_channels=512 shortcut='duplicating' is_video=True temporal_upsample=True
ðŸŸ© InterpolateConvUpSampleLayer.__init__ in_channels=1024 out_channels=512 kernel_size=3 factor=2 mode='nearest' is_video=True temporal_upsample=True
ðŸŸ© ConvLayer.__init__ in_channels=1024 out_channels=512 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=512 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ChannelDuplicatingPixelShuffleUpSampleLayer.__init__ in_channels=1024 out_channels=512 factor=2 temporal_upsample=True
ðŸŸ© ResidualBlock.__init__ main=InterpolateConvUpSampleLayer(factor=2, mode=nearest, temporal_upsample=True) shortcut=ChannelDuplicatingPixelShuffleUpSampleLayer(in_channels=1024, out_channels=512, factor=2, temporal_upsample=True) post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© build_stage_main width=512 depth=3 block_type='EViTS5_GLU' norm='rms3d' act='silu' input_width=512 is_video=True
ðŸŸ© build_block block_type='EViTS5_GLU' in_channels=512 out_channels=512 norm='rms3d' act='silu' is_video=True
ðŸŸ© EfficientViTBlock.__init__ in_channels=512 heads_ratio=1.0 dim=32 expand_ratio=4 scales=(5,) norm='rms3d' act_func='silu' context_module='LiteMLA' local_module='GLUMBConv' is_video=True
ðŸŸ© LiteMLA.__init__ in_channels=512 out_channels=512 heads=None heads_ratio=1.0 dim=32 use_bias=False norm=(None, 'rms3d') act_func=(None, None) kernel_func='relu' scales=(5,) eps=1e-15 is_video=True
ðŸŸ© ConvLayer.__init__ in_channels=512 out_channels=1536 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=1536 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© get_same_padding kernel_size=5
ðŸŸ¦ get_same_padding result: 2
ðŸŸ© build_act name='relu' kwargs={'inplace': False}
ðŸŸ© build_kwargs_from_config config={'inplace': False} target_func=<class 'torch.nn.modules.activation.ReLU'>
ðŸŸ© ConvLayer.__init__ in_channels=1024 out_channels=512 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=512 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 512} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=512, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=LiteMLA(
  (qkv): ConvLayer(
    (conv): ChannelChunkConv3d(512, 1536, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
  (aggreg): ModuleList(
    (0): Sequential(
      (0): ChannelChunkConv3d(1536, 1536, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=1536, bias=False)
      (1): ChannelChunkConv3d(1536, 1536, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=48, bias=False)
    )
  )
  (kernel_func): ReLU()
  (proj): ConvLayer(
    (conv): ChannelChunkConv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© GLUMBConv.__init__ in_channels=512 out_channels=512 kernel_size=3 stride=1 mid_channels=None expand_ratio=4 use_bias=(True, True, False) norm=(None, None, 'rms3d') act_func=('silu', 'silu', None) is_video=True
ðŸŸ© build_act name='silu' kwargs={'inplace': False}
ðŸŸ© build_kwargs_from_config config={'inplace': False} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=512 out_channels=4096 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func='silu' is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=4096 kwargs={}
ðŸŸ© build_act name='silu' kwargs={}
ðŸŸ© build_kwargs_from_config config={} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=4096 out_channels=4096 kernel_size=3 stride=1 dilation=1 groups=4096 use_bias=True dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=4096 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ConvLayer.__init__ in_channels=2048 out_channels=512 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=512 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 512} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=512, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=GLUMBConv(
  (glu_act): SiLU()
  (inverted_conv): ConvLayer(
    (conv): ChannelChunkConv3d(512, 4096, kernel_size=(1, 1, 1), stride=(1, 1, 1))
    (act): SiLU()
  )
  (depth_conv): ConvLayer(
    (conv): ChannelChunkConv3d(4096, 4096, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=4096)
  )
  (point_conv): ConvLayer(
    (conv): ChannelChunkConv3d(2048, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© build_block block_type='EViTS5_GLU' in_channels=512 out_channels=512 norm='rms3d' act='silu' is_video=True
ðŸŸ© EfficientViTBlock.__init__ in_channels=512 heads_ratio=1.0 dim=32 expand_ratio=4 scales=(5,) norm='rms3d' act_func='silu' context_module='LiteMLA' local_module='GLUMBConv' is_video=True
ðŸŸ© LiteMLA.__init__ in_channels=512 out_channels=512 heads=None heads_ratio=1.0 dim=32 use_bias=False norm=(None, 'rms3d') act_func=(None, None) kernel_func='relu' scales=(5,) eps=1e-15 is_video=True
ðŸŸ© ConvLayer.__init__ in_channels=512 out_channels=1536 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=1536 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© get_same_padding kernel_size=5
ðŸŸ¦ get_same_padding result: 2
ðŸŸ© build_act name='relu' kwargs={'inplace': False}
ðŸŸ© build_kwargs_from_config config={'inplace': False} target_func=<class 'torch.nn.modules.activation.ReLU'>
ðŸŸ© ConvLayer.__init__ in_channels=1024 out_channels=512 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=512 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 512} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=512, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=LiteMLA(
  (qkv): ConvLayer(
    (conv): ChannelChunkConv3d(512, 1536, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
  (aggreg): ModuleList(
    (0): Sequential(
      (0): ChannelChunkConv3d(1536, 1536, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=1536, bias=False)
      (1): ChannelChunkConv3d(1536, 1536, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=48, bias=False)
    )
  )
  (kernel_func): ReLU()
  (proj): ConvLayer(
    (conv): ChannelChunkConv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© GLUMBConv.__init__ in_channels=512 out_channels=512 kernel_size=3 stride=1 mid_channels=None expand_ratio=4 use_bias=(True, True, False) norm=(None, None, 'rms3d') act_func=('silu', 'silu', None) is_video=True
ðŸŸ© build_act name='silu' kwargs={'inplace': False}
ðŸŸ© build_kwargs_from_config config={'inplace': False} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=512 out_channels=4096 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func='silu' is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=4096 kwargs={}
ðŸŸ© build_act name='silu' kwargs={}
ðŸŸ© build_kwargs_from_config config={} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=4096 out_channels=4096 kernel_size=3 stride=1 dilation=1 groups=4096 use_bias=True dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=4096 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ConvLayer.__init__ in_channels=2048 out_channels=512 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=512 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 512} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=512, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=GLUMBConv(
  (glu_act): SiLU()
  (inverted_conv): ConvLayer(
    (conv): ChannelChunkConv3d(512, 4096, kernel_size=(1, 1, 1), stride=(1, 1, 1))
    (act): SiLU()
  )
  (depth_conv): ConvLayer(
    (conv): ChannelChunkConv3d(4096, 4096, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=4096)
  )
  (point_conv): ConvLayer(
    (conv): ChannelChunkConv3d(2048, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© build_block block_type='EViTS5_GLU' in_channels=512 out_channels=512 norm='rms3d' act='silu' is_video=True
ðŸŸ© EfficientViTBlock.__init__ in_channels=512 heads_ratio=1.0 dim=32 expand_ratio=4 scales=(5,) norm='rms3d' act_func='silu' context_module='LiteMLA' local_module='GLUMBConv' is_video=True
ðŸŸ© LiteMLA.__init__ in_channels=512 out_channels=512 heads=None heads_ratio=1.0 dim=32 use_bias=False norm=(None, 'rms3d') act_func=(None, None) kernel_func='relu' scales=(5,) eps=1e-15 is_video=True
ðŸŸ© ConvLayer.__init__ in_channels=512 out_channels=1536 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=1536 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© get_same_padding kernel_size=5
ðŸŸ¦ get_same_padding result: 2
ðŸŸ© build_act name='relu' kwargs={'inplace': False}
ðŸŸ© build_kwargs_from_config config={'inplace': False} target_func=<class 'torch.nn.modules.activation.ReLU'>
ðŸŸ© ConvLayer.__init__ in_channels=1024 out_channels=512 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=512 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 512} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=512, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=LiteMLA(
  (qkv): ConvLayer(
    (conv): ChannelChunkConv3d(512, 1536, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
  (aggreg): ModuleList(
    (0): Sequential(
      (0): ChannelChunkConv3d(1536, 1536, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=1536, bias=False)
      (1): ChannelChunkConv3d(1536, 1536, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=48, bias=False)
    )
  )
  (kernel_func): ReLU()
  (proj): ConvLayer(
    (conv): ChannelChunkConv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© GLUMBConv.__init__ in_channels=512 out_channels=512 kernel_size=3 stride=1 mid_channels=None expand_ratio=4 use_bias=(True, True, False) norm=(None, None, 'rms3d') act_func=('silu', 'silu', None) is_video=True
ðŸŸ© build_act name='silu' kwargs={'inplace': False}
ðŸŸ© build_kwargs_from_config config={'inplace': False} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=512 out_channels=4096 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func='silu' is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=4096 kwargs={}
ðŸŸ© build_act name='silu' kwargs={}
ðŸŸ© build_kwargs_from_config config={} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=4096 out_channels=4096 kernel_size=3 stride=1 dilation=1 groups=4096 use_bias=True dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=4096 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ConvLayer.__init__ in_channels=2048 out_channels=512 kernel_size=1 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=512 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 512} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=512, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=GLUMBConv(
  (glu_act): SiLU()
  (inverted_conv): ConvLayer(
    (conv): ChannelChunkConv3d(512, 4096, kernel_size=(1, 1, 1), stride=(1, 1, 1))
    (act): SiLU()
  )
  (depth_conv): ConvLayer(
    (conv): ChannelChunkConv3d(4096, 4096, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=4096)
  )
  (point_conv): ConvLayer(
    (conv): ChannelChunkConv3d(2048, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© OpSequential init with op_list=[ResidualBlock(
  (main): InterpolateConvUpSampleLayer(factor=2, mode=nearest, temporal_upsample=True)
  (shortcut): ChannelDuplicatingPixelShuffleUpSampleLayer(in_channels=1024, out_channels=512, factor=2, temporal_upsample=True)
), EfficientViTBlock(
  (context_module): ResidualBlock(
    (main): LiteMLA(
      (qkv): ConvLayer(
        (conv): ChannelChunkConv3d(512, 1536, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
      )
      (aggreg): ModuleList(
        (0): Sequential(
          (0): ChannelChunkConv3d(1536, 1536, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=1536, bias=False)
          (1): ChannelChunkConv3d(1536, 1536, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=48, bias=False)
        )
      )
      (kernel_func): ReLU()
      (proj): ConvLayer(
        (conv): ChannelChunkConv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (norm): RMSNorm3d()
      )
    )
    (shortcut): IdentityLayer()
  )
  (local_module): ResidualBlock(
    (main): GLUMBConv(
      (glu_act): SiLU()
      (inverted_conv): ConvLayer(
        (conv): ChannelChunkConv3d(512, 4096, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): SiLU()
      )
      (depth_conv): ConvLayer(
        (conv): ChannelChunkConv3d(4096, 4096, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=4096)
      )
      (point_conv): ConvLayer(
        (conv): ChannelChunkConv3d(2048, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (norm): RMSNorm3d()
      )
    )
    (shortcut): IdentityLayer()
  )
), EfficientViTBlock(
  (context_module): ResidualBlock(
    (main): LiteMLA(
      (qkv): ConvLayer(
        (conv): ChannelChunkConv3d(512, 1536, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
      )
      (aggreg): ModuleList(
        (0): Sequential(
          (0): ChannelChunkConv3d(1536, 1536, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=1536, bias=False)
          (1): ChannelChunkConv3d(1536, 1536, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=48, bias=False)
        )
      )
      (kernel_func): ReLU()
      (proj): ConvLayer(
        (conv): ChannelChunkConv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (norm): RMSNorm3d()
      )
    )
    (shortcut): IdentityLayer()
  )
  (local_module): ResidualBlock(
    (main): GLUMBConv(
      (glu_act): SiLU()
      (inverted_conv): ConvLayer(
        (conv): ChannelChunkConv3d(512, 4096, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): SiLU()
      )
      (depth_conv): ConvLayer(
        (conv): ChannelChunkConv3d(4096, 4096, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=4096)
      )
      (point_conv): ConvLayer(
        (conv): ChannelChunkConv3d(2048, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (norm): RMSNorm3d()
      )
    )
    (shortcut): IdentityLayer()
  )
), EfficientViTBlock(
  (context_module): ResidualBlock(
    (main): LiteMLA(
      (qkv): ConvLayer(
        (conv): ChannelChunkConv3d(512, 1536, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
      )
      (aggreg): ModuleList(
        (0): Sequential(
          (0): ChannelChunkConv3d(1536, 1536, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=1536, bias=False)
          (1): ChannelChunkConv3d(1536, 1536, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=48, bias=False)
        )
      )
      (kernel_func): ReLU()
      (proj): ConvLayer(
        (conv): ChannelChunkConv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (norm): RMSNorm3d()
      )
    )
    (shortcut): IdentityLayer()
  )
  (local_module): ResidualBlock(
    (main): GLUMBConv(
      (glu_act): SiLU()
      (inverted_conv): ConvLayer(
        (conv): ChannelChunkConv3d(512, 4096, kernel_size=(1, 1, 1), stride=(1, 1, 1))
        (act): SiLU()
      )
      (depth_conv): ConvLayer(
        (conv): ChannelChunkConv3d(4096, 4096, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=4096)
      )
      (point_conv): ConvLayer(
        (conv): ChannelChunkConv3d(2048, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (norm): RMSNorm3d()
      )
    )
    (shortcut): IdentityLayer()
  )
)]
ðŸŸ© build_upsample_block block_type='InterpolateConv' in_channels=512 out_channels=512 shortcut='duplicating' is_video=True temporal_upsample=False
ðŸŸ© InterpolateConvUpSampleLayer.__init__ in_channels=512 out_channels=512 kernel_size=3 factor=2 mode='nearest' is_video=True temporal_upsample=False
ðŸŸ© ConvLayer.__init__ in_channels=512 out_channels=512 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=512 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ChannelDuplicatingPixelShuffleUpSampleLayer.__init__ in_channels=512 out_channels=512 factor=2 temporal_upsample=False
ðŸŸ© ResidualBlock.__init__ main=InterpolateConvUpSampleLayer(factor=2, mode=nearest, temporal_upsample=False) shortcut=ChannelDuplicatingPixelShuffleUpSampleLayer(in_channels=512, out_channels=512, factor=2, temporal_upsample=False) post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© build_stage_main width=512 depth=3 block_type='ResBlock' norm='rms3d' act='silu' input_width=512 is_video=True
ðŸŸ© build_block block_type='ResBlock' in_channels=512 out_channels=512 norm='rms3d' act='silu' is_video=True
ðŸŸ© ResBlock.__init__ in_channels=512 out_channels=512 kernel_size=3 stride=1 mid_channels=None expand_ratio=1 use_bias=(True, False) norm=(None, 'rms3d') act_func=('silu', None) is_video=True
ðŸŸ© ConvLayer.__init__ in_channels=512 out_channels=512 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func='silu' is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=512 kwargs={}
ðŸŸ© build_act name='silu' kwargs={}
ðŸŸ© build_kwargs_from_config config={} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=512 out_channels=512 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=512 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 512} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=512, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=ResBlock(
  (conv1): ConvLayer(
    (conv): ChannelChunkConv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1))
    (act): SiLU()
  )
  (conv2): ConvLayer(
    (conv): ChannelChunkConv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© build_block block_type='ResBlock' in_channels=512 out_channels=512 norm='rms3d' act='silu' is_video=True
ðŸŸ© ResBlock.__init__ in_channels=512 out_channels=512 kernel_size=3 stride=1 mid_channels=None expand_ratio=1 use_bias=(True, False) norm=(None, 'rms3d') act_func=('silu', None) is_video=True
ðŸŸ© ConvLayer.__init__ in_channels=512 out_channels=512 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func='silu' is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=512 kwargs={}
ðŸŸ© build_act name='silu' kwargs={}
ðŸŸ© build_kwargs_from_config config={} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=512 out_channels=512 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=512 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 512} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=512, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=ResBlock(
  (conv1): ConvLayer(
    (conv): ChannelChunkConv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1))
    (act): SiLU()
  )
  (conv2): ConvLayer(
    (conv): ChannelChunkConv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© build_block block_type='ResBlock' in_channels=512 out_channels=512 norm='rms3d' act='silu' is_video=True
ðŸŸ© ResBlock.__init__ in_channels=512 out_channels=512 kernel_size=3 stride=1 mid_channels=None expand_ratio=1 use_bias=(True, False) norm=(None, 'rms3d') act_func=('silu', None) is_video=True
ðŸŸ© ConvLayer.__init__ in_channels=512 out_channels=512 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func='silu' is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=512 kwargs={}
ðŸŸ© build_act name='silu' kwargs={}
ðŸŸ© build_kwargs_from_config config={} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=512 out_channels=512 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=512 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 512} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=512, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=ResBlock(
  (conv1): ConvLayer(
    (conv): ChannelChunkConv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1))
    (act): SiLU()
  )
  (conv2): ConvLayer(
    (conv): ChannelChunkConv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© OpSequential init with op_list=[ResidualBlock(
  (main): InterpolateConvUpSampleLayer(factor=2, mode=nearest, temporal_upsample=False)
  (shortcut): ChannelDuplicatingPixelShuffleUpSampleLayer(in_channels=512, out_channels=512, factor=2, temporal_upsample=False)
), ResidualBlock(
  (main): ResBlock(
    (conv1): ConvLayer(
      (conv): ChannelChunkConv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      (act): SiLU()
    )
    (conv2): ConvLayer(
      (conv): ChannelChunkConv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)
      (norm): RMSNorm3d()
    )
  )
  (shortcut): IdentityLayer()
), ResidualBlock(
  (main): ResBlock(
    (conv1): ConvLayer(
      (conv): ChannelChunkConv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      (act): SiLU()
    )
    (conv2): ConvLayer(
      (conv): ChannelChunkConv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)
      (norm): RMSNorm3d()
    )
  )
  (shortcut): IdentityLayer()
), ResidualBlock(
  (main): ResBlock(
    (conv1): ConvLayer(
      (conv): ChannelChunkConv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      (act): SiLU()
    )
    (conv2): ConvLayer(
      (conv): ChannelChunkConv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)
      (norm): RMSNorm3d()
    )
  )
  (shortcut): IdentityLayer()
)]
ðŸŸ© build_upsample_block block_type='InterpolateConv' in_channels=512 out_channels=256 shortcut='duplicating' is_video=True temporal_upsample=False
ðŸŸ© InterpolateConvUpSampleLayer.__init__ in_channels=512 out_channels=256 kernel_size=3 factor=2 mode='nearest' is_video=True temporal_upsample=False
ðŸŸ© ConvLayer.__init__ in_channels=512 out_channels=256 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=256 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ChannelDuplicatingPixelShuffleUpSampleLayer.__init__ in_channels=512 out_channels=256 factor=2 temporal_upsample=False
ðŸŸ© ResidualBlock.__init__ main=InterpolateConvUpSampleLayer(factor=2, mode=nearest, temporal_upsample=False) shortcut=ChannelDuplicatingPixelShuffleUpSampleLayer(in_channels=512, out_channels=256, factor=2, temporal_upsample=False) post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© build_stage_main width=256 depth=3 block_type='ResBlock' norm='rms3d' act='silu' input_width=256 is_video=True
ðŸŸ© build_block block_type='ResBlock' in_channels=256 out_channels=256 norm='rms3d' act='silu' is_video=True
ðŸŸ© ResBlock.__init__ in_channels=256 out_channels=256 kernel_size=3 stride=1 mid_channels=None expand_ratio=1 use_bias=(True, False) norm=(None, 'rms3d') act_func=('silu', None) is_video=True
ðŸŸ© ConvLayer.__init__ in_channels=256 out_channels=256 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func='silu' is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=256 kwargs={}
ðŸŸ© build_act name='silu' kwargs={}
ðŸŸ© build_kwargs_from_config config={} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=256 out_channels=256 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=256 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 256} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=256, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=ResBlock(
  (conv1): ConvLayer(
    (conv): ChannelChunkConv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1))
    (act): SiLU()
  )
  (conv2): ConvLayer(
    (conv): ChannelChunkConv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© build_block block_type='ResBlock' in_channels=256 out_channels=256 norm='rms3d' act='silu' is_video=True
ðŸŸ© ResBlock.__init__ in_channels=256 out_channels=256 kernel_size=3 stride=1 mid_channels=None expand_ratio=1 use_bias=(True, False) norm=(None, 'rms3d') act_func=('silu', None) is_video=True
ðŸŸ© ConvLayer.__init__ in_channels=256 out_channels=256 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func='silu' is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=256 kwargs={}
ðŸŸ© build_act name='silu' kwargs={}
ðŸŸ© build_kwargs_from_config config={} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=256 out_channels=256 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=256 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 256} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=256, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=ResBlock(
  (conv1): ConvLayer(
    (conv): ChannelChunkConv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1))
    (act): SiLU()
  )
  (conv2): ConvLayer(
    (conv): ChannelChunkConv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© build_block block_type='ResBlock' in_channels=256 out_channels=256 norm='rms3d' act='silu' is_video=True
ðŸŸ© ResBlock.__init__ in_channels=256 out_channels=256 kernel_size=3 stride=1 mid_channels=None expand_ratio=1 use_bias=(True, False) norm=(None, 'rms3d') act_func=('silu', None) is_video=True
ðŸŸ© ConvLayer.__init__ in_channels=256 out_channels=256 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func='silu' is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=256 kwargs={}
ðŸŸ© build_act name='silu' kwargs={}
ðŸŸ© build_kwargs_from_config config={} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=256 out_channels=256 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=256 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 256} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=256, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=ResBlock(
  (conv1): ConvLayer(
    (conv): ChannelChunkConv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1))
    (act): SiLU()
  )
  (conv2): ConvLayer(
    (conv): ChannelChunkConv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© OpSequential init with op_list=[ResidualBlock(
  (main): InterpolateConvUpSampleLayer(factor=2, mode=nearest, temporal_upsample=False)
  (shortcut): ChannelDuplicatingPixelShuffleUpSampleLayer(in_channels=512, out_channels=256, factor=2, temporal_upsample=False)
), ResidualBlock(
  (main): ResBlock(
    (conv1): ConvLayer(
      (conv): ChannelChunkConv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      (act): SiLU()
    )
    (conv2): ConvLayer(
      (conv): ChannelChunkConv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)
      (norm): RMSNorm3d()
    )
  )
  (shortcut): IdentityLayer()
), ResidualBlock(
  (main): ResBlock(
    (conv1): ConvLayer(
      (conv): ChannelChunkConv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      (act): SiLU()
    )
    (conv2): ConvLayer(
      (conv): ChannelChunkConv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)
      (norm): RMSNorm3d()
    )
  )
  (shortcut): IdentityLayer()
), ResidualBlock(
  (main): ResBlock(
    (conv1): ConvLayer(
      (conv): ChannelChunkConv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      (act): SiLU()
    )
    (conv2): ConvLayer(
      (conv): ChannelChunkConv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)
      (norm): RMSNorm3d()
    )
  )
  (shortcut): IdentityLayer()
)]
ðŸŸ© build_upsample_block block_type='InterpolateConv' in_channels=256 out_channels=128 shortcut='duplicating' is_video=True temporal_upsample=False
ðŸŸ© InterpolateConvUpSampleLayer.__init__ in_channels=256 out_channels=128 kernel_size=3 factor=2 mode='nearest' is_video=True temporal_upsample=False
ðŸŸ© ConvLayer.__init__ in_channels=256 out_channels=128 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=128 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ChannelDuplicatingPixelShuffleUpSampleLayer.__init__ in_channels=256 out_channels=128 factor=2 temporal_upsample=False
ðŸŸ© ResidualBlock.__init__ main=InterpolateConvUpSampleLayer(factor=2, mode=nearest, temporal_upsample=False) shortcut=ChannelDuplicatingPixelShuffleUpSampleLayer(in_channels=256, out_channels=128, factor=2, temporal_upsample=False) post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© build_stage_main width=128 depth=3 block_type='ResBlock' norm='rms3d' act='silu' input_width=128 is_video=True
ðŸŸ© build_block block_type='ResBlock' in_channels=128 out_channels=128 norm='rms3d' act='silu' is_video=True
ðŸŸ© ResBlock.__init__ in_channels=128 out_channels=128 kernel_size=3 stride=1 mid_channels=None expand_ratio=1 use_bias=(True, False) norm=(None, 'rms3d') act_func=('silu', None) is_video=True
ðŸŸ© ConvLayer.__init__ in_channels=128 out_channels=128 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func='silu' is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=128 kwargs={}
ðŸŸ© build_act name='silu' kwargs={}
ðŸŸ© build_kwargs_from_config config={} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=128 out_channels=128 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=128 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 128} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=128, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=ResBlock(
  (conv1): ConvLayer(
    (conv): ChannelChunkConv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1))
    (act): SiLU()
  )
  (conv2): ConvLayer(
    (conv): ChannelChunkConv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© build_block block_type='ResBlock' in_channels=128 out_channels=128 norm='rms3d' act='silu' is_video=True
ðŸŸ© ResBlock.__init__ in_channels=128 out_channels=128 kernel_size=3 stride=1 mid_channels=None expand_ratio=1 use_bias=(True, False) norm=(None, 'rms3d') act_func=('silu', None) is_video=True
ðŸŸ© ConvLayer.__init__ in_channels=128 out_channels=128 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func='silu' is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=128 kwargs={}
ðŸŸ© build_act name='silu' kwargs={}
ðŸŸ© build_kwargs_from_config config={} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=128 out_channels=128 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=128 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 128} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=128, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=ResBlock(
  (conv1): ConvLayer(
    (conv): ChannelChunkConv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1))
    (act): SiLU()
  )
  (conv2): ConvLayer(
    (conv): ChannelChunkConv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© build_block block_type='ResBlock' in_channels=128 out_channels=128 norm='rms3d' act='silu' is_video=True
ðŸŸ© ResBlock.__init__ in_channels=128 out_channels=128 kernel_size=3 stride=1 mid_channels=None expand_ratio=1 use_bias=(True, False) norm=(None, 'rms3d') act_func=('silu', None) is_video=True
ðŸŸ© ConvLayer.__init__ in_channels=128 out_channels=128 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func='silu' is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=128 kwargs={}
ðŸŸ© build_act name='silu' kwargs={}
ðŸŸ© build_kwargs_from_config config={} target_func=<class 'torch.nn.modules.activation.SiLU'>
ðŸŸ© ConvLayer.__init__ in_channels=128 out_channels=128 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=False dropout=0 norm='rms3d' act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name='rms3d' num_features=128 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 128} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=128, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© ResidualBlock.__init__ main=ResBlock(
  (conv1): ConvLayer(
    (conv): ChannelChunkConv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1))
    (act): SiLU()
  )
  (conv2): ConvLayer(
    (conv): ChannelChunkConv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)
    (norm): RMSNorm3d()
  )
) shortcut=IdentityLayer() post_act=None pre_norm=None
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© OpSequential init with op_list=[ResidualBlock(
  (main): InterpolateConvUpSampleLayer(factor=2, mode=nearest, temporal_upsample=False)
  (shortcut): ChannelDuplicatingPixelShuffleUpSampleLayer(in_channels=256, out_channels=128, factor=2, temporal_upsample=False)
), ResidualBlock(
  (main): ResBlock(
    (conv1): ConvLayer(
      (conv): ChannelChunkConv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      (act): SiLU()
    )
    (conv2): ConvLayer(
      (conv): ChannelChunkConv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)
      (norm): RMSNorm3d()
    )
  )
  (shortcut): IdentityLayer()
), ResidualBlock(
  (main): ResBlock(
    (conv1): ConvLayer(
      (conv): ChannelChunkConv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      (act): SiLU()
    )
    (conv2): ConvLayer(
      (conv): ChannelChunkConv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)
      (norm): RMSNorm3d()
    )
  )
  (shortcut): IdentityLayer()
), ResidualBlock(
  (main): ResBlock(
    (conv1): ConvLayer(
      (conv): ChannelChunkConv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      (act): SiLU()
    )
    (conv2): ConvLayer(
      (conv): ChannelChunkConv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)
      (norm): RMSNorm3d()
    )
  )
  (shortcut): IdentityLayer()
)]
ðŸŸ© build_decoder_project_out_block in_channels=128 out_channels=3 factor=1 upsample_block_type='InterpolateConv' norm='rms3d' act='relu' is_video=True
ðŸŸ© build_norm name='rms3d' num_features=128 kwargs={}
ðŸŸ© build_kwargs_from_config config={'num_features': 128} target_func=<class '__main__.RMSNorm3d'>
ðŸŸ© RMSNorm2d init with num_features=128, eps=1e-05, elementwise_affine=True, bias=True
ðŸŸ© build_act name='relu' kwargs={}
ðŸŸ© build_kwargs_from_config config={} target_func=<class 'torch.nn.modules.activation.ReLU'>
ðŸŸ© ConvLayer.__init__ in_channels=128 out_channels=3 kernel_size=3 stride=1 dilation=1 groups=1 use_bias=True dropout=0 norm=None act_func=None is_video=True pad_mode_3d='constant'
ðŸŸ© build_norm name=None num_features=3 kwargs={}
ðŸŸ© build_act name=None kwargs={}
ðŸŸ© OpSequential init with op_list=[RMSNorm3d(), ReLU(), ConvLayer(
  (conv): ChannelChunkConv3d(128, 3, kernel_size=(3, 3, 3), stride=(1, 1, 1))
)]
ðŸŸ© init_modules model=DCAE_HF(
  (encoder): Encoder(
    (project_in): ConvLayer(
      (conv): ChannelChunkConv3d(3, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1))
    )
    (stages): ModuleList(
      (0): OpSequential(
        (op_list): ModuleList(
          (0-1): 2 x ResidualBlock(
            (main): ResBlock(
              (conv1): ConvLayer(
                (conv): ChannelChunkConv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1))
                (act): SiLU()
              )
              (conv2): ConvLayer(
                (conv): ChannelChunkConv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)
                (norm): RMSNorm3d()
              )
            )
            (shortcut): IdentityLayer()
          )
          (2): ResidualBlock(
            (main): ConvLayer(
              (conv): ChannelChunkConv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 2, 2))
            )
            (shortcut): PixelUnshuffleChannelAveragingDownSampleLayer(in_channels=128, out_channels=256, factor=2), temporal_downsample=False
          )
        )
      )
      (1): OpSequential(
        (op_list): ModuleList(
          (0-1): 2 x ResidualBlock(
            (main): ResBlock(
              (conv1): ConvLayer(
                (conv): ChannelChunkConv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1))
                (act): SiLU()
              )
              (conv2): ConvLayer(
                (conv): ChannelChunkConv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)
                (norm): RMSNorm3d()
              )
            )
            (shortcut): IdentityLayer()
          )
          (2): ResidualBlock(
            (main): ConvLayer(
              (conv): ChannelChunkConv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 2, 2))
            )
            (shortcut): PixelUnshuffleChannelAveragingDownSampleLayer(in_channels=256, out_channels=512, factor=2), temporal_downsample=False
          )
        )
      )
      (2): OpSequential(
        (op_list): ModuleList(
          (0-1): 2 x ResidualBlock(
            (main): ResBlock(
              (conv1): ConvLayer(
                (conv): ChannelChunkConv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1))
                (act): SiLU()
              )
              (conv2): ConvLayer(
                (conv): ChannelChunkConv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)
                (norm): RMSNorm3d()
              )
            )
            (shortcut): IdentityLayer()
          )
          (2): ResidualBlock(
            (main): ConvLayer(
              (conv): ChannelChunkConv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 2, 2))
            )
            (shortcut): PixelUnshuffleChannelAveragingDownSampleLayer(in_channels=512, out_channels=512, factor=2), temporal_downsample=False
          )
        )
      )
      (3): OpSequential(
        (op_list): ModuleList(
          (0-2): 3 x EfficientViTBlock(
            (context_module): ResidualBlock(
              (main): LiteMLA(
                (qkv): ConvLayer(
                  (conv): ChannelChunkConv3d(512, 1536, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                )
                (aggreg): ModuleList(
                  (0): Sequential(
                    (0): ChannelChunkConv3d(1536, 1536, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=1536, bias=False)
                    (1): ChannelChunkConv3d(1536, 1536, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=48, bias=False)
                  )
                )
                (kernel_func): ReLU()
                (proj): ConvLayer(
                  (conv): ChannelChunkConv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): RMSNorm3d()
                )
              )
              (shortcut): IdentityLayer()
            )
            (local_module): ResidualBlock(
              (main): GLUMBConv(
                (glu_act): SiLU()
                (inverted_conv): ConvLayer(
                  (conv): ChannelChunkConv3d(512, 4096, kernel_size=(1, 1, 1), stride=(1, 1, 1))
                  (act): SiLU()
                )
                (depth_conv): ConvLayer(
                  (conv): ChannelChunkConv3d(4096, 4096, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=4096)
                )
                (point_conv): ConvLayer(
                  (conv): ChannelChunkConv3d(2048, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): RMSNorm3d()
                )
              )
              (shortcut): IdentityLayer()
            )
          )
          (3): ResidualBlock(
            (main): ConvLayer(
              (conv): ChannelChunkConv3d(512, 1024, kernel_size=(3, 3, 3), stride=(2, 2, 2))
            )
            (shortcut): PixelUnshuffleChannelAveragingDownSampleLayer(in_channels=512, out_channels=1024, factor=2), temporal_downsample=True
          )
        )
      )
      (4): OpSequential(
        (op_list): ModuleList(
          (0-2): 3 x EfficientViTBlock(
            (context_module): ResidualBlock(
              (main): LiteMLA(
                (qkv): ConvLayer(
                  (conv): ChannelChunkConv3d(1024, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                )
                (aggreg): ModuleList(
                  (0): Sequential(
                    (0): ChannelChunkConv3d(3072, 3072, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=3072, bias=False)
                    (1): ChannelChunkConv3d(3072, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=96, bias=False)
                  )
                )
                (kernel_func): ReLU()
                (proj): ConvLayer(
                  (conv): ChannelChunkConv3d(2048, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): RMSNorm3d()
                )
              )
              (shortcut): IdentityLayer()
            )
            (local_module): ResidualBlock(
              (main): GLUMBConv(
                (glu_act): SiLU()
                (inverted_conv): ConvLayer(
                  (conv): ChannelChunkConv3d(1024, 8192, kernel_size=(1, 1, 1), stride=(1, 1, 1))
                  (act): SiLU()
                )
                (depth_conv): ConvLayer(
                  (conv): ChannelChunkConv3d(8192, 8192, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=8192)
                )
                (point_conv): ConvLayer(
                  (conv): ChannelChunkConv3d(4096, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): RMSNorm3d()
                )
              )
              (shortcut): IdentityLayer()
            )
          )
          (3): ResidualBlock(
            (main): ConvLayer(
              (conv): ChannelChunkConv3d(1024, 1024, kernel_size=(3, 3, 3), stride=(2, 2, 2))
            )
            (shortcut): PixelUnshuffleChannelAveragingDownSampleLayer(in_channels=1024, out_channels=1024, factor=2), temporal_downsample=True
          )
        )
      )
      (5): OpSequential(
        (op_list): ModuleList(
          (0-2): 3 x EfficientViTBlock(
            (context_module): ResidualBlock(
              (main): LiteMLA(
                (qkv): ConvLayer(
                  (conv): ChannelChunkConv3d(1024, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                )
                (aggreg): ModuleList(
                  (0): Sequential(
                    (0): ChannelChunkConv3d(3072, 3072, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=3072, bias=False)
                    (1): ChannelChunkConv3d(3072, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=96, bias=False)
                  )
                )
                (kernel_func): ReLU()
                (proj): ConvLayer(
                  (conv): ChannelChunkConv3d(2048, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): RMSNorm3d()
                )
              )
              (shortcut): IdentityLayer()
            )
            (local_module): ResidualBlock(
              (main): GLUMBConv(
                (glu_act): SiLU()
                (inverted_conv): ConvLayer(
                  (conv): ChannelChunkConv3d(1024, 8192, kernel_size=(1, 1, 1), stride=(1, 1, 1))
                  (act): SiLU()
                )
                (depth_conv): ConvLayer(
                  (conv): ChannelChunkConv3d(8192, 8192, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=8192)
                )
                (point_conv): ConvLayer(
                  (conv): ChannelChunkConv3d(4096, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): RMSNorm3d()
                )
              )
              (shortcut): IdentityLayer()
            )
          )
        )
      )
    )
    (project_out): ResidualBlock(
      (main): OpSequential(
        (op_list): ModuleList(
          (0): ConvLayer(
            (conv): ChannelChunkConv3d(1024, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1))
          )
        )
      )
      (shortcut): PixelUnshuffleChannelAveragingDownSampleLayer(in_channels=1024, out_channels=128, factor=1), temporal_downsample=False
    )
  )
  (decoder): Decoder(
    (project_in): ResidualBlock(
      (main): ConvLayer(
        (conv): ChannelChunkConv3d(128, 1024, kernel_size=(3, 3, 3), stride=(1, 1, 1))
      )
      (shortcut): ChannelDuplicatingPixelShuffleUpSampleLayer(in_channels=128, out_channels=1024, factor=1, temporal_upsample=False)
    )
    (stages): ModuleList(
      (0): OpSequential(
        (op_list): ModuleList(
          (0): ResidualBlock(
            (main): InterpolateConvUpSampleLayer(factor=2, mode=nearest, temporal_upsample=False)
            (shortcut): ChannelDuplicatingPixelShuffleUpSampleLayer(in_channels=256, out_channels=128, factor=2, temporal_upsample=False)
          )
          (1-3): 3 x ResidualBlock(
            (main): ResBlock(
              (conv1): ConvLayer(
                (conv): ChannelChunkConv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1))
                (act): SiLU()
              )
              (conv2): ConvLayer(
                (conv): ChannelChunkConv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)
                (norm): RMSNorm3d()
              )
            )
            (shortcut): IdentityLayer()
          )
        )
      )
      (1): OpSequential(
        (op_list): ModuleList(
          (0): ResidualBlock(
            (main): InterpolateConvUpSampleLayer(factor=2, mode=nearest, temporal_upsample=False)
            (shortcut): ChannelDuplicatingPixelShuffleUpSampleLayer(in_channels=512, out_channels=256, factor=2, temporal_upsample=False)
          )
          (1-3): 3 x ResidualBlock(
            (main): ResBlock(
              (conv1): ConvLayer(
                (conv): ChannelChunkConv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1))
                (act): SiLU()
              )
              (conv2): ConvLayer(
                (conv): ChannelChunkConv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)
                (norm): RMSNorm3d()
              )
            )
            (shortcut): IdentityLayer()
          )
        )
      )
      (2): OpSequential(
        (op_list): ModuleList(
          (0): ResidualBlock(
            (main): InterpolateConvUpSampleLayer(factor=2, mode=nearest, temporal_upsample=False)
            (shortcut): ChannelDuplicatingPixelShuffleUpSampleLayer(in_channels=512, out_channels=512, factor=2, temporal_upsample=False)
          )
          (1-3): 3 x ResidualBlock(
            (main): ResBlock(
              (conv1): ConvLayer(
                (conv): ChannelChunkConv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1))
                (act): SiLU()
              )
              (conv2): ConvLayer(
                (conv): ChannelChunkConv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)
                (norm): RMSNorm3d()
              )
            )
            (shortcut): IdentityLayer()
          )
        )
      )
      (3): OpSequential(
        (op_list): ModuleList(
          (0): ResidualBlock(
            (main): InterpolateConvUpSampleLayer(factor=2, mode=nearest, temporal_upsample=True)
            (shortcut): ChannelDuplicatingPixelShuffleUpSampleLayer(in_channels=1024, out_channels=512, factor=2, temporal_upsample=True)
          )
          (1-3): 3 x EfficientViTBlock(
            (context_module): ResidualBlock(
              (main): LiteMLA(
                (qkv): ConvLayer(
                  (conv): ChannelChunkConv3d(512, 1536, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                )
                (aggreg): ModuleList(
                  (0): Sequential(
                    (0): ChannelChunkConv3d(1536, 1536, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=1536, bias=False)
                    (1): ChannelChunkConv3d(1536, 1536, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=48, bias=False)
                  )
                )
                (kernel_func): ReLU()
                (proj): ConvLayer(
                  (conv): ChannelChunkConv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): RMSNorm3d()
                )
              )
              (shortcut): IdentityLayer()
            )
            (local_module): ResidualBlock(
              (main): GLUMBConv(
                (glu_act): SiLU()
                (inverted_conv): ConvLayer(
                  (conv): ChannelChunkConv3d(512, 4096, kernel_size=(1, 1, 1), stride=(1, 1, 1))
                  (act): SiLU()
                )
                (depth_conv): ConvLayer(
                  (conv): ChannelChunkConv3d(4096, 4096, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=4096)
                )
                (point_conv): ConvLayer(
                  (conv): ChannelChunkConv3d(2048, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): RMSNorm3d()
                )
              )
              (shortcut): IdentityLayer()
            )
          )
        )
      )
      (4): OpSequential(
        (op_list): ModuleList(
          (0): ResidualBlock(
            (main): InterpolateConvUpSampleLayer(factor=2, mode=nearest, temporal_upsample=True)
            (shortcut): ChannelDuplicatingPixelShuffleUpSampleLayer(in_channels=1024, out_channels=1024, factor=2, temporal_upsample=True)
          )
          (1-3): 3 x EfficientViTBlock(
            (context_module): ResidualBlock(
              (main): LiteMLA(
                (qkv): ConvLayer(
                  (conv): ChannelChunkConv3d(1024, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                )
                (aggreg): ModuleList(
                  (0): Sequential(
                    (0): ChannelChunkConv3d(3072, 3072, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=3072, bias=False)
                    (1): ChannelChunkConv3d(3072, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=96, bias=False)
                  )
                )
                (kernel_func): ReLU()
                (proj): ConvLayer(
                  (conv): ChannelChunkConv3d(2048, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): RMSNorm3d()
                )
              )
              (shortcut): IdentityLayer()
            )
            (local_module): ResidualBlock(
              (main): GLUMBConv(
                (glu_act): SiLU()
                (inverted_conv): ConvLayer(
                  (conv): ChannelChunkConv3d(1024, 8192, kernel_size=(1, 1, 1), stride=(1, 1, 1))
                  (act): SiLU()
                )
                (depth_conv): ConvLayer(
                  (conv): ChannelChunkConv3d(8192, 8192, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=8192)
                )
                (point_conv): ConvLayer(
                  (conv): ChannelChunkConv3d(4096, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): RMSNorm3d()
                )
              )
              (shortcut): IdentityLayer()
            )
          )
        )
      )
      (5): OpSequential(
        (op_list): ModuleList(
          (0-2): 3 x EfficientViTBlock(
            (context_module): ResidualBlock(
              (main): LiteMLA(
                (qkv): ConvLayer(
                  (conv): ChannelChunkConv3d(1024, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                )
                (aggreg): ModuleList(
                  (0): Sequential(
                    (0): ChannelChunkConv3d(3072, 3072, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2), groups=3072, bias=False)
                    (1): ChannelChunkConv3d(3072, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1), groups=96, bias=False)
                  )
                )
                (kernel_func): ReLU()
                (proj): ConvLayer(
                  (conv): ChannelChunkConv3d(2048, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): RMSNorm3d()
                )
              )
              (shortcut): IdentityLayer()
            )
            (local_module): ResidualBlock(
              (main): GLUMBConv(
                (glu_act): SiLU()
                (inverted_conv): ConvLayer(
                  (conv): ChannelChunkConv3d(1024, 8192, kernel_size=(1, 1, 1), stride=(1, 1, 1))
                  (act): SiLU()
                )
                (depth_conv): ConvLayer(
                  (conv): ChannelChunkConv3d(8192, 8192, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=8192)
                )
                (point_conv): ConvLayer(
                  (conv): ChannelChunkConv3d(4096, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
                  (norm): RMSNorm3d()
                )
              )
              (shortcut): IdentityLayer()
            )
          )
        )
      )
    )
    (project_out): OpSequential(
      (op_list): ModuleList(
        (0): RMSNorm3d()
        (1): ReLU()
        (2): ConvLayer(
          (conv): ChannelChunkConv3d(128, 3, kernel_size=(3, 3, 3), stride=(1, 1, 1))
        )
      )
    )
  )
) init_type='trunc_normal'
ðŸŸ© DCAE.forward x.shape=torch.Size([1, 3, 4, 256, 256])
ðŸŸ© DCAE.encode x.shape=torch.Size([1, 3, 4, 256, 256])
ðŸŸ© DCAE._encode x.shape=torch.Size([1, 3, 4, 256, 256])
ðŸŸ© DCAE.encode_single x.shape=torch.Size([1, 3, 4, 256, 256]) is_video_encoder=True
ðŸŸ© Encoder.forward x.shape=torch.Size([1, 3, 4, 256, 256])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 3, 4, 256, 256])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 3, 6, 258, 258])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 3, 6, 258, 258])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 128, 4, 256, 256])
ðŸŸ¦ Encoder.forward after project_in x.shape=torch.Size([1, 128, 4, 256, 256])
ðŸŸ© auto_grad_checkpoint len(args)=1 args[0].shape=torch.Size([1, 128, 4, 256, 256]) kwargs={}
ðŸŸ© OpSequential forward x.shape=torch.Size([1, 128, 4, 256, 256])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 128, 4, 256, 256])
ðŸŸ© ResBlock.forward x.shape=torch.Size([1, 128, 4, 256, 256])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 128, 4, 256, 256])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 128, 6, 258, 258])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 128, 6, 258, 258])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 128, 4, 256, 256])
ðŸŸ¦ After conv1: torch.Size([1, 128, 4, 256, 256])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 128, 4, 256, 256])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 128, 6, 258, 258])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 128, 6, 258, 258])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 128, 4, 256, 256])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 128, 4, 256, 256])
ðŸŸ¦ After conv2: torch.Size([1, 128, 4, 256, 256])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 128, 4, 256, 256])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 128, 4, 256, 256])
ðŸŸ© ResBlock.forward x.shape=torch.Size([1, 128, 4, 256, 256])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 128, 4, 256, 256])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 128, 6, 258, 258])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 128, 6, 258, 258])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 128, 4, 256, 256])
ðŸŸ¦ After conv1: torch.Size([1, 128, 4, 256, 256])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 128, 4, 256, 256])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 128, 6, 258, 258])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 128, 6, 258, 258])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 128, 4, 256, 256])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 128, 4, 256, 256])
ðŸŸ¦ After conv2: torch.Size([1, 128, 4, 256, 256])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 128, 4, 256, 256])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 128, 4, 256, 256])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 128, 4, 256, 256])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 128, 6, 258, 258])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 128, 6, 258, 258])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 256, 4, 128, 128])
ðŸŸ© PixelUnshuffleChannelAveragingDownSampleLayer.forward x.shape=torch.Size([1, 128, 4, 256, 256])
ðŸŸ¦ After pixel unshuffle: x.shape=torch.Size([1, 512, 4, 128, 128])
ðŸŸ¦ After channel averaging: x.shape=torch.Size([1, 256, 4, 128, 128])
ðŸŸ¦ Encoder.forward after stage 0 x.shape=torch.Size([1, 256, 4, 128, 128])
ðŸŸ© auto_grad_checkpoint len(args)=1 args[0].shape=torch.Size([1, 256, 4, 128, 128]) kwargs={}
ðŸŸ© OpSequential forward x.shape=torch.Size([1, 256, 4, 128, 128])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 256, 4, 128, 128])
ðŸŸ© ResBlock.forward x.shape=torch.Size([1, 256, 4, 128, 128])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 256, 4, 128, 128])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 256, 6, 130, 130])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 256, 6, 130, 130])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 256, 4, 128, 128])
ðŸŸ¦ After conv1: torch.Size([1, 256, 4, 128, 128])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 256, 4, 128, 128])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 256, 6, 130, 130])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 256, 6, 130, 130])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 256, 4, 128, 128])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 256, 4, 128, 128])
ðŸŸ¦ After conv2: torch.Size([1, 256, 4, 128, 128])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 256, 4, 128, 128])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 256, 4, 128, 128])
ðŸŸ© ResBlock.forward x.shape=torch.Size([1, 256, 4, 128, 128])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 256, 4, 128, 128])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 256, 6, 130, 130])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 256, 6, 130, 130])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 256, 4, 128, 128])
ðŸŸ¦ After conv1: torch.Size([1, 256, 4, 128, 128])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 256, 4, 128, 128])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 256, 6, 130, 130])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 256, 6, 130, 130])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 256, 4, 128, 128])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 256, 4, 128, 128])
ðŸŸ¦ After conv2: torch.Size([1, 256, 4, 128, 128])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 256, 4, 128, 128])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 256, 4, 128, 128])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 256, 4, 128, 128])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 256, 6, 130, 130])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 256, 6, 130, 130])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 512, 4, 64, 64])
ðŸŸ© PixelUnshuffleChannelAveragingDownSampleLayer.forward x.shape=torch.Size([1, 256, 4, 128, 128])
ðŸŸ¦ After pixel unshuffle: x.shape=torch.Size([1, 1024, 4, 64, 64])
ðŸŸ¦ After channel averaging: x.shape=torch.Size([1, 512, 4, 64, 64])
ðŸŸ¦ Encoder.forward after stage 1 x.shape=torch.Size([1, 512, 4, 64, 64])
ðŸŸ© auto_grad_checkpoint len(args)=1 args[0].shape=torch.Size([1, 512, 4, 64, 64]) kwargs={}
ðŸŸ© OpSequential forward x.shape=torch.Size([1, 512, 4, 64, 64])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 512, 4, 64, 64])
ðŸŸ© ResBlock.forward x.shape=torch.Size([1, 512, 4, 64, 64])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 512, 4, 64, 64])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 512, 6, 66, 66])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 512, 6, 66, 66])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 512, 4, 64, 64])
ðŸŸ¦ After conv1: torch.Size([1, 512, 4, 64, 64])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 512, 4, 64, 64])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 512, 6, 66, 66])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 512, 6, 66, 66])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 512, 4, 64, 64])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 512, 4, 64, 64])
ðŸŸ¦ After conv2: torch.Size([1, 512, 4, 64, 64])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 512, 4, 64, 64])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 512, 4, 64, 64])
ðŸŸ© ResBlock.forward x.shape=torch.Size([1, 512, 4, 64, 64])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 512, 4, 64, 64])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 512, 6, 66, 66])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 512, 6, 66, 66])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 512, 4, 64, 64])
ðŸŸ¦ After conv1: torch.Size([1, 512, 4, 64, 64])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 512, 4, 64, 64])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 512, 6, 66, 66])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 512, 6, 66, 66])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 512, 4, 64, 64])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 512, 4, 64, 64])
ðŸŸ¦ After conv2: torch.Size([1, 512, 4, 64, 64])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 512, 4, 64, 64])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 512, 4, 64, 64])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 512, 4, 64, 64])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 512, 6, 66, 66])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 512, 6, 66, 66])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© PixelUnshuffleChannelAveragingDownSampleLayer.forward x.shape=torch.Size([1, 512, 4, 64, 64])
ðŸŸ¦ After pixel unshuffle: x.shape=torch.Size([1, 2048, 4, 32, 32])
ðŸŸ¦ After channel averaging: x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ¦ Encoder.forward after stage 2 x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© auto_grad_checkpoint len(args)=1 args[0].shape=torch.Size([1, 512, 4, 32, 32]) kwargs={}
ðŸŸ© OpSequential forward x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© EfficientViTBlock.forward x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© LiteMLA.forward x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 1536, 4, 32, 32])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1536, 4, 32, 32])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1536, 4, 32, 32])
ðŸŸ¦ LiteMLA.forward after aggreg op oped.shape=torch.Size([1, 1536, 4, 32, 32])
ðŸŸ¦ LiteMLA.forward after multi-scale conv qkv.shape=torch.Size([1, 3072, 4, 32, 32])
ðŸŸ© LiteMLA.relu_linear_att qkv.shape=torch.Size([1, 3072, 4, 32, 32])
ðŸŸ¦ LiteMLA.relu_linear_att after reshape qkv.shape=torch.Size([1, 32, 96, 4096])
ðŸŸ¦ LiteMLA.relu_linear_att after split q.shape=torch.Size([1, 32, 32, 4096]) k.shape=torch.Size([1, 32, 32, 4096]) v.shape=torch.Size([1, 32, 32, 4096])
ðŸŸ¦ q.shape=torch.Size([1, 32, 32, 4096]) k.shape=torch.Size([1, 32, 32, 4096]) after kernel_func
ðŸŸ¦ LiteMLA.relu_linear_att after v pad v.shape=torch.Size([1, 32, 33, 4096])
ðŸŸ¦ LiteMLA.relu_linear_att after vk matmul vk.shape=torch.Size([1, 32, 33, 32])
ðŸŸ¦ LiteMLA.relu_linear_att after out matmul out.shape=torch.Size([1, 32, 33, 4096])
ðŸŸ¦ LiteMLA.relu_linear_att after normalization out.shape=torch.Size([1, 32, 32, 4096])
ðŸŸ¦ LiteMLA.relu_linear_att after reshape out.shape=torch.Size([1, 1024, 4, 32, 32])
ðŸŸ¦ LiteMLA.forward after relu_linear_att out.shape=torch.Size([1, 1024, 4, 32, 32])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 1024, 4, 32, 32])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 1024, 4, 32, 32])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1024, 4, 32, 32])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ¦ LiteMLA.forward after proj out.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ¦ After context_module: x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© GLUMBConv.forward x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 4096, 4, 32, 32])
ðŸŸ¦ GLUMBConv.forward after inverted_conv x.shape=torch.Size([1, 4096, 4, 32, 32])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 4096, 4, 32, 32])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 4096, 6, 34, 34])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 4096, 6, 34, 34])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 4096, 4, 32, 32])
ðŸŸ¦ GLUMBConv.forward after depth_conv x.shape=torch.Size([1, 4096, 4, 32, 32])
ðŸŸ¦ GLUMBConv.forward after GLU x.shape=torch.Size([1, 2048, 4, 32, 32])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 2048, 4, 32, 32])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 2048, 4, 32, 32])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 2048, 4, 32, 32])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ¦ GLUMBConv.forward after point_conv x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ¦ After local_module: x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© EfficientViTBlock.forward x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© LiteMLA.forward x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 1536, 4, 32, 32])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1536, 4, 32, 32])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1536, 4, 32, 32])
ðŸŸ¦ LiteMLA.forward after aggreg op oped.shape=torch.Size([1, 1536, 4, 32, 32])
ðŸŸ¦ LiteMLA.forward after multi-scale conv qkv.shape=torch.Size([1, 3072, 4, 32, 32])
ðŸŸ© LiteMLA.relu_linear_att qkv.shape=torch.Size([1, 3072, 4, 32, 32])
ðŸŸ¦ LiteMLA.relu_linear_att after reshape qkv.shape=torch.Size([1, 32, 96, 4096])
ðŸŸ¦ LiteMLA.relu_linear_att after split q.shape=torch.Size([1, 32, 32, 4096]) k.shape=torch.Size([1, 32, 32, 4096]) v.shape=torch.Size([1, 32, 32, 4096])
ðŸŸ¦ q.shape=torch.Size([1, 32, 32, 4096]) k.shape=torch.Size([1, 32, 32, 4096]) after kernel_func
ðŸŸ¦ LiteMLA.relu_linear_att after v pad v.shape=torch.Size([1, 32, 33, 4096])
ðŸŸ¦ LiteMLA.relu_linear_att after vk matmul vk.shape=torch.Size([1, 32, 33, 32])
ðŸŸ¦ LiteMLA.relu_linear_att after out matmul out.shape=torch.Size([1, 32, 33, 4096])
ðŸŸ¦ LiteMLA.relu_linear_att after normalization out.shape=torch.Size([1, 32, 32, 4096])
ðŸŸ¦ LiteMLA.relu_linear_att after reshape out.shape=torch.Size([1, 1024, 4, 32, 32])
ðŸŸ¦ LiteMLA.forward after relu_linear_att out.shape=torch.Size([1, 1024, 4, 32, 32])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 1024, 4, 32, 32])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 1024, 4, 32, 32])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1024, 4, 32, 32])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ¦ LiteMLA.forward after proj out.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ¦ After context_module: x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© GLUMBConv.forward x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 4096, 4, 32, 32])
ðŸŸ¦ GLUMBConv.forward after inverted_conv x.shape=torch.Size([1, 4096, 4, 32, 32])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 4096, 4, 32, 32])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 4096, 6, 34, 34])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 4096, 6, 34, 34])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 4096, 4, 32, 32])
ðŸŸ¦ GLUMBConv.forward after depth_conv x.shape=torch.Size([1, 4096, 4, 32, 32])
ðŸŸ¦ GLUMBConv.forward after GLU x.shape=torch.Size([1, 2048, 4, 32, 32])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 2048, 4, 32, 32])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 2048, 4, 32, 32])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 2048, 4, 32, 32])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ¦ GLUMBConv.forward after point_conv x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ¦ After local_module: x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© EfficientViTBlock.forward x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© LiteMLA.forward x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 1536, 4, 32, 32])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1536, 4, 32, 32])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1536, 4, 32, 32])
ðŸŸ¦ LiteMLA.forward after aggreg op oped.shape=torch.Size([1, 1536, 4, 32, 32])
ðŸŸ¦ LiteMLA.forward after multi-scale conv qkv.shape=torch.Size([1, 3072, 4, 32, 32])
ðŸŸ© LiteMLA.relu_linear_att qkv.shape=torch.Size([1, 3072, 4, 32, 32])
ðŸŸ¦ LiteMLA.relu_linear_att after reshape qkv.shape=torch.Size([1, 32, 96, 4096])
ðŸŸ¦ LiteMLA.relu_linear_att after split q.shape=torch.Size([1, 32, 32, 4096]) k.shape=torch.Size([1, 32, 32, 4096]) v.shape=torch.Size([1, 32, 32, 4096])
ðŸŸ¦ q.shape=torch.Size([1, 32, 32, 4096]) k.shape=torch.Size([1, 32, 32, 4096]) after kernel_func
ðŸŸ¦ LiteMLA.relu_linear_att after v pad v.shape=torch.Size([1, 32, 33, 4096])
ðŸŸ¦ LiteMLA.relu_linear_att after vk matmul vk.shape=torch.Size([1, 32, 33, 32])
ðŸŸ¦ LiteMLA.relu_linear_att after out matmul out.shape=torch.Size([1, 32, 33, 4096])
ðŸŸ¦ LiteMLA.relu_linear_att after normalization out.shape=torch.Size([1, 32, 32, 4096])
ðŸŸ¦ LiteMLA.relu_linear_att after reshape out.shape=torch.Size([1, 1024, 4, 32, 32])
ðŸŸ¦ LiteMLA.forward after relu_linear_att out.shape=torch.Size([1, 1024, 4, 32, 32])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 1024, 4, 32, 32])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 1024, 4, 32, 32])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1024, 4, 32, 32])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ¦ LiteMLA.forward after proj out.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ¦ After context_module: x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© GLUMBConv.forward x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 4096, 4, 32, 32])
ðŸŸ¦ GLUMBConv.forward after inverted_conv x.shape=torch.Size([1, 4096, 4, 32, 32])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 4096, 4, 32, 32])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 4096, 6, 34, 34])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 4096, 6, 34, 34])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 4096, 4, 32, 32])
ðŸŸ¦ GLUMBConv.forward after depth_conv x.shape=torch.Size([1, 4096, 4, 32, 32])
ðŸŸ¦ GLUMBConv.forward after GLU x.shape=torch.Size([1, 2048, 4, 32, 32])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 2048, 4, 32, 32])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 2048, 4, 32, 32])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 2048, 4, 32, 32])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ¦ GLUMBConv.forward after point_conv x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ¦ After local_module: x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 512, 6, 34, 34])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 512, 6, 34, 34])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© PixelUnshuffleChannelAveragingDownSampleLayer.forward x.shape=torch.Size([1, 512, 4, 32, 32])
ðŸŸ© pixel_unshuffle_3d x.shape=torch.Size([1, 512, 4, 32, 32]) downsample_factor=2
ðŸŸ¦ pixel_unshuffle_3d output y.shape=torch.Size([1, 4096, 2, 16, 16])
ðŸŸ¦ After pixel unshuffle: x.shape=torch.Size([1, 4096, 2, 16, 16])
ðŸŸ¦ After channel averaging: x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ¦ Encoder.forward after stage 3 x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© auto_grad_checkpoint len(args)=1 args[0].shape=torch.Size([1, 1024, 2, 16, 16]) kwargs={}
ðŸŸ© OpSequential forward x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© EfficientViTBlock.forward x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© LiteMLA.forward x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 3072, 2, 16, 16])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 3072, 2, 16, 16])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 3072, 2, 16, 16])
ðŸŸ¦ LiteMLA.forward after aggreg op oped.shape=torch.Size([1, 3072, 2, 16, 16])
ðŸŸ¦ LiteMLA.forward after multi-scale conv qkv.shape=torch.Size([1, 6144, 2, 16, 16])
ðŸŸ© LiteMLA.relu_linear_att qkv.shape=torch.Size([1, 6144, 2, 16, 16])
ðŸŸ¦ LiteMLA.relu_linear_att after reshape qkv.shape=torch.Size([1, 64, 96, 512])
ðŸŸ¦ LiteMLA.relu_linear_att after split q.shape=torch.Size([1, 64, 32, 512]) k.shape=torch.Size([1, 64, 32, 512]) v.shape=torch.Size([1, 64, 32, 512])
ðŸŸ¦ q.shape=torch.Size([1, 64, 32, 512]) k.shape=torch.Size([1, 64, 32, 512]) after kernel_func
ðŸŸ¦ LiteMLA.relu_linear_att after v pad v.shape=torch.Size([1, 64, 33, 512])
ðŸŸ¦ LiteMLA.relu_linear_att after vk matmul vk.shape=torch.Size([1, 64, 33, 32])
ðŸŸ¦ LiteMLA.relu_linear_att after out matmul out.shape=torch.Size([1, 64, 33, 512])
ðŸŸ¦ LiteMLA.relu_linear_att after normalization out.shape=torch.Size([1, 64, 32, 512])
ðŸŸ¦ LiteMLA.relu_linear_att after reshape out.shape=torch.Size([1, 2048, 2, 16, 16])
ðŸŸ¦ LiteMLA.forward after relu_linear_att out.shape=torch.Size([1, 2048, 2, 16, 16])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 2048, 2, 16, 16])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 2048, 2, 16, 16])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 2048, 2, 16, 16])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ¦ LiteMLA.forward after proj out.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ¦ After context_module: x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© GLUMBConv.forward x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 8192, 2, 16, 16])
ðŸŸ¦ GLUMBConv.forward after inverted_conv x.shape=torch.Size([1, 8192, 2, 16, 16])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 8192, 2, 16, 16])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 8192, 4, 18, 18])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 8192, 4, 18, 18])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 8192, 2, 16, 16])
ðŸŸ¦ GLUMBConv.forward after depth_conv x.shape=torch.Size([1, 8192, 2, 16, 16])
ðŸŸ¦ GLUMBConv.forward after GLU x.shape=torch.Size([1, 4096, 2, 16, 16])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 4096, 2, 16, 16])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 4096, 2, 16, 16])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 4096, 2, 16, 16])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ¦ GLUMBConv.forward after point_conv x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ¦ After local_module: x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© EfficientViTBlock.forward x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© LiteMLA.forward x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 3072, 2, 16, 16])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 3072, 2, 16, 16])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 3072, 2, 16, 16])
ðŸŸ¦ LiteMLA.forward after aggreg op oped.shape=torch.Size([1, 3072, 2, 16, 16])
ðŸŸ¦ LiteMLA.forward after multi-scale conv qkv.shape=torch.Size([1, 6144, 2, 16, 16])
ðŸŸ© LiteMLA.relu_linear_att qkv.shape=torch.Size([1, 6144, 2, 16, 16])
ðŸŸ¦ LiteMLA.relu_linear_att after reshape qkv.shape=torch.Size([1, 64, 96, 512])
ðŸŸ¦ LiteMLA.relu_linear_att after split q.shape=torch.Size([1, 64, 32, 512]) k.shape=torch.Size([1, 64, 32, 512]) v.shape=torch.Size([1, 64, 32, 512])
ðŸŸ¦ q.shape=torch.Size([1, 64, 32, 512]) k.shape=torch.Size([1, 64, 32, 512]) after kernel_func
ðŸŸ¦ LiteMLA.relu_linear_att after v pad v.shape=torch.Size([1, 64, 33, 512])
ðŸŸ¦ LiteMLA.relu_linear_att after vk matmul vk.shape=torch.Size([1, 64, 33, 32])
ðŸŸ¦ LiteMLA.relu_linear_att after out matmul out.shape=torch.Size([1, 64, 33, 512])
ðŸŸ¦ LiteMLA.relu_linear_att after normalization out.shape=torch.Size([1, 64, 32, 512])
ðŸŸ¦ LiteMLA.relu_linear_att after reshape out.shape=torch.Size([1, 2048, 2, 16, 16])
ðŸŸ¦ LiteMLA.forward after relu_linear_att out.shape=torch.Size([1, 2048, 2, 16, 16])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 2048, 2, 16, 16])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 2048, 2, 16, 16])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 2048, 2, 16, 16])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ¦ LiteMLA.forward after proj out.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ¦ After context_module: x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© GLUMBConv.forward x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 8192, 2, 16, 16])
ðŸŸ¦ GLUMBConv.forward after inverted_conv x.shape=torch.Size([1, 8192, 2, 16, 16])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 8192, 2, 16, 16])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 8192, 4, 18, 18])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 8192, 4, 18, 18])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 8192, 2, 16, 16])
ðŸŸ¦ GLUMBConv.forward after depth_conv x.shape=torch.Size([1, 8192, 2, 16, 16])
ðŸŸ¦ GLUMBConv.forward after GLU x.shape=torch.Size([1, 4096, 2, 16, 16])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 4096, 2, 16, 16])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 4096, 2, 16, 16])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 4096, 2, 16, 16])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ¦ GLUMBConv.forward after point_conv x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ¦ After local_module: x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© EfficientViTBlock.forward x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© LiteMLA.forward x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 3072, 2, 16, 16])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 3072, 2, 16, 16])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 3072, 2, 16, 16])
ðŸŸ¦ LiteMLA.forward after aggreg op oped.shape=torch.Size([1, 3072, 2, 16, 16])
ðŸŸ¦ LiteMLA.forward after multi-scale conv qkv.shape=torch.Size([1, 6144, 2, 16, 16])
ðŸŸ© LiteMLA.relu_linear_att qkv.shape=torch.Size([1, 6144, 2, 16, 16])
ðŸŸ¦ LiteMLA.relu_linear_att after reshape qkv.shape=torch.Size([1, 64, 96, 512])
ðŸŸ¦ LiteMLA.relu_linear_att after split q.shape=torch.Size([1, 64, 32, 512]) k.shape=torch.Size([1, 64, 32, 512]) v.shape=torch.Size([1, 64, 32, 512])
ðŸŸ¦ q.shape=torch.Size([1, 64, 32, 512]) k.shape=torch.Size([1, 64, 32, 512]) after kernel_func
ðŸŸ¦ LiteMLA.relu_linear_att after v pad v.shape=torch.Size([1, 64, 33, 512])
ðŸŸ¦ LiteMLA.relu_linear_att after vk matmul vk.shape=torch.Size([1, 64, 33, 32])
ðŸŸ¦ LiteMLA.relu_linear_att after out matmul out.shape=torch.Size([1, 64, 33, 512])
ðŸŸ¦ LiteMLA.relu_linear_att after normalization out.shape=torch.Size([1, 64, 32, 512])
ðŸŸ¦ LiteMLA.relu_linear_att after reshape out.shape=torch.Size([1, 2048, 2, 16, 16])
ðŸŸ¦ LiteMLA.forward after relu_linear_att out.shape=torch.Size([1, 2048, 2, 16, 16])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 2048, 2, 16, 16])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 2048, 2, 16, 16])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 2048, 2, 16, 16])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ¦ LiteMLA.forward after proj out.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ¦ After context_module: x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© GLUMBConv.forward x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 8192, 2, 16, 16])
ðŸŸ¦ GLUMBConv.forward after inverted_conv x.shape=torch.Size([1, 8192, 2, 16, 16])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 8192, 2, 16, 16])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 8192, 4, 18, 18])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 8192, 4, 18, 18])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 8192, 2, 16, 16])
ðŸŸ¦ GLUMBConv.forward after depth_conv x.shape=torch.Size([1, 8192, 2, 16, 16])
ðŸŸ¦ GLUMBConv.forward after GLU x.shape=torch.Size([1, 4096, 2, 16, 16])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 4096, 2, 16, 16])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 4096, 2, 16, 16])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 4096, 2, 16, 16])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ¦ GLUMBConv.forward after point_conv x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ¦ After local_module: x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 1024, 4, 18, 18])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1024, 4, 18, 18])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© PixelUnshuffleChannelAveragingDownSampleLayer.forward x.shape=torch.Size([1, 1024, 2, 16, 16])
ðŸŸ© pixel_unshuffle_3d x.shape=torch.Size([1, 1024, 2, 16, 16]) downsample_factor=2
ðŸŸ¦ pixel_unshuffle_3d output y.shape=torch.Size([1, 8192, 1, 8, 8])
ðŸŸ¦ After pixel unshuffle: x.shape=torch.Size([1, 8192, 1, 8, 8])
ðŸŸ¦ After channel averaging: x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ Encoder.forward after stage 4 x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© auto_grad_checkpoint len(args)=1 args[0].shape=torch.Size([1, 1024, 1, 8, 8]) kwargs={}
ðŸŸ© OpSequential forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© EfficientViTBlock.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© LiteMLA.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 3072, 1, 8, 8])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 3072, 1, 8, 8])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 3072, 1, 8, 8])
ðŸŸ¦ LiteMLA.forward after aggreg op oped.shape=torch.Size([1, 3072, 1, 8, 8])
ðŸŸ¦ LiteMLA.forward after multi-scale conv qkv.shape=torch.Size([1, 6144, 1, 8, 8])
ðŸŸ© LiteMLA.relu_linear_att qkv.shape=torch.Size([1, 6144, 1, 8, 8])
ðŸŸ¦ LiteMLA.relu_linear_att after reshape qkv.shape=torch.Size([1, 64, 96, 64])
ðŸŸ¦ LiteMLA.relu_linear_att after split q.shape=torch.Size([1, 64, 32, 64]) k.shape=torch.Size([1, 64, 32, 64]) v.shape=torch.Size([1, 64, 32, 64])
ðŸŸ¦ q.shape=torch.Size([1, 64, 32, 64]) k.shape=torch.Size([1, 64, 32, 64]) after kernel_func
ðŸŸ¦ LiteMLA.relu_linear_att after v pad v.shape=torch.Size([1, 64, 33, 64])
ðŸŸ¦ LiteMLA.relu_linear_att after vk matmul vk.shape=torch.Size([1, 64, 33, 32])
ðŸŸ¦ LiteMLA.relu_linear_att after out matmul out.shape=torch.Size([1, 64, 33, 64])
ðŸŸ¦ LiteMLA.relu_linear_att after normalization out.shape=torch.Size([1, 64, 32, 64])
ðŸŸ¦ LiteMLA.relu_linear_att after reshape out.shape=torch.Size([1, 2048, 1, 8, 8])
ðŸŸ¦ LiteMLA.forward after relu_linear_att out.shape=torch.Size([1, 2048, 1, 8, 8])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 2048, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 2048, 1, 8, 8])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 2048, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ LiteMLA.forward after proj out.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ After context_module: x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© GLUMBConv.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 8192, 1, 8, 8])
ðŸŸ¦ GLUMBConv.forward after inverted_conv x.shape=torch.Size([1, 8192, 1, 8, 8])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 8192, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 8192, 3, 10, 10])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 8192, 3, 10, 10])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 8192, 1, 8, 8])
ðŸŸ¦ GLUMBConv.forward after depth_conv x.shape=torch.Size([1, 8192, 1, 8, 8])
ðŸŸ¦ GLUMBConv.forward after GLU x.shape=torch.Size([1, 4096, 1, 8, 8])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 4096, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 4096, 1, 8, 8])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 4096, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ GLUMBConv.forward after point_conv x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ After local_module: x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© EfficientViTBlock.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© LiteMLA.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 3072, 1, 8, 8])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 3072, 1, 8, 8])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 3072, 1, 8, 8])
ðŸŸ¦ LiteMLA.forward after aggreg op oped.shape=torch.Size([1, 3072, 1, 8, 8])
ðŸŸ¦ LiteMLA.forward after multi-scale conv qkv.shape=torch.Size([1, 6144, 1, 8, 8])
ðŸŸ© LiteMLA.relu_linear_att qkv.shape=torch.Size([1, 6144, 1, 8, 8])
ðŸŸ¦ LiteMLA.relu_linear_att after reshape qkv.shape=torch.Size([1, 64, 96, 64])
ðŸŸ¦ LiteMLA.relu_linear_att after split q.shape=torch.Size([1, 64, 32, 64]) k.shape=torch.Size([1, 64, 32, 64]) v.shape=torch.Size([1, 64, 32, 64])
ðŸŸ¦ q.shape=torch.Size([1, 64, 32, 64]) k.shape=torch.Size([1, 64, 32, 64]) after kernel_func
ðŸŸ¦ LiteMLA.relu_linear_att after v pad v.shape=torch.Size([1, 64, 33, 64])
ðŸŸ¦ LiteMLA.relu_linear_att after vk matmul vk.shape=torch.Size([1, 64, 33, 32])
ðŸŸ¦ LiteMLA.relu_linear_att after out matmul out.shape=torch.Size([1, 64, 33, 64])
ðŸŸ¦ LiteMLA.relu_linear_att after normalization out.shape=torch.Size([1, 64, 32, 64])
ðŸŸ¦ LiteMLA.relu_linear_att after reshape out.shape=torch.Size([1, 2048, 1, 8, 8])
ðŸŸ¦ LiteMLA.forward after relu_linear_att out.shape=torch.Size([1, 2048, 1, 8, 8])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 2048, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 2048, 1, 8, 8])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 2048, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ LiteMLA.forward after proj out.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ After context_module: x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© GLUMBConv.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 8192, 1, 8, 8])
ðŸŸ¦ GLUMBConv.forward after inverted_conv x.shape=torch.Size([1, 8192, 1, 8, 8])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 8192, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 8192, 3, 10, 10])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 8192, 3, 10, 10])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 8192, 1, 8, 8])
ðŸŸ¦ GLUMBConv.forward after depth_conv x.shape=torch.Size([1, 8192, 1, 8, 8])
ðŸŸ¦ GLUMBConv.forward after GLU x.shape=torch.Size([1, 4096, 1, 8, 8])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 4096, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 4096, 1, 8, 8])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 4096, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ GLUMBConv.forward after point_conv x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ After local_module: x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© EfficientViTBlock.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© LiteMLA.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 3072, 1, 8, 8])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 3072, 1, 8, 8])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 3072, 1, 8, 8])
ðŸŸ¦ LiteMLA.forward after aggreg op oped.shape=torch.Size([1, 3072, 1, 8, 8])
ðŸŸ¦ LiteMLA.forward after multi-scale conv qkv.shape=torch.Size([1, 6144, 1, 8, 8])
ðŸŸ© LiteMLA.relu_linear_att qkv.shape=torch.Size([1, 6144, 1, 8, 8])
ðŸŸ¦ LiteMLA.relu_linear_att after reshape qkv.shape=torch.Size([1, 64, 96, 64])
ðŸŸ¦ LiteMLA.relu_linear_att after split q.shape=torch.Size([1, 64, 32, 64]) k.shape=torch.Size([1, 64, 32, 64]) v.shape=torch.Size([1, 64, 32, 64])
ðŸŸ¦ q.shape=torch.Size([1, 64, 32, 64]) k.shape=torch.Size([1, 64, 32, 64]) after kernel_func
ðŸŸ¦ LiteMLA.relu_linear_att after v pad v.shape=torch.Size([1, 64, 33, 64])
ðŸŸ¦ LiteMLA.relu_linear_att after vk matmul vk.shape=torch.Size([1, 64, 33, 32])
ðŸŸ¦ LiteMLA.relu_linear_att after out matmul out.shape=torch.Size([1, 64, 33, 64])
ðŸŸ¦ LiteMLA.relu_linear_att after normalization out.shape=torch.Size([1, 64, 32, 64])
ðŸŸ¦ LiteMLA.relu_linear_att after reshape out.shape=torch.Size([1, 2048, 1, 8, 8])
ðŸŸ¦ LiteMLA.forward after relu_linear_att out.shape=torch.Size([1, 2048, 1, 8, 8])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 2048, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 2048, 1, 8, 8])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 2048, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ LiteMLA.forward after proj out.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ After context_module: x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© GLUMBConv.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 8192, 1, 8, 8])
ðŸŸ¦ GLUMBConv.forward after inverted_conv x.shape=torch.Size([1, 8192, 1, 8, 8])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 8192, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 8192, 3, 10, 10])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 8192, 3, 10, 10])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 8192, 1, 8, 8])
ðŸŸ¦ GLUMBConv.forward after depth_conv x.shape=torch.Size([1, 8192, 1, 8, 8])
ðŸŸ¦ GLUMBConv.forward after GLU x.shape=torch.Size([1, 4096, 1, 8, 8])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 4096, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 4096, 1, 8, 8])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 4096, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ GLUMBConv.forward after point_conv x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ After local_module: x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ Encoder.forward after stage 5 x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© auto_grad_checkpoint len(args)=1 args[0].shape=torch.Size([1, 1024, 1, 8, 8]) kwargs={}
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© OpSequential forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 1024, 3, 10, 10])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1024, 3, 10, 10])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 128, 1, 8, 8])
ðŸŸ© PixelUnshuffleChannelAveragingDownSampleLayer.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ After pixel unshuffle: x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ After channel averaging: x.shape=torch.Size([1, 128, 1, 8, 8])
ðŸŸ¦ Encoder.forward after project_out x.shape=torch.Size([1, 128, 1, 8, 8])
ðŸŸ© DCAE.decode z.shape=torch.Size([1, 128, 1, 8, 8])
ðŸŸ© DCAE._decode z.shape=torch.Size([1, 128, 1, 8, 8])
ðŸŸ© DCAE.decode_single z.shape=torch.Size([1, 128, 1, 8, 8]) is_video_decoder=True
ðŸŸ© Decoder.forward x.shape=torch.Size([1, 128, 1, 8, 8])
ðŸŸ© auto_grad_checkpoint len(args)=1 args[0].shape=torch.Size([1, 128, 1, 8, 8]) kwargs={}
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 128, 1, 8, 8])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 128, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 128, 3, 10, 10])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 128, 3, 10, 10])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ChannelDuplicatingPixelShuffleUpSampleLayer.forward x.shape=torch.Size([1, 128, 1, 8, 8])
ðŸŸ¦ temporal_upsample repeats (T==1): 8
ðŸŸ¦ after repeat_interleave: torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ after pixel_shuffle: torch.Size([1, 1, 1024, 8, 8])
ðŸŸ¦ Decoder.forward after project_in x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© auto_grad_checkpoint len(args)=1 args[0].shape=torch.Size([1, 1024, 1, 8, 8]) kwargs={}
ðŸŸ© OpSequential forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© EfficientViTBlock.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© LiteMLA.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 3072, 1, 8, 8])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 3072, 1, 8, 8])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 3072, 1, 8, 8])
ðŸŸ¦ LiteMLA.forward after aggreg op oped.shape=torch.Size([1, 3072, 1, 8, 8])
ðŸŸ¦ LiteMLA.forward after multi-scale conv qkv.shape=torch.Size([1, 6144, 1, 8, 8])
ðŸŸ© LiteMLA.relu_linear_att qkv.shape=torch.Size([1, 6144, 1, 8, 8])
ðŸŸ¦ LiteMLA.relu_linear_att after reshape qkv.shape=torch.Size([1, 64, 96, 64])
ðŸŸ¦ LiteMLA.relu_linear_att after split q.shape=torch.Size([1, 64, 32, 64]) k.shape=torch.Size([1, 64, 32, 64]) v.shape=torch.Size([1, 64, 32, 64])
ðŸŸ¦ q.shape=torch.Size([1, 64, 32, 64]) k.shape=torch.Size([1, 64, 32, 64]) after kernel_func
ðŸŸ¦ LiteMLA.relu_linear_att after v pad v.shape=torch.Size([1, 64, 33, 64])
ðŸŸ¦ LiteMLA.relu_linear_att after vk matmul vk.shape=torch.Size([1, 64, 33, 32])
ðŸŸ¦ LiteMLA.relu_linear_att after out matmul out.shape=torch.Size([1, 64, 33, 64])
ðŸŸ¦ LiteMLA.relu_linear_att after normalization out.shape=torch.Size([1, 64, 32, 64])
ðŸŸ¦ LiteMLA.relu_linear_att after reshape out.shape=torch.Size([1, 2048, 1, 8, 8])
ðŸŸ¦ LiteMLA.forward after relu_linear_att out.shape=torch.Size([1, 2048, 1, 8, 8])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 2048, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 2048, 1, 8, 8])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 2048, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ LiteMLA.forward after proj out.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ After context_module: x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© GLUMBConv.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 8192, 1, 8, 8])
ðŸŸ¦ GLUMBConv.forward after inverted_conv x.shape=torch.Size([1, 8192, 1, 8, 8])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 8192, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 8192, 3, 10, 10])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 8192, 3, 10, 10])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 8192, 1, 8, 8])
ðŸŸ¦ GLUMBConv.forward after depth_conv x.shape=torch.Size([1, 8192, 1, 8, 8])
ðŸŸ¦ GLUMBConv.forward after GLU x.shape=torch.Size([1, 4096, 1, 8, 8])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 4096, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 4096, 1, 8, 8])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 4096, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ GLUMBConv.forward after point_conv x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ After local_module: x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© EfficientViTBlock.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© LiteMLA.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 3072, 1, 8, 8])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 3072, 1, 8, 8])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 3072, 1, 8, 8])
ðŸŸ¦ LiteMLA.forward after aggreg op oped.shape=torch.Size([1, 3072, 1, 8, 8])
ðŸŸ¦ LiteMLA.forward after multi-scale conv qkv.shape=torch.Size([1, 6144, 1, 8, 8])
ðŸŸ© LiteMLA.relu_linear_att qkv.shape=torch.Size([1, 6144, 1, 8, 8])
ðŸŸ¦ LiteMLA.relu_linear_att after reshape qkv.shape=torch.Size([1, 64, 96, 64])
ðŸŸ¦ LiteMLA.relu_linear_att after split q.shape=torch.Size([1, 64, 32, 64]) k.shape=torch.Size([1, 64, 32, 64]) v.shape=torch.Size([1, 64, 32, 64])
ðŸŸ¦ q.shape=torch.Size([1, 64, 32, 64]) k.shape=torch.Size([1, 64, 32, 64]) after kernel_func
ðŸŸ¦ LiteMLA.relu_linear_att after v pad v.shape=torch.Size([1, 64, 33, 64])
ðŸŸ¦ LiteMLA.relu_linear_att after vk matmul vk.shape=torch.Size([1, 64, 33, 32])
ðŸŸ¦ LiteMLA.relu_linear_att after out matmul out.shape=torch.Size([1, 64, 33, 64])
ðŸŸ¦ LiteMLA.relu_linear_att after normalization out.shape=torch.Size([1, 64, 32, 64])
ðŸŸ¦ LiteMLA.relu_linear_att after reshape out.shape=torch.Size([1, 2048, 1, 8, 8])
ðŸŸ¦ LiteMLA.forward after relu_linear_att out.shape=torch.Size([1, 2048, 1, 8, 8])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 2048, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 2048, 1, 8, 8])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 2048, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ LiteMLA.forward after proj out.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ After context_module: x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© GLUMBConv.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 8192, 1, 8, 8])
ðŸŸ¦ GLUMBConv.forward after inverted_conv x.shape=torch.Size([1, 8192, 1, 8, 8])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 8192, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 8192, 3, 10, 10])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 8192, 3, 10, 10])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 8192, 1, 8, 8])
ðŸŸ¦ GLUMBConv.forward after depth_conv x.shape=torch.Size([1, 8192, 1, 8, 8])
ðŸŸ¦ GLUMBConv.forward after GLU x.shape=torch.Size([1, 4096, 1, 8, 8])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 4096, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 4096, 1, 8, 8])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 4096, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ GLUMBConv.forward after point_conv x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ After local_module: x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© EfficientViTBlock.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© LiteMLA.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 3072, 1, 8, 8])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 3072, 1, 8, 8])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 3072, 1, 8, 8])
ðŸŸ¦ LiteMLA.forward after aggreg op oped.shape=torch.Size([1, 3072, 1, 8, 8])
ðŸŸ¦ LiteMLA.forward after multi-scale conv qkv.shape=torch.Size([1, 6144, 1, 8, 8])
ðŸŸ© LiteMLA.relu_linear_att qkv.shape=torch.Size([1, 6144, 1, 8, 8])
ðŸŸ¦ LiteMLA.relu_linear_att after reshape qkv.shape=torch.Size([1, 64, 96, 64])
ðŸŸ¦ LiteMLA.relu_linear_att after split q.shape=torch.Size([1, 64, 32, 64]) k.shape=torch.Size([1, 64, 32, 64]) v.shape=torch.Size([1, 64, 32, 64])
ðŸŸ¦ q.shape=torch.Size([1, 64, 32, 64]) k.shape=torch.Size([1, 64, 32, 64]) after kernel_func
ðŸŸ¦ LiteMLA.relu_linear_att after v pad v.shape=torch.Size([1, 64, 33, 64])
ðŸŸ¦ LiteMLA.relu_linear_att after vk matmul vk.shape=torch.Size([1, 64, 33, 32])
ðŸŸ¦ LiteMLA.relu_linear_att after out matmul out.shape=torch.Size([1, 64, 33, 64])
ðŸŸ¦ LiteMLA.relu_linear_att after normalization out.shape=torch.Size([1, 64, 32, 64])
ðŸŸ¦ LiteMLA.relu_linear_att after reshape out.shape=torch.Size([1, 2048, 1, 8, 8])
ðŸŸ¦ LiteMLA.forward after relu_linear_att out.shape=torch.Size([1, 2048, 1, 8, 8])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 2048, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 2048, 1, 8, 8])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 2048, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ LiteMLA.forward after proj out.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ After context_module: x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© GLUMBConv.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 8192, 1, 8, 8])
ðŸŸ¦ GLUMBConv.forward after inverted_conv x.shape=torch.Size([1, 8192, 1, 8, 8])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 8192, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 8192, 3, 10, 10])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 8192, 3, 10, 10])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 8192, 1, 8, 8])
ðŸŸ¦ GLUMBConv.forward after depth_conv x.shape=torch.Size([1, 8192, 1, 8, 8])
ðŸŸ¦ GLUMBConv.forward after GLU x.shape=torch.Size([1, 4096, 1, 8, 8])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 4096, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 4096, 1, 8, 8])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 4096, 1, 8, 8])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ GLUMBConv.forward after point_conv x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ After local_module: x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ Decoder.forward after stage i=0 x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© auto_grad_checkpoint len(args)=1 args[0].shape=torch.Size([1, 1024, 1, 8, 8]) kwargs={}
ðŸŸ© OpSequential forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© InterpolateConvUpSampleLayer.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ© chunked_interpolate x.shape=torch.Size([1, 1024, 1, 8, 8]) scale_factor=[1, 2, 2] mode='nearest'
ðŸŸ¦ After interpolate: torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 1024, 3, 18, 18])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1024, 3, 18, 18])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ¦ After conv: torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© ChannelDuplicatingPixelShuffleUpSampleLayer.forward x.shape=torch.Size([1, 1024, 1, 8, 8])
ðŸŸ¦ temporal_upsample repeats (T==1): 4
ðŸŸ¦ after repeat_interleave: torch.Size([1, 4096, 1, 8, 8])
ðŸŸ¦ after pixel_shuffle: torch.Size([1, 1, 1024, 16, 16])
ðŸŸ© EfficientViTBlock.forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© LiteMLA.forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 3072, 1, 16, 16])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 3072, 1, 16, 16])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 3072, 1, 16, 16])
ðŸŸ¦ LiteMLA.forward after aggreg op oped.shape=torch.Size([1, 3072, 1, 16, 16])
ðŸŸ¦ LiteMLA.forward after multi-scale conv qkv.shape=torch.Size([1, 6144, 1, 16, 16])
ðŸŸ© LiteMLA.relu_linear_att qkv.shape=torch.Size([1, 6144, 1, 16, 16])
ðŸŸ¦ LiteMLA.relu_linear_att after reshape qkv.shape=torch.Size([1, 64, 96, 256])
ðŸŸ¦ LiteMLA.relu_linear_att after split q.shape=torch.Size([1, 64, 32, 256]) k.shape=torch.Size([1, 64, 32, 256]) v.shape=torch.Size([1, 64, 32, 256])
ðŸŸ¦ q.shape=torch.Size([1, 64, 32, 256]) k.shape=torch.Size([1, 64, 32, 256]) after kernel_func
ðŸŸ¦ LiteMLA.relu_linear_att after v pad v.shape=torch.Size([1, 64, 33, 256])
ðŸŸ¦ LiteMLA.relu_linear_att after vk matmul vk.shape=torch.Size([1, 64, 33, 32])
ðŸŸ¦ LiteMLA.relu_linear_att after out matmul out.shape=torch.Size([1, 64, 33, 256])
ðŸŸ¦ LiteMLA.relu_linear_att after normalization out.shape=torch.Size([1, 64, 32, 256])
ðŸŸ¦ LiteMLA.relu_linear_att after reshape out.shape=torch.Size([1, 2048, 1, 16, 16])
ðŸŸ¦ LiteMLA.forward after relu_linear_att out.shape=torch.Size([1, 2048, 1, 16, 16])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 2048, 1, 16, 16])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 2048, 1, 16, 16])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 2048, 1, 16, 16])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ¦ LiteMLA.forward after proj out.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ¦ After context_module: x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© GLUMBConv.forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 8192, 1, 16, 16])
ðŸŸ¦ GLUMBConv.forward after inverted_conv x.shape=torch.Size([1, 8192, 1, 16, 16])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 8192, 1, 16, 16])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 8192, 3, 18, 18])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 8192, 3, 18, 18])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 8192, 1, 16, 16])
ðŸŸ¦ GLUMBConv.forward after depth_conv x.shape=torch.Size([1, 8192, 1, 16, 16])
ðŸŸ¦ GLUMBConv.forward after GLU x.shape=torch.Size([1, 4096, 1, 16, 16])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 4096, 1, 16, 16])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 4096, 1, 16, 16])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 4096, 1, 16, 16])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ¦ GLUMBConv.forward after point_conv x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ¦ After local_module: x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© EfficientViTBlock.forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© LiteMLA.forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 3072, 1, 16, 16])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 3072, 1, 16, 16])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 3072, 1, 16, 16])
ðŸŸ¦ LiteMLA.forward after aggreg op oped.shape=torch.Size([1, 3072, 1, 16, 16])
ðŸŸ¦ LiteMLA.forward after multi-scale conv qkv.shape=torch.Size([1, 6144, 1, 16, 16])
ðŸŸ© LiteMLA.relu_linear_att qkv.shape=torch.Size([1, 6144, 1, 16, 16])
ðŸŸ¦ LiteMLA.relu_linear_att after reshape qkv.shape=torch.Size([1, 64, 96, 256])
ðŸŸ¦ LiteMLA.relu_linear_att after split q.shape=torch.Size([1, 64, 32, 256]) k.shape=torch.Size([1, 64, 32, 256]) v.shape=torch.Size([1, 64, 32, 256])
ðŸŸ¦ q.shape=torch.Size([1, 64, 32, 256]) k.shape=torch.Size([1, 64, 32, 256]) after kernel_func
ðŸŸ¦ LiteMLA.relu_linear_att after v pad v.shape=torch.Size([1, 64, 33, 256])
ðŸŸ¦ LiteMLA.relu_linear_att after vk matmul vk.shape=torch.Size([1, 64, 33, 32])
ðŸŸ¦ LiteMLA.relu_linear_att after out matmul out.shape=torch.Size([1, 64, 33, 256])
ðŸŸ¦ LiteMLA.relu_linear_att after normalization out.shape=torch.Size([1, 64, 32, 256])
ðŸŸ¦ LiteMLA.relu_linear_att after reshape out.shape=torch.Size([1, 2048, 1, 16, 16])
ðŸŸ¦ LiteMLA.forward after relu_linear_att out.shape=torch.Size([1, 2048, 1, 16, 16])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 2048, 1, 16, 16])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 2048, 1, 16, 16])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 2048, 1, 16, 16])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ¦ LiteMLA.forward after proj out.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ¦ After context_module: x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© GLUMBConv.forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 8192, 1, 16, 16])
ðŸŸ¦ GLUMBConv.forward after inverted_conv x.shape=torch.Size([1, 8192, 1, 16, 16])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 8192, 1, 16, 16])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 8192, 3, 18, 18])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 8192, 3, 18, 18])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 8192, 1, 16, 16])
ðŸŸ¦ GLUMBConv.forward after depth_conv x.shape=torch.Size([1, 8192, 1, 16, 16])
ðŸŸ¦ GLUMBConv.forward after GLU x.shape=torch.Size([1, 4096, 1, 16, 16])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 4096, 1, 16, 16])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 4096, 1, 16, 16])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 4096, 1, 16, 16])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ¦ GLUMBConv.forward after point_conv x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ¦ After local_module: x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© EfficientViTBlock.forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© LiteMLA.forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 3072, 1, 16, 16])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 3072, 1, 16, 16])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 3072, 1, 16, 16])
ðŸŸ¦ LiteMLA.forward after aggreg op oped.shape=torch.Size([1, 3072, 1, 16, 16])
ðŸŸ¦ LiteMLA.forward after multi-scale conv qkv.shape=torch.Size([1, 6144, 1, 16, 16])
ðŸŸ© LiteMLA.relu_linear_att qkv.shape=torch.Size([1, 6144, 1, 16, 16])
ðŸŸ¦ LiteMLA.relu_linear_att after reshape qkv.shape=torch.Size([1, 64, 96, 256])
ðŸŸ¦ LiteMLA.relu_linear_att after split q.shape=torch.Size([1, 64, 32, 256]) k.shape=torch.Size([1, 64, 32, 256]) v.shape=torch.Size([1, 64, 32, 256])
ðŸŸ¦ q.shape=torch.Size([1, 64, 32, 256]) k.shape=torch.Size([1, 64, 32, 256]) after kernel_func
ðŸŸ¦ LiteMLA.relu_linear_att after v pad v.shape=torch.Size([1, 64, 33, 256])
ðŸŸ¦ LiteMLA.relu_linear_att after vk matmul vk.shape=torch.Size([1, 64, 33, 32])
ðŸŸ¦ LiteMLA.relu_linear_att after out matmul out.shape=torch.Size([1, 64, 33, 256])
ðŸŸ¦ LiteMLA.relu_linear_att after normalization out.shape=torch.Size([1, 64, 32, 256])
ðŸŸ¦ LiteMLA.relu_linear_att after reshape out.shape=torch.Size([1, 2048, 1, 16, 16])
ðŸŸ¦ LiteMLA.forward after relu_linear_att out.shape=torch.Size([1, 2048, 1, 16, 16])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 2048, 1, 16, 16])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 2048, 1, 16, 16])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 2048, 1, 16, 16])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ¦ LiteMLA.forward after proj out.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ¦ After context_module: x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© GLUMBConv.forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 8192, 1, 16, 16])
ðŸŸ¦ GLUMBConv.forward after inverted_conv x.shape=torch.Size([1, 8192, 1, 16, 16])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 8192, 1, 16, 16])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 8192, 3, 18, 18])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 8192, 3, 18, 18])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 8192, 1, 16, 16])
ðŸŸ¦ GLUMBConv.forward after depth_conv x.shape=torch.Size([1, 8192, 1, 16, 16])
ðŸŸ¦ GLUMBConv.forward after GLU x.shape=torch.Size([1, 4096, 1, 16, 16])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 4096, 1, 16, 16])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 4096, 1, 16, 16])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 4096, 1, 16, 16])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ¦ GLUMBConv.forward after point_conv x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ¦ After local_module: x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ¦ Decoder.forward after stage i=1 x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© auto_grad_checkpoint len(args)=1 args[0].shape=torch.Size([1, 1024, 1, 16, 16]) kwargs={}
ðŸŸ© OpSequential forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© InterpolateConvUpSampleLayer.forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ© chunked_interpolate x.shape=torch.Size([1, 1024, 1, 16, 16]) scale_factor=[1, 2, 2] mode='nearest'
ðŸŸ¦ After interpolate: torch.Size([1, 1024, 1, 32, 32])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 1024, 1, 32, 32])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 1024, 3, 34, 34])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1024, 3, 34, 34])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ¦ After conv: torch.Size([1, 512, 1, 32, 32])
ðŸŸ© ChannelDuplicatingPixelShuffleUpSampleLayer.forward x.shape=torch.Size([1, 1024, 1, 16, 16])
ðŸŸ¦ temporal_upsample repeats (T==1): 2
ðŸŸ¦ after repeat_interleave: torch.Size([1, 2048, 1, 16, 16])
ðŸŸ¦ after pixel_shuffle: torch.Size([1, 1, 512, 32, 32])
ðŸŸ© EfficientViTBlock.forward x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© LiteMLA.forward x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 1536, 1, 32, 32])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1536, 1, 32, 32])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1536, 1, 32, 32])
ðŸŸ¦ LiteMLA.forward after aggreg op oped.shape=torch.Size([1, 1536, 1, 32, 32])
ðŸŸ¦ LiteMLA.forward after multi-scale conv qkv.shape=torch.Size([1, 3072, 1, 32, 32])
ðŸŸ© LiteMLA.relu_linear_att qkv.shape=torch.Size([1, 3072, 1, 32, 32])
ðŸŸ¦ LiteMLA.relu_linear_att after reshape qkv.shape=torch.Size([1, 32, 96, 1024])
ðŸŸ¦ LiteMLA.relu_linear_att after split q.shape=torch.Size([1, 32, 32, 1024]) k.shape=torch.Size([1, 32, 32, 1024]) v.shape=torch.Size([1, 32, 32, 1024])
ðŸŸ¦ q.shape=torch.Size([1, 32, 32, 1024]) k.shape=torch.Size([1, 32, 32, 1024]) after kernel_func
ðŸŸ¦ LiteMLA.relu_linear_att after v pad v.shape=torch.Size([1, 32, 33, 1024])
ðŸŸ¦ LiteMLA.relu_linear_att after vk matmul vk.shape=torch.Size([1, 32, 33, 32])
ðŸŸ¦ LiteMLA.relu_linear_att after out matmul out.shape=torch.Size([1, 32, 33, 1024])
ðŸŸ¦ LiteMLA.relu_linear_att after normalization out.shape=torch.Size([1, 32, 32, 1024])
ðŸŸ¦ LiteMLA.relu_linear_att after reshape out.shape=torch.Size([1, 1024, 1, 32, 32])
ðŸŸ¦ LiteMLA.forward after relu_linear_att out.shape=torch.Size([1, 1024, 1, 32, 32])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 1024, 1, 32, 32])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 1024, 1, 32, 32])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1024, 1, 32, 32])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ¦ LiteMLA.forward after proj out.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ¦ After context_module: x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© GLUMBConv.forward x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 4096, 1, 32, 32])
ðŸŸ¦ GLUMBConv.forward after inverted_conv x.shape=torch.Size([1, 4096, 1, 32, 32])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 4096, 1, 32, 32])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 4096, 3, 34, 34])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 4096, 3, 34, 34])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 4096, 1, 32, 32])
ðŸŸ¦ GLUMBConv.forward after depth_conv x.shape=torch.Size([1, 4096, 1, 32, 32])
ðŸŸ¦ GLUMBConv.forward after GLU x.shape=torch.Size([1, 2048, 1, 32, 32])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 2048, 1, 32, 32])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 2048, 1, 32, 32])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 2048, 1, 32, 32])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ¦ GLUMBConv.forward after point_conv x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ¦ After local_module: x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© EfficientViTBlock.forward x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© LiteMLA.forward x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 1536, 1, 32, 32])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1536, 1, 32, 32])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1536, 1, 32, 32])
ðŸŸ¦ LiteMLA.forward after aggreg op oped.shape=torch.Size([1, 1536, 1, 32, 32])
ðŸŸ¦ LiteMLA.forward after multi-scale conv qkv.shape=torch.Size([1, 3072, 1, 32, 32])
ðŸŸ© LiteMLA.relu_linear_att qkv.shape=torch.Size([1, 3072, 1, 32, 32])
ðŸŸ¦ LiteMLA.relu_linear_att after reshape qkv.shape=torch.Size([1, 32, 96, 1024])
ðŸŸ¦ LiteMLA.relu_linear_att after split q.shape=torch.Size([1, 32, 32, 1024]) k.shape=torch.Size([1, 32, 32, 1024]) v.shape=torch.Size([1, 32, 32, 1024])
ðŸŸ¦ q.shape=torch.Size([1, 32, 32, 1024]) k.shape=torch.Size([1, 32, 32, 1024]) after kernel_func
ðŸŸ¦ LiteMLA.relu_linear_att after v pad v.shape=torch.Size([1, 32, 33, 1024])
ðŸŸ¦ LiteMLA.relu_linear_att after vk matmul vk.shape=torch.Size([1, 32, 33, 32])
ðŸŸ¦ LiteMLA.relu_linear_att after out matmul out.shape=torch.Size([1, 32, 33, 1024])
ðŸŸ¦ LiteMLA.relu_linear_att after normalization out.shape=torch.Size([1, 32, 32, 1024])
ðŸŸ¦ LiteMLA.relu_linear_att after reshape out.shape=torch.Size([1, 1024, 1, 32, 32])
ðŸŸ¦ LiteMLA.forward after relu_linear_att out.shape=torch.Size([1, 1024, 1, 32, 32])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 1024, 1, 32, 32])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 1024, 1, 32, 32])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1024, 1, 32, 32])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ¦ LiteMLA.forward after proj out.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ¦ After context_module: x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© GLUMBConv.forward x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 4096, 1, 32, 32])
ðŸŸ¦ GLUMBConv.forward after inverted_conv x.shape=torch.Size([1, 4096, 1, 32, 32])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 4096, 1, 32, 32])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 4096, 3, 34, 34])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 4096, 3, 34, 34])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 4096, 1, 32, 32])
ðŸŸ¦ GLUMBConv.forward after depth_conv x.shape=torch.Size([1, 4096, 1, 32, 32])
ðŸŸ¦ GLUMBConv.forward after GLU x.shape=torch.Size([1, 2048, 1, 32, 32])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 2048, 1, 32, 32])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 2048, 1, 32, 32])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 2048, 1, 32, 32])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ¦ GLUMBConv.forward after point_conv x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ¦ After local_module: x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© EfficientViTBlock.forward x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© LiteMLA.forward x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 1536, 1, 32, 32])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1536, 1, 32, 32])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1536, 1, 32, 32])
ðŸŸ¦ LiteMLA.forward after aggreg op oped.shape=torch.Size([1, 1536, 1, 32, 32])
ðŸŸ¦ LiteMLA.forward after multi-scale conv qkv.shape=torch.Size([1, 3072, 1, 32, 32])
ðŸŸ© LiteMLA.relu_linear_att qkv.shape=torch.Size([1, 3072, 1, 32, 32])
ðŸŸ¦ LiteMLA.relu_linear_att after reshape qkv.shape=torch.Size([1, 32, 96, 1024])
ðŸŸ¦ LiteMLA.relu_linear_att after split q.shape=torch.Size([1, 32, 32, 1024]) k.shape=torch.Size([1, 32, 32, 1024]) v.shape=torch.Size([1, 32, 32, 1024])
ðŸŸ¦ q.shape=torch.Size([1, 32, 32, 1024]) k.shape=torch.Size([1, 32, 32, 1024]) after kernel_func
ðŸŸ¦ LiteMLA.relu_linear_att after v pad v.shape=torch.Size([1, 32, 33, 1024])
ðŸŸ¦ LiteMLA.relu_linear_att after vk matmul vk.shape=torch.Size([1, 32, 33, 32])
ðŸŸ¦ LiteMLA.relu_linear_att after out matmul out.shape=torch.Size([1, 32, 33, 1024])
ðŸŸ¦ LiteMLA.relu_linear_att after normalization out.shape=torch.Size([1, 32, 32, 1024])
ðŸŸ¦ LiteMLA.relu_linear_att after reshape out.shape=torch.Size([1, 1024, 1, 32, 32])
ðŸŸ¦ LiteMLA.forward after relu_linear_att out.shape=torch.Size([1, 1024, 1, 32, 32])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 1024, 1, 32, 32])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 1024, 1, 32, 32])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 1024, 1, 32, 32])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ¦ LiteMLA.forward after proj out.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ¦ After context_module: x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© GLUMBConv.forward x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 4096, 1, 32, 32])
ðŸŸ¦ GLUMBConv.forward after inverted_conv x.shape=torch.Size([1, 4096, 1, 32, 32])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 4096, 1, 32, 32])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 4096, 3, 34, 34])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 4096, 3, 34, 34])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 4096, 1, 32, 32])
ðŸŸ¦ GLUMBConv.forward after depth_conv x.shape=torch.Size([1, 4096, 1, 32, 32])
ðŸŸ¦ GLUMBConv.forward after GLU x.shape=torch.Size([1, 2048, 1, 32, 32])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 2048, 1, 32, 32])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 2048, 1, 32, 32])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 2048, 1, 32, 32])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ¦ GLUMBConv.forward after point_conv x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ¦ After local_module: x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ¦ Decoder.forward after stage i=2 x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© auto_grad_checkpoint len(args)=1 args[0].shape=torch.Size([1, 512, 1, 32, 32]) kwargs={}
ðŸŸ© OpSequential forward x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© InterpolateConvUpSampleLayer.forward x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ© chunked_interpolate x.shape=torch.Size([1, 512, 1, 32, 32]) scale_factor=[1, 2, 2] mode='nearest'
ðŸŸ¦ After interpolate: torch.Size([1, 512, 1, 64, 64])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 512, 1, 64, 64])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 512, 3, 66, 66])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 512, 3, 66, 66])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 512, 1, 64, 64])
ðŸŸ¦ After conv: torch.Size([1, 512, 1, 64, 64])
ðŸŸ© ChannelDuplicatingPixelShuffleUpSampleLayer.forward x.shape=torch.Size([1, 512, 1, 32, 32])
ðŸŸ¦ temporal_upsample repeats (T==1): 4
ðŸŸ¦ after repeat_interleave: torch.Size([1, 2048, 1, 32, 32])
ðŸŸ¦ after pixel_shuffle: torch.Size([1, 1, 512, 64, 64])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 512, 1, 64, 64])
ðŸŸ© ResBlock.forward x.shape=torch.Size([1, 512, 1, 64, 64])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 512, 1, 64, 64])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 512, 3, 66, 66])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 512, 3, 66, 66])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 512, 1, 64, 64])
ðŸŸ¦ After conv1: torch.Size([1, 512, 1, 64, 64])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 512, 1, 64, 64])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 512, 3, 66, 66])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 512, 3, 66, 66])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 512, 1, 64, 64])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 512, 1, 64, 64])
ðŸŸ¦ After conv2: torch.Size([1, 512, 1, 64, 64])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 512, 1, 64, 64])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 512, 1, 64, 64])
ðŸŸ© ResBlock.forward x.shape=torch.Size([1, 512, 1, 64, 64])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 512, 1, 64, 64])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 512, 3, 66, 66])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 512, 3, 66, 66])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 512, 1, 64, 64])
ðŸŸ¦ After conv1: torch.Size([1, 512, 1, 64, 64])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 512, 1, 64, 64])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 512, 3, 66, 66])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 512, 3, 66, 66])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 512, 1, 64, 64])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 512, 1, 64, 64])
ðŸŸ¦ After conv2: torch.Size([1, 512, 1, 64, 64])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 512, 1, 64, 64])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 512, 1, 64, 64])
ðŸŸ© ResBlock.forward x.shape=torch.Size([1, 512, 1, 64, 64])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 512, 1, 64, 64])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 512, 3, 66, 66])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 512, 3, 66, 66])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 512, 1, 64, 64])
ðŸŸ¦ After conv1: torch.Size([1, 512, 1, 64, 64])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 512, 1, 64, 64])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 512, 3, 66, 66])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 512, 3, 66, 66])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 512, 1, 64, 64])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 512, 1, 64, 64])
ðŸŸ¦ After conv2: torch.Size([1, 512, 1, 64, 64])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 512, 1, 64, 64])
ðŸŸ¦ Decoder.forward after stage i=3 x.shape=torch.Size([1, 512, 1, 64, 64])
ðŸŸ© auto_grad_checkpoint len(args)=1 args[0].shape=torch.Size([1, 512, 1, 64, 64]) kwargs={}
ðŸŸ© OpSequential forward x.shape=torch.Size([1, 512, 1, 64, 64])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 512, 1, 64, 64])
ðŸŸ© InterpolateConvUpSampleLayer.forward x.shape=torch.Size([1, 512, 1, 64, 64])
ðŸŸ© chunked_interpolate x.shape=torch.Size([1, 512, 1, 64, 64]) scale_factor=[1, 2, 2] mode='nearest'
ðŸŸ¦ After interpolate: torch.Size([1, 512, 1, 128, 128])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 512, 1, 128, 128])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 512, 3, 130, 130])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 512, 3, 130, 130])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 256, 1, 128, 128])
ðŸŸ¦ After conv: torch.Size([1, 256, 1, 128, 128])
ðŸŸ© ChannelDuplicatingPixelShuffleUpSampleLayer.forward x.shape=torch.Size([1, 512, 1, 64, 64])
ðŸŸ¦ temporal_upsample repeats (T==1): 2
ðŸŸ¦ after repeat_interleave: torch.Size([1, 1024, 1, 64, 64])
ðŸŸ¦ after pixel_shuffle: torch.Size([1, 1, 256, 128, 128])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 256, 1, 128, 128])
ðŸŸ© ResBlock.forward x.shape=torch.Size([1, 256, 1, 128, 128])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 256, 1, 128, 128])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 256, 3, 130, 130])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 256, 3, 130, 130])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 256, 1, 128, 128])
ðŸŸ¦ After conv1: torch.Size([1, 256, 1, 128, 128])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 256, 1, 128, 128])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 256, 3, 130, 130])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 256, 3, 130, 130])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 256, 1, 128, 128])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 256, 1, 128, 128])
ðŸŸ¦ After conv2: torch.Size([1, 256, 1, 128, 128])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 256, 1, 128, 128])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 256, 1, 128, 128])
ðŸŸ© ResBlock.forward x.shape=torch.Size([1, 256, 1, 128, 128])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 256, 1, 128, 128])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 256, 3, 130, 130])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 256, 3, 130, 130])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 256, 1, 128, 128])
ðŸŸ¦ After conv1: torch.Size([1, 256, 1, 128, 128])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 256, 1, 128, 128])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 256, 3, 130, 130])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 256, 3, 130, 130])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 256, 1, 128, 128])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 256, 1, 128, 128])
ðŸŸ¦ After conv2: torch.Size([1, 256, 1, 128, 128])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 256, 1, 128, 128])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 256, 1, 128, 128])
ðŸŸ© ResBlock.forward x.shape=torch.Size([1, 256, 1, 128, 128])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 256, 1, 128, 128])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 256, 3, 130, 130])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 256, 3, 130, 130])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 256, 1, 128, 128])
ðŸŸ¦ After conv1: torch.Size([1, 256, 1, 128, 128])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 256, 1, 128, 128])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 256, 3, 130, 130])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 256, 3, 130, 130])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 256, 1, 128, 128])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 256, 1, 128, 128])
ðŸŸ¦ After conv2: torch.Size([1, 256, 1, 128, 128])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 256, 1, 128, 128])
ðŸŸ¦ Decoder.forward after stage i=4 x.shape=torch.Size([1, 256, 1, 128, 128])
ðŸŸ© auto_grad_checkpoint len(args)=1 args[0].shape=torch.Size([1, 256, 1, 128, 128]) kwargs={}
ðŸŸ© OpSequential forward x.shape=torch.Size([1, 256, 1, 128, 128])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 256, 1, 128, 128])
ðŸŸ© InterpolateConvUpSampleLayer.forward x.shape=torch.Size([1, 256, 1, 128, 128])
ðŸŸ© chunked_interpolate x.shape=torch.Size([1, 256, 1, 128, 128]) scale_factor=[1, 2, 2] mode='nearest'
ðŸŸ¦ After interpolate: torch.Size([1, 256, 1, 256, 256])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 256, 1, 256, 256])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 256, 3, 258, 258])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 256, 3, 258, 258])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 128, 1, 256, 256])
ðŸŸ¦ After conv: torch.Size([1, 128, 1, 256, 256])
ðŸŸ© ChannelDuplicatingPixelShuffleUpSampleLayer.forward x.shape=torch.Size([1, 256, 1, 128, 128])
ðŸŸ¦ temporal_upsample repeats (T==1): 2
ðŸŸ¦ after repeat_interleave: torch.Size([1, 512, 1, 128, 128])
ðŸŸ¦ after pixel_shuffle: torch.Size([1, 1, 128, 256, 256])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 128, 1, 256, 256])
ðŸŸ© ResBlock.forward x.shape=torch.Size([1, 128, 1, 256, 256])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 128, 1, 256, 256])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 128, 3, 258, 258])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 128, 3, 258, 258])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 128, 1, 256, 256])
ðŸŸ¦ After conv1: torch.Size([1, 128, 1, 256, 256])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 128, 1, 256, 256])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 128, 3, 258, 258])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 128, 3, 258, 258])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 128, 1, 256, 256])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 128, 1, 256, 256])
ðŸŸ¦ After conv2: torch.Size([1, 128, 1, 256, 256])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 128, 1, 256, 256])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 128, 1, 256, 256])
ðŸŸ© ResBlock.forward x.shape=torch.Size([1, 128, 1, 256, 256])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 128, 1, 256, 256])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 128, 3, 258, 258])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 128, 3, 258, 258])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 128, 1, 256, 256])
ðŸŸ¦ After conv1: torch.Size([1, 128, 1, 256, 256])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 128, 1, 256, 256])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 128, 3, 258, 258])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 128, 3, 258, 258])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 128, 1, 256, 256])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 128, 1, 256, 256])
ðŸŸ¦ After conv2: torch.Size([1, 128, 1, 256, 256])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 128, 1, 256, 256])
ðŸŸ© ResidualBlock.forward x.shape=torch.Size([1, 128, 1, 256, 256])
ðŸŸ© ResBlock.forward x.shape=torch.Size([1, 128, 1, 256, 256])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 128, 1, 256, 256])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 128, 3, 258, 258])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 128, 3, 258, 258])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 128, 1, 256, 256])
ðŸŸ¦ After conv1: torch.Size([1, 128, 1, 256, 256])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 128, 1, 256, 256])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 128, 3, 258, 258])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 128, 3, 258, 258])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 128, 1, 256, 256])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 128, 1, 256, 256])
ðŸŸ¦ After conv2: torch.Size([1, 128, 1, 256, 256])
ðŸŸ© IdentityLayer forward x.shape=torch.Size([1, 128, 1, 256, 256])
ðŸŸ¦ Decoder.forward after stage i=5 x.shape=torch.Size([1, 128, 1, 256, 256])
ðŸŸ© auto_grad_checkpoint len(args)=1 args[0].shape=torch.Size([1, 128, 1, 256, 256]) kwargs={}
ðŸŸ© OpSequential forward x.shape=torch.Size([1, 128, 1, 256, 256])
ðŸŸ© RMSNorm3d forward x.shape=torch.Size([1, 128, 1, 256, 256])
ðŸŸ© ConvLayer.forward x.shape=torch.Size([1, 128, 1, 256, 256])
ðŸŸ¦ ConvLayer.forward after pad x.shape=torch.Size([1, 128, 3, 258, 258])
ðŸŸ© ChannelChunkConv3d.forward input.shape=torch.Size([1, 128, 3, 258, 258])
ðŸŸ¦ ConvLayer.forward after conv x.shape=torch.Size([1, 3, 1, 256, 256])
ðŸŸ© MMDiTã‚’æ¤œè¨¼
ðŸŸ© Initializing MMDiT Model with from_pretrained=None, cache_dir=None, device_map='cuda', torch_dtype=torch.bfloat16, strict_load=False, kwargs={'guidance_embed': False, 'fused_qkv': False, 'use_liger_rope': True, 'grad_ckpt_settings': (8, 100), 'in_channels': 64, 'vec_in_dim': 768, 'context_in_dim': 4096, 'hidden_size': 384, 'mlp_ratio': 4.0, 'num_heads': 3, 'depth': 1, 'depth_single_blocks': 38, 'axes_dim': [16, 56, 56], 'theta': 10000, 'qkv_bias': True}
ðŸŸ© MMDiTModel.__init__ config=MMDiTConfig(from_pretrained=None, cache_dir=None, in_channels=64, vec_in_dim=768, context_in_dim=4096, hidden_size=384, mlp_ratio=4.0, num_heads=3, depth=1, depth_single_blocks=38, axes_dim=[16, 56, 56], theta=10000, qkv_bias=True, guidance_embed=False, cond_embed=False, fused_qkv=False, grad_ckpt_settings=(8, 100), use_liger_rope=True, patch_size=2)
ðŸŸ© LigerEmbedND.__init__ dim=128 theta=10000 axes_dim=[16, 56, 56]
ðŸŸ© MLPEmbedder.__init__ in_dim=256 hidden_dim=384
ðŸŸ© MLPEmbedder.__init__ in_dim=768 hidden_dim=384
ðŸŸ© DoubleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qkv_bias=True fused_qkv=False
ðŸŸ© Modulation.__init__ dim=384 double=True
ðŸŸ© SelfAttention.__init__ dim=384 num_heads=3 qkv_bias=True fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=True
ðŸŸ© SelfAttention.__init__ dim=384 num_heads=3 qkv_bias=True fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© SingleStreamBlock.__init__ hidden_size=384 num_heads=3 mlp_ratio=4.0 qk_scale=None fused_qkv=False
ðŸŸ© QKNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© RMSNorm.__init__ dim=128
ðŸŸ© Modulation.__init__ dim=384 double=False
ðŸŸ© LastLayer.__init__ hidden_size=384 patch_size=1 out_channels=64
ðŸŸ© MMDiTModel.forward_selective_ckpt img.shape=torch.Size([1, 16, 64]) txt.shape=torch.Size([1, 16, 4096]) timesteps.shape=torch.Size([1]) y_vec.shape=torch.Size([1, 768])
ðŸŸ© MMDiTModel.prepare_block_inputs img.shape=torch.Size([1, 16, 64]) txt.shape=torch.Size([1, 16, 4096]) timesteps.shape=torch.Size([1]) y_vec.shape=torch.Size([1, 768])
ðŸŸ¦ After img_in: img.shape=torch.Size([1, 16, 384])
ðŸŸ© timestep_embedding t.shape=torch.Size([1]) dim=256 max_period=10000 time_factor=1000.0
ðŸŸ© MLPEmbedder.forward x.shape=torch.Size([1, 256])
ðŸŸ¦ After time_in: vec.shape=torch.Size([1, 384])
ðŸŸ© MLPEmbedder.forward x.shape=torch.Size([1, 768])
ðŸŸ¦ After vector_in: vec.shape=torch.Size([1, 384])
ðŸŸ¦ After txt_in: txt.shape=torch.Size([1, 16, 384])
ðŸŸ¦ Before pe_embedder: ids.shape=torch.Size([1, 32, 3])
ðŸŸ© LigerEmbedND.forward ids.shape=torch.Size([1, 32, 3])
ðŸŸ© liger_rope pos.shape=torch.Size([1, 32]) dim=16 theta=10000
ðŸŸ© liger_rope pos.shape=torch.Size([1, 32]) dim=56 theta=10000
ðŸŸ© liger_rope pos.shape=torch.Size([1, 32]) dim=56 theta=10000
ðŸŸ¦ After pe_embedder: pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ¦ After prepare_block_inputs: img.shape=torch.Size([1, 16, 384]) txt.shape=torch.Size([1, 16, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© auto_grad_checkpoint len(args)=4 args[0].shape=torch.Size([1, 16, 384]) kwargs={}
ðŸŸ© DoubleStreamBlock.forward img.shape=torch.Size([1, 16, 384]) txt.shape=torch.Size([1, 16, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© DoubleStreamBlockProcessor.__call__ img.shape=torch.Size([1, 16, 384]) txt.shape=torch.Size([1, 16, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ img_q.shape=torch.Size([1, 16, 3, 128]) img_k.shape=torch.Size([1, 16, 3, 128]) img_v.shape=torch.Size([1, 16, 3, 128])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 16, 3, 128]) k.shape=torch.Size([1, 16, 3, 128]) v.shape=torch.Size([1, 16, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 16, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 16, 3, 128])
ðŸŸ¦ img_q.shape=torch.Size([1, 3, 16, 128]) img_k.shape=torch.Size([1, 3, 16, 128]) img_v.shape=torch.Size([1, 3, 16, 128])
ðŸŸ¦ Before QKNorm txt_q.shape=torch.Size([1, 16, 3, 128]) txt_k.shape=torch.Size([1, 16, 3, 128]) txt_v.shape=torch.Size([1, 16, 3, 128])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 16, 3, 128]) k.shape=torch.Size([1, 16, 3, 128]) v.shape=torch.Size([1, 16, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 16, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 16, 3, 128])
ðŸŸ¦ After QKNorm txt_q.shape=torch.Size([1, 3, 16, 128]) txt_k.shape=torch.Size([1, 3, 16, 128]) txt_v.shape=torch.Size([1, 3, 16, 128])
ðŸŸ¦ Before Attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ After double_blocks: img.shape=torch.Size([1, 16, 384]) txt.shape=torch.Size([1, 16, 384])
ðŸŸ¦ After concat txt and img: img.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ© auto_grad_checkpoint len(args)=3 args[0].shape=torch.Size([1, 32, 384]) kwargs={}
ðŸŸ© SingleStreamBlock.forward x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© SingleStreamBlockProcessor.__call__ x.shape=torch.Size([1, 32, 384]) vec.shape=torch.Size([1, 384]) pe[0].shape=torch.Size([1, 32, 128]) pe[1].shape=torch.Size([1, 32, 128])
ðŸŸ© Modulation.forward vec.shape=torch.Size([1, 384])
ðŸŸ¦ Modulation.forward out shapes: [torch.Size([1, 1, 384]), torch.Size([1, 1, 384]), torch.Size([1, 1, 384])]
ðŸŸ¦ Before QKNorm q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 384]) mlp.shape=torch.Size([1, 32, 1536])
ðŸŸ© QKNorm.forward q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ© FusedRMSNorm.forward x.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ After QKNorm q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128])
ðŸŸ© attention q.shape=torch.Size([1, 3, 32, 128]) k.shape=torch.Size([1, 3, 32, 128]) v.shape=torch.Size([1, 3, 32, 128]) type(pe)=<class 'tuple'>
ðŸŸ¦ LigerRopeã‚’é©ç”¨ cos.shape=torch.Size([1, 32, 128]) sin.shape=torch.Size([1, 32, 128])
ðŸŸ© flash_attn_func q.shape=torch.Size([1, 32, 3, 128]) k.shape=torch.Size([1, 32, 3, 128]) v.shape=torch.Size([1, 32, 3, 128])
ðŸŸ¦ attention output x.shape=torch.Size([1, 32, 384])
ðŸŸ¦ SingleStreamBlock output output.shape=torch.Size([1, 32, 384])
ðŸŸ¦ After single_blocks: img.shape=torch.Size([1, 32, 384])
ðŸŸ¦ After single_blocks: img.shape=torch.Size([1, 16, 384])
ðŸŸ© LastLayer.forward x.shape=torch.Size([1, 16, 384]) vec.shape=torch.Size([1, 384])
ðŸŸ¦ After final_layer: img.shape=torch.Size([1, 16, 64])
ðŸŸ© torch.Size([1, 16, 64])
