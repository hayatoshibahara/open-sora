{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f68b9b2b",
   "metadata": {},
   "source": [
    "# Open-Sora 2.0 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f47a17e",
   "metadata": {},
   "source": [
    "## „É¢„Éá„É´„ÅÆË®ìÁ∑¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646fd000",
   "metadata": {},
   "source": [
    "Ë®ìÁ∑¥„ÅØ3„Å§„ÅÆ„Çπ„ÉÜ„Éº„Ç∏„ÅßÊßãÊàê:\n",
    "\n",
    "1. ‰ΩéËß£ÂÉèÂ∫¶ÂãïÁîª„Å´„Çà„ÇãT2VÔºàText to VideoÔºâ„É¢„Éá„É´„ÅÆË®ìÁ∑¥\n",
    "1. ‰ΩéËß£ÂÉèÂ∫¶ÂãïÁîª„Å´„Çà„ÇãI2VÔºàImage to VideoÔºâ„É¢„Éá„É´„ÅÆË®ìÁ∑¥\n",
    "1. È´òËß£ÂÉèÂ∫¶ÂãïÁîª„Å´„Çà„ÇãI2V„É¢„Éá„É´„ÅÆ„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574c78b7",
   "metadata": {},
   "source": [
    "Ë®ìÁ∑¥„ÅÆ„Ç≥„Çπ„Éà„ÅÆÂÜÖË®≥„ÅØ„ÄÅ„Çπ„ÉÜ„Éº„Ç∏1„ÅÆ„Ç≥„Çπ„Éà„Åå50%„ÇíÂç†„ÇÅ„Çã:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ef9cce",
   "metadata": {},
   "source": [
    "![](image/table3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1e9a74",
   "metadata": {},
   "source": [
    "Ë®ìÁ∑¥„ÅÆÂäπÁéáÂåñ„ÅÆÊà¶Áï•:\n",
    "\n",
    "- 11BÔºà110ÂÑÑÔºâ„Éë„É©„É°„Éº„Çø„ÅÆFlux„ÅÆËí∏Áïô„É¢„Éá„É´„Åß„ÄÅT2V„É¢„Éá„É´„ÇíÂàùÊúüÂåñ\n",
    "    - Flux„ÅØ„ÄÅ„Ç™„Éº„Éó„É≥„ÇΩ„Éº„ÇπT2IÔºàText to ImageÔºâ„É¢„Éá„É´\n",
    "- È´òÂìÅË≥™„Å™ÂãïÁîª„Éá„Éº„Çø„Åß„ÄÅË®ìÁ∑¥ÂäπÁéá„ÇíÂêë‰∏ä\n",
    "    - PixArt„Åã„ÇâÁùÄÊÉ≥\n",
    "    - „Çπ„ÉÜ„ÉÉ„Éó1, 2„Åß„ÅØ„ÄÅÂ§ßË¶èÊ®°„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åã„ÇâÈ´òÂìÅË≥™„Å™„Çµ„Éñ„Çª„ÉÉ„Éà„ÇíÊäΩÂá∫\n",
    "    - „Çπ„ÉÜ„ÉÉ„Éó3„Åß„ÅØ„ÄÅ„Çà„ÇäÂé≥„Åó„ÅÑ„Éï„Ç£„É´„Çø„É™„É≥„Ç∞„Åß„Çµ„Éñ„Çª„ÉÉ„Éà„ÇíÊäΩÂá∫\n",
    "- ‰ΩéËß£ÂÉèÂ∫¶ÂãïÁîª„Åß„ÅÆÂãï„Åç„ÅÆÂ≠¶Áøí\n",
    "    - È´òËß£ÂÉèÂ∫¶ÂãïÁîª„ÅÆË®ìÁ∑¥„Ç≥„Çπ„Éà„ÅØÈ´ò„ÅÑ„Åü„ÇÅ„ÄÅÂ§öÊßò„Å™Âãï„Åç„ÅØ‰ΩéËß£ÂÉèÂ∫¶ÂãïÁîª„Çí‰∏≠ÂøÉ„Å´Ë®ìÁ∑¥\n",
    "        - 128„Éï„É¨„Éº„É†768px„ÅÆÂãïÁîª„ÅÆË®ìÁ∑¥„ÅØ„ÄÅ256px„ÅÆÂ†¥Âêà„Çà„Çä„ÇÇ40ÂÄçÈÅÖ„ÅÑ\n",
    "        - „Çª„É´„Éï„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥„ÅÆË®àÁÆóË§áÈõëÂ∫¶„Åå‰∫åÊ¨°Èñ¢Êï∞ÁöÑ„Å´Â¢óÂ§ß„Åô„Çã„Åü„ÇÅ\n",
    "    - Âá∫Âäõ„Åå„Åº„ÇÑ„Åë„Çã„Åü„ÇÅ„ÄÅÈ´òËß£ÂÉèÂ∫¶ÂãïÁîª„ÅÆË®ìÁ∑¥„ÅßÂìÅË≥™„ÇíÂêë‰∏ä\n",
    "- „Çπ„ÉÜ„ÉÉ„Éó2„ÅÆ„Ç¢„ÉÉ„Éó„Çπ„Ç±„Éº„É™„É≥„Ç∞Â≠¶Áøí„ÅØ„ÄÅT2V„Åß„ÅØ„Å™„ÅèI2V„ÇíÊé°Áî®„ÅóÂäπÁéáÂåñ:\n",
    "    - ÈùôÊ≠¢Áîª„ÇíÊù°‰ª∂‰ªò„Åë„Åô„Çã„Åì„Å®„Åß„ÄÅÂãï„Åç„ÅÆÁîüÊàê„Å´ÈõÜ‰∏≠„Åß„Åç„Çã„Åü„ÇÅ\n",
    "- È´òËß£ÂÉèÂ∫¶ÂãïÁîª„Åß„ÅÆ„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„ÇíÁü≠Á∏Æ„Åó„ÄÅË®ìÁ∑¥„ÇíÂäπÁéáÂåñ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bee32fa",
   "metadata": {},
   "source": [
    "Open-Sora 2.0„ÅÆË®ìÁ∑¥„Ç≥„Çπ„Éà„ÅØ„ÄÅ5~10ÂÄç‰Ωé„ÅÑ:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7a4a13",
   "metadata": {},
   "source": [
    "![](image/fig7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50432d84",
   "metadata": {},
   "source": [
    "Ë®ìÁ∑¥Ë®≠ÂÆö„ÅØ„ÄÅOpen-Sora 1.2„Å´Âü∫„Å•„Åè:\n",
    "\n",
    "- ÁõÆÁöÑÈñ¢Êï∞„ÅØFlow Matching„ÇíÊé°Áî®\n",
    "- „Ç™„Éó„ÉÜ„Ç£„Éû„Ç§„Ç∂„ÅØAdamW„Çí‰ΩøÁî®\n",
    "    - $\\beta$ÂÄ§„ÅØ$(0.9, 0.999)$\n",
    "    - $\\epsilon$„ÅØ$1\\times 10^{-15}$\n",
    "    - Èáç„ÅøÊ∏õË°∞„ÅØ‰ΩøÁî®„Åó„Å™„ÅÑ\n",
    "    - Â≠¶ÁøíÁéá\n",
    "        - „Çπ„ÉÜ„Éº„Ç∏1, 2\n",
    "            - ÊúÄÂàù„ÅÆ4‰∏á„Çπ„ÉÜ„ÉÉ„Éó„ÅØ$5\\times 10^{-5}$\n",
    "            - ÊúÄÂæå„ÅÆ4.5‰∏á„Çπ„ÉÜ„ÉÉ„Éó„ÅØ$3\\times 10^{-5}$\n",
    "        - „Çπ„ÉÜ„Éº„Ç∏3\n",
    "            - $1\\times 10^{-5}$\n",
    "    - Ë≥ºË≤∑„Éé„É´„É†„ÇØ„É™„ÉÉ„Éî„É≥„Ç∞„ÅÆÈñæÂÄ§„ÅØ$1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298a6ed7",
   "metadata": {},
   "source": [
    "Flow matching„ÅØ„ÄÅStable Diffusion 3ÔºàSD3Ôºâ„Å®È°û‰ºº:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\mathbb{E}_{t,X_{0},X_{1}}[||f_{\\theta}(X_{t},t,y)-(X_{0}-X_{1})||]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16825ea",
   "metadata": {},
   "source": [
    "- $X_0$: ÂãïÁîª„ÅÆÊΩúÂú®Ë°®Áèæ\n",
    "- $X_1 \\sim \\mathcal{N}(0,1)$: „Ç¨„Ç¶„Çπ„Éé„Ç§„Ç∫\n",
    "- $t$: „Çø„Ç§„É†„Çπ„ÉÜ„ÉÉ„ÉóÔºà„Éé„Ç§„Ç∫Âº∑Â∫¶Ôºâ\n",
    "- $X_t=(1-t)X_0 + tX_1$: Ë£úÈñì„Åï„Çå„ÅüÊΩúÂú®Ë°®ÁèæÔºà„Éé„Ç§„Ç∫„ÅÇ„Çä„ÅÆÊΩúÂú®Ë°®ÁèæÔºâ\n",
    "- $y$: „ÉÜ„Ç≠„Çπ„Éà„ÇÑÁîªÂÉè„Å™„Å©„ÅÆÊù°‰ª∂\n",
    "- $(X_0 - X_1)$: Ê≠£Ëß£„ÅÆÈÄüÂ∫¶Ôºà„Éá„Éº„Çø„Åã„Çâ„Éé„Ç§„Ç∫„Å∏„ÅÆÂ§âÂåñ„ÅÆÊñπÂêëÔºâ\n",
    "- $\\mathbb{E}_{t, X_0, X_1}$: Êù°‰ª∂„ÇíÂ§â„Åà„ÅüÊôÇ„ÅÆÂπ≥Âùá"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cae6e92",
   "metadata": {},
   "source": [
    "$t$„ÅØ„ÄÅÂØæÊï∞Ê≠£Ë¶èÂàÜÂ∏É„Åã„Çâ„Çµ„É≥„Éó„É™„É≥„Ç∞„Åó„ÄÅ$X_0$„ÅÆÂΩ¢Áä∂„Å´Âêà„Çè„Åõ„Å¶„Çπ„Ç±„Éº„É™„É≥„Ç∞Ôºö\n",
    "\n",
    "$$\n",
    "t' = \\frac{at}{1 + (a-1)t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cd5f9a",
   "metadata": {},
   "source": [
    "- $a$: $T\\times H\\times W$„Å´ÊØî‰æã„Åô„Çã„Éë„É©„É°„Éº„Çø\n",
    "- È´òËß£ÂÉèÂ∫¶„Åß„ÅÆÈï∑ÊôÇÈñì„ÅÆÁîªÂÉè„ÅØ„Éé„Ç§„Ç∫„ÅÆÂΩ±Èüø„ÇíÂèó„Åë„ÇÑ„Åô„ÅÑ„Åü„ÇÅ\n",
    "- Êé®Ë´ñÊôÇ„ÇÇÂêåÊßò„ÅÆÊñπÊ≥ï„ÅåÈÅ©Áî®"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb03915",
   "metadata": {},
   "source": [
    "„Éû„É´„ÉÅ„Éê„Ç±„ÉÉ„Éà„Éà„É¨„Éº„Éã„É≥„Ç∞„ÇíÊé°Áî®:\n",
    "\n",
    "- „Éï„É¨„Éº„É†Êï∞„ÉªËß£ÂÉèÂ∫¶„Éª„Ç¢„Çπ„Éö„ÇØ„ÉàÊØî„Åå‰ºº„ÅüÂãïÁîª„Çí„Éê„Ç±„ÉÑ„Å´ÂàÜ„Åë„Å¶„ÄÅ„Éê„Ç±„ÉÑ„Åî„Å®„Å´„Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫„ÇíË™øÊï¥„Åô„ÇãÊâãÊ≥ï\n",
    "    - „É°„É¢„É™‰∏çË∂≥ÔºàOOMÔºâ„ÅÆÂõûÈÅø\n",
    "    - ‰∏¶ÂàóË®àÁÆóÊôÇ„ÅÆË®ìÁ∑¥ÊôÇÈñì„ÅÆÂùá‰∏ÄÂåñ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580846f9",
   "metadata": {},
   "source": [
    "„Çπ„ÉÜ„Éº„Ç∏1, 2„ÅÆ„Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫„Å®„Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫„Å®„Çπ„É´„Éº„Éó„ÉÉ„Éà:\n",
    "\n",
    "![](image/table4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece6997d",
   "metadata": {},
   "source": [
    "„Çπ„ÉÜ„Éº„Ç∏3„ÅÆ„Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫„Å®„Çπ„É´„Éº„Éó„ÉÉ„ÉàÔºàContext parallelism 4Ôºâ:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0decbf9",
   "metadata": {},
   "source": [
    "![](image/table5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f930f7dd",
   "metadata": {},
   "source": [
    "È´òÂúßÁ∏Æ„Ç™„Éº„Éà„Ç®„É≥„Ç≥„Éº„ÉÄ„ÉºÔºàDC-AEÔºâ„Çí‰ΩøÁî®„Åó„ÄÅ„Éà„É¨„Éº„Éã„É≥„Ç∞„Ç≥„Çπ„Éà„Çí„Åï„Çâ„Å´ÂâäÊ∏õ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382509ce",
   "metadata": {},
   "source": [
    "DC-AE„Çí‰ΩøÁî®„Åô„Çã„Å®„ÄÅÂãïÁîªÁîüÊàê„Å´ÂøÖË¶Å„Å™„Éà„Éº„ÇØ„É≥Êï∞„ÅåÂ§ßÂπÖ„Å´ÂâäÊ∏õ„Åß„Åç„Çã:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2af738",
   "metadata": {},
   "source": [
    "$$\n",
    "D_{token} = D_{T} \\times D_{H} \\times D_{W} \\times P_{T} \\times P_{H} \\times P_{W}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c126dd2e",
   "metadata": {},
   "source": [
    "- $D_{\\text{token}}$: „Éà„Éº„ÇØ„É≥„Éª„ÉÄ„Ç¶„É≥„Çµ„É≥„Éó„É´ÊØîÔºà„Å©„ÅÆ„Åè„Çâ„ÅÑÂ∞è„Åï„ÅèÂúßÁ∏Æ„Åó„Å¶„Éà„Éº„ÇØ„É≥Âåñ„Åô„Çã„Åã„ÅÆÊØîÁéáÔºâ\n",
    "- $D_T, D_H, H_W$: ÊôÇÈñì„ÉªÈ´ò„Åï„ÉªÂπÖ„ÅÆÊ¨°ÂÖÉ„Å´„Åä„Åë„Çã„Ç™„Éº„Éà„Ç®„É≥„Ç≥„Éº„ÉÄ„ÅÆÂúßÁ∏ÆÁéá\n",
    "- $P_T, P_H, P_W$: ÁîüÊàê„É¢„Éá„É´„Å´ÂÖ•Âäõ„Åô„ÇãÈöõ„ÅÆ„Éë„ÉÉ„ÉÅÂàÜÂâ≤„Çµ„Ç§„Ç∫\n",
    "- Hunyuan Video„ÅÆ$D_{\\text{token}}$„ÅØ$4096$„ÄÅDC-AE„ÅÆ$D_{\\text{token}}$„ÅØ$1024$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d73b6eb",
   "metadata": {},
   "source": [
    "„Ç™„Éº„Éà„Ç®„É≥„Ç≥„Éº„ÉÄ„Éº„ÅÆÊÄßËÉΩ„ÅØ„ÄÅÊÉÖÂ†±„ÉÄ„Ç¶„É≥„Çµ„É≥„Éó„É™„É≥„Ç∞ÊØî„Åß‰∫àÊ∏¨„Åß„Åç„Çã:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc2ad92",
   "metadata": {},
   "source": [
    "$$\n",
    "D_{info} = \\frac{D_{T} \\times D_{H} \\times D_{W} \\times C_{in}}{C_{out}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f77401f",
   "metadata": {},
   "source": [
    "- $D_{\\text{info}}$: ÂÖ•Âäõ„Éá„Éº„Çø„ÅåÊΩúÂú®Ë°®Áèæ„Å´„Å™„Çã„Å®„Åç„Å´ÊÉÖÂ†±Èáè„ÅåÂúßÁ∏Æ„Åï„Çå„ÅüÊØîÁéá\n",
    "- $C_{\\text{in}}$: ÂÖ•Âäõ„ÉÅ„É£„É≥„Éç„É´Êï∞ÔºàRGB„ÅÆÂ†¥Âêà$3$Ôºâ\n",
    "- $C_{\\text{out}}$: Âá∫Âäõ„ÉÅ„É£„É≥„Éç„É´Êï∞ÔºàÊΩúÂú®Ë°®Áèæ„ÅÆ„ÉÅ„É£„É≥„Éç„É´Êï∞Ôºâ\n",
    "- DC-AE„ÅÆ„Ç™„Éº„Éà„Ç®„É≥„Ç≥„Éº„ÉÄ„ÅØ„ÄÅHunyuan Video VAE„Å®StepVideo VAE„ÅÆ$D_{\\text{info}}$„Å®‰∏ÄËá¥„Åô„Çã„Çà„ÅÜ„Å´Ë®≠Ë®à\n",
    "- ÂÖ∑‰ΩìÁöÑ„Å´„ÅØÂá∫Âäõ„ÉÅ„É£„É≥„Éç„É´Êï∞„ÇíÂ¢ó„ÇÑ„Åô"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82e52bc",
   "metadata": {},
   "source": [
    "DC-AE„ÅÆ„ÉÄ„Ç¶„É≥„Çµ„É≥„Éó„É´„Éñ„É≠„ÉÉ„ÇØ„Å®„Ç¢„ÉÉ„Éó„Çµ„É≥„Éó„É´„Éñ„É≠„ÉÉ„ÇØ„Å´„ÅØ„ÄÅÂãæÈÖç‰ºùÊí≠„ÅÆÂïèÈ°å„Åå„ÅÇ„Çã:\n",
    "\n",
    "- „Éî„ÇØ„Çª„É´„Ç∑„É£„ÉÉ„Éï„É´„Å®„Éî„ÇØ„Çª„É´„Ç¢„É≥„Ç∑„É£„ÉÉ„Éï„É´„Çí‰ΩøÁî®„Åó„ÅüÊÆãÂ∑ÆÊé•Á∂ö„ÇíÂ∞éÂÖ•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4b044c",
   "metadata": {},
   "source": [
    "DC-AE„ÅßÂá∫Âäõ„Åó„ÅüÈ´òÂúßÁ∏Æ„ÅÆÊΩúÂú®Â§âÊï∞„Åß„É¢„Éá„É´„ÇíË®ìÁ∑¥„Åô„Çã„Å®„ÄÅÂãïÁîªÁîüÊàê„ÅÆÂìÅË≥™„Åå‰Ωé„Åè„Å™„Çã:\n",
    "\n",
    "- „ÉÅ„É£„É≥„Éç„É´Êï∞„ÅåÂ§ö„ÅÑ„Å®„ÄÅÊΩúÂú®Á©∫ÈñìÊßãÈÄ†Ôºàlatent space structureÔºâ„ÅÆÊúÄÈÅ©Âåñ„ÅåÈõ£„Åó„Åè„Å™„Çã„Åü„ÇÅ\n",
    "- DC-AEË®ìÁ∑¥Âæå„Å´„ÄÅÁ¨¨3Â±§„ÅÆÊΩúÂú®Ë°®Áèæ„ÇíDINOv2„Å´Âêà„Çè„Åõ„Çã„Åü„ÇÅ„ÅÆËí∏ÁïôÊêçÂ§±„ÇíÈÅ©Áî®„Åó„ÄÅÊúÄÈÅ©Âåñ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8639f8",
   "metadata": {},
   "source": [
    "DC-AE„Çí‰ΩøÁî®„Åó„ÅüÂãïÁîªÁîüÊàê„É¢„Éá„É´„ÅÆË®ìÁ∑¥:\n",
    "\n",
    "1. 2000‰∏á‰ª∂„ÅÆÊúÄÂ§ß33„Éï„É¨„Éº„É†„ÅÆÂãïÁîª„Çí„ÄÅ1.7‰∏á„Çπ„ÉÜ„ÉÉ„Éó„ÅßË®ìÁ∑¥\n",
    "2. 200‰∏á‰ª∂„ÅÆÊúÄÂ§ß128„Éï„É¨„Éº„É†„ÅÆÂãïÁîª„Çí„ÄÅ8000„Çπ„ÉÜ„ÉÉ„Éó„ÅßË®ìÁ∑¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1697a8a5",
   "metadata": {},
   "source": [
    "DC-AE„Çí‰ΩøÁî®„Åó„ÅüÂãïÁîªÁîüÊàê„É¢„Éá„É´„ÅØ„ÄÅHunyuan Video VAE„Çà„Çä„ÇÇ„ÄÅË®ìÁ∑¥„Å®Êé®Ë´ñ„ÅßÈ´ò„ÅÑÂäπÁéáÊÄß:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46688e6",
   "metadata": {},
   "source": [
    "![](image/fig8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e26d444",
   "metadata": {},
   "source": [
    "DC-AE„ÅØ„ÄÅ256„ÉÅ„É£„Éç„É´„Å®128„ÉÅ„É£„Éç„É´„ÅÆ2„Å§„ÇíÊúÄÂàù„Åã„ÇâË®ìÁ∑¥:\n",
    "\n",
    "1. ÂÜçÊßãÊàêÊêçÂ§±$\\mathcal{L}_1$„Å®Áü•Ë¶öÊêçÂ§±$\\mathcal{L}_{\\text{LIPIS}}$„Çí‰ΩøÁî®„Åó„Å¶„ÄÅ25‰∏á„Çπ„ÉÜ„ÉÉ„ÉóË®ìÁ∑¥:\n",
    "    $$\n",
    "    \\mathcal{L} = \\mathcal{L}_{1} + 0.5\\mathcal{L}_{LPIPS}\n",
    "    $$\n",
    "2. ÊïµÂØæÁöÑÊêçÂ§±$\\mathcal{L}_{\\text{adv}}$„Çí‰ΩøÁî®„Åó„Å¶„ÄÅ20‰∏á„Çπ„ÉÜ„ÉÉ„ÉóË®ìÁ∑¥:\n",
    "    $$\n",
    "    \\mathcal{L} = \\mathcal{L_1} + 0.5\\mathcal{L_{LPIPS}} + 0.05\\mathcal{L_{\\text{adv}}}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73af25fc",
   "metadata": {},
   "source": [
    "DC-AE„ÅÆË®ìÁ∑¥Ë®≠ÂÆö:\n",
    "\n",
    "- „É≠„Éº„Ç´„É´„Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫$1$„Åß$8$ÂÄã„ÅÆGPU„Çí‰ΩøÁî®\n",
    "- „Ç¢„Çπ„Éö„ÇØ„ÉàÊØî$1:1$„ÄÅ32„Éï„É¨„Éº„É†„ÄÅ256px„ÅÆÂãïÁîª„Çí‰ΩøÁî®\n",
    "- „Ç™„Éó„ÉÜ„Ç£„Éû„Ç§„Ç∂„ÅØAdamW„Çí‰ΩøÁî®Ôºà$\\beta=(0.9, 0.999)$, $\\epsilon=1\\times 10^{-15}$, Èáç„ÅøÊ∏õË°∞„Å™„ÅóÔºâ\n",
    "- „Ç™„Éº„Éà„Ç®„É≥„Ç≥„Éº„ÉÄ„ÅÆÂ≠¶ÁøíÁéá„ÅØ5e-5\n",
    "- „Éá„Ç£„Çπ„ÇØ„É™„Éü„Éç„Éº„Çø„ÅÆÂ≠¶ÁøíÁéá„ÅØ1e-4\n",
    "- ÂãæÈÖç„Éé„É´„É†„ÇØ„É™„ÉÉ„Éó„ÅØÈñæÂÄ§1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd76e520",
   "metadata": {},
   "source": [
    "## Êù°‰ª∂‰ªò„Åë"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b965deb5",
   "metadata": {},
   "source": [
    "Êù°‰ª∂‰ªò„Åë„ÅØ„ÄÅÊΩúÂú®Â§âÊï∞„Å´ÂØæ„Åó„Å¶Êù°‰ª∂„Éô„ÇØ„Éà„É´„Å®„Çø„Çπ„ÇØ„ÅÆÁ®ÆÈ°û„ÇíÁ§∫„Åô„ÉÅ„É£„É≥„Éç„É´„ÇíÈÄ£Áµê„Åó„Å¶Ë°å„ÅÜ:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2c960a",
   "metadata": {},
   "source": [
    "![](image/fig10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8651a48c",
   "metadata": {},
   "source": [
    "- Â∑¶: ÁîªÂÉè„Åã„ÇâÂãïÁîª„ÇíÁîüÊàê„Åô„Çã„Çø„Çπ„ÇØ\n",
    "- ‰∏≠: ÂãïÁîª„ÇíÂª∂Èï∑„Åô„Çã„Çø„Çπ„ÇØ\n",
    "- Âè≥: ÊúÄÂàù„Å®ÊúÄÂæå„ÅÆÁîªÂÉè„ÅÆÈñì„ÇíË£úÂÆå„Åô„Çã„Çø„Çπ„ÇØ\n",
    "- ÈÄ£ÁµêÂæå„ÅÆ„ÉÅ„É£„É≥„Éç„É´Êï∞„ÅØ$2k + 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4afc63",
   "metadata": {},
   "source": [
    "Ê±éÂåñÊÄßËÉΩ„ÇíÂêë‰∏ä„Åï„Åõ„Çã„Åü„ÇÅ„Å´„ÄÅ„ÉÜ„Ç≠„Çπ„ÉàÊù°‰ª∂„ÅÆ„Éâ„É≠„ÉÉ„Éó„Ç¢„Ç¶„Éà„ÇÑÁîªÂÉèÊù°‰ª∂„ÅÆ„Éâ„É≠„ÉÉ„Éó„Ç¢„Ç¶„Éà„ÇíÂ∞éÂÖ•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c229f9d",
   "metadata": {},
   "source": [
    "Êé®Ë´ñ„Å´„ÅØ„ÄÅClassifier-free GuidanceÔºàCFGÔºâ„ÇíÊé°Áî®:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532dd227",
   "metadata": {},
   "source": [
    "$$\n",
    "v_{t} = v_{\\theta}(x_{t},t,\\emptyset,\\emptyset) + g_{img}\\cdot(v_{\\theta}(x_{t},t,\\emptyset,img)-v_{\\theta}(x_{t},t,\\emptyset,\\emptyset)) + g_{txt}\\cdot(v_{\\theta}(x_{t},t,txt,img)-v_{\\theta}(x_{t},t,\\emptyset,img))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cd2e12",
   "metadata": {},
   "source": [
    "- $g_{\\text{img}}$: ÁîªÂÉèÊù°‰ª∂„ÅÆÂº∑„Åï„ÇíË™øÊï¥„Åô„Çã‰øÇÊï∞\n",
    "- $g_{\\text{txt}}$: „ÉÜ„Ç≠„Çπ„ÉàÊù°‰ª∂„ÅÆÂº∑„Åï„ÇíË™øÊï¥„Åô„Çã‰øÇÊï∞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a04814",
   "metadata": {},
   "source": [
    "ÁîªÂÉè„Ç¨„Ç§„ÉÄ„É≥„Çπ„ÇíÂº∑„Åè„Åô„Çã„Å®„ÄÅÁîüÊàê„Åï„Çå„ÅüÂãïÁîª„Å´„ÉÅ„É©„Å§„ÅçÔºàflickerÔºâ„ÅåÁîü„Åò„Çã:\n",
    "\n",
    "- Guidance Oscillation„ÇíÂ∞éÂÖ•„Åó„Å¶ÂÆâÂÆöÂåñ\n",
    "    - ‰æã„Åà„Å∞„ÄÅ50„Çπ„ÉÜ„ÉÉ„Éó„ÅÆ„Çµ„É≥„Éó„É™„É≥„Ç∞„ÅÆ„ÅÜ„Å°„ÄÅ10„Çπ„ÉÜ„ÉÉ„Éó„Åî„Å®„Å´$g_{\\text{img}}$„Çí$1$„Å´ËêΩ„Å®„Åô"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b8f8b1",
   "metadata": {},
   "source": [
    "I2V„Åß„ÅØÂãïÁöÑ„Å™ÁîªÂÉè„Ç¨„Ç§„ÉÄ„É≥„ÇπÔºàdynamic image guidanceÔºâ„ÇíÊé°Áî®„Åó„ÄÅÂãïÁîª„ÅÆÂæåÂçä„Å´„Å™„Çã„Å´„Å§„Çå„Å¶Êù°‰ª∂‰ªò„Åë„ÇíÂº∑„Åè„Åô„Çã:\n",
    "\n",
    "![](image/fig11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68ce8cd",
   "metadata": {},
   "source": [
    "- Ê®™Ëª∏„ÅØ„Éï„É¨„Éº„É†Êï∞\n",
    "- Á∏¶Ëª∏„ÅØÊé®Ë´ñ„Çπ„ÉÜ„ÉÉ„ÉóÊï∞\n",
    "- ÁîªÂÉè„ÅÆ„Ç¨„Ç§„ÉÄ„É≥„Çπ„Çπ„Ç±„Éº„É´„ÅØ$1$„Åã„Çâ$k$„Åæ„ÅßÁ∑öÂΩ¢„Å´„Çπ„Ç±„Éº„É™„É≥„Ç∞\n",
    "- Êé®Ë´ñ„ÅåÁµÇ„Çè„Çä„Å´„Å™„Çã„Å´„Å§„Çå„Å¶„ÄÅ„Ç¨„Ç§„ÉÄ„É≥„Çπ„Çπ„Ç±„Éº„É™„É≥„Ç∞„Åå$1$„Å´„Å™„Çã„Çà„ÅÜ„Å´Ê∏õË°∞\n",
    "- „Ç¨„Ç§„ÉÄ„É≥„Çπ„ÅÆ„Éá„Éï„Ç©„É´„ÉàÂÄ§„ÅØ$g_{\\text{img}}=3$, $g_{\\text{txt}}=7.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d241562",
   "metadata": {},
   "source": [
    "ÂãïÁîª„ÅÆÂãï„Åç„ÅÆÂº∑„Åï„ÅØ„É¢„Éº„Ç∑„Éß„É≥„Çπ„Ç≥„Ç¢„Çí„Ç≠„É£„Éó„Ç∑„Éß„É≥„Å´ËøΩÂä†„Åô„Çã„Åì„Å®„ÅßÂà∂Âæ°:\n",
    "\n",
    "![](image/fig12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0abfb4",
   "metadata": {},
   "source": [
    "## „Ç∑„Çπ„ÉÜ„É†ÊúÄÈÅ©Âåñ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f02982a",
   "metadata": {},
   "source": [
    "Ë®ìÁ∑¥„ÅØ„ÄÅColossalAI„Çí‰ΩøÁî®:\n",
    "\n",
    "- 141GB„ÅÆ„É°„É¢„É™ÂÆπÈáè„ÇíÊåÅ„Å§H200 GPU„Çí‰ΩøÁî®„Åó„ÄÅ„Éá„Éº„Çø‰∏¶ÂàóÔºàDP, Data ParallelismÔºâ„ÇíÂ∞éÂÖ•\n",
    "- ÈÅ∏ÊäûÁöÑ„Ç¢„ÇØ„ÉÜ„Ç£„Éô„Éº„Ç∑„Éß„É≥„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„ÉÜ„Ç£„É≥„Ç∞ÔºàSelective Activation CheckpointingÔºâ„ÇíÂ∞éÂÖ•\n",
    "    - ‰∏ÄÈÉ®„ÅÆÈ†Ü‰ºùÊí≠„ÅÆË®àÁÆóÁµêÊûú„ÇíÁ†¥Ê£Ñ„Åó„ÄÅÂøÖË¶Å„Å´Âøú„Åò„Å¶ÂÜçË®àÁÆó„Åô„ÇãÊäÄË°ì„Åß„É°„É¢„É™ÂäπÁéáÂåñ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6636c0c3",
   "metadata": {},
   "source": [
    "È´òËß£ÂÉèÂ∫¶„ÅÆÂãïÁîª„ÇíÂäπÁéáÁöÑ„Å´Âá¶ÁêÜ„Åô„Çã„Åü„ÇÅ„Å´„ÄÅË§áÊï∞„ÅÆ‰∏¶ÂàóÂåñÊäÄË°ì„ÇíÊé°Áî®:\n",
    "\n",
    "- AE„Åß„ÅØ„ÄÅ„ÉÜ„É≥„ÇΩ„É´‰∏¶ÂàóÂåñÔºàTP, Tensor ParallelismÔºâ„ÇíÁï≥„ÅøËæº„ÅøÂ±§„Å´ÈÅ©Áî®\n",
    "    - ÂÖ•Âäõ„Åæ„Åü„ÅØÂá∫Âäõ„ÅÆ„ÉÅ„É£„Éç„É´Ê¨°ÂÖÉ„ÅßÈáç„Åø„ÇíÂàÜÂâ≤„Åô„Çã\n",
    "- MMDiT„Åß„ÅØ„ÄÅZero Redundancy OpitimizerÔºàZeroDPÔºâ„Å®„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà‰∏¶ÂàóÔºàCP, Context ParallelismÔºâ„ÇíÈÅ©Áî®\n",
    "    - ZeroDP„ÅØ„ÄÅDeepSpeed„ÅÆZeRO2„Å´„Çà„ÇäÂãæÈÖç„ÇíÂàÜÂâ≤„Åó„ÄÅÈáçË§á„Å™„ÅèGPU„Åå‰øùÊåÅ„Åô„ÇãÊäÄË°ì\n",
    "    - CP„ÅØ„ÄÅÂãïÁîª„Å®„ÉÜ„Ç≠„Çπ„Éà„ÅÆ„Ç∑„Éº„Ç±„É≥„Çπ„ÇíÂàÜÂâ≤„Åó„ÄÅÂêÑGPU„ÅåÁã¨Á´ã„Åó„Å¶„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥Ë®àÁÆó„Åô„ÇãÊäÄË°ì"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f902c3e9",
   "metadata": {},
   "source": [
    "„Ç¢„ÇØ„ÉÜ„Ç£„Éô„Éº„Ç∑„Éß„É≥„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„ÉÜ„Ç£„É≥„Ç∞„ÅØ„ÄÅ„Éñ„É≠„ÉÉ„ÇØ„ÅÆÂÖ•Âäõ„ÅÆ„Åø„Çí‰øùÊåÅ„Åó„ÄÅ„Åù„Çå‰ª•Â§ñ„ÅØÂÜçË®àÁÆó„Åô„Çã:\n",
    "\n",
    "- „Çπ„ÉÜ„Éº„Ç∏1, 2„Åß„ÅØ„ÄÅ„Éá„É•„Ç¢„É´„Éñ„É≠„ÉÉ„ÇØ„ÅÆ8Â±§„Å®„Åô„Åπ„Å¶„ÅÆ„Ç∑„É≥„Ç∞„É´„Éñ„É≠„ÉÉ„ÇØ„Å´ÈÅ©Áî®\n",
    "- „Çπ„ÉÜ„Éº„Ç∏3„Åß„ÅØ„ÄÅ„Åô„Åπ„Å¶„ÅÆ„Éñ„É≠„ÉÉ„ÇØ„ÅßÊúâÂäπÂåñ„Åó„ÄÅCPU„Å∏„ÅÆ„Ç™„Éï„É≠„Éº„Éâ„ÇÇ‰ΩøÁî®\n",
    "    - „Ç™„Éï„É≠„Éº„Éâ„Åß„ÅØ„ÄÅPinned Memory„Å®ÈùûÂêåÊúü„Éá„Éº„ÇøËª¢ÈÄÅ„Çí‰ΩøÁî®„Åó„ÄÅ„Åï„Çâ„Å´ÂäπÁéáÂåñ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6969e364",
   "metadata": {},
   "source": [
    "ÂàÜÊï£Ë®ìÁ∑¥„ÅÆÂ†¥Âêà„ÄÅÁâ©ÁêÜÈöúÂÆ≥„ÅåÁô∫Áîü„Åó„ÇÑ„Åô„ÅÑ„ÅÆ„ÅßËá™ÂãïÂæ©ÊóßÊ©üËÉΩÔºàAuto RecoveryÔºâ„ÇíÂ∞éÂÖ•\n",
    "\n",
    "- InfiniBand„ÅÆÈöúÂÆ≥„ÄÅ„Çπ„Éà„É¨„Éº„Ç∏„Ç∑„Çπ„ÉÜ„É†„ÅÆ„ÇØ„É©„ÉÉ„Ç∑„É•„ÄÅNCCL„Ç®„É©„Éº\n",
    "- Ë®ìÁ∑¥„ÅÆÁä∂ÊÖã„ÇíÁõ£Ë¶ñ„Åó„ÄÅÈÄüÂ∫¶‰Ωé‰∏ã„ÉªÊêçÂ§±„ÅÆÊÄ•ÊøÄ„Å™ÊÇ™Âåñ„Éª„É¨„Çπ„Éù„É≥„Çπ„Åå„Å™„ÅÑ„Åã„ÇíÊ§úË®º\n",
    "- ÂïèÈ°å„ÅåÊ§úÂá∫„Åï„Çå„ÅüÂ†¥Âêà„ÅØ„ÄÅ„Åô„Åπ„Å¶„ÅÆ„Éó„É≠„Çª„Çπ„ÇíÂÅúÊ≠¢„Åó„ÄÅ„Éé„Éº„Éâ„ÇíË®∫Êñ≠\n",
    "- ÂøÖË¶Å„Å´Âøú„Åò„Å¶‰∫àÂÇô„Çí„Éá„Éó„É≠„Ç§„Åó„ÄÅÊúÄÁµÇ„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà„Åã„ÇâËá™ÂãïÁöÑ„Å´Ë®ìÁ∑¥„ÇíÂÜçÈñã„Åô„Çã\n",
    "- GPUÁ®ºÂÉçÁéá99%„Åß„ÄÅ„ÉÄ„Ç¶„É≥„Çø„Ç§„É†„ÇíÊúÄÂ∞èÈôê„Å´Êäë„Åà„Åü"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58002756",
   "metadata": {},
   "source": [
    "CPU„Å®GPUÈñì„ÅÆ„Éá„Éº„ÇøËª¢ÈÄÅ„ÇíÈ´òÈÄüÂåñ„Åô„Çã„Åü„ÇÅ„Å´PyTorch„ÅÆ„Éá„Éº„Çø„É≠„Éº„ÉÄ„Éº„ÇíÊúÄÈÅ©Âåñ:\n",
    "\n",
    "- PyTorch„Éá„Éï„Ç©„É´„Éà„ÅÆPinned Memory„ÅØ„ÄÅCUDA„Ç´„Éº„Éç„É´ÂÆüË°å„Çí„Éñ„É≠„ÉÉ„ÇØ„Åô„ÇãÂèØËÉΩÊÄß„Åå„ÅÇ„Çã\n",
    "- „É°„É¢„É™„Éê„ÉÉ„Éï„Ç°Ôºàpre-allocated pinned memory bufferÔºâ„Çí‰ΩøÁî®„Åó„ÄÅ„Ç™„Éº„Éê„Éº„Éò„ÉÉ„Éâ„ÇíÂâäÊ∏õ\n",
    "- „Éá„Éº„ÇøËª¢ÈÄÅ„Å®Ë®àÁÆó„Çí„Ç™„Éº„Éê„Éº„É©„ÉÉ„Éó„Åï„Åõ„Å¶ÂäπÁéáÂåñ\n",
    "- „Ç∞„É≠„Éº„Éê„É´„Å™GCÔºà„Ç¨„Éô„Éº„Ç∏„Ç≥„É¨„ÇØ„Ç∑„Éß„É≥Ôºâ„ÅØ„ÄÅÂàÜÊï£Ë®ìÁ∑¥„Åß„ÅØ„Éê„Ç∞„ÅåÂ§ö„ÅÑ„Åü„ÇÅÊâãÂãï„Åß„É°„É¢„É™„Åã„Çì„Çä"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55af23f0",
   "metadata": {},
   "source": [
    "„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà„ÅÆ‰øùÂ≠ò„ÅØ„ÄÅ„Éî„É≥Áïô„ÇÅ„Åï„Çå„Åü„É°„É¢„É™„Éê„ÉÉ„Éï„Ç°„Å´ÂØæ„Åó„Å¶Áõ¥Êé•„Ç¢„ÇØ„Çª„Çπ„Åô„Çã„Åì„Å®„ÅßÈ´òÈÄüÂåñ:\n",
    "\n",
    "- „Éá„Ç£„Çπ„ÇØ„Å∏„ÅÆÊõ∏„ÅçËæº„Åø„ÅØ„ÄÅC++„Å´„Çà„ÇãÈùûÂêåÊúü„Éá„Ç£„Çπ„ÇØÊõ∏„ÅçËæº„Åø„Å´„Çà„ÇäË®ìÁ∑¥„Éó„É≠„Çª„Çπ„Çí„Éñ„É≠„ÉÉ„ÇØ„Åõ„Åö„Å´ÂäπÁéáÂåñ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629e8584",
   "metadata": {},
   "source": [
    "## ÂÆüË£Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eaff96bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/open-sora/Open-Sora\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    !git clone https://github.com/hpcaitech/Open-Sora.git\n",
    "    WORK_DIR = \"/content/Open-Sora\"\n",
    "    %cd $WORK_DIR\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    WORK_DIR = \"/workspaces/open-sora/Open-Sora\"\n",
    "    %cd $WORK_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "298268ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üü© Python 3.12.11\n",
      "üü© Thu Dec 25 15:36:17 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off |   00000000:01:00.0 Off |                  Off |\n",
      "|  0%   31C    P8              8W /  450W |     733MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import platform\n",
    "import subprocess\n",
    "\n",
    "DEBUG_LOG_PATH = os.path.join(WORK_DIR, \"debug.log\")\n",
    "\n",
    "if os.path.exists(DEBUG_LOG_PATH):\n",
    "    os.remove(DEBUG_LOG_PATH)\n",
    "\n",
    "def custom_format(record):\n",
    "    match record.levelno:\n",
    "        case logging.DEBUG:\n",
    "            level = \"üü¶\"\n",
    "        case logging.INFO:\n",
    "            level = \"üü©\"\n",
    "        case logging.WARNING:\n",
    "            level = \"üü®\"\n",
    "        case logging.ERROR:\n",
    "            level = \"üü•\"\n",
    "        case logging.CRITICAL:\n",
    "            level = \"üõë\"\n",
    "    return f\"{level} {record.getMessage()}\"\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "for handler in logger.handlers:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "formatter = logging.Formatter()\n",
    "formatter.format = custom_format\n",
    "\n",
    "file_handler = logging.FileHandler(DEBUG_LOG_PATH)\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "PYTHON_VERSION = platform.python_version()\n",
    "logger.info(f\"Python {PYTHON_VERSION}\")\n",
    "\n",
    "NVIDIA_SMI = subprocess.run(\"nvidia-smi\", capture_output=True, text=True).stdout\n",
    "logger.info(f\"{NVIDIA_SMI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a2bab8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    %pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "    %pip install \\\n",
    "        accelerate \\\n",
    "        av==13.1.0 \\\n",
    "        colossalai \\\n",
    "        ftfy \\\n",
    "        liger-kernel \\\n",
    "        omegaconf \\\n",
    "        mmengine \\\n",
    "        openai \\\n",
    "        pandas \\\n",
    "        pandarallel \\\n",
    "        pyarrow \\\n",
    "        tensorboard \\\n",
    "        wandb \\\n",
    "        --extra-index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "    %pip install flash-attn --no-build-isolation\n",
    "\n",
    "    %pip install -e . --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4318c538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from einops import rearrange\n",
    "from flash_attn import flash_attn_func as flash_attn_func_v2\n",
    "from functools import partial\n",
    "from huggingface_hub import PyTorchModelHubMixin\n",
    "from inspect import signature\n",
    "from liger_kernel.ops.rms_norm import LigerRMSNormFunction\n",
    "from liger_kernel.ops.rope import LigerRopeFunction\n",
    "from omegaconf import MISSING, OmegaConf\n",
    "from torch import Tensor, nn\n",
    "from torch.nn.modules.batchnorm import _BatchNorm\n",
    "from typing import Any, Callable, Optional, Union, Tuple\n",
    "import diffusers\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "from mmengine.config import Config\n",
    "import ast\n",
    "\n",
    "try:\n",
    "    from flash_attn_interface import flash_attn_func as flash_attn_func_v3\n",
    "    SUPPORT_FA3 = True\n",
    "except:\n",
    "    SUPPORT_FA3 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8a3051",
   "metadata": {},
   "source": [
    "## „Éá„Éº„Çø„Çª„ÉÉ„Éà"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a360a8b",
   "metadata": {},
   "source": [
    "ÂãïÁîª„Éï„Ç°„Ç§„É´„ÅØ[hcpai-tech/open-sora-pexels-45k][1]„Å´„ÅÇ„Çã\n",
    "\n",
    "[1]: https://modelscope.cn/datasets/hpcai-tech/open-sora-pexels-45k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e6ac0fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 269.53GB„ÅÆÂãïÁîª„Éá„Éº„Çø„Çª„ÉÉ„Éà„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ\n",
    "# https://modelscope.cn/datasets/hpcai-tech/open-sora-pexels-45k\n",
    "\n",
    "if False:\n",
    "    !apt update && apt install git-lfs\n",
    "    !git-lfs install\n",
    "    !git clone \"https://www.modelscope.cn/datasets/hpcai-tech/open-sora-pexels-45k.git\"\n",
    "    !cd open-sora-pexels-45k && \\\n",
    "        cat tar/pexels_45k.tar.* > pexels_45k.tar && \\\n",
    "        mkdir ../datasets && \\\n",
    "        mv pexels_45k ../datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c0a57f",
   "metadata": {},
   "source": [
    "Èáç„ÅÑ„ÅÆ„ÅßÊúÄÂ∞èÈôê„ÅÆÁµ±Ë®à„Éá„Éº„Çø„ÅÆ„Åø„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1fa393d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‚Äòpexels_45k_necessary.csv‚Äô already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -nc https://modelscope.cn/datasets/hpcai-tech/open-sora-pexels-45k/resolve/master/pexels_45k_necessary.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "216fe272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45817\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>text</th>\n",
       "      <th>num_frames</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>aspect_ratio</th>\n",
       "      <th>resolution</th>\n",
       "      <th>fps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./datasets/pexels_45k/popular_3/853857_scene-0...</td>\n",
       "      <td>an aerial view of a large, ancient stone struc...</td>\n",
       "      <td>330.0</td>\n",
       "      <td>1036.0</td>\n",
       "      <td>1102.0</td>\n",
       "      <td>0.940109</td>\n",
       "      <td>1141672.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./datasets/pexels_45k/popular_3/5751131_scene-...</td>\n",
       "      <td>a nighttime scene on a city street, where the ...</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1038.0</td>\n",
       "      <td>1520.0</td>\n",
       "      <td>0.682895</td>\n",
       "      <td>1577760.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./datasets/pexels_45k/popular_3/6828868_scene-...</td>\n",
       "      <td>a shot of two individuals seated at a wooden t...</td>\n",
       "      <td>270.0</td>\n",
       "      <td>1026.0</td>\n",
       "      <td>886.0</td>\n",
       "      <td>1.158014</td>\n",
       "      <td>909036.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./datasets/pexels_45k/popular_3/20011847_scene...</td>\n",
       "      <td>a series of videos depicting a harbor scene wi...</td>\n",
       "      <td>351.0</td>\n",
       "      <td>1048.0</td>\n",
       "      <td>1746.0</td>\n",
       "      <td>0.600229</td>\n",
       "      <td>1829808.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./datasets/pexels_45k/popular_3/5183212_scene-...</td>\n",
       "      <td>a scene of two individuals standing on a beach...</td>\n",
       "      <td>390.0</td>\n",
       "      <td>1048.0</td>\n",
       "      <td>992.0</td>\n",
       "      <td>1.056452</td>\n",
       "      <td>1039616.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  \\\n",
       "0  ./datasets/pexels_45k/popular_3/853857_scene-0...   \n",
       "1  ./datasets/pexels_45k/popular_3/5751131_scene-...   \n",
       "2  ./datasets/pexels_45k/popular_3/6828868_scene-...   \n",
       "3  ./datasets/pexels_45k/popular_3/20011847_scene...   \n",
       "4  ./datasets/pexels_45k/popular_3/5183212_scene-...   \n",
       "\n",
       "                                                text  num_frames  height  \\\n",
       "0  an aerial view of a large, ancient stone struc...       330.0  1036.0   \n",
       "1  a nighttime scene on a city street, where the ...        60.0  1038.0   \n",
       "2  a shot of two individuals seated at a wooden t...       270.0  1026.0   \n",
       "3  a series of videos depicting a harbor scene wi...       351.0  1048.0   \n",
       "4  a scene of two individuals standing on a beach...       390.0  1048.0   \n",
       "\n",
       "    width  aspect_ratio  resolution   fps  \n",
       "0  1102.0      0.940109   1141672.0  30.0  \n",
       "1  1520.0      0.682895   1577760.0  30.0  \n",
       "2   886.0      1.158014    909036.0  30.0  \n",
       "3  1746.0      0.600229   1829808.0  30.0  \n",
       "4   992.0      1.056452   1039616.0  30.0  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ÂãïÁîª„ÇíË©ï‰æ°Âæå„ÄÅ„Éï„Ç£„É´„Çø„É™„É≥„Ç∞„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆCSV„ÇíË™≠„ÅøËæº„ÇÄ\n",
    "pexels_45k_necessary_df = pd.read_csv(\"pexels_45k_necessary.csv\")\n",
    "print(len(pexels_45k_necessary_df)) # 45,817‰ª∂\n",
    "pexels_45k_necessary_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b9192662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>num_frames</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>aspect_ratio</th>\n",
       "      <th>fps</th>\n",
       "      <th>resolution</th>\n",
       "      <th>score_aes</th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>...</th>\n",
       "      <th>duration</th>\n",
       "      <th>bitrate</th>\n",
       "      <th>bpp</th>\n",
       "      <th>profile</th>\n",
       "      <th>filesize</th>\n",
       "      <th>score_blur_all</th>\n",
       "      <th>ocr_bbox_ar_max</th>\n",
       "      <th>score_cj</th>\n",
       "      <th>score_vmafmotion</th>\n",
       "      <th>score_naive_motion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9244</th>\n",
       "      <td>./datasets/pexels_5/popular_1/7989384_scene-5_...</td>\n",
       "      <td>90.0</td>\n",
       "      <td>466.0</td>\n",
       "      <td>402.0</td>\n",
       "      <td>1.159204</td>\n",
       "      <td>30.0</td>\n",
       "      <td>187332.0</td>\n",
       "      <td>5.23</td>\n",
       "      <td>a person wearing a black leather jacket with v...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.00</td>\n",
       "      <td>233685.0</td>\n",
       "      <td>0.042</td>\n",
       "      <td>High</td>\n",
       "      <td>0.084</td>\n",
       "      <td>[152.14, 122.27, 109.96, 106.05, 78.34]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333</td>\n",
       "      <td>1.719</td>\n",
       "      <td>-0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8695</th>\n",
       "      <td>./datasets/pexels_5/popular_1/7989384_scene-0_...</td>\n",
       "      <td>90.0</td>\n",
       "      <td>674.0</td>\n",
       "      <td>446.0</td>\n",
       "      <td>1.511211</td>\n",
       "      <td>30.0</td>\n",
       "      <td>300604.0</td>\n",
       "      <td>5.01</td>\n",
       "      <td>a person wearing a black leather jacket with v...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.00</td>\n",
       "      <td>371800.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>High</td>\n",
       "      <td>0.133</td>\n",
       "      <td>[138.07, 166.78, 179.51, 206.05, 162.95]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333</td>\n",
       "      <td>2.276</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>./datasets/pexels_5/popular_3/7569560_scene-5_...</td>\n",
       "      <td>60.0</td>\n",
       "      <td>634.0</td>\n",
       "      <td>724.0</td>\n",
       "      <td>0.875691</td>\n",
       "      <td>30.0</td>\n",
       "      <td>459016.0</td>\n",
       "      <td>5.05</td>\n",
       "      <td>a sequence of videos showing two individuals i...</td>\n",
       "      <td>7569560_scene-5</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>700244.0</td>\n",
       "      <td>0.051</td>\n",
       "      <td>High</td>\n",
       "      <td>0.167</td>\n",
       "      <td>[51.5, 59.83, 82.67, 61.74, 35.84]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>2.620</td>\n",
       "      <td>0.409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15641</th>\n",
       "      <td>./datasets/pexels_5/popular_2/7984166_scene-11...</td>\n",
       "      <td>90.0</td>\n",
       "      <td>624.0</td>\n",
       "      <td>612.0</td>\n",
       "      <td>1.019608</td>\n",
       "      <td>30.0</td>\n",
       "      <td>381888.0</td>\n",
       "      <td>5.22</td>\n",
       "      <td>a sequence of videos showing two individuals i...</td>\n",
       "      <td>7984166_scene-11</td>\n",
       "      <td>...</td>\n",
       "      <td>3.00</td>\n",
       "      <td>551834.0</td>\n",
       "      <td>0.048</td>\n",
       "      <td>High</td>\n",
       "      <td>0.197</td>\n",
       "      <td>[163.05, 157.17, 150.84, 141.88, 126.16]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333</td>\n",
       "      <td>1.136</td>\n",
       "      <td>0.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12389</th>\n",
       "      <td>./datasets/pexels_5/popular_1/7390668_scene-0_...</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1048.0</td>\n",
       "      <td>586.0</td>\n",
       "      <td>1.788396</td>\n",
       "      <td>30.0</td>\n",
       "      <td>614128.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>two individuals, one with dark hair and the ot...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.77</td>\n",
       "      <td>934138.0</td>\n",
       "      <td>0.051</td>\n",
       "      <td>High</td>\n",
       "      <td>0.197</td>\n",
       "      <td>[91.8, 96.93, 93.71, 88.11, 86.61]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.566</td>\n",
       "      <td>2.312</td>\n",
       "      <td>0.255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    path  num_frames  height  \\\n",
       "9244   ./datasets/pexels_5/popular_1/7989384_scene-5_...        90.0   466.0   \n",
       "8695   ./datasets/pexels_5/popular_1/7989384_scene-0_...        90.0   674.0   \n",
       "2495   ./datasets/pexels_5/popular_3/7569560_scene-5_...        60.0   634.0   \n",
       "15641  ./datasets/pexels_5/popular_2/7984166_scene-11...        90.0   624.0   \n",
       "12389  ./datasets/pexels_5/popular_1/7390668_scene-0_...        53.0  1048.0   \n",
       "\n",
       "       width  aspect_ratio   fps  resolution  score_aes  \\\n",
       "9244   402.0      1.159204  30.0    187332.0       5.23   \n",
       "8695   446.0      1.511211  30.0    300604.0       5.01   \n",
       "2495   724.0      0.875691  30.0    459016.0       5.05   \n",
       "15641  612.0      1.019608  30.0    381888.0       5.22   \n",
       "12389  586.0      1.788396  30.0    614128.0       5.00   \n",
       "\n",
       "                                                    text                id  \\\n",
       "9244   a person wearing a black leather jacket with v...               NaN   \n",
       "8695   a person wearing a black leather jacket with v...               NaN   \n",
       "2495   a sequence of videos showing two individuals i...   7569560_scene-5   \n",
       "15641  a sequence of videos showing two individuals i...  7984166_scene-11   \n",
       "12389  two individuals, one with dark hair and the ot...               NaN   \n",
       "\n",
       "       ... duration   bitrate    bpp  profile  filesize  \\\n",
       "9244   ...     3.00  233685.0  0.042     High     0.084   \n",
       "8695   ...     3.00  371800.0  0.041     High     0.133   \n",
       "2495   ...     2.00  700244.0  0.051     High     0.167   \n",
       "15641  ...     3.00  551834.0  0.048     High     0.197   \n",
       "12389  ...     1.77  934138.0  0.051     High     0.197   \n",
       "\n",
       "                                 score_blur_all ocr_bbox_ar_max  score_cj  \\\n",
       "9244    [152.14, 122.27, 109.96, 106.05, 78.34]             0.0     0.333   \n",
       "8695   [138.07, 166.78, 179.51, 206.05, 162.95]             0.0     0.333   \n",
       "2495         [51.5, 59.83, 82.67, 61.74, 35.84]             0.0     0.500   \n",
       "15641  [163.05, 157.17, 150.84, 141.88, 126.16]             0.0     0.333   \n",
       "12389        [91.8, 96.93, 93.71, 88.11, 86.61]             0.0     0.566   \n",
       "\n",
       "      score_vmafmotion  score_naive_motion  \n",
       "9244             1.719              -0.000  \n",
       "8695             2.276               0.000  \n",
       "2495             2.620               0.409  \n",
       "15641            1.136               0.526  \n",
       "12389            2.312               0.255  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pexels_45k_score_df = pexels_45k_score_df.sort_values(by=[\"filesize\"])\n",
    "pexels_5 = pexels_45k_score_df.head(5).copy()\n",
    "pexels_5['path'] = pexels_5['path'].str.replace('datasets/pexels_45k/', 'datasets/pexels_5/', regex=False)\n",
    "pexels_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a8652538",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_DIR = os.path.join(WORK_DIR, \"datasets\")\n",
    "PEXELS_DATASET_DIR = os.path.join(DATASETS_DIR, \"pexels_5\")\n",
    "os.makedirs(PEXELS_DATASET_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cd1e20a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>num_frames</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>aspect_ratio</th>\n",
       "      <th>fps</th>\n",
       "      <th>resolution</th>\n",
       "      <th>score_aes</th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>...</th>\n",
       "      <th>duration</th>\n",
       "      <th>bitrate</th>\n",
       "      <th>bpp</th>\n",
       "      <th>profile</th>\n",
       "      <th>filesize</th>\n",
       "      <th>score_blur_all</th>\n",
       "      <th>ocr_bbox_ar_max</th>\n",
       "      <th>score_cj</th>\n",
       "      <th>score_vmafmotion</th>\n",
       "      <th>score_naive_motion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/workspaces/open-sora/Open-Sora/datasets/pexel...</td>\n",
       "      <td>90.0</td>\n",
       "      <td>466.0</td>\n",
       "      <td>402.0</td>\n",
       "      <td>1.159204</td>\n",
       "      <td>30.0</td>\n",
       "      <td>187332.0</td>\n",
       "      <td>5.23</td>\n",
       "      <td>a person wearing a black leather jacket with v...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.00</td>\n",
       "      <td>233685.0</td>\n",
       "      <td>0.042</td>\n",
       "      <td>High</td>\n",
       "      <td>0.084</td>\n",
       "      <td>[152.14, 122.27, 109.96, 106.05, 78.34]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333</td>\n",
       "      <td>1.719</td>\n",
       "      <td>-0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/workspaces/open-sora/Open-Sora/datasets/pexel...</td>\n",
       "      <td>90.0</td>\n",
       "      <td>674.0</td>\n",
       "      <td>446.0</td>\n",
       "      <td>1.511211</td>\n",
       "      <td>30.0</td>\n",
       "      <td>300604.0</td>\n",
       "      <td>5.01</td>\n",
       "      <td>a person wearing a black leather jacket with v...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.00</td>\n",
       "      <td>371800.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>High</td>\n",
       "      <td>0.133</td>\n",
       "      <td>[138.07, 166.78, 179.51, 206.05, 162.95]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333</td>\n",
       "      <td>2.276</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/workspaces/open-sora/Open-Sora/datasets/pexel...</td>\n",
       "      <td>60.0</td>\n",
       "      <td>634.0</td>\n",
       "      <td>724.0</td>\n",
       "      <td>0.875691</td>\n",
       "      <td>30.0</td>\n",
       "      <td>459016.0</td>\n",
       "      <td>5.05</td>\n",
       "      <td>a sequence of videos showing two individuals i...</td>\n",
       "      <td>7569560_scene-5</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>700244.0</td>\n",
       "      <td>0.051</td>\n",
       "      <td>High</td>\n",
       "      <td>0.167</td>\n",
       "      <td>[51.5, 59.83, 82.67, 61.74, 35.84]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>2.620</td>\n",
       "      <td>0.409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/workspaces/open-sora/Open-Sora/datasets/pexel...</td>\n",
       "      <td>90.0</td>\n",
       "      <td>624.0</td>\n",
       "      <td>612.0</td>\n",
       "      <td>1.019608</td>\n",
       "      <td>30.0</td>\n",
       "      <td>381888.0</td>\n",
       "      <td>5.22</td>\n",
       "      <td>a sequence of videos showing two individuals i...</td>\n",
       "      <td>7984166_scene-11</td>\n",
       "      <td>...</td>\n",
       "      <td>3.00</td>\n",
       "      <td>551834.0</td>\n",
       "      <td>0.048</td>\n",
       "      <td>High</td>\n",
       "      <td>0.197</td>\n",
       "      <td>[163.05, 157.17, 150.84, 141.88, 126.16]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333</td>\n",
       "      <td>1.136</td>\n",
       "      <td>0.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/workspaces/open-sora/Open-Sora/datasets/pexel...</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1048.0</td>\n",
       "      <td>586.0</td>\n",
       "      <td>1.788396</td>\n",
       "      <td>30.0</td>\n",
       "      <td>614128.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>two individuals, one with dark hair and the ot...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.77</td>\n",
       "      <td>934138.0</td>\n",
       "      <td>0.051</td>\n",
       "      <td>High</td>\n",
       "      <td>0.197</td>\n",
       "      <td>[91.8, 96.93, 93.71, 88.11, 86.61]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.566</td>\n",
       "      <td>2.312</td>\n",
       "      <td>0.255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  num_frames  height  \\\n",
       "0  /workspaces/open-sora/Open-Sora/datasets/pexel...        90.0   466.0   \n",
       "1  /workspaces/open-sora/Open-Sora/datasets/pexel...        90.0   674.0   \n",
       "2  /workspaces/open-sora/Open-Sora/datasets/pexel...        60.0   634.0   \n",
       "3  /workspaces/open-sora/Open-Sora/datasets/pexel...        90.0   624.0   \n",
       "4  /workspaces/open-sora/Open-Sora/datasets/pexel...        53.0  1048.0   \n",
       "\n",
       "   width  aspect_ratio   fps  resolution  score_aes  \\\n",
       "0  402.0      1.159204  30.0    187332.0       5.23   \n",
       "1  446.0      1.511211  30.0    300604.0       5.01   \n",
       "2  724.0      0.875691  30.0    459016.0       5.05   \n",
       "3  612.0      1.019608  30.0    381888.0       5.22   \n",
       "4  586.0      1.788396  30.0    614128.0       5.00   \n",
       "\n",
       "                                                text                id  ...  \\\n",
       "0  a person wearing a black leather jacket with v...               NaN  ...   \n",
       "1  a person wearing a black leather jacket with v...               NaN  ...   \n",
       "2  a sequence of videos showing two individuals i...   7569560_scene-5  ...   \n",
       "3  a sequence of videos showing two individuals i...  7984166_scene-11  ...   \n",
       "4  two individuals, one with dark hair and the ot...               NaN  ...   \n",
       "\n",
       "  duration   bitrate    bpp  profile  filesize  \\\n",
       "0     3.00  233685.0  0.042     High     0.084   \n",
       "1     3.00  371800.0  0.041     High     0.133   \n",
       "2     2.00  700244.0  0.051     High     0.167   \n",
       "3     3.00  551834.0  0.048     High     0.197   \n",
       "4     1.77  934138.0  0.051     High     0.197   \n",
       "\n",
       "                             score_blur_all ocr_bbox_ar_max  score_cj  \\\n",
       "0   [152.14, 122.27, 109.96, 106.05, 78.34]             0.0     0.333   \n",
       "1  [138.07, 166.78, 179.51, 206.05, 162.95]             0.0     0.333   \n",
       "2        [51.5, 59.83, 82.67, 61.74, 35.84]             0.0     0.500   \n",
       "3  [163.05, 157.17, 150.84, 141.88, 126.16]             0.0     0.333   \n",
       "4        [91.8, 96.93, 93.71, 88.11, 86.61]             0.0     0.566   \n",
       "\n",
       "  score_vmafmotion  score_naive_motion  \n",
       "0            1.719              -0.000  \n",
       "1            2.276               0.000  \n",
       "2            2.620               0.409  \n",
       "3            1.136               0.526  \n",
       "4            2.312               0.255  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "PEXELS_CSV_PATH = os.path.join(WORK_DIR, \"pexels_5.csv\")\n",
    "\n",
    "if not os.path.exists(PEXELS_CSV_PATH):\n",
    "    for path in pexels_5[\"path\"]:\n",
    "        match = re.search(r\"/(\\d+)_\", path)\n",
    "        video_id = match.group(1)\n",
    "        video_url = f\"https://www.pexels.com/download/video/{video_id}\"\n",
    "        video_path = os.path.join(PEXELS_DATASET_DIR, f\"{video_id}.mp4\")\n",
    "        !wget -nc -O $video_path $video_url\n",
    "        pexels_5[\"path\"] = pexels_5[\"path\"].replace(path, video_path)\n",
    "\n",
    "    pexels_5.to_csv(os.path.join(WORK_DIR, \"pexels_5.csv\"), index=False)\n",
    "else:\n",
    "    pexels_5 = pd.read_csv(PEXELS_CSV_PATH)\n",
    "\n",
    "pexels_5.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd75697f",
   "metadata": {},
   "source": [
    "## CLI„Éë„Éº„Çµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45c0e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(args) -> tuple[str, argparse.Namespace]:\n",
    "    \"\"\"\n",
    "    This function parses the command line arguments.\n",
    "\n",
    "    Returns:\n",
    "        tuple[str, argparse.Namespace]: The path to the configuration file and the command line arguments.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"config\", type=str, help=\"model config file path\")\n",
    "    args, unknown_args = parser.parse_known_args(args)\n",
    "    return args.config, unknown_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4e518b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config(config_path: str) -> Config:\n",
    "    \"\"\"\n",
    "    This function reads the configuration file.\n",
    "\n",
    "    Args:\n",
    "        config_path (str): The path to the configuration file.\n",
    "\n",
    "    Returns:\n",
    "        Config: The configuration object.\n",
    "    \"\"\"\n",
    "    cfg = Config.fromfile(config_path)\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80074784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_convert(value: str) -> int | float | bool | list | dict | None:\n",
    "    \"\"\"\n",
    "    Automatically convert a string to the appropriate Python data type,\n",
    "    including int, float, bool, list, dict, etc.\n",
    "\n",
    "    Args:\n",
    "        value (str): The string to convert.\n",
    "\n",
    "    Returns:\n",
    "        int, float, bool, list |  dict: The converted value.\n",
    "    \"\"\"\n",
    "    # Handle empty string\n",
    "    if value == \"\":\n",
    "        return value\n",
    "\n",
    "    # Handle None\n",
    "    if value.lower() == \"none\":\n",
    "        return None\n",
    "\n",
    "    # Handle boolean values\n",
    "    lower_value = value.lower()\n",
    "    if lower_value == \"true\":\n",
    "        return True\n",
    "    elif lower_value == \"false\":\n",
    "        return False\n",
    "\n",
    "    # Try to convert the string to an integer or float\n",
    "    try:\n",
    "        # Try converting to an integer\n",
    "        return int(value)\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        # Try converting to a float\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    # Try to convert the string to a list, dict, tuple, etc.\n",
    "    try:\n",
    "        return ast.literal_eval(value)\n",
    "    except (ValueError, SyntaxError):\n",
    "        pass\n",
    "\n",
    "    # If all attempts fail, return the original string\n",
    "    return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7b6093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_args(cfg: Config, args: argparse.Namespace) -> Config:\n",
    "    \"\"\"\n",
    "    This function merges the configuration file and command line arguments.\n",
    "\n",
    "    Args:\n",
    "        cfg (Config): The configuration object.\n",
    "        args (argparse.Namespace): The command line arguments.\n",
    "\n",
    "    Returns:\n",
    "        Config: The configuration object.\n",
    "    \"\"\"\n",
    "    for k, v in zip(args[::2], args[1::2]):\n",
    "        assert k.startswith(\"--\"), f\"Invalid argument: {k}\"\n",
    "        k = k[2:].replace(\"-\", \"_\")\n",
    "        k_split = k.split(\".\")\n",
    "        target = cfg\n",
    "        for key in k_split[:-1]:\n",
    "            assert key in cfg, f\"Key {key} not found in config\"\n",
    "            target = target[key]\n",
    "        if v.lower() == \"none\":\n",
    "            v = None\n",
    "        elif k in target:\n",
    "            v_type = type(target[k])\n",
    "            if v_type == bool:\n",
    "                v = auto_convert(v)\n",
    "            else:\n",
    "                v = type(target[k])(v)\n",
    "        else:\n",
    "            v = auto_convert(v)\n",
    "        target[k_split[-1]] = v\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f075d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_configs(args) -> Config:\n",
    "    \"\"\"\n",
    "    This function parses the configuration file and command line arguments.\n",
    "\n",
    "    Returns:\n",
    "        Config: The configuration object.\n",
    "    \"\"\"\n",
    "    config, args = parse_args(args)\n",
    "    cfg = read_config(config)\n",
    "    cfg = merge_args(cfg, args)\n",
    "    cfg.config_path = config\n",
    "\n",
    "    # hard-coded for spatial compression\n",
    "    if cfg.get(\"ae_spatial_compression\", None) is not None:\n",
    "        os.environ[\"AE_SPATIAL_COMPRESSION\"] = str(cfg.ae_spatial_compression)\n",
    "\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f46383",
   "metadata": {},
   "source": [
    "## DC-AE„ÅÆË®ìÁ∑¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfe17fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DC-AEË®ìÁ∑¥\n",
    "\n",
    "# https://github.com/hpcaitech/Open-Sora/blob/main/docs/hcae.md\n",
    "\n",
    "# torchrun --nproc_per_node 8 scripts/vae/train.py configs/vae/train/video_dc_ae.py\n",
    "\n",
    "dcae_cfg = parse_configs([\n",
    "    \"configs/vae/train/video_dc_ae.py\",\n",
    "])\n",
    "dcae_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1842d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import random\n",
    "import subprocess\n",
    "import warnings\n",
    "from contextlib import nullcontext\n",
    "from copy import deepcopy\n",
    "from pprint import pformat\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "gc.disable()\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from colossalai.booster import Booster\n",
    "from colossalai.utils import set_seed\n",
    "from torch.profiler import ProfilerActivity, profile, schedule\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "from opensora.acceleration.checkpoint import set_grad_checkpoint\n",
    "from opensora.acceleration.parallel_states import get_data_parallel_group\n",
    "from opensora.datasets.dataloader import prepare_dataloader\n",
    "from opensora.datasets.pin_memory_cache import PinMemoryCache\n",
    "from opensora.models.vae.losses import DiscriminatorLoss, GeneratorLoss, VAELoss\n",
    "from opensora.registry import DATASETS, MODELS, build_module\n",
    "from opensora.utils.ckpt import CheckpointIO, model_sharding, record_model_param_shape, rm_checkpoints\n",
    "# from opensora.utils.config import config_to_name, create_experiment_workspace, parse_configs\n",
    "from opensora.utils.config import config_to_name, create_experiment_workspace\n",
    "from opensora.utils.logger import create_logger\n",
    "from opensora.utils.misc import (\n",
    "    Timer,\n",
    "    all_reduce_sum,\n",
    "    create_tensorboard_writer,\n",
    "    is_log_process,\n",
    "    log_model_params,\n",
    "    to_torch_dtype,\n",
    ")\n",
    "from opensora.utils.optimizer import create_lr_scheduler, create_optimizer\n",
    "from opensora.utils.train import create_colossalai_plugin, set_lr, set_warmup_steps, setup_device, update_ema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9809ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "WAIT = 1\n",
    "WARMUP = 10\n",
    "ACTIVE = 20\n",
    "\n",
    "my_schedule = schedule(\n",
    "    wait=WAIT,  # number of warmup steps\n",
    "    warmup=WARMUP,  # number of warmup steps with profiling\n",
    "    active=ACTIVE,  # number of active steps with profiling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc13ff7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 1. configs & runtime variables\n",
    "# ======================================================\n",
    "# == parse configs ==\n",
    "# cfg = parse_configs()\n",
    "cfg = parse_configs([\n",
    "    \"configs/vae/train/video_dc_ae.py\",\n",
    "])\n",
    "\n",
    "cfg.bucket_config = {\n",
    "    \"256px_ar1:1\": {\n",
    "        8: (1.0, 1),\n",
    "        32: (0.0, 1),\n",
    "    }\n",
    "}\n",
    "cfg.epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250b4dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == get dtype & device ==\n",
    "dtype = to_torch_dtype(cfg.get(\"dtype\", \"bf16\"))\n",
    "\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '12345'\n",
    "\n",
    "device, coordinator = setup_device()\n",
    "checkpoint_io = CheckpointIO()\n",
    "set_seed(cfg.get(\"seed\", 1024))\n",
    "PinMemoryCache.force_dtype = dtype\n",
    "pin_memory_cache_pre_alloc_numels = cfg.get(\"pin_memory_cache_pre_alloc_numels\", None)\n",
    "PinMemoryCache.pre_alloc_numels = pin_memory_cache_pre_alloc_numels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d544b486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == init ColossalAI booster ==\n",
    "plugin_type = cfg.get(\"plugin\", \"zero2\")\n",
    "plugin_config = cfg.get(\"plugin_config\", {})\n",
    "plugin = (\n",
    "    create_colossalai_plugin(\n",
    "        plugin=plugin_type,\n",
    "        dtype=cfg.get(\"dtype\", \"bf16\"),\n",
    "        grad_clip=cfg.get(\"grad_clip\", 0),\n",
    "        **plugin_config,\n",
    "    )\n",
    "    if plugin_type != \"none\"\n",
    "    else None\n",
    ")\n",
    "booster = Booster(plugin=plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49473b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == init exp_dir ==\n",
    "exp_name, exp_dir = create_experiment_workspace(\n",
    "    cfg.get(\"outputs\", \"./outputs\"),\n",
    "    model_name=config_to_name(cfg),\n",
    "    config=cfg.to_dict(),\n",
    ")\n",
    "if is_log_process(plugin_type, plugin_config):\n",
    "    print(f\"changing {exp_dir} to share\")\n",
    "    os.system(f\"chgrp -R share {exp_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab51d1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == init logger, tensorboard & wandb ==\n",
    "logger = create_logger(exp_dir)\n",
    "logger.info(\"Training configuration:\\n %s\", pformat(cfg.to_dict()))\n",
    "tb_writer = None\n",
    "if coordinator.is_master():\n",
    "    tb_writer = create_tensorboard_writer(exp_dir)\n",
    "    if cfg.get(\"wandb\", False):\n",
    "        wandb.init(\n",
    "            project=cfg.get(\"wandb_project\", \"Open-Sora\"),\n",
    "            name=cfg.get(\"wandb_expr_name\", exp_name),\n",
    "            config=cfg.to_dict(),\n",
    "            dir=exp_dir,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccabf78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 2. build dataset and dataloader\n",
    "# ======================================================\n",
    "logger.info(f\"Building dataset... {cfg.dataset=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4743fc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: pexels_45k_necessary.csv„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ\n",
    "\n",
    "assert os.path.join(WORK_DIR, cfg.dataset.data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13016093",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe57d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == build dataset ==\n",
    "\n",
    "# VideoTextDataset„ÇíÂÆü‰ΩìÂåñ\n",
    "dataset = build_module(cfg.dataset, DATASETS)\n",
    "logger.info(\"Dataset contains %s samples.\", len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07538605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == build dataloader ==\n",
    "cache_pin_memory = pin_memory_cache_pre_alloc_numels is not None\n",
    "dataloader_args = dict(\n",
    "    dataset=dataset,\n",
    "    batch_size=cfg.get(\"batch_size\", None),\n",
    "    num_workers=cfg.get(\"num_workers\", 4),\n",
    "    seed=cfg.get(\"seed\", 1024),\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    process_group=get_data_parallel_group(),\n",
    "    prefetch_factor=cfg.get(\"prefetch_factor\", None),\n",
    "    cache_pin_memory=cache_pin_memory,\n",
    ")\n",
    "dataloader, sampler = prepare_dataloader(\n",
    "    bucket_config=cfg.get(\"bucket_config\", None),\n",
    "    num_bucket_build_workers=cfg.get(\"num_bucket_build_workers\", 1),\n",
    "    **dataloader_args,\n",
    ")\n",
    "num_steps_per_epoch = len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a5af0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch = next(iter(dataloader))\n",
    "first_batch[\"sampling_interval\"], first_batch[\"video\"].shape, first_batch[\"path\"], first_batch[\"index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471580c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_frame = first_batch[\"video\"][:, :, 0].to(device=device, dtype=torch.float32)\n",
    "first_frame.shape\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(first_frame[0].permute(1, 2, 0).cpu().numpy())\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ee1c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 3. build model\n",
    "# ======================================================\n",
    "logger.info(\"Building models...\")\n",
    "\n",
    "# == build vae model ==\n",
    "model = build_module(cfg.model, MODELS, device_map=device, torch_dtype=dtype).train()\n",
    "log_model_params(model)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e257a57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.get(\"grad_checkpoint\", False):\n",
    "    set_grad_checkpoint(model)\n",
    "\n",
    "vae_loss_fn = VAELoss(**cfg.vae_loss_config, device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc340ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == build EMA model ==\n",
    "if cfg.get(\"ema_decay\", None) is not None:\n",
    "    ema = deepcopy(model).cpu().eval().requires_grad_(False)\n",
    "    ema_shape_dict = record_model_param_shape(ema)\n",
    "    logger.info(\"EMA model created.\")\n",
    "else:\n",
    "    ema = ema_shape_dict = None\n",
    "    logger.info(\"No EMA model created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a32a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == build discriminator model ==\n",
    "use_discriminator = cfg.get(\"discriminator\", None) is not None\n",
    "if use_discriminator:\n",
    "    discriminator = build_module(cfg.discriminator, MODELS).to(device, dtype).train()\n",
    "    log_model_params(discriminator)\n",
    "    generator_loss_fn = GeneratorLoss(**cfg.gen_loss_config)\n",
    "    discriminator_loss_fn = DiscriminatorLoss(**cfg.disc_loss_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b7f953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == setup optimizer ==\n",
    "optimizer = create_optimizer(model, cfg.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26438bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == setup lr scheduler ==\n",
    "lr_scheduler = create_lr_scheduler(\n",
    "    optimizer=optimizer, num_steps_per_epoch=num_steps_per_epoch, epochs=cfg.get(\"epochs\", 1000), **cfg.lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1f0e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == setup discriminator optimizer ==\n",
    "if use_discriminator:\n",
    "    disc_optimizer = create_optimizer(discriminator, cfg.optim_discriminator)\n",
    "    disc_lr_scheduler = create_lr_scheduler(\n",
    "        optimizer=disc_optimizer,\n",
    "        num_steps_per_epoch=num_steps_per_epoch,\n",
    "        epochs=cfg.get(\"epochs\", 1000),\n",
    "        **cfg.disc_lr_scheduler,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319e1931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# 4. distributed training preparation with colossalai\n",
    "# =======================================================\n",
    "logger.info(\"Preparing for distributed training...\")\n",
    "# == boosting ==\n",
    "torch.set_default_dtype(dtype)\n",
    "model, optimizer, _, dataloader, lr_scheduler = booster.boost(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    dataloader=dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73f47b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_discriminator:\n",
    "    discriminator, disc_optimizer, _, _, disc_lr_scheduler = booster.boost(\n",
    "        model=discriminator,\n",
    "        optimizer=disc_optimizer,\n",
    "        lr_scheduler=disc_lr_scheduler,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2a9344",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float)\n",
    "logger.info(\"Boosted model for distributed training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d6fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == global variables ==\n",
    "cfg_epochs = cfg.get(\"epochs\", 1000)\n",
    "mixed_strategy = cfg.get(\"mixed_strategy\", None)\n",
    "mixed_image_ratio = cfg.get(\"mixed_image_ratio\", 0.0)\n",
    "# modulate mixed image ratio since we force rank 0 to be video\n",
    "num_ranks = dist.get_world_size()\n",
    "modulated_mixed_image_ratio = (\n",
    "    num_ranks * mixed_image_ratio / (num_ranks - 1) if num_ranks > 1 else mixed_image_ratio\n",
    ")\n",
    "if is_log_process(plugin_type, plugin_config):\n",
    "    print(\"modulated mixed image ratio:\", modulated_mixed_image_ratio)\n",
    "\n",
    "start_epoch = start_step = log_step = acc_step = 0\n",
    "running_loss = dict(  # loss accumulated over config.log_every steps\n",
    "    all=0.0,\n",
    "    nll=0.0,\n",
    "    nll_rec=0.0,\n",
    "    nll_per=0.0,\n",
    "    kl=0.0,\n",
    "    gen=0.0,\n",
    "    gen_w=0.0,\n",
    "    disc=0.0,\n",
    "    debug=0.0,\n",
    ")\n",
    "\n",
    "def log_loss(name, loss, loss_dict, use_video):\n",
    "    # only calculate loss for video\n",
    "    if use_video == 0:\n",
    "        loss.data = torch.tensor(0.0, device=device, dtype=dtype)\n",
    "    all_reduce_sum(loss.data)\n",
    "    num_video = torch.tensor(use_video, device=device, dtype=dtype)\n",
    "    all_reduce_sum(num_video)\n",
    "    loss_item = loss.item() / num_video.item()\n",
    "    loss_dict[name] = loss_item\n",
    "    running_loss[name] += loss_item\n",
    "\n",
    "logger.info(\"Training for %s epochs with %s steps per epoch\", cfg_epochs, num_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26bb06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == sharding EMA model ==\n",
    "if ema is not None:\n",
    "    model_sharding(ema)\n",
    "    ema = ema.to(device)\n",
    "\n",
    "if cfg.get(\"freeze_layers\", None) == \"all\":\n",
    "    for param in model.module.parameters():\n",
    "        param.requires_grad = False\n",
    "    print(\"all layers frozen\")\n",
    "\n",
    "# model.module.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73b9521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == resume ==\n",
    "if cfg.get(\"load\", None) is not None:\n",
    "    logger.info(\"Loading checkpoint from %s\", cfg.load)\n",
    "    start_epoch = cfg.get(\"start_epoch\", None)\n",
    "    start_step = cfg.get(\"start_step\", None)\n",
    "    ret = checkpoint_io.load(\n",
    "        booster,\n",
    "        cfg.load,\n",
    "        model=model,\n",
    "        ema=ema,\n",
    "        optimizer=optimizer,\n",
    "        lr_scheduler=lr_scheduler,\n",
    "        sampler=(\n",
    "            None if start_step is not None else sampler\n",
    "        ),  # if specify start step, set last_micro_batch_access_index of a new sampler instead\n",
    "    )\n",
    "    if start_step is not None:\n",
    "        # if start step exceeds data length, go to next epoch\n",
    "        if start_step > num_steps_per_epoch:\n",
    "            start_epoch = (\n",
    "                start_epoch + start_step // num_steps_per_epoch\n",
    "                if start_epoch is not None\n",
    "                else start_step // num_steps_per_epoch\n",
    "            )\n",
    "            start_step = start_step % num_steps_per_epoch\n",
    "        sampler.set_step(start_step)\n",
    "\n",
    "    start_epoch = start_epoch if start_epoch is not None else ret[0]\n",
    "    start_step = start_step if start_step is not None else ret[1]\n",
    "\n",
    "    if (\n",
    "        use_discriminator\n",
    "        and os.path.exists(os.path.join(cfg.load, \"discriminator\"))\n",
    "        and not cfg.get(\"restart_disc\", False)\n",
    "    ):\n",
    "        booster.load_model(discriminator, os.path.join(cfg.load, \"discriminator\"))\n",
    "        if cfg.get(\"load_optimizer\", True):\n",
    "            booster.load_optimizer(disc_optimizer, os.path.join(cfg.load, \"disc_optimizer\"))\n",
    "            if disc_lr_scheduler is not None:\n",
    "                booster.load_lr_scheduler(disc_lr_scheduler, os.path.join(cfg.load, \"disc_lr_scheduler\"))\n",
    "            if cfg.get(\"disc_lr\", None) is not None:\n",
    "                set_lr(disc_optimizer, disc_lr_scheduler, cfg.disc_lr)\n",
    "\n",
    "    logger.info(\"Loaded checkpoint %s at epoch %s step %s\", cfg.load, start_epoch, start_step)\n",
    "\n",
    "    if cfg.get(\"lr\", None) is not None:\n",
    "        set_lr(optimizer, lr_scheduler, cfg.lr, cfg.get(\"initial_lr\", None))\n",
    "\n",
    "    if cfg.get(\"update_warmup_steps\", False):\n",
    "        assert (\n",
    "            cfg.lr_scheduler.get(\"warmup_steps\", None) is not None\n",
    "        ), \"you need to set lr_scheduler.warmup_steps in order to pass --update-warmup-steps True\"\n",
    "        set_warmup_steps(lr_scheduler, cfg.lr_scheduler.warmup_steps)\n",
    "        if use_discriminator:\n",
    "            assert (\n",
    "                cfg.disc_lr_scheduler.get(\"warmup_steps\", None) is not None\n",
    "            ), \"you need to set disc_lr_scheduler.warmup_steps in order to pass --update-warmup-steps True\"\n",
    "            set_warmup_steps(disc_lr_scheduler, cfg.disc_lr_scheduler.warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e12596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# 5. training loop\n",
    "# =======================================================\n",
    "dist.barrier()\n",
    "\n",
    "cfg[\"accumulation_steps\"] = 128\n",
    "accumulation_steps = int(cfg.get(\"accumulation_steps\", 1))\n",
    "logger.info(\"Using gradient accumulation steps: %s\", accumulation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7127104",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(start_epoch, cfg_epochs):\n",
    "    # == set dataloader to new epoch ==\n",
    "    sampler.set_epoch(epoch)\n",
    "    dataiter = iter(dataloader)\n",
    "    logger.info(\"Beginning epoch %s...\", epoch)\n",
    "    random.seed(1024 + dist.get_rank())  # load vid/img for each rank\n",
    "\n",
    "    # == training loop in an epoch ==\n",
    "    with tqdm(\n",
    "        enumerate(dataiter, start=start_step),\n",
    "        desc=f\"Epoch {epoch}\",\n",
    "        disable=not coordinator.is_master(),\n",
    "        total=num_steps_per_epoch,\n",
    "        initial=start_step,\n",
    "    ) as pbar:\n",
    "        pbar_iter = iter(pbar)\n",
    "\n",
    "        def fetch_data():\n",
    "            step, batch = next(pbar_iter)\n",
    "            pinned_video = batch[\"video\"]\n",
    "            batch[\"video\"] = pinned_video.to(device, dtype, non_blocking=True)\n",
    "            return batch, step, pinned_video\n",
    "\n",
    "        batch_, step_, pinned_video_ = fetch_data()\n",
    "\n",
    "        profiler_ctxt = (\n",
    "            profile(\n",
    "                activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "                schedule=my_schedule,\n",
    "                on_trace_ready=torch.profiler.tensorboard_trace_handler(\"./log/profile\"),\n",
    "                record_shapes=True,\n",
    "                profile_memory=True,\n",
    "                with_stack=True,\n",
    "            )\n",
    "            if cfg.get(\"profile\", False)\n",
    "            else nullcontext()\n",
    "        )\n",
    "\n",
    "        with profiler_ctxt:\n",
    "            for _ in range(start_step, num_steps_per_epoch):\n",
    "                if cfg.get(\"profile\", False) and _ == WARMUP + ACTIVE + WAIT + 3:\n",
    "                    break\n",
    "\n",
    "                # == load data ===\n",
    "                batch, step, pinned_video = batch_, step_, pinned_video_\n",
    "                if step + 1 < num_steps_per_epoch:\n",
    "                    batch_, step_, pinned_video_ = fetch_data()\n",
    "\n",
    "                # == log config ==\n",
    "                global_step = epoch * num_steps_per_epoch + step\n",
    "                actual_update_step = (global_step + 1) // accumulation_steps\n",
    "                log_step += 1\n",
    "                acc_step += 1\n",
    "\n",
    "                # == mixed strategy ==\n",
    "                x = batch[\"video\"]\n",
    "                t_length = x.size(2)\n",
    "                use_video = 1\n",
    "                if mixed_strategy == \"mixed_video_image\":\n",
    "                    if random.random() < modulated_mixed_image_ratio and dist.get_rank() != 0:\n",
    "                        # NOTE: enable the first rank to use video\n",
    "                        t_length = 1\n",
    "                        use_video = 0\n",
    "                elif mixed_strategy == \"mixed_video_random\":\n",
    "                    t_length = random.randint(1, x.size(2))\n",
    "                x = x[:, :, :t_length, :, :]\n",
    "\n",
    "                with Timer(\"model\", log=True) if cfg.get(\"profile\", False) else nullcontext():\n",
    "                    # == forward pass ==\n",
    "                    x_rec, posterior, z = model(x)\n",
    "\n",
    "                    if cfg.get(\"profile\", False):\n",
    "                        profiler_ctxt.step()\n",
    "\n",
    "                    if cache_pin_memory:\n",
    "                        dataiter.remove_cache(pinned_video)\n",
    "\n",
    "                    # == loss initialization ==\n",
    "                    vae_loss = torch.tensor(0.0, device=device, dtype=dtype)\n",
    "                    loss_dict = {}  # loss at every step\n",
    "\n",
    "                    # == reconstruction loss ==\n",
    "                    ret = vae_loss_fn(x, x_rec, posterior)\n",
    "                    nll_loss = ret[\"nll_loss\"]\n",
    "                    kl_loss = ret[\"kl_loss\"]\n",
    "                    recon_loss = ret[\"recon_loss\"]\n",
    "                    perceptual_loss = ret[\"perceptual_loss\"]\n",
    "                    vae_loss += nll_loss + kl_loss\n",
    "\n",
    "                    # == generator loss ==\n",
    "                    if use_discriminator:\n",
    "                        # turn off grad update for disc\n",
    "                        discriminator.requires_grad_(False)\n",
    "                        fake_logits = discriminator(x_rec.contiguous())\n",
    "\n",
    "                        generator_loss, g_loss = generator_loss_fn(\n",
    "                            fake_logits,\n",
    "                            nll_loss,\n",
    "                            model.module.get_last_layer(),\n",
    "                            actual_update_step,\n",
    "                            is_training=model.training,\n",
    "                        )\n",
    "                        # print(f\"generator_loss: {generator_loss}, recon_loss: {recon_loss}, perceptual_loss: {perceptual_loss}\")\n",
    "\n",
    "                        vae_loss += generator_loss\n",
    "                        # turn on disc training\n",
    "                        discriminator.requires_grad_(True)\n",
    "\n",
    "                    # == generator backward & update ==\n",
    "                    ctx = (\n",
    "                        booster.no_sync(model, optimizer)\n",
    "                        if cfg.get(\"plugin\", \"zero2\") in (\"zero1\", \"zero1-seq\")\n",
    "                        and (step + 1) % accumulation_steps != 0\n",
    "                        else nullcontext()\n",
    "                    )\n",
    "                    with Timer(\"backward\", log=True) if cfg.get(\"profile\", False) else nullcontext():\n",
    "                        with ctx:\n",
    "                            booster.backward(loss=vae_loss / accumulation_steps, optimizer=optimizer)\n",
    "\n",
    "                    with Timer(\"optimizer\", log=True) if cfg.get(\"profile\", False) else nullcontext():\n",
    "                        if (step + 1) % accumulation_steps == 0:\n",
    "                            optimizer.step()\n",
    "                            optimizer.zero_grad()\n",
    "                            if lr_scheduler is not None:\n",
    "                                lr_scheduler.step(\n",
    "                                    actual_update_step,\n",
    "                                )\n",
    "                            # == update EMA ==\n",
    "                            if ema is not None:\n",
    "                                update_ema(\n",
    "                                    ema,\n",
    "                                    model.unwrap(),\n",
    "                                    optimizer=optimizer,\n",
    "                                    decay=cfg.get(\"ema_decay\", 0.9999),\n",
    "                                )\n",
    "\n",
    "                # == logging ==\n",
    "                log_loss(\"all\", vae_loss, loss_dict, use_video)\n",
    "                log_loss(\"nll\", nll_loss, loss_dict, use_video)\n",
    "                log_loss(\"nll_rec\", recon_loss, loss_dict, use_video)\n",
    "                log_loss(\"nll_per\", perceptual_loss, loss_dict, use_video)\n",
    "                log_loss(\"kl\", kl_loss, loss_dict, use_video)\n",
    "                if use_discriminator:\n",
    "                    log_loss(\"gen_w\", generator_loss, loss_dict, use_video)\n",
    "                    log_loss(\"gen\", g_loss, loss_dict, use_video)\n",
    "\n",
    "                # == loss: discriminator adversarial ==\n",
    "                if use_discriminator:\n",
    "                    real_logits = discriminator(x.detach().contiguous())\n",
    "                    fake_logits = discriminator(x_rec.detach().contiguous())\n",
    "                    disc_loss = discriminator_loss_fn(\n",
    "                        real_logits,\n",
    "                        fake_logits,\n",
    "                        actual_update_step,\n",
    "                    )\n",
    "\n",
    "                    # == discriminator backward & update ==\n",
    "                    ctx = (\n",
    "                        booster.no_sync(discriminator, disc_optimizer)\n",
    "                        if cfg.get(\"plugin\", \"zero2\") in (\"zero1\", \"zero1-seq\")\n",
    "                        and (step + 1) % accumulation_steps != 0\n",
    "                        else nullcontext()\n",
    "                    )\n",
    "                    with ctx:\n",
    "                        booster.backward(loss=disc_loss / accumulation_steps, optimizer=disc_optimizer)\n",
    "                    if (step + 1) % accumulation_steps == 0:\n",
    "                        disc_optimizer.step()\n",
    "                        disc_optimizer.zero_grad()\n",
    "                        if disc_lr_scheduler is not None:\n",
    "                            disc_lr_scheduler.step(actual_update_step)\n",
    "\n",
    "                    # log\n",
    "                    log_loss(\"disc\", disc_loss, loss_dict, use_video)\n",
    "\n",
    "                # == logging ==\n",
    "                if (global_step + 1) % accumulation_steps == 0:\n",
    "                    if coordinator.is_master() and actual_update_step % cfg.get(\"log_every\", 1) == 0:\n",
    "                        avg_loss = {k: v / log_step for k, v in running_loss.items()}\n",
    "                        # progress bar\n",
    "                        pbar.set_postfix(\n",
    "                            {\n",
    "                                # \"step\": step,\n",
    "                                # \"global_step\": global_step,\n",
    "                                # \"actual_update_step\": actual_update_step,\n",
    "                                # \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "                                **{k: f\"{v:.2f}\" for k, v in avg_loss.items()},\n",
    "                            }\n",
    "                        )\n",
    "                        # tensorboard\n",
    "                        tb_writer.add_scalar(\"loss\", vae_loss.item(), actual_update_step)\n",
    "                        # wandb\n",
    "                        if cfg.get(\"wandb\", False):\n",
    "                            wandb.log(\n",
    "                                {\n",
    "                                    \"iter\": global_step,\n",
    "                                    \"epoch\": epoch,\n",
    "                                    \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "                                    \"avg_loss_\": avg_loss,\n",
    "                                    \"avg_loss\": avg_loss[\"all\"],\n",
    "                                    \"loss_\": loss_dict,\n",
    "                                    \"loss\": vae_loss.item(),\n",
    "                                    \"global_grad_norm\": optimizer.get_grad_norm(),\n",
    "                                },\n",
    "                                step=actual_update_step,\n",
    "                            )\n",
    "\n",
    "                        running_loss = {k: 0.0 for k in running_loss}\n",
    "                        log_step = 0\n",
    "\n",
    "                    # == checkpoint saving ==\n",
    "                    ckpt_every = cfg.get(\"ckpt_every\", 0)\n",
    "                    if ckpt_every > 0 and actual_update_step % ckpt_every == 0 and coordinator.is_master():\n",
    "                        subprocess.run(\"sudo drop_cache\", shell=True)\n",
    "\n",
    "                    if ckpt_every > 0 and actual_update_step % ckpt_every == 0:\n",
    "                        # mannually garbage collection\n",
    "                        gc.collect()\n",
    "\n",
    "                        save_dir = checkpoint_io.save(\n",
    "                            booster,\n",
    "                            exp_dir,\n",
    "                            model=model,\n",
    "                            ema=ema,\n",
    "                            optimizer=optimizer,\n",
    "                            lr_scheduler=lr_scheduler,\n",
    "                            sampler=sampler,\n",
    "                            epoch=epoch,\n",
    "                            step=step + 1,\n",
    "                            global_step=global_step + 1,\n",
    "                            batch_size=cfg.get(\"batch_size\", None),\n",
    "                            actual_update_step=actual_update_step,\n",
    "                            ema_shape_dict=ema_shape_dict,\n",
    "                            async_io=True,\n",
    "                        )\n",
    "\n",
    "                        if is_log_process(plugin_type, plugin_config):\n",
    "                            os.system(f\"chgrp -R share {save_dir}\")\n",
    "\n",
    "                        if use_discriminator:\n",
    "                            booster.save_model(discriminator, os.path.join(save_dir, \"discriminator\"), shard=True)\n",
    "                            booster.save_optimizer(\n",
    "                                disc_optimizer,\n",
    "                                os.path.join(save_dir, \"disc_optimizer\"),\n",
    "                                shard=True,\n",
    "                                size_per_shard=4096,\n",
    "                            )\n",
    "                            if disc_lr_scheduler is not None:\n",
    "                                booster.save_lr_scheduler(\n",
    "                                    disc_lr_scheduler, os.path.join(save_dir, \"disc_lr_scheduler\")\n",
    "                                )\n",
    "                        dist.barrier()\n",
    "\n",
    "                        logger.info(\n",
    "                            \"Saved checkpoint at epoch %s, step %s, global_step %s to %s\",\n",
    "                            epoch,\n",
    "                            step + 1,\n",
    "                            actual_update_step,\n",
    "                            save_dir,\n",
    "                        )\n",
    "\n",
    "                        # remove old checkpoints\n",
    "                        rm_checkpoints(exp_dir, keep_n_latest=cfg.get(\"keep_n_latest\", -1))\n",
    "                        logger.info(\n",
    "                            \"Removed old checkpoints and kept %s latest ones.\", cfg.get(\"keep_n_latest\", -1)\n",
    "                        )\n",
    "\n",
    "        if cfg.get(\"profile\", False):\n",
    "            profiler_ctxt.export_chrome_trace(\"./log/profile/trace.json\")\n",
    "\n",
    "    sampler.reset()\n",
    "    start_step = 0\n",
    "\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
