{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f68b9b2b",
   "metadata": {},
   "source": [
    "# Open-Sora 2.0 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f47a17e",
   "metadata": {},
   "source": [
    "## ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646fd000",
   "metadata": {},
   "source": [
    "è¨“ç·´ã¯3ã¤ã®ã‚¹ãƒ†ãƒ¼ã‚¸ã§æ§‹æˆ:\n",
    "\n",
    "1. ä½Žè§£åƒåº¦å‹•ç”»ã«ã‚ˆã‚‹T2Vï¼ˆText to Videoï¼‰ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´\n",
    "1. ä½Žè§£åƒåº¦å‹•ç”»ã«ã‚ˆã‚‹I2Vï¼ˆImage to Videoï¼‰ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´\n",
    "1. é«˜è§£åƒåº¦å‹•ç”»ã«ã‚ˆã‚‹I2Vãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574c78b7",
   "metadata": {},
   "source": [
    "è¨“ç·´ã®ã‚³ã‚¹ãƒˆã®å†…è¨³ã¯ã€ã‚¹ãƒ†ãƒ¼ã‚¸1ã®ã‚³ã‚¹ãƒˆãŒ50%ã‚’å ã‚ã‚‹:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ef9cce",
   "metadata": {},
   "source": [
    "![](image/table3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1e9a74",
   "metadata": {},
   "source": [
    "è¨“ç·´ã®åŠ¹çŽ‡åŒ–ã®æˆ¦ç•¥:\n",
    "\n",
    "- 11Bï¼ˆ110å„„ï¼‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®Fluxã®è’¸ç•™ãƒ¢ãƒ‡ãƒ«ã§ã€T2Vãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–\n",
    "    - Fluxã¯ã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹T2Iï¼ˆText to Imageï¼‰ãƒ¢ãƒ‡ãƒ«\n",
    "- é«˜å“è³ªãªå‹•ç”»ãƒ‡ãƒ¼ã‚¿ã§ã€è¨“ç·´åŠ¹çŽ‡ã‚’å‘ä¸Š\n",
    "    - PixArtã‹ã‚‰ç€æƒ³\n",
    "    - ã‚¹ãƒ†ãƒƒãƒ—1, 2ã§ã¯ã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰é«˜å“è³ªãªã‚µãƒ–ã‚»ãƒƒãƒˆã‚’æŠ½å‡º\n",
    "    - ã‚¹ãƒ†ãƒƒãƒ—3ã§ã¯ã€ã‚ˆã‚ŠåŽ³ã—ã„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã§ã‚µãƒ–ã‚»ãƒƒãƒˆã‚’æŠ½å‡º\n",
    "- ä½Žè§£åƒåº¦å‹•ç”»ã§ã®å‹•ãã®å­¦ç¿’\n",
    "    - é«˜è§£åƒåº¦å‹•ç”»ã®è¨“ç·´ã‚³ã‚¹ãƒˆã¯é«˜ã„ãŸã‚ã€å¤šæ§˜ãªå‹•ãã¯ä½Žè§£åƒåº¦å‹•ç”»ã‚’ä¸­å¿ƒã«è¨“ç·´\n",
    "        - 128ãƒ•ãƒ¬ãƒ¼ãƒ 768pxã®å‹•ç”»ã®è¨“ç·´ã¯ã€256pxã®å ´åˆã‚ˆã‚Šã‚‚40å€é…ã„\n",
    "        - ã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®è¨ˆç®—è¤‡é›‘åº¦ãŒäºŒæ¬¡é–¢æ•°çš„ã«å¢—å¤§ã™ã‚‹ãŸã‚\n",
    "    - å‡ºåŠ›ãŒã¼ã‚„ã‘ã‚‹ãŸã‚ã€é«˜è§£åƒåº¦å‹•ç”»ã®è¨“ç·´ã§å“è³ªã‚’å‘ä¸Š\n",
    "- ã‚¹ãƒ†ãƒƒãƒ—2ã®ã‚¢ãƒƒãƒ—ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å­¦ç¿’ã¯ã€T2Vã§ã¯ãªãI2Vã‚’æŽ¡ç”¨ã—åŠ¹çŽ‡åŒ–:\n",
    "    - é™æ­¢ç”»ã‚’æ¡ä»¶ä»˜ã‘ã™ã‚‹ã“ã¨ã§ã€å‹•ãã®ç”Ÿæˆã«é›†ä¸­ã§ãã‚‹ãŸã‚\n",
    "- é«˜è§£åƒåº¦å‹•ç”»ã§ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’çŸ­ç¸®ã—ã€è¨“ç·´ã‚’åŠ¹çŽ‡åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bee32fa",
   "metadata": {},
   "source": [
    "Open-Sora 2.0ã®è¨“ç·´ã‚³ã‚¹ãƒˆã¯ã€5~10å€ä½Žã„:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7a4a13",
   "metadata": {},
   "source": [
    "![](image/fig7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50432d84",
   "metadata": {},
   "source": [
    "è¨“ç·´è¨­å®šã¯ã€Open-Sora 1.2ã«åŸºã¥ã:\n",
    "\n",
    "- ç›®çš„é–¢æ•°ã¯Flow Matchingã‚’æŽ¡ç”¨\n",
    "- ã‚ªãƒ—ãƒ†ã‚£ãƒžã‚¤ã‚¶ã¯AdamWã‚’ä½¿ç”¨\n",
    "    - $\\beta$å€¤ã¯$(0.9, 0.999)$\n",
    "    - $\\epsilon$ã¯$1\\times 10^{-15}$\n",
    "    - é‡ã¿æ¸›è¡°ã¯ä½¿ç”¨ã—ãªã„\n",
    "    - å­¦ç¿’çŽ‡\n",
    "        - ã‚¹ãƒ†ãƒ¼ã‚¸1, 2\n",
    "            - æœ€åˆã®4ä¸‡ã‚¹ãƒ†ãƒƒãƒ—ã¯$5\\times 10^{-5}$\n",
    "            - æœ€å¾Œã®4.5ä¸‡ã‚¹ãƒ†ãƒƒãƒ—ã¯$3\\times 10^{-5}$\n",
    "        - ã‚¹ãƒ†ãƒ¼ã‚¸3\n",
    "            - $1\\times 10^{-5}$\n",
    "    - è³¼è²·ãƒŽãƒ«ãƒ ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã®é–¾å€¤ã¯$1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298a6ed7",
   "metadata": {},
   "source": [
    "Flow matchingã¯ã€Stable Diffusion 3ï¼ˆSD3ï¼‰ã¨é¡žä¼¼:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\mathbb{E}_{t,X_{0},X_{1}}[||f_{\\theta}(X_{t},t,y)-(X_{0}-X_{1})||]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16825ea",
   "metadata": {},
   "source": [
    "- $X_0$: å‹•ç”»ã®æ½œåœ¨è¡¨ç¾\n",
    "- $X_1 \\sim \\mathcal{N}(0,1)$: ã‚¬ã‚¦ã‚¹ãƒŽã‚¤ã‚º\n",
    "- $t$: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆãƒŽã‚¤ã‚ºå¼·åº¦ï¼‰\n",
    "- $X_t=(1-t)X_0 + tX_1$: è£œé–“ã•ã‚ŒãŸæ½œåœ¨è¡¨ç¾ï¼ˆãƒŽã‚¤ã‚ºã‚ã‚Šã®æ½œåœ¨è¡¨ç¾ï¼‰\n",
    "- $y$: ãƒ†ã‚­ã‚¹ãƒˆã‚„ç”»åƒãªã©ã®æ¡ä»¶\n",
    "- $(X_0 - X_1)$: æ­£è§£ã®é€Ÿåº¦ï¼ˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒŽã‚¤ã‚ºã¸ã®å¤‰åŒ–ã®æ–¹å‘ï¼‰\n",
    "- $\\mathbb{E}_{t, X_0, X_1}$: æ¡ä»¶ã‚’å¤‰ãˆãŸæ™‚ã®å¹³å‡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cae6e92",
   "metadata": {},
   "source": [
    "$t$ã¯ã€å¯¾æ•°æ­£è¦åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€$X_0$ã®å½¢çŠ¶ã«åˆã‚ã›ã¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼š\n",
    "\n",
    "$$\n",
    "t' = \\frac{at}{1 + (a-1)t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cd5f9a",
   "metadata": {},
   "source": [
    "- $a$: $T\\times H\\times W$ã«æ¯”ä¾‹ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "- é«˜è§£åƒåº¦ã§ã®é•·æ™‚é–“ã®ç”»åƒã¯ãƒŽã‚¤ã‚ºã®å½±éŸ¿ã‚’å—ã‘ã‚„ã™ã„ãŸã‚\n",
    "- æŽ¨è«–æ™‚ã‚‚åŒæ§˜ã®æ–¹æ³•ãŒé©ç”¨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb03915",
   "metadata": {},
   "source": [
    "ãƒžãƒ«ãƒãƒã‚±ãƒƒãƒˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’æŽ¡ç”¨:\n",
    "\n",
    "- ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ãƒ»è§£åƒåº¦ãƒ»ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ãŒä¼¼ãŸå‹•ç”»ã‚’ãƒã‚±ãƒ„ã«åˆ†ã‘ã¦ã€ãƒã‚±ãƒ„ã”ã¨ã«ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’èª¿æ•´ã™ã‚‹æ‰‹æ³•\n",
    "    - ãƒ¡ãƒ¢ãƒªä¸è¶³ï¼ˆOOMï¼‰ã®å›žé¿\n",
    "    - ä¸¦åˆ—è¨ˆç®—æ™‚ã®è¨“ç·´æ™‚é–“ã®å‡ä¸€åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580846f9",
   "metadata": {},
   "source": [
    "ã‚¹ãƒ†ãƒ¼ã‚¸1, 2ã®ãƒãƒƒãƒã‚µã‚¤ã‚ºã¨ãƒãƒƒãƒã‚µã‚¤ã‚ºã¨ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ:\n",
    "\n",
    "![](image/table4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece6997d",
   "metadata": {},
   "source": [
    "ã‚¹ãƒ†ãƒ¼ã‚¸3ã®ãƒãƒƒãƒã‚µã‚¤ã‚ºã¨ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆï¼ˆContext parallelism 4ï¼‰:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0decbf9",
   "metadata": {},
   "source": [
    "![](image/table5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f930f7dd",
   "metadata": {},
   "source": [
    "é«˜åœ§ç¸®ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ï¼ˆDC-AEï¼‰ã‚’ä½¿ç”¨ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚³ã‚¹ãƒˆã‚’ã•ã‚‰ã«å‰Šæ¸›"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382509ce",
   "metadata": {},
   "source": [
    "DC-AEã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€å‹•ç”»ç”Ÿæˆã«å¿…è¦ãªãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒå¤§å¹…ã«å‰Šæ¸›ã§ãã‚‹:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2af738",
   "metadata": {},
   "source": [
    "$$\n",
    "D_{token} = D_{T} \\times D_{H} \\times D_{W} \\times P_{T} \\times P_{H} \\times P_{W}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c126dd2e",
   "metadata": {},
   "source": [
    "- $D_{\\text{token}}$: ãƒˆãƒ¼ã‚¯ãƒ³ãƒ»ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒ«æ¯”ï¼ˆã©ã®ãã‚‰ã„å°ã•ãåœ§ç¸®ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã™ã‚‹ã‹ã®æ¯”çŽ‡ï¼‰\n",
    "- $D_T, D_H, H_W$: æ™‚é–“ãƒ»é«˜ã•ãƒ»å¹…ã®æ¬¡å…ƒã«ãŠã‘ã‚‹ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®åœ§ç¸®çŽ‡\n",
    "- $P_T, P_H, P_W$: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›ã™ã‚‹éš›ã®ãƒ‘ãƒƒãƒåˆ†å‰²ã‚µã‚¤ã‚º\n",
    "- Hunyuan Videoã®$D_{\\text{token}}$ã¯$4096$ã€DC-AEã®$D_{\\text{token}}$ã¯$1024$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d73b6eb",
   "metadata": {},
   "source": [
    "ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®æ€§èƒ½ã¯ã€æƒ…å ±ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¯”ã§äºˆæ¸¬ã§ãã‚‹:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc2ad92",
   "metadata": {},
   "source": [
    "$$\n",
    "D_{info} = \\frac{D_{T} \\times D_{H} \\times D_{W} \\times C_{in}}{C_{out}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f77401f",
   "metadata": {},
   "source": [
    "- $D_{\\text{info}}$: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ãŒæ½œåœ¨è¡¨ç¾ã«ãªã‚‹ã¨ãã«æƒ…å ±é‡ãŒåœ§ç¸®ã•ã‚ŒãŸæ¯”çŽ‡\n",
    "- $C_{\\text{in}}$: å…¥åŠ›ãƒãƒ£ãƒ³ãƒãƒ«æ•°ï¼ˆRGBã®å ´åˆ$3$ï¼‰\n",
    "- $C_{\\text{out}}$: å‡ºåŠ›ãƒãƒ£ãƒ³ãƒãƒ«æ•°ï¼ˆæ½œåœ¨è¡¨ç¾ã®ãƒãƒ£ãƒ³ãƒãƒ«æ•°ï¼‰\n",
    "- DC-AEã®ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¯ã€Hunyuan Video VAEã¨StepVideo VAEã®$D_{\\text{info}}$ã¨ä¸€è‡´ã™ã‚‹ã‚ˆã†ã«è¨­è¨ˆ\n",
    "- å…·ä½“çš„ã«ã¯å‡ºåŠ›ãƒãƒ£ãƒ³ãƒãƒ«æ•°ã‚’å¢—ã‚„ã™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82e52bc",
   "metadata": {},
   "source": [
    "DC-AEã®ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒ«ãƒ–ãƒ­ãƒƒã‚¯ã¨ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒ«ãƒ–ãƒ­ãƒƒã‚¯ã«ã¯ã€å‹¾é…ä¼æ’­ã®å•é¡ŒãŒã‚ã‚‹:\n",
    "\n",
    "- ãƒ”ã‚¯ã‚»ãƒ«ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã¨ãƒ”ã‚¯ã‚»ãƒ«ã‚¢ãƒ³ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã‚’ä½¿ç”¨ã—ãŸæ®‹å·®æŽ¥ç¶šã‚’å°Žå…¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4b044c",
   "metadata": {},
   "source": [
    "DC-AEã§å‡ºåŠ›ã—ãŸé«˜åœ§ç¸®ã®æ½œåœ¨å¤‰æ•°ã§ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã™ã‚‹ã¨ã€å‹•ç”»ç”Ÿæˆã®å“è³ªãŒä½Žããªã‚‹:\n",
    "\n",
    "- ãƒãƒ£ãƒ³ãƒãƒ«æ•°ãŒå¤šã„ã¨ã€æ½œåœ¨ç©ºé–“æ§‹é€ ï¼ˆlatent space structureï¼‰ã®æœ€é©åŒ–ãŒé›£ã—ããªã‚‹ãŸã‚\n",
    "- DC-AEè¨“ç·´å¾Œã«ã€ç¬¬3å±¤ã®æ½œåœ¨è¡¨ç¾ã‚’DINOv2ã«åˆã‚ã›ã‚‹ãŸã‚ã®è’¸ç•™æå¤±ã‚’é©ç”¨ã—ã€æœ€é©åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8639f8",
   "metadata": {},
   "source": [
    "DC-AEã‚’ä½¿ç”¨ã—ãŸå‹•ç”»ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´:\n",
    "\n",
    "1. 2000ä¸‡ä»¶ã®æœ€å¤§33ãƒ•ãƒ¬ãƒ¼ãƒ ã®å‹•ç”»ã‚’ã€1.7ä¸‡ã‚¹ãƒ†ãƒƒãƒ—ã§è¨“ç·´\n",
    "2. 200ä¸‡ä»¶ã®æœ€å¤§128ãƒ•ãƒ¬ãƒ¼ãƒ ã®å‹•ç”»ã‚’ã€8000ã‚¹ãƒ†ãƒƒãƒ—ã§è¨“ç·´"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1697a8a5",
   "metadata": {},
   "source": [
    "DC-AEã‚’ä½¿ç”¨ã—ãŸå‹•ç”»ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¯ã€Hunyuan Video VAEã‚ˆã‚Šã‚‚ã€è¨“ç·´ã¨æŽ¨è«–ã§é«˜ã„åŠ¹çŽ‡æ€§:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46688e6",
   "metadata": {},
   "source": [
    "![](image/fig8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e26d444",
   "metadata": {},
   "source": [
    "DC-AEã¯ã€256ãƒãƒ£ãƒãƒ«ã¨128ãƒãƒ£ãƒãƒ«ã®2ã¤ã‚’æœ€åˆã‹ã‚‰è¨“ç·´:\n",
    "\n",
    "1. å†æ§‹æˆæå¤±$\\mathcal{L}_1$ã¨çŸ¥è¦šæå¤±$\\mathcal{L}_{\\text{LIPIS}}$ã‚’ä½¿ç”¨ã—ã¦ã€25ä¸‡ã‚¹ãƒ†ãƒƒãƒ—è¨“ç·´:\n",
    "    $$\n",
    "    \\mathcal{L} = \\mathcal{L}_{1} + 0.5\\mathcal{L}_{LPIPS}\n",
    "    $$\n",
    "2. æ•µå¯¾çš„æå¤±$\\mathcal{L}_{\\text{adv}}$ã‚’ä½¿ç”¨ã—ã¦ã€20ä¸‡ã‚¹ãƒ†ãƒƒãƒ—è¨“ç·´:\n",
    "    $$\n",
    "    \\mathcal{L} = \\mathcal{L_1} + 0.5\\mathcal{L_{LPIPS}} + 0.05\\mathcal{L_{\\text{adv}}}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73af25fc",
   "metadata": {},
   "source": [
    "DC-AEã®è¨“ç·´è¨­å®š:\n",
    "\n",
    "- ãƒ­ãƒ¼ã‚«ãƒ«ãƒãƒƒãƒã‚µã‚¤ã‚º$1$ã§$8$å€‹ã®GPUã‚’ä½¿ç”¨\n",
    "- ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”$1:1$ã€32ãƒ•ãƒ¬ãƒ¼ãƒ ã€256pxã®å‹•ç”»ã‚’ä½¿ç”¨\n",
    "- ã‚ªãƒ—ãƒ†ã‚£ãƒžã‚¤ã‚¶ã¯AdamWã‚’ä½¿ç”¨ï¼ˆ$\\beta=(0.9, 0.999)$, $\\epsilon=1\\times 10^{-15}$, é‡ã¿æ¸›è¡°ãªã—ï¼‰\n",
    "- ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å­¦ç¿’çŽ‡ã¯5e-5\n",
    "- ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒŸãƒãƒ¼ã‚¿ã®å­¦ç¿’çŽ‡ã¯1e-4\n",
    "- å‹¾é…ãƒŽãƒ«ãƒ ã‚¯ãƒªãƒƒãƒ—ã¯é–¾å€¤1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd76e520",
   "metadata": {},
   "source": [
    "## æ¡ä»¶ä»˜ã‘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b965deb5",
   "metadata": {},
   "source": [
    "æ¡ä»¶ä»˜ã‘ã¯ã€æ½œåœ¨å¤‰æ•°ã«å¯¾ã—ã¦æ¡ä»¶ãƒ™ã‚¯ãƒˆãƒ«ã¨ã‚¿ã‚¹ã‚¯ã®ç¨®é¡žã‚’ç¤ºã™ãƒãƒ£ãƒ³ãƒãƒ«ã‚’é€£çµã—ã¦è¡Œã†:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2c960a",
   "metadata": {},
   "source": [
    "![](image/fig10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8651a48c",
   "metadata": {},
   "source": [
    "- å·¦: ç”»åƒã‹ã‚‰å‹•ç”»ã‚’ç”Ÿæˆã™ã‚‹ã‚¿ã‚¹ã‚¯\n",
    "- ä¸­: å‹•ç”»ã‚’å»¶é•·ã™ã‚‹ã‚¿ã‚¹ã‚¯\n",
    "- å³: æœ€åˆã¨æœ€å¾Œã®ç”»åƒã®é–“ã‚’è£œå®Œã™ã‚‹ã‚¿ã‚¹ã‚¯\n",
    "- é€£çµå¾Œã®ãƒãƒ£ãƒ³ãƒãƒ«æ•°ã¯$2k + 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4afc63",
   "metadata": {},
   "source": [
    "æ±ŽåŒ–æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ã€ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ã®ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚„ç”»åƒæ¡ä»¶ã®ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’å°Žå…¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c229f9d",
   "metadata": {},
   "source": [
    "æŽ¨è«–ã«ã¯ã€Classifier-free Guidanceï¼ˆCFGï¼‰ã‚’æŽ¡ç”¨:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532dd227",
   "metadata": {},
   "source": [
    "$$\n",
    "v_{t} = v_{\\theta}(x_{t},t,\\emptyset,\\emptyset) + g_{img}\\cdot(v_{\\theta}(x_{t},t,\\emptyset,img)-v_{\\theta}(x_{t},t,\\emptyset,\\emptyset)) + g_{txt}\\cdot(v_{\\theta}(x_{t},t,txt,img)-v_{\\theta}(x_{t},t,\\emptyset,img))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cd2e12",
   "metadata": {},
   "source": [
    "- $g_{\\text{img}}$: ç”»åƒæ¡ä»¶ã®å¼·ã•ã‚’èª¿æ•´ã™ã‚‹ä¿‚æ•°\n",
    "- $g_{\\text{txt}}$: ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ã®å¼·ã•ã‚’èª¿æ•´ã™ã‚‹ä¿‚æ•°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a04814",
   "metadata": {},
   "source": [
    "ç”»åƒã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚’å¼·ãã™ã‚‹ã¨ã€ç”Ÿæˆã•ã‚ŒãŸå‹•ç”»ã«ãƒãƒ©ã¤ãï¼ˆflickerï¼‰ãŒç”Ÿã˜ã‚‹:\n",
    "\n",
    "- Guidance Oscillationã‚’å°Žå…¥ã—ã¦å®‰å®šåŒ–\n",
    "    - ä¾‹ãˆã°ã€50ã‚¹ãƒ†ãƒƒãƒ—ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ã†ã¡ã€10ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«$g_{\\text{img}}$ã‚’$1$ã«è½ã¨ã™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b8f8b1",
   "metadata": {},
   "source": [
    "I2Vã§ã¯å‹•çš„ãªç”»åƒã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ï¼ˆdynamic image guidanceï¼‰ã‚’æŽ¡ç”¨ã—ã€å‹•ç”»ã®å¾ŒåŠã«ãªã‚‹ã«ã¤ã‚Œã¦æ¡ä»¶ä»˜ã‘ã‚’å¼·ãã™ã‚‹:\n",
    "\n",
    "![](image/fig11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68ce8cd",
   "metadata": {},
   "source": [
    "- æ¨ªè»¸ã¯ãƒ•ãƒ¬ãƒ¼ãƒ æ•°\n",
    "- ç¸¦è»¸ã¯æŽ¨è«–ã‚¹ãƒ†ãƒƒãƒ—æ•°\n",
    "- ç”»åƒã®ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚¹ã‚±ãƒ¼ãƒ«ã¯$1$ã‹ã‚‰$k$ã¾ã§ç·šå½¢ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
    "- æŽ¨è«–ãŒçµ‚ã‚ã‚Šã«ãªã‚‹ã«ã¤ã‚Œã¦ã€ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãŒ$1$ã«ãªã‚‹ã‚ˆã†ã«æ¸›è¡°\n",
    "- ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¯$g_{\\text{img}}=3$, $g_{\\text{txt}}=7.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d241562",
   "metadata": {},
   "source": [
    "å‹•ç”»ã®å‹•ãã®å¼·ã•ã¯ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã«è¿½åŠ ã™ã‚‹ã“ã¨ã§åˆ¶å¾¡:\n",
    "\n",
    "![](image/fig12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0abfb4",
   "metadata": {},
   "source": [
    "## ã‚·ã‚¹ãƒ†ãƒ æœ€é©åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f02982a",
   "metadata": {},
   "source": [
    "è¨“ç·´ã¯ã€ColossalAIã‚’ä½¿ç”¨:\n",
    "\n",
    "- 141GBã®ãƒ¡ãƒ¢ãƒªå®¹é‡ã‚’æŒã¤H200 GPUã‚’ä½¿ç”¨ã—ã€ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—ï¼ˆDP, Data Parallelismï¼‰ã‚’å°Žå…¥\n",
    "- é¸æŠžçš„ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒ†ã‚£ãƒ³ã‚°ï¼ˆSelective Activation Checkpointingï¼‰ã‚’å°Žå…¥\n",
    "    - ä¸€éƒ¨ã®é †ä¼æ’­ã®è¨ˆç®—çµæžœã‚’ç ´æ£„ã—ã€å¿…è¦ã«å¿œã˜ã¦å†è¨ˆç®—ã™ã‚‹æŠ€è¡“ã§ãƒ¡ãƒ¢ãƒªåŠ¹çŽ‡åŒ–\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6636c0c3",
   "metadata": {},
   "source": [
    "é«˜è§£åƒåº¦ã®å‹•ç”»ã‚’åŠ¹çŽ‡çš„ã«å‡¦ç†ã™ã‚‹ãŸã‚ã«ã€è¤‡æ•°ã®ä¸¦åˆ—åŒ–æŠ€è¡“ã‚’æŽ¡ç”¨:\n",
    "\n",
    "- AEã§ã¯ã€ãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—åŒ–ï¼ˆTP, Tensor Parallelismï¼‰ã‚’ç•³ã¿è¾¼ã¿å±¤ã«é©ç”¨\n",
    "    - å…¥åŠ›ã¾ãŸã¯å‡ºåŠ›ã®ãƒãƒ£ãƒãƒ«æ¬¡å…ƒã§é‡ã¿ã‚’åˆ†å‰²ã™ã‚‹\n",
    "- MMDiTã§ã¯ã€Zero Redundancy Opitimizerï¼ˆZeroDPï¼‰ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¸¦åˆ—ï¼ˆCP, Context Parallelismï¼‰ã‚’é©ç”¨\n",
    "    - ZeroDPã¯ã€DeepSpeedã®ZeRO2ã«ã‚ˆã‚Šå‹¾é…ã‚’åˆ†å‰²ã—ã€é‡è¤‡ãªãGPUãŒä¿æŒã™ã‚‹æŠ€è¡“\n",
    "    - CPã¯ã€å‹•ç”»ã¨ãƒ†ã‚­ã‚¹ãƒˆã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’åˆ†å‰²ã—ã€å„GPUãŒç‹¬ç«‹ã—ã¦ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¨ˆç®—ã™ã‚‹æŠ€è¡“"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f902c3e9",
   "metadata": {},
   "source": [
    "ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒ†ã‚£ãƒ³ã‚°ã¯ã€ãƒ–ãƒ­ãƒƒã‚¯ã®å…¥åŠ›ã®ã¿ã‚’ä¿æŒã—ã€ãã‚Œä»¥å¤–ã¯å†è¨ˆç®—ã™ã‚‹:\n",
    "\n",
    "- ã‚¹ãƒ†ãƒ¼ã‚¸1, 2ã§ã¯ã€ãƒ‡ãƒ¥ã‚¢ãƒ«ãƒ–ãƒ­ãƒƒã‚¯ã®8å±¤ã¨ã™ã¹ã¦ã®ã‚·ãƒ³ã‚°ãƒ«ãƒ–ãƒ­ãƒƒã‚¯ã«é©ç”¨\n",
    "- ã‚¹ãƒ†ãƒ¼ã‚¸3ã§ã¯ã€ã™ã¹ã¦ã®ãƒ–ãƒ­ãƒƒã‚¯ã§æœ‰åŠ¹åŒ–ã—ã€CPUã¸ã®ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã‚‚ä½¿ç”¨\n",
    "    - ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã§ã¯ã€Pinned Memoryã¨éžåŒæœŸãƒ‡ãƒ¼ã‚¿è»¢é€ã‚’ä½¿ç”¨ã—ã€ã•ã‚‰ã«åŠ¹çŽ‡åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6969e364",
   "metadata": {},
   "source": [
    "åˆ†æ•£è¨“ç·´ã®å ´åˆã€ç‰©ç†éšœå®³ãŒç™ºç”Ÿã—ã‚„ã™ã„ã®ã§è‡ªå‹•å¾©æ—§æ©Ÿèƒ½ï¼ˆAuto Recoveryï¼‰ã‚’å°Žå…¥\n",
    "\n",
    "- InfiniBandã®éšœå®³ã€ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã€NCCLã‚¨ãƒ©ãƒ¼\n",
    "- è¨“ç·´ã®çŠ¶æ…‹ã‚’ç›£è¦–ã—ã€é€Ÿåº¦ä½Žä¸‹ãƒ»æå¤±ã®æ€¥æ¿€ãªæ‚ªåŒ–ãƒ»ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒãªã„ã‹ã‚’æ¤œè¨¼\n",
    "- å•é¡ŒãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã¯ã€ã™ã¹ã¦ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚’åœæ­¢ã—ã€ãƒŽãƒ¼ãƒ‰ã‚’è¨ºæ–­\n",
    "- å¿…è¦ã«å¿œã˜ã¦äºˆå‚™ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã€æœ€çµ‚ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰è‡ªå‹•çš„ã«è¨“ç·´ã‚’å†é–‹ã™ã‚‹\n",
    "- GPUç¨¼åƒçŽ‡99%ã§ã€ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ ã‚’æœ€å°é™ã«æŠ‘ãˆãŸ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58002756",
   "metadata": {},
   "source": [
    "CPUã¨GPUé–“ã®ãƒ‡ãƒ¼ã‚¿è»¢é€ã‚’é«˜é€ŸåŒ–ã™ã‚‹ãŸã‚ã«PyTorchã®ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’æœ€é©åŒ–:\n",
    "\n",
    "- PyTorchãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®Pinned Memoryã¯ã€CUDAã‚«ãƒ¼ãƒãƒ«å®Ÿè¡Œã‚’ãƒ–ãƒ­ãƒƒã‚¯ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹\n",
    "- ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ•ã‚¡ï¼ˆpre-allocated pinned memory bufferï¼‰ã‚’ä½¿ç”¨ã—ã€ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’å‰Šæ¸›\n",
    "- ãƒ‡ãƒ¼ã‚¿è»¢é€ã¨è¨ˆç®—ã‚’ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã•ã›ã¦åŠ¹çŽ‡åŒ–\n",
    "- ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªGCï¼ˆã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ï¼‰ã¯ã€åˆ†æ•£è¨“ç·´ã§ã¯ãƒã‚°ãŒå¤šã„ãŸã‚æ‰‹å‹•ã§ãƒ¡ãƒ¢ãƒªã‹ã‚“ã‚Š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55af23f0",
   "metadata": {},
   "source": [
    "ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ä¿å­˜ã¯ã€ãƒ”ãƒ³ç•™ã‚ã•ã‚ŒãŸãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ•ã‚¡ã«å¯¾ã—ã¦ç›´æŽ¥ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã“ã¨ã§é«˜é€ŸåŒ–:\n",
    "\n",
    "- ãƒ‡ã‚£ã‚¹ã‚¯ã¸ã®æ›¸ãè¾¼ã¿ã¯ã€C++ã«ã‚ˆã‚‹éžåŒæœŸãƒ‡ã‚£ã‚¹ã‚¯æ›¸ãè¾¼ã¿ã«ã‚ˆã‚Šè¨“ç·´ãƒ—ãƒ­ã‚»ã‚¹ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã›ãšã«åŠ¹çŽ‡åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629e8584",
   "metadata": {},
   "source": [
    "## å®Ÿè£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaff96bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    !git clone https://github.com/hpcaitech/Open-Sora.git\n",
    "    WORK_DIR = \"/content/Open-Sora\"\n",
    "    %cd $WORK_DIR\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    WORK_DIR = \"/workspaces/open-sora/Open-Sora\"\n",
    "    %cd $WORK_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298268ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import platform\n",
    "import subprocess\n",
    "\n",
    "DEBUG_LOG_PATH = os.path.join(WORK_DIR, \"debug.log\")\n",
    "\n",
    "if os.path.exists(DEBUG_LOG_PATH):\n",
    "    os.remove(DEBUG_LOG_PATH)\n",
    "\n",
    "def custom_format(record):\n",
    "    match record.levelno:\n",
    "        case logging.DEBUG:\n",
    "            level = \"ðŸŸ¦\"\n",
    "        case logging.INFO:\n",
    "            level = \"ðŸŸ©\"\n",
    "        case logging.WARNING:\n",
    "            level = \"ðŸŸ¨\"\n",
    "        case logging.ERROR:\n",
    "            level = \"ðŸŸ¥\"\n",
    "        case logging.CRITICAL:\n",
    "            level = \"ðŸ›‘\"\n",
    "    return f\"{level} {record.getMessage()}\"\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "for handler in logger.handlers:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "formatter = logging.Formatter()\n",
    "formatter.format = custom_format\n",
    "\n",
    "file_handler = logging.FileHandler(DEBUG_LOG_PATH)\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "PYTHON_VERSION = platform.python_version()\n",
    "logger.info(f\"Python {PYTHON_VERSION}\")\n",
    "\n",
    "NVIDIA_SMI = subprocess.run(\"nvidia-smi\", capture_output=True, text=True).stdout\n",
    "logger.info(f\"{NVIDIA_SMI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bab8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    %pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "    %pip install \\\n",
    "        accelerate \\\n",
    "        av==13.1.0 \\\n",
    "        colossalai \\\n",
    "        ftfy \\\n",
    "        liger-kernel \\\n",
    "        omegaconf \\\n",
    "        mmengine \\\n",
    "        openai \\\n",
    "        pandas \\\n",
    "        pandarallel \\\n",
    "        pyarrow \\\n",
    "        tensorboard \\\n",
    "        wandb \\\n",
    "        --extra-index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "    %pip install flash-attn --no-build-isolation\n",
    "\n",
    "    %pip install -e . --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4318c538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from einops import rearrange\n",
    "from flash_attn import flash_attn_func as flash_attn_func_v2\n",
    "from functools import partial\n",
    "from huggingface_hub import PyTorchModelHubMixin\n",
    "from inspect import signature\n",
    "from liger_kernel.ops.rms_norm import LigerRMSNormFunction\n",
    "from liger_kernel.ops.rope import LigerRopeFunction\n",
    "from omegaconf import MISSING, OmegaConf\n",
    "from torch import Tensor, nn\n",
    "from torch.nn.modules.batchnorm import _BatchNorm\n",
    "from typing import Any, Callable, Optional, Union, Tuple\n",
    "import diffusers\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "from mmengine.config import Config\n",
    "import ast\n",
    "\n",
    "try:\n",
    "    from flash_attn_interface import flash_attn_func as flash_attn_func_v3\n",
    "    SUPPORT_FA3 = True\n",
    "except:\n",
    "    SUPPORT_FA3 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8a3051",
   "metadata": {},
   "source": [
    "## ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a360a8b",
   "metadata": {},
   "source": [
    "å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã¯[hcpai-tech/open-sora-pexels-45k][1]ã«ã‚ã‚‹\n",
    "\n",
    "[1]: https://modelscope.cn/datasets/hpcai-tech/open-sora-pexels-45k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ac0fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 269.53GBã®å‹•ç”»ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "# https://modelscope.cn/datasets/hpcai-tech/open-sora-pexels-45k\n",
    "\n",
    "if False:\n",
    "    !apt update && apt install git-lfs\n",
    "    !git-lfs install\n",
    "    !git clone \"https://www.modelscope.cn/datasets/hpcai-tech/open-sora-pexels-45k.git\"\n",
    "    !cd open-sora-pexels-45k && \\\n",
    "        cat tar/pexels_45k.tar.* > pexels_45k.tar && \\\n",
    "        mkdir ../datasets && \\\n",
    "        mv pexels_45k ../datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c0a57f",
   "metadata": {},
   "source": [
    "é‡ã„ã®ã§æœ€å°é™ã®çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa393d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc https://modelscope.cn/datasets/hpcai-tech/open-sora-pexels-45k/resolve/master/pexels_45k_necessary.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216fe272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# å‹•ç”»ã‚’è©•ä¾¡å¾Œã€ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®CSVã‚’èª­ã¿è¾¼ã‚€\n",
    "pexels_45k_necessary_df = pd.read_csv(\"pexels_45k_necessary.csv\")\n",
    "print(len(pexels_45k_necessary_df)) # 45,817ä»¶\n",
    "pexels_45k_necessary_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9192662",
   "metadata": {},
   "outputs": [],
   "source": [
    "pexels_45k_score_df = pexels_45k_necessary_df.sort_values(by=[\"num_frames\"])\n",
    "pexels_5 = pexels_45k_score_df.head(5).copy()\n",
    "pexels_5['path'] = pexels_5['path'].str.replace('datasets/pexels_45k/', 'datasets/pexels_5/', regex=False)\n",
    "pexels_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8652538",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_DIR = os.path.join(WORK_DIR, \"datasets\")\n",
    "PEXELS_DATASET_DIR = os.path.join(DATASETS_DIR, \"pexels_5\")\n",
    "os.makedirs(PEXELS_DATASET_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1e20a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "PEXELS_CSV_PATH = os.path.join(WORK_DIR, \"pexels_5.csv\")\n",
    "\n",
    "if not os.path.exists(PEXELS_CSV_PATH):\n",
    "    for path in pexels_5[\"path\"]:\n",
    "        match = re.search(r\"/(\\d+)_\", path)\n",
    "        video_id = match.group(1)\n",
    "        video_url = f\"https://www.pexels.com/download/video/{video_id}\"\n",
    "        video_path = os.path.join(PEXELS_DATASET_DIR, f\"{video_id}.mp4\")\n",
    "        !wget -nc -O $video_path $video_url\n",
    "        pexels_5[\"path\"] = pexels_5[\"path\"].replace(path, video_path)\n",
    "\n",
    "    pexels_5.to_csv(os.path.join(WORK_DIR, \"pexels_5.csv\"), index=False)\n",
    "else:\n",
    "    pexels_5 = pd.read_csv(PEXELS_CSV_PATH)\n",
    "\n",
    "pexels_5.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd75697f",
   "metadata": {},
   "source": [
    "## DC-AEã®è¨“ç·´"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675ee5cf",
   "metadata": {},
   "source": [
    "[ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ][1]ã‚ˆã‚Šä»¥ä¸‹ã®ã‚³ãƒžãƒ³ãƒ‰ã§é–‹å§‹ã§ãã‚‹\n",
    "\n",
    "```sh\n",
    "torchrun --nproc_per_node 8 scripts/vae/train.py configs/vae/train/video_dc_ae.py\n",
    "```\n",
    "\n",
    "[1]: https://github.com/hpcaitech/Open-Sora/blob/main/docs/hcae.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f070e0e",
   "metadata": {},
   "source": [
    "### CLIãƒ‘ãƒ¼ã‚µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45c0e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(args) -> tuple[str, argparse.Namespace]:\n",
    "    \"\"\"\n",
    "    This function parses the command line arguments.\n",
    "\n",
    "    Returns:\n",
    "        tuple[str, argparse.Namespace]: The path to the configuration file and the command line arguments.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"config\", type=str, help=\"model config file path\")\n",
    "    args, unknown_args = parser.parse_known_args(args)\n",
    "    return args.config, unknown_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4e518b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config(config_path: str) -> Config:\n",
    "    \"\"\"\n",
    "    This function reads the configuration file.\n",
    "\n",
    "    Args:\n",
    "        config_path (str): The path to the configuration file.\n",
    "\n",
    "    Returns:\n",
    "        Config: The configuration object.\n",
    "    \"\"\"\n",
    "    cfg = Config.fromfile(config_path)\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80074784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_convert(value: str) -> int | float | bool | list | dict | None:\n",
    "    \"\"\"\n",
    "    Automatically convert a string to the appropriate Python data type,\n",
    "    including int, float, bool, list, dict, etc.\n",
    "\n",
    "    Args:\n",
    "        value (str): The string to convert.\n",
    "\n",
    "    Returns:\n",
    "        int, float, bool, list |  dict: The converted value.\n",
    "    \"\"\"\n",
    "    # Handle empty string\n",
    "    if value == \"\":\n",
    "        return value\n",
    "\n",
    "    # Handle None\n",
    "    if value.lower() == \"none\":\n",
    "        return None\n",
    "\n",
    "    # Handle boolean values\n",
    "    lower_value = value.lower()\n",
    "    if lower_value == \"true\":\n",
    "        return True\n",
    "    elif lower_value == \"false\":\n",
    "        return False\n",
    "\n",
    "    # Try to convert the string to an integer or float\n",
    "    try:\n",
    "        # Try converting to an integer\n",
    "        return int(value)\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        # Try converting to a float\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    # Try to convert the string to a list, dict, tuple, etc.\n",
    "    try:\n",
    "        return ast.literal_eval(value)\n",
    "    except (ValueError, SyntaxError):\n",
    "        pass\n",
    "\n",
    "    # If all attempts fail, return the original string\n",
    "    return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7b6093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_args(cfg: Config, args: argparse.Namespace) -> Config:\n",
    "    \"\"\"\n",
    "    This function merges the configuration file and command line arguments.\n",
    "\n",
    "    Args:\n",
    "        cfg (Config): The configuration object.\n",
    "        args (argparse.Namespace): The command line arguments.\n",
    "\n",
    "    Returns:\n",
    "        Config: The configuration object.\n",
    "    \"\"\"\n",
    "    for k, v in zip(args[::2], args[1::2]):\n",
    "        assert k.startswith(\"--\"), f\"Invalid argument: {k}\"\n",
    "        k = k[2:].replace(\"-\", \"_\")\n",
    "        k_split = k.split(\".\")\n",
    "        target = cfg\n",
    "        for key in k_split[:-1]:\n",
    "            assert key in cfg, f\"Key {key} not found in config\"\n",
    "            target = target[key]\n",
    "        if v.lower() == \"none\":\n",
    "            v = None\n",
    "        elif k in target:\n",
    "            v_type = type(target[k])\n",
    "            if v_type == bool:\n",
    "                v = auto_convert(v)\n",
    "            else:\n",
    "                v = type(target[k])(v)\n",
    "        else:\n",
    "            v = auto_convert(v)\n",
    "        target[k_split[-1]] = v\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f075d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_configs(args) -> Config:\n",
    "    \"\"\"\n",
    "    This function parses the configuration file and command line arguments.\n",
    "\n",
    "    Returns:\n",
    "        Config: The configuration object.\n",
    "    \"\"\"\n",
    "    config, args = parse_args(args)\n",
    "    cfg = read_config(config)\n",
    "    cfg = merge_args(cfg, args)\n",
    "    cfg.config_path = config\n",
    "\n",
    "    # hard-coded for spatial compression\n",
    "    if cfg.get(\"ae_spatial_compression\", None) is not None:\n",
    "        os.environ[\"AE_SPATIAL_COMPRESSION\"] = str(cfg.ae_spatial_compression)\n",
    "\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa2c9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 1. configs & runtime variables\n",
    "# ======================================================\n",
    "# == parse configs ==\n",
    "# cfg = parse_configs()\n",
    "# cfg = parse_configs([\n",
    "#     \"configs/vae/train/video_dc_ae.py\",\n",
    "# ])\n",
    "\n",
    "# # ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—ç”¨ã«ä¸€éƒ¨ã‚’ä¸Šæ›¸ã\n",
    "\n",
    "# cfg.bucket_config = {\n",
    "#     \"256px_ar1:1\": {\n",
    "#         8: (1.0, 1),\n",
    "#         32: (0.0, 1),\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# cfg.epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1842d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# import os\n",
    "# import random\n",
    "# import subprocess\n",
    "# import warnings\n",
    "# from contextlib import nullcontext\n",
    "# from copy import deepcopy\n",
    "# from pprint import pformat\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "# gc.disable()\n",
    "\n",
    "\n",
    "# import torch\n",
    "# import torch.distributed as dist\n",
    "# from colossalai.booster import Booster\n",
    "# from colossalai.utils import set_seed\n",
    "# from torch.profiler import ProfilerActivity, profile, schedule\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# import wandb\n",
    "# from opensora.acceleration.checkpoint import set_grad_checkpoint\n",
    "# from opensora.acceleration.parallel_states import get_data_parallel_group\n",
    "# from opensora.datasets.dataloader import prepare_dataloader\n",
    "# from opensora.datasets.pin_memory_cache import PinMemoryCache\n",
    "# from opensora.models.vae.losses import DiscriminatorLoss, GeneratorLoss, VAELoss\n",
    "# from opensora.registry import DATASETS, MODELS, build_module\n",
    "# from opensora.utils.ckpt import CheckpointIO, model_sharding, record_model_param_shape, rm_checkpoints\n",
    "# # from opensora.utils.config import config_to_name, create_experiment_workspace, parse_configs\n",
    "# from opensora.utils.config import config_to_name, create_experiment_workspace\n",
    "# from opensora.utils.logger import create_logger\n",
    "# from opensora.utils.misc import (\n",
    "#     Timer,\n",
    "#     all_reduce_sum,\n",
    "#     create_tensorboard_writer,\n",
    "#     is_log_process,\n",
    "#     log_model_params,\n",
    "#     to_torch_dtype,\n",
    "# )\n",
    "# from opensora.utils.optimizer import create_lr_scheduler, create_optimizer\n",
    "# from opensora.utils.train import create_colossalai_plugin, set_lr, set_warmup_steps, setup_device, update_ema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9809ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# WAIT = 1\n",
    "# WARMUP = 10\n",
    "# ACTIVE = 20\n",
    "\n",
    "# my_schedule = schedule(\n",
    "#     wait=WAIT,  # number of warmup steps\n",
    "#     warmup=WARMUP,  # number of warmup steps with profiling\n",
    "#     active=ACTIVE,  # number of active steps with profiling\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc13ff7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 1. configs & runtime variables\n",
    "# ======================================================\n",
    "# == parse configs ==\n",
    "# cfg = parse_configs()\n",
    "cfg = parse_configs([\n",
    "    \"configs/vae/train/video_dc_ae.py\",\n",
    "])\n",
    "\n",
    "cfg.bucket_config = {\n",
    "    \"256px_ar1:1\": {\n",
    "        8: (1.0, 1),\n",
    "        32: (0.0, 1),\n",
    "    }\n",
    "}\n",
    "cfg.epochs = 1\n",
    "\n",
    "cfg.pin_memory_cache_pre_alloc_numels = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ef7c9e",
   "metadata": {},
   "source": [
    "### ãƒ‡ãƒã‚¤ã‚¹è¨­å®š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ea3d5c",
   "metadata": {},
   "source": [
    "### ãƒ‡ãƒ¼ã‚¿åž‹è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab26330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_torch_dtype(dtype: str | torch.dtype) -> torch.dtype:\n",
    "    \"\"\"\n",
    "    Convert a string or a torch.dtype to a torch.dtype.\n",
    "\n",
    "    Args:\n",
    "        dtype (str | torch.dtype): The input dtype.\n",
    "\n",
    "    Returns:\n",
    "        torch.dtype: The converted dtype.\n",
    "    \"\"\"\n",
    "    if isinstance(dtype, torch.dtype):\n",
    "        return dtype\n",
    "    elif isinstance(dtype, str):\n",
    "        dtype_mapping = {\n",
    "            \"float64\": torch.float64,\n",
    "            \"float32\": torch.float32,\n",
    "            \"float16\": torch.float16,\n",
    "            \"fp32\": torch.float32,\n",
    "            \"fp16\": torch.float16,\n",
    "            \"half\": torch.float16,\n",
    "            \"bf16\": torch.bfloat16,\n",
    "        }\n",
    "        if dtype not in dtype_mapping:\n",
    "            raise ValueError(f\"Unsupported dtype {dtype}\")\n",
    "        dtype = dtype_mapping[dtype]\n",
    "        return dtype\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dtype {dtype}\")\n",
    "\n",
    "# == get dtype & device ==\n",
    "dtype = to_torch_dtype(cfg.get(\"dtype\", \"bf16\"))\n",
    "dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0d7602",
   "metadata": {},
   "source": [
    "### ãƒ‡ãƒã‚¤ã‚¹è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812d7b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '12345'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e9ec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colossalai.cluster import DistCoordinator\n",
    "from colossalai.utils import get_current_device\n",
    "from datetime import timedelta\n",
    "import torch.distributed as dist\n",
    "\n",
    "def setup_device() -> tuple[torch.device, DistCoordinator]:\n",
    "    \"\"\"\n",
    "    Setup the device and the distributed coordinator.\n",
    "\n",
    "    Returns:\n",
    "        tuple[torch.device, DistCoordinator]: The device and the distributed coordinator.\n",
    "    \"\"\"\n",
    "    assert torch.cuda.is_available(), \"Training currently requires at least one GPU.\"\n",
    "    # NOTE: A very large timeout is set to avoid some processes exit early\n",
    "    dist.init_process_group(backend=\"nccl\", timeout=timedelta(hours=24))\n",
    "    torch.cuda.set_device(dist.get_rank() % torch.cuda.device_count())\n",
    "    coordinator = DistCoordinator()\n",
    "    device = get_current_device()\n",
    "\n",
    "    return device, coordinator\n",
    "\n",
    "try:\n",
    "    device, coordinator = setup_device()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    pass\n",
    "\n",
    "device, coordinator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df22215b",
   "metadata": {},
   "source": [
    "### ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b694d51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colossalai.zero.low_level import LowLevelZeroOptimizer\n",
    "from colossalai.booster import Booster\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "class CheckpointIO:\n",
    "    def __init__(self, n_write_entries: int = 32):\n",
    "        self.n_write_entries = n_write_entries\n",
    "        self.writer: Optional[Any] = None\n",
    "        self.pinned_state_dict: Optional[Dict[str, torch.Tensor]] = None\n",
    "        self.master_pinned_state_dict: Optional[Dict[str, torch.Tensor]] = None\n",
    "        self.master_writer: Optional[Any] = None\n",
    "\n",
    "    def _sync_io(self):\n",
    "        if self.writer is not None:\n",
    "            self.writer.synchronize()\n",
    "            self.writer = None\n",
    "        if self.master_writer is not None:\n",
    "            self.master_writer.synchronize()\n",
    "            self.master_writer = None\n",
    "\n",
    "    def __del__(self):\n",
    "        self._sync_io()\n",
    "\n",
    "    def _prepare_pinned_state_dict(self, ema: nn.Module, ema_shape_dict: dict):\n",
    "        if self.pinned_state_dict is None and dist.get_rank() == 0:\n",
    "            self.pinned_state_dict = _prepare_ema_pinned_state_dict(ema, ema_shape_dict)\n",
    "\n",
    "    def _prepare_master_pinned_state_dict(self, model: nn.Module, optimizer: LowLevelZeroOptimizer):\n",
    "        if self.master_pinned_state_dict is None and dist.get_rank() == 0:\n",
    "            sd = {}\n",
    "            w2m = optimizer.get_working_to_master_map()\n",
    "            for n, p in model.named_parameters():\n",
    "                master_p = w2m[id(p)]\n",
    "                sd[n] = torch.empty(p.shape, dtype=master_p.dtype, pin_memory=True, device=\"cpu\")\n",
    "            self.master_pinned_state_dict = sd\n",
    "\n",
    "    def save(\n",
    "        self,\n",
    "        booster: Booster,\n",
    "        save_dir: str,\n",
    "        model: nn.Module = None,\n",
    "        ema: nn.Module = None,\n",
    "        optimizer: Optimizer = None,\n",
    "        lr_scheduler: _LRScheduler = None,\n",
    "        sampler=None,\n",
    "        epoch: int = None,\n",
    "        step: int = None,\n",
    "        global_step: int = None,\n",
    "        batch_size: int = None,\n",
    "        lora: bool = False,\n",
    "        actual_update_step: int = None,\n",
    "        ema_shape_dict: dict = None,\n",
    "        async_io: bool = True,\n",
    "        include_master_weights: bool = False,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Save a checkpoint.\n",
    "\n",
    "        Args:\n",
    "            booster (Booster): The Booster object.\n",
    "            save_dir (str): The directory to save the checkpoint to.\n",
    "            model (nn.Module): The model to save the checkpoint from.\n",
    "            ema (nn.Module): The EMA model to save the checkpoint from.\n",
    "            optimizer (Optimizer): The optimizer to save the checkpoint from.\n",
    "            lr_scheduler (_LRScheduler): The learning rate scheduler to save the checkpoint from.\n",
    "            sampler: The sampler to save the checkpoint from.\n",
    "            epoch (int): The epoch of the checkpoint.\n",
    "            step (int): The step of the checkpoint.\n",
    "            global_step (int): The global step of the checkpoint.\n",
    "            batch_size (int): The batch size of the checkpoint.\n",
    "            lora (bool): Whether the model is trained with LoRA.\n",
    "\n",
    "        Returns:\n",
    "            str: The path to the saved checkpoint\n",
    "        \"\"\"\n",
    "        self._sync_io()\n",
    "        save_dir = os.path.join(save_dir, f\"epoch{epoch}-global_step{actual_update_step}\")\n",
    "        os.environ[\"TENSORNVME_DEBUG_LOG\"] = os.path.join(save_dir, \"async_file_io.log\")\n",
    "        if model is not None:\n",
    "            if not lora:\n",
    "                os.makedirs(os.path.join(save_dir, \"model\"), exist_ok=True)\n",
    "                booster.save_model(\n",
    "                    model,\n",
    "                    os.path.join(save_dir, \"model\"),\n",
    "                    shard=True,\n",
    "                    use_safetensors=True,\n",
    "                    size_per_shard=4096,\n",
    "                    use_async=async_io,\n",
    "                )\n",
    "            else:\n",
    "                os.makedirs(os.path.join(save_dir, \"lora\"), exist_ok=True)\n",
    "                booster.save_lora_as_pretrained(model, os.path.join(save_dir, \"lora\"))\n",
    "        if optimizer is not None:\n",
    "            booster.save_optimizer(\n",
    "                optimizer, os.path.join(save_dir, \"optimizer\"), shard=True, size_per_shard=4096, use_async=async_io\n",
    "            )\n",
    "            if include_master_weights:\n",
    "                self._prepare_master_pinned_state_dict(model, optimizer)\n",
    "                master_weights_gathering(model, optimizer, self.master_pinned_state_dict)\n",
    "        if lr_scheduler is not None:\n",
    "            booster.save_lr_scheduler(lr_scheduler, os.path.join(save_dir, \"lr_scheduler\"))\n",
    "        if ema is not None:\n",
    "            self._prepare_pinned_state_dict(ema, ema_shape_dict)\n",
    "            model_gathering(ema, ema_shape_dict, self.pinned_state_dict)\n",
    "        if dist.get_rank() == 0:\n",
    "            running_states = {\n",
    "                \"epoch\": epoch,\n",
    "                \"step\": step,\n",
    "                \"global_step\": global_step,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"actual_update_step\": actual_update_step,\n",
    "            }\n",
    "            save_json(running_states, os.path.join(save_dir, \"running_states.json\"))\n",
    "\n",
    "            if ema is not None:\n",
    "                if async_io:\n",
    "                    self.writer = async_save(os.path.join(save_dir, \"ema.safetensors\"), self.pinned_state_dict)\n",
    "                else:\n",
    "                    torch.save(ema.state_dict(), os.path.join(save_dir, \"ema.pt\"))\n",
    "\n",
    "            if sampler is not None:\n",
    "                # only for VariableVideoBatchSampler\n",
    "                torch.save(sampler.state_dict(step), os.path.join(save_dir, \"sampler\"))\n",
    "\n",
    "            if optimizer is not None and include_master_weights:\n",
    "                self.master_writer = async_save(\n",
    "                    os.path.join(save_dir, \"master.safetensors\"), self.master_pinned_state_dict\n",
    "                )\n",
    "\n",
    "        dist.barrier()\n",
    "        return save_dir\n",
    "\n",
    "    def load(\n",
    "        self,\n",
    "        booster: Booster,\n",
    "        load_dir: str,\n",
    "        model: nn.Module = None,\n",
    "        ema: nn.Module = None,\n",
    "        optimizer: Optimizer = None,\n",
    "        lr_scheduler: _LRScheduler = None,\n",
    "        sampler=None,\n",
    "        strict: bool = False,\n",
    "        include_master_weights: bool = False,\n",
    "    ) -> tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Load a checkpoint.\n",
    "\n",
    "        Args:\n",
    "            booster (Booster): The Booster object.\n",
    "            load_dir (str): The directory to load the checkpoint from.\n",
    "            model (nn.Module): The model to load the checkpoint into.\n",
    "            ema (nn.Module): The EMA model to load the checkpoint into.\n",
    "            optimizer (Optimizer): The optimizer to load the checkpoint into.\n",
    "            lr_scheduler (_LRScheduler): The learning rate scheduler to load the checkpoint into.\n",
    "            sampler: The sampler to load the checkpoint into.\n",
    "\n",
    "        Returns:\n",
    "            tuple[int, int]: The epoch and step of the checkpoint.\n",
    "        \"\"\"\n",
    "        assert os.path.exists(load_dir), f\"Checkpoint directory {load_dir} does not exist\"\n",
    "        assert os.path.exists(os.path.join(load_dir, \"running_states.json\")), \"running_states.json does not exist\"\n",
    "\n",
    "        running_states = load_json(os.path.join(load_dir, \"running_states.json\"))\n",
    "        if model is not None:\n",
    "            booster.load_model(\n",
    "                model,\n",
    "                _search_valid_path(os.path.join(load_dir, \"model\")),\n",
    "                strict=strict,\n",
    "                low_cpu_mem_mode=False,\n",
    "                num_threads=32,\n",
    "            )\n",
    "        if ema is not None:\n",
    "            if os.path.exists(os.path.join(load_dir, \"ema.safetensors\")):\n",
    "                ema_state_dict = load_file(os.path.join(load_dir, \"ema.safetensors\"))\n",
    "            else:\n",
    "                ema_state_dict = torch.load(os.path.join(load_dir, \"ema.pt\"), map_location=torch.device(\"cpu\"))\n",
    "            # ema is not boosted, so we don't use booster.load_model\n",
    "            ema.load_state_dict(ema_state_dict, strict=strict, assign=True)\n",
    "\n",
    "        if optimizer is not None:\n",
    "            booster.load_optimizer(\n",
    "                optimizer, os.path.join(load_dir, \"optimizer\"), low_cpu_mem_mode=False, num_threads=32\n",
    "            )\n",
    "            if include_master_weights:\n",
    "                master_state_dict = load_file(os.path.join(load_dir, \"master.safetensors\"))\n",
    "                load_master_weights(model, optimizer, master_state_dict)\n",
    "        if lr_scheduler is not None:\n",
    "            booster.load_lr_scheduler(lr_scheduler, os.path.join(load_dir, \"lr_scheduler\"))\n",
    "        if sampler is not None:\n",
    "            sampler.load_state_dict(torch.load(os.path.join(load_dir, \"sampler\")))\n",
    "\n",
    "        dist.barrier()\n",
    "\n",
    "        return (running_states[\"epoch\"], running_states[\"step\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb418ffc",
   "metadata": {},
   "source": [
    "### ã‚·ãƒ¼ãƒ‰å›ºå®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb12a148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colossalai.utils import set_seed\n",
    "set_seed(cfg.get(\"seed\", 1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c715793e",
   "metadata": {},
   "source": [
    "### ãƒ”ãƒ³ãƒ¡ãƒ¢ãƒªãƒ¼è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c15fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from typing import Dict, List, Optional\n",
    "import torch\n",
    "\n",
    "class PinMemoryCache:\n",
    "    force_dtype: Optional[torch.dtype] = None\n",
    "    min_cache_numel: int = 0\n",
    "    pre_alloc_numels: List[int] = []\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cache: Dict[int, torch.Tensor] = {}\n",
    "        self.output_to_cache: Dict[int, int] = {}\n",
    "        self.cache_to_output: Dict[int, int] = {}\n",
    "        self.lock = threading.Lock()\n",
    "        self.total_cnt = 0\n",
    "        self.hit_cnt = 0\n",
    "\n",
    "        if len(self.pre_alloc_numels) > 0 and self.force_dtype is not None:\n",
    "            for n in self.pre_alloc_numels:\n",
    "                cache_tensor = torch.empty(n, dtype=self.force_dtype, device=\"cpu\", pin_memory=True)\n",
    "                with self.lock:\n",
    "                    self.cache[id(cache_tensor)] = cache_tensor\n",
    "\n",
    "    def get(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Receive a cpu tensor and return the corresponding pinned tensor. Note that this only manage memory allocation, doesn't copy content.\n",
    "\n",
    "        Args:\n",
    "            tensor (torch.Tensor): The tensor to be pinned.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The pinned tensor.\n",
    "        \"\"\"\n",
    "        self.total_cnt += 1\n",
    "        with self.lock:\n",
    "            # find free cache\n",
    "            for cache_id, cache_tensor in self.cache.items():\n",
    "                if cache_id not in self.cache_to_output and cache_tensor.numel() >= tensor.numel():\n",
    "                    target_cache_tensor = cache_tensor[: tensor.numel()].view(tensor.shape)\n",
    "                    out_id = id(target_cache_tensor)\n",
    "                    self.output_to_cache[out_id] = cache_id\n",
    "                    self.cache_to_output[cache_id] = out_id\n",
    "                    self.hit_cnt += 1\n",
    "                    return target_cache_tensor\n",
    "        # no free cache, create a new one\n",
    "        dtype = self.force_dtype if self.force_dtype is not None else tensor.dtype\n",
    "        cache_numel = max(tensor.numel(), self.min_cache_numel)\n",
    "        cache_tensor = torch.empty(cache_numel, dtype=dtype, device=\"cpu\", pin_memory=True)\n",
    "        target_cache_tensor = cache_tensor[: tensor.numel()].view(tensor.shape)\n",
    "        out_id = id(target_cache_tensor)\n",
    "        with self.lock:\n",
    "            self.cache[id(cache_tensor)] = cache_tensor\n",
    "            self.output_to_cache[out_id] = id(cache_tensor)\n",
    "            self.cache_to_output[id(cache_tensor)] = out_id\n",
    "        return target_cache_tensor\n",
    "\n",
    "    def remove(self, output_tensor: torch.Tensor) -> None:\n",
    "        \"\"\"Release corresponding cache tensor.\n",
    "\n",
    "        Args:\n",
    "            output_tensor (torch.Tensor): The tensor to be released.\n",
    "        \"\"\"\n",
    "        out_id = id(output_tensor)\n",
    "        with self.lock:\n",
    "            if out_id not in self.output_to_cache:\n",
    "                raise ValueError(\"Tensor not found in cache.\")\n",
    "            cache_id = self.output_to_cache.pop(out_id)\n",
    "            del self.cache_to_output[cache_id]\n",
    "\n",
    "    def __str__(self):\n",
    "        with self.lock:\n",
    "            num_cached = len(self.cache)\n",
    "            num_used = len(self.output_to_cache)\n",
    "            total_cache_size = sum([v.numel() * v.element_size() for v in self.cache.values()])\n",
    "        return f\"PinMemoryCache(num_cached={num_cached}, num_used={num_used}, total_cache_size={total_cache_size / 1024**3:.2f} GB, hit rate={self.hit_cnt / self.total_cnt:.2f})\"\n",
    "\n",
    "PinMemoryCache.force_dtype = dtype\n",
    "pin_memory_cache_pre_alloc_numels = cfg.get(\"pin_memory_cache_pre_alloc_numels\", None)\n",
    "PinMemoryCache.pre_alloc_numels = pin_memory_cache_pre_alloc_numels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d0950f",
   "metadata": {},
   "source": [
    "### CollosalAI Boosterã®åˆæœŸåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aab6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == init ColossalAI booster ==\n",
    "plugin_type = cfg.get(\"plugin\", \"zero2\")\n",
    "plugin_config = cfg.get(\"plugin_config\", {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4be0847",
   "metadata": {},
   "outputs": [],
   "source": [
    "_GLOBAL_PARALLEL_GROUPS = dict()\n",
    "\n",
    "def set_data_parallel_group(group: dist.ProcessGroup):\n",
    "    _GLOBAL_PARALLEL_GROUPS[\"data\"] = group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fac278",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import warnings\n",
    "from collections import OrderedDict\n",
    "from datetime import timedelta\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn.functional as F\n",
    "from colossalai.booster.plugin import HybridParallelPlugin, LowLevelZeroPlugin\n",
    "from colossalai.cluster import DistCoordinator\n",
    "from colossalai.utils import get_current_device\n",
    "from einops import rearrange\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_colossalai_plugin(\n",
    "    plugin: str,\n",
    "    dtype: str,\n",
    "    grad_clip: float,\n",
    "    **kwargs,\n",
    ") -> LowLevelZeroPlugin | HybridParallelPlugin:\n",
    "    \"\"\"\n",
    "    Create a ColossalAI plugin.\n",
    "\n",
    "    Args:\n",
    "        plugin (str): The plugin name.\n",
    "        dtype (str): The data type.\n",
    "        grad_clip (float): The gradient clip value.\n",
    "\n",
    "    Returns:\n",
    "        LowLevelZeroPlugin |  HybridParallelPlugin: The plugin.\n",
    "    \"\"\"\n",
    "    plugin_kwargs = dict(\n",
    "        precision=dtype,\n",
    "        initial_scale=2**16,\n",
    "        max_norm=grad_clip,\n",
    "        overlap_allgather=True,\n",
    "        cast_inputs=False,\n",
    "        reduce_bucket_size_in_m=20,\n",
    "    )\n",
    "    plugin_kwargs.update(kwargs)\n",
    "    sp_size = plugin_kwargs.get(\"sp_size\", 1)\n",
    "    if plugin == \"zero1\" or plugin == \"zero2\":\n",
    "        assert sp_size == 1, \"Zero plugin does not support sequence parallelism\"\n",
    "        stage = 1 if plugin == \"zero1\" else 2\n",
    "        plugin = LowLevelZeroPlugin(\n",
    "            stage=stage,\n",
    "            **plugin_kwargs,\n",
    "        )\n",
    "        set_data_parallel_group(dist.group.WORLD)\n",
    "    elif plugin == \"hybrid\":\n",
    "        plugin_kwargs[\"find_unused_parameters\"] = True\n",
    "        reduce_bucket_size_in_m = plugin_kwargs.pop(\"reduce_bucket_size_in_m\")\n",
    "        if \"zero_bucket_size_in_m\" not in plugin_kwargs:\n",
    "            plugin_kwargs[\"zero_bucket_size_in_m\"] = reduce_bucket_size_in_m\n",
    "        plugin_kwargs.pop(\"cast_inputs\")\n",
    "        plugin_kwargs[\"enable_metadata_cache\"] = False\n",
    "\n",
    "        custom_policy = plugin_kwargs.pop(\"custom_policy\", None)\n",
    "        if custom_policy is not None:\n",
    "            custom_policy = custom_policy()\n",
    "        plugin = HybridParallelPlugin(\n",
    "            custom_policy=custom_policy,\n",
    "            **plugin_kwargs,\n",
    "        )\n",
    "        set_tensor_parallel_group(plugin.tp_group)\n",
    "        set_sequence_parallel_group(plugin.sp_group)\n",
    "        set_data_parallel_group(plugin.dp_group)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown plugin {plugin}\")\n",
    "    return plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d544b486",
   "metadata": {},
   "outputs": [],
   "source": [
    "plugin = (\n",
    "    create_colossalai_plugin(\n",
    "        plugin=plugin_type,\n",
    "        dtype=cfg.get(\"dtype\", \"bf16\"),\n",
    "        grad_clip=cfg.get(\"grad_clip\", 0),\n",
    "        **plugin_config,\n",
    "    )\n",
    "    if plugin_type != \"none\"\n",
    "    else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f02f646",
   "metadata": {},
   "outputs": [],
   "source": [
    "booster = Booster(plugin=plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd105e65",
   "metadata": {},
   "source": [
    "### å®Ÿé¨“ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe9fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import ast\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from mmengine.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35ae1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_distributed() -> bool:\n",
    "    \"\"\"\n",
    "    Check if the code is running in a distributed setting.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if running in a distributed setting, False otherwise\n",
    "    \"\"\"\n",
    "    return os.environ.get(\"WORLD_SIZE\", None) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029d68e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_main_process() -> bool:\n",
    "    \"\"\"\n",
    "    Check if the current process is the main process.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the current process is the main process, False otherwise.\n",
    "    \"\"\"\n",
    "    return not is_distributed() or dist.get_rank() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce900e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_to_name(cfg: Config) -> str:\n",
    "    filename = cfg._filename\n",
    "    filename = filename.replace(\"configs/\", \"\")\n",
    "    filename = filename.replace(\".py\", \"\")\n",
    "    filename = filename.replace(\"/\", \"_\")\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91b1b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync_string(value: str):\n",
    "    \"\"\"\n",
    "    This function synchronizes a string across all processes.\n",
    "    \"\"\"\n",
    "    if not is_distributed():\n",
    "        return value\n",
    "    bytes_value = value.encode(\"utf-8\")\n",
    "    max_len = 256\n",
    "    bytes_tensor = torch.zeros(max_len, dtype=torch.uint8).cuda()\n",
    "    bytes_tensor[: len(bytes_value)] = torch.tensor(\n",
    "        list(bytes_value), dtype=torch.uint8\n",
    "    )\n",
    "    torch.distributed.broadcast(bytes_tensor, 0)\n",
    "    synced_value = bytes_tensor.cpu().numpy().tobytes().decode(\"utf-8\").rstrip(\"\\x00\")\n",
    "    return synced_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951af3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_experiment_workspace(\n",
    "    output_dir: str, model_name: str = None, config: dict = None, exp_name: str = None\n",
    ") -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    This function creates a folder for experiment tracking.\n",
    "\n",
    "    Args:\n",
    "        output_dir: The path to the output directory.\n",
    "        model_name: The name of the model.\n",
    "        exp_name: The given name of the experiment, if None will use default.\n",
    "\n",
    "    Returns:\n",
    "        tuple[str, str]: The experiment name and the experiment directory.\n",
    "    \"\"\"\n",
    "    if exp_name is None:\n",
    "        # Make outputs folder (holds all experiment subfolders)\n",
    "        experiment_index = datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
    "        experiment_index = sync_string(experiment_index)\n",
    "        # Create an experiment folder\n",
    "        model_name = (\n",
    "            \"-\" + model_name.replace(\"/\", \"-\") if model_name is not None else \"\"\n",
    "        )\n",
    "        exp_name = f\"{experiment_index}{model_name}\"\n",
    "    exp_dir = f\"{output_dir}/{exp_name}\"\n",
    "    if is_main_process():\n",
    "        os.makedirs(exp_dir, exist_ok=True)\n",
    "        # Save the config\n",
    "        with open(f\"{exp_dir}/config.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(config, f, indent=4)\n",
    "\n",
    "    return exp_name, exp_dir\n",
    "\n",
    "# == init exp_dir ==\n",
    "exp_name, exp_dir = create_experiment_workspace(\n",
    "    cfg.get(\"outputs\", \"./outputs\"),\n",
    "    model_name=config_to_name(cfg),\n",
    "    config=cfg.to_dict(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb11acf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pipeline_enabled(plugin_type: str, plugin_config: dict) -> bool:\n",
    "    return plugin_type == \"hybrid\" and plugin_config.get(\"pp_size\", 1) > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179e430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_log_process(plugin_type: str, plugin_config: dict) -> bool:\n",
    "    if is_pipeline_enabled(plugin_type, plugin_config):\n",
    "        return dist.get_rank() == dist.get_world_size() - 1\n",
    "    return dist.get_rank() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49473b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_log_process(plugin_type, plugin_config):\n",
    "    print(f\"changing {exp_dir} to share\")\n",
    "    os.system(f\"chgrp -R share {exp_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bde602",
   "metadata": {},
   "source": [
    "### ãƒ­ã‚¬ãƒ¼ã®åˆæœŸåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdd85d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_logger(logging_dir: str = None) -> logging.Logger:\n",
    "    \"\"\"\n",
    "    Create a logger that writes to a log file and stdout. Only the main process logs.\n",
    "\n",
    "    Args:\n",
    "        logging_dir (str): The directory to save the log file.\n",
    "\n",
    "    Returns:\n",
    "        logging.Logger: The logger.\n",
    "    \"\"\"\n",
    "    if is_main_process():\n",
    "        additional_args = dict()\n",
    "        if logging_dir is not None:\n",
    "            additional_args[\"handlers\"] = [\n",
    "                logging.StreamHandler(),\n",
    "                logging.FileHandler(f\"{logging_dir}/log.txt\"),\n",
    "            ]\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format=\"[\\033[34m%(asctime)s\\033[0m] %(message)s\",\n",
    "            datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "            **additional_args,\n",
    "        )\n",
    "        logger = logging.getLogger(__name__)\n",
    "        if logging_dir is not None:\n",
    "            logger.info(\"Experiment directory created at %s\", logging_dir)\n",
    "    else:\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.addHandler(logging.NullHandler())\n",
    "    return logger\n",
    "\n",
    "from pprint import pformat\n",
    "\n",
    "logger = create_logger(exp_dir)\n",
    "logger.info(\"Training configuration:\\n %s\", pformat(cfg.to_dict()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3471e241",
   "metadata": {},
   "source": [
    "### Tensorboardã¨WandBã®åˆæœŸåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922484f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19da0ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensorboard_writer(exp_dir: str) -> SummaryWriter:\n",
    "    \"\"\"\n",
    "    Create a tensorboard writer.\n",
    "\n",
    "    Args:\n",
    "        exp_dir (str): The directory to save tensorboard logs.\n",
    "\n",
    "    Returns:\n",
    "        SummaryWriter: The tensorboard writer.\n",
    "    \"\"\"\n",
    "    tensorboard_dir = f\"{exp_dir}/tensorboard\"\n",
    "    os.makedirs(tensorboard_dir, exist_ok=True)\n",
    "    writer = SummaryWriter(tensorboard_dir)\n",
    "    return writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab51d1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == init logger, tensorboard & wandb ==\n",
    "tb_writer = None\n",
    "\n",
    "# True\n",
    "if coordinator.is_master():\n",
    "    tb_writer = create_tensorboard_writer(exp_dir)\n",
    "\n",
    "    # False\n",
    "    if cfg.get(\"wandb\", False):\n",
    "        wandb.init(\n",
    "            project=cfg.get(\"wandb_project\", \"Open-Sora\"),\n",
    "            name=cfg.get(\"wandb_expr_name\", exp_name),\n",
    "            config=cfg.to_dict(),\n",
    "            dir=exp_dir,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84fc906",
   "metadata": {},
   "source": [
    "### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ§‹ç¯‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccabf78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 2. build dataset and dataloader\n",
    "# ======================================================\n",
    "logger.info(f\"Building dataset... {cfg.dataset=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b89691",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.dataset[\"data_path\"] = PEXELS_DATASET_DIR\n",
    "cfg.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f0740c",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_KEYS = (\"neg\", \"path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127799df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @DATASETS.register_module(\"text\")\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for text data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str = None,\n",
    "        tokenize_fn: callable = None,\n",
    "        fps_max: int = 16,\n",
    "        vmaf: bool = False,\n",
    "        memory_efficient: bool = False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.data_path = data_path\n",
    "        self.data = read_file(data_path, memory_efficient=memory_efficient)\n",
    "        self.memory_efficient = memory_efficient\n",
    "        self.tokenize_fn = tokenize_fn\n",
    "        self.vmaf = vmaf\n",
    "\n",
    "        if fps_max is not None:\n",
    "            self.fps_max = fps_max\n",
    "        else:\n",
    "            self.fps_max = 999999999\n",
    "\n",
    "    def to_efficient(self):\n",
    "        if self.memory_efficient:\n",
    "            addition_data_path = self.data_path.split(\".\")[0]\n",
    "            self._data = self.data\n",
    "            self.data = EfficientParquet(self._data, addition_data_path)\n",
    "\n",
    "    def getitem(self, index: int) -> dict:\n",
    "        ret = dict()\n",
    "        sample = self.data.iloc[index].to_dict()\n",
    "        sample_fps = sample.get(\"fps\", np.nan)\n",
    "        new_fps, sampling_interval = map_target_fps(sample_fps, self.fps_max)\n",
    "        ret.update({\"sampling_interval\": sampling_interval})\n",
    "\n",
    "        if \"text\" in sample:\n",
    "            ret[\"text\"] = sample.pop(\"text\")\n",
    "            postfixs = []\n",
    "            if new_fps != 0 and self.fps_max < 999:\n",
    "                postfixs.append(f\"{new_fps} FPS\")\n",
    "            if self.vmaf and \"score_vmafmotion\" in sample and not np.isnan(sample[\"score_vmafmotion\"]):\n",
    "                postfixs.append(f\"{int(sample['score_vmafmotion'] + 0.5)} motion score\")\n",
    "            postfix = \" \" + \", \".join(postfixs) + \".\" if postfixs else \"\"\n",
    "            ret[\"text\"] = ret[\"text\"] + postfix\n",
    "            if self.tokenize_fn is not None:\n",
    "                ret.update({k: v.squeeze(0) for k, v in self.tokenize_fn(ret[\"text\"]).items()})\n",
    "\n",
    "        if \"ref\" in sample:  # i2v & v2v reference\n",
    "            ret[\"ref\"] = sample.pop(\"ref\")\n",
    "\n",
    "        # name of the generated sample\n",
    "        if \"name\" in sample:  # sample name (`dataset_idx`)\n",
    "            ret[\"name\"] = sample.pop(\"name\")\n",
    "        else:\n",
    "            ret[\"index\"] = index  # use index for name\n",
    "        valid_sample = {k: v for k, v in sample.items() if k in VALID_KEYS}\n",
    "        ret.update(valid_sample)\n",
    "        return ret\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.getitem(index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e1ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_file(input_path, memory_efficient=False):\n",
    "    if input_path.endswith(\".csv\"):\n",
    "        assert not memory_efficient, \"Memory efficient mode is not supported for CSV files\"\n",
    "        return pd.read_csv(input_path)\n",
    "    elif input_path.endswith(\".parquet\"):\n",
    "        columns = None\n",
    "        if memory_efficient:\n",
    "            columns = [\"path\", \"num_frames\", \"height\", \"width\", \"aspect_ratio\", \"fps\", \"resolution\"]\n",
    "        if SUPPORT_DASK:\n",
    "            ret = dd.read_parquet(input_path, columns=columns).compute()\n",
    "        else:\n",
    "            ret = pd.read_parquet(input_path, columns=columns)\n",
    "        return ret\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Unsupported file format: {input_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f065bb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_img(path):\n",
    "    ext = os.path.splitext(path)[-1].lower()\n",
    "    return ext in IMG_EXTENSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718b6c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from fractions import Fraction\n",
    "\n",
    "import av\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import get_video_backend\n",
    "from torchvision.io.video import _check_av_available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f3be00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_from_stream(\n",
    "    video_frames,\n",
    "    container: \"av.container.Container\",\n",
    "    start_offset: float,\n",
    "    end_offset: float,\n",
    "    pts_unit: str,\n",
    "    stream: \"av.stream.Stream\",\n",
    "    stream_name: dict[str, int | tuple[int, ...] | list[int] | None],\n",
    "    filename: str | None = None,\n",
    ") -> list[\"av.frame.Frame\"]:\n",
    "    if pts_unit == \"sec\":\n",
    "        # TODO: we should change all of this from ground up to simply take\n",
    "        # sec and convert to MS in C++\n",
    "        start_offset = int(math.floor(start_offset * (1 / stream.time_base)))\n",
    "        if end_offset != float(\"inf\"):\n",
    "            end_offset = int(math.ceil(end_offset * (1 / stream.time_base)))\n",
    "    else:\n",
    "        warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
    "\n",
    "    should_buffer = True\n",
    "    max_buffer_size = 5\n",
    "    if stream.type == \"video\":\n",
    "        # DivX-style packed B-frames can have out-of-order pts (2 frames in a single pkt)\n",
    "        # so need to buffer some extra frames to sort everything\n",
    "        # properly\n",
    "        extradata = stream.codec_context.extradata\n",
    "        # overly complicated way of finding if `divx_packed` is set, following\n",
    "        # https://github.com/FFmpeg/FFmpeg/commit/d5a21172283572af587b3d939eba0091484d3263\n",
    "        if extradata and b\"DivX\" in extradata:\n",
    "            # can't use regex directly because of some weird characters sometimes...\n",
    "            pos = extradata.find(b\"DivX\")\n",
    "            d = extradata[pos:]\n",
    "            o = re.search(rb\"DivX(\\d+)Build(\\d+)(\\w)\", d)\n",
    "            if o is None:\n",
    "                o = re.search(rb\"DivX(\\d+)b(\\d+)(\\w)\", d)\n",
    "            if o is not None:\n",
    "                should_buffer = o.group(3) == b\"p\"\n",
    "    seek_offset = start_offset\n",
    "    # some files don't seek to the right location, so better be safe here\n",
    "    seek_offset = max(seek_offset - 1, 0)\n",
    "    if should_buffer:\n",
    "        # FIXME this is kind of a hack, but we will jump to the previous keyframe\n",
    "        # so this will be safe\n",
    "        seek_offset = max(seek_offset - max_buffer_size, 0)\n",
    "    try:\n",
    "        # TODO check if stream needs to always be the video stream here or not\n",
    "        container.seek(seek_offset, any_frame=False, backward=True, stream=stream)\n",
    "    except av.AVError as e:\n",
    "        print(f\"[Warning] Error while seeking video {filename}: {e}\")\n",
    "        return []\n",
    "\n",
    "    # == main ==\n",
    "    buffer_count = 0\n",
    "    frames_pts = []\n",
    "    cnt = 0\n",
    "    try:\n",
    "        for _idx, frame in enumerate(container.decode(**stream_name)):\n",
    "            frames_pts.append(frame.pts)\n",
    "            video_frames[cnt] = frame.to_rgb().to_ndarray()\n",
    "            cnt += 1\n",
    "            if cnt >= len(video_frames):\n",
    "                break\n",
    "            if frame.pts >= end_offset:\n",
    "                if should_buffer and buffer_count < max_buffer_size:\n",
    "                    buffer_count += 1\n",
    "                    continue\n",
    "                break\n",
    "    except av.AVError as e:\n",
    "        print(f\"[Warning] Error while reading video {filename}: {e}\")\n",
    "\n",
    "    # garbage collection for thread leakage\n",
    "    container.close()\n",
    "    del container\n",
    "    # NOTE: manually garbage collect to close pyav threads\n",
    "    gc.collect()\n",
    "\n",
    "    # ensure that the results are sorted wrt the pts\n",
    "    # NOTE: here we assert frames_pts is sorted\n",
    "    start_ptr = 0\n",
    "    end_ptr = cnt\n",
    "    while start_ptr < end_ptr and frames_pts[start_ptr] < start_offset:\n",
    "        start_ptr += 1\n",
    "    while start_ptr < end_ptr and frames_pts[end_ptr - 1] > end_offset:\n",
    "        end_ptr -= 1\n",
    "    if start_offset > 0 and start_offset not in frames_pts[start_ptr:end_ptr]:\n",
    "        # if there is no frame that exactly matches the pts of start_offset\n",
    "        # add the last frame smaller than start_offset, to guarantee that\n",
    "        # we will have all the necessary data. This is most useful for audio\n",
    "        if start_ptr > 0:\n",
    "            start_ptr -= 1\n",
    "    result = video_frames[start_ptr:end_ptr].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392ecae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video_av(\n",
    "    filename: str,\n",
    "    start_pts: float | Fraction = 0,\n",
    "    end_pts: float | Fraction | None = None,\n",
    "    pts_unit: str = \"pts\",\n",
    "    output_format: str = \"THWC\",\n",
    ") -> tuple[torch.Tensor, torch.Tensor, dict]:\n",
    "    \"\"\"\n",
    "    Reads a video from a file, returning both the video frames and the audio frames\n",
    "\n",
    "    This method is modified from torchvision.io.video.read_video, with the following changes:\n",
    "\n",
    "    1. will not extract audio frames and return empty for aframes\n",
    "    2. remove checks and only support pyav\n",
    "    3. add container.close() and gc.collect() to avoid thread leakage\n",
    "    4. try our best to avoid memory leak\n",
    "\n",
    "    Args:\n",
    "        filename (str): path to the video file\n",
    "        start_pts (int if pts_unit = 'pts', float / Fraction if pts_unit = 'sec', optional):\n",
    "            The start presentation time of the video\n",
    "        end_pts (int if pts_unit = 'pts', float / Fraction if pts_unit = 'sec', optional):\n",
    "            The end presentation time\n",
    "        pts_unit (str, optional): unit in which start_pts and end_pts values will be interpreted,\n",
    "            either 'pts' or 'sec'. Defaults to 'pts'.\n",
    "        output_format (str, optional): The format of the output video tensors. Can be either \"THWC\" (default) or \"TCHW\".\n",
    "\n",
    "    Returns:\n",
    "        vframes (Tensor[T, H, W, C] or Tensor[T, C, H, W]): the `T` video frames\n",
    "        aframes (Tensor[K, L]): the audio frames, where `K` is the number of channels and `L` is the number of points\n",
    "        info (dict): metadata for the video and audio. Can contain the fields video_fps (float) and audio_fps (int)\n",
    "    \"\"\"\n",
    "    # format\n",
    "    output_format = output_format.upper()\n",
    "    if output_format not in (\"THWC\", \"TCHW\"):\n",
    "        raise ValueError(f\"output_format should be either 'THWC' or 'TCHW', got {output_format}.\")\n",
    "    # file existence\n",
    "    if not os.path.exists(filename):\n",
    "        raise RuntimeError(f\"File not found: {filename}\")\n",
    "    # backend check\n",
    "    assert get_video_backend() == \"pyav\", \"pyav backend is required for read_video_av\"\n",
    "    _check_av_available()\n",
    "    # end_pts check\n",
    "    if end_pts is None:\n",
    "        end_pts = float(\"inf\")\n",
    "    if end_pts < start_pts:\n",
    "        raise ValueError(f\"end_pts should be larger than start_pts, got start_pts={start_pts} and end_pts={end_pts}\")\n",
    "\n",
    "    # == get video info ==\n",
    "    info = {}\n",
    "    # TODO: creating an container leads to memory leak (1G for 8 workers 1 GPU)\n",
    "    container = av.open(filename, metadata_errors=\"ignore\")\n",
    "    # fps\n",
    "    video_fps = container.streams.video[0].average_rate\n",
    "    # guard against potentially corrupted files\n",
    "    if video_fps is not None:\n",
    "        info[\"video_fps\"] = float(video_fps)\n",
    "    iter_video = container.decode(**{\"video\": 0})\n",
    "    frame = next(iter_video).to_rgb().to_ndarray()\n",
    "    height, width = frame.shape[:2]\n",
    "    total_frames = container.streams.video[0].frames\n",
    "    if total_frames == 0:\n",
    "        total_frames = MAX_NUM_FRAMES\n",
    "        warnings.warn(f\"total_frames is 0, using {MAX_NUM_FRAMES} as a fallback\")\n",
    "    container.close()\n",
    "    del container\n",
    "\n",
    "    # HACK: must create before iterating stream\n",
    "    # use np.zeros will not actually allocate memory\n",
    "    # use np.ones will lead to a little memory leak\n",
    "    video_frames = np.zeros((total_frames, height, width, 3), dtype=np.uint8)\n",
    "\n",
    "    # == read ==\n",
    "    try:\n",
    "        # TODO: The reading has memory leak (4G for 8 workers 1 GPU)\n",
    "        container = av.open(filename, metadata_errors=\"ignore\")\n",
    "        assert container.streams.video is not None\n",
    "        video_frames = _read_from_stream(\n",
    "            video_frames,\n",
    "            container,\n",
    "            start_pts,\n",
    "            end_pts,\n",
    "            pts_unit,\n",
    "            container.streams.video[0],\n",
    "            {\"video\": 0},\n",
    "            filename=filename,\n",
    "        )\n",
    "    except av.AVError as e:\n",
    "        print(f\"[Warning] Error while reading video {filename}: {e}\")\n",
    "\n",
    "    vframes = torch.from_numpy(video_frames).clone()\n",
    "    del video_frames\n",
    "    if output_format == \"TCHW\":\n",
    "        # [T,H,W,C] --> [T,C,H,W]\n",
    "        vframes = vframes.permute(0, 3, 1, 2)\n",
    "\n",
    "    aframes = torch.empty((1, 0), dtype=torch.float32)\n",
    "    return vframes, aframes, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef793e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video(video_path, backend=\"av\"):\n",
    "    if backend == \"cv2\":\n",
    "        vframes, vinfo = read_video_cv2(video_path)\n",
    "    elif backend == \"av\":\n",
    "        vframes, _, vinfo = read_video_av(filename=video_path, pts_unit=\"sec\", output_format=\"TCHW\")\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    return vframes, vinfo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaf5872",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalRandomCrop(object):\n",
    "    \"\"\"Temporally crop the given frame indices at a random location.\n",
    "\n",
    "    Args:\n",
    "            size (int): Desired length of frames will be seen in the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, total_frames):\n",
    "        rand_end = max(0, total_frames - self.size - 1)\n",
    "        begin_index = random.randint(0, rand_end)\n",
    "        end_index = min(begin_index + self.size, total_frames)\n",
    "        return begin_index, end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401ad364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_random_crop(\n",
    "    vframes: torch.Tensor, num_frames: int, frame_interval: int, return_frame_indices: bool = False\n",
    ") -> torch.Tensor | tuple[torch.Tensor, np.ndarray]:\n",
    "    # temporal_sample = video_transforms.temporalrandomcrop(num_frames * frame_interval)\n",
    "    temporal_sample = TemporalRandomCrop(num_frames * frame_interval)\n",
    "\n",
    "    total_frames = len(vframes)\n",
    "    start_frame_ind, end_frame_ind = temporal_sample(total_frames)\n",
    "\n",
    "    assert (\n",
    "        end_frame_ind - start_frame_ind >= num_frames\n",
    "    ), f\"Not enough frames to sample, {end_frame_ind} - {start_frame_ind} < {num_frames}\"\n",
    "\n",
    "    frame_indices = np.linspace(start_frame_ind, end_frame_ind - 1, num_frames, dtype=int)\n",
    "    video = vframes[frame_indices]\n",
    "    if return_frame_indices:\n",
    "        return video, frame_indices\n",
    "    else:\n",
    "        return video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8377620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_tensor_video_clip(clip):\n",
    "    if not torch.is_tensor(clip):\n",
    "        raise TypeError(\"clip should be Tensor. Got %s\" % type(clip))\n",
    "\n",
    "    if not clip.ndimension() == 4:\n",
    "        raise ValueError(\"clip should be 4D. Got %dD\" % clip.dim())\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9071d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(clip):\n",
    "    \"\"\"\n",
    "    Convert tensor data type from uint8 to float, divide value by 255.0 and\n",
    "    permute the dimensions of clip tensor\n",
    "    Args:\n",
    "        clip (torch.tensor, dtype=torch.uint8): Size is (T, C, H, W)\n",
    "    Return:\n",
    "        clip (torch.tensor, dtype=torch.float): Size is (T, C, H, W)\n",
    "    \"\"\"\n",
    "    _is_tensor_video_clip(clip)\n",
    "    if not clip.dtype == torch.uint8:\n",
    "        raise TypeError(\"clip tensor should have data type uint8. Got %s\" % str(clip.dtype))\n",
    "    # return clip.float().permute(3, 0, 1, 2) / 255.0\n",
    "    return clip.float() / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8923557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensorVideo:\n",
    "    \"\"\"\n",
    "    Convert tensor data type from uint8 to float, divide value by 255.0 and\n",
    "    permute the dimensions of clip tensor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            clip (torch.tensor, dtype=torch.uint8): Size is (T, C, H, W)\n",
    "        Return:\n",
    "            clip (torch.tensor, dtype=torch.float): Size is (T, C, H, W)\n",
    "        \"\"\"\n",
    "        return to_tensor(clip)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aceaf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UCFCenterCropVideo:\n",
    "    \"\"\"\n",
    "    First scale to the specified size in equal proportion to the short edge,\n",
    "    then center cropping\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        size,\n",
    "        interpolation_mode=\"bilinear\",\n",
    "    ):\n",
    "        if isinstance(size, tuple):\n",
    "            if len(size) != 2:\n",
    "                raise ValueError(f\"size should be tuple (height, width), instead got {size}\")\n",
    "            self.size = size\n",
    "        else:\n",
    "            self.size = (size, size)\n",
    "\n",
    "        self.interpolation_mode = interpolation_mode\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            clip (torch.tensor): Video clip to be cropped. Size is (T, C, H, W)\n",
    "        Returns:\n",
    "            torch.tensor: scale resized / center cropped video clip.\n",
    "                size is (T, C, crop_size, crop_size)\n",
    "        \"\"\"\n",
    "        clip_resize = resize_scale(clip=clip, target_size=self.size, interpolation_mode=self.interpolation_mode)\n",
    "        clip_center_crop = center_crop(clip_resize, self.size)\n",
    "        return clip_center_crop\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}(size={self.size}, interpolation_mode={self.interpolation_mode}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d33f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numbers\n",
    "\n",
    "# - This file is adapted from https://github.com/Vchitect/Latte/blob/main/datasets/video_transforms.py\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a274c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(clip, target_size, interpolation_mode):\n",
    "    if len(target_size) != 2:\n",
    "        raise ValueError(f\"target size should be tuple (height, width), instead got {target_size}\")\n",
    "    return torch.nn.functional.interpolate(clip, size=target_size, mode=interpolation_mode, align_corners=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906de606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(clip, i, j, h, w):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        clip (torch.tensor): Video clip to be cropped. Size is (T, C, H, W)\n",
    "    \"\"\"\n",
    "    if len(clip.size()) != 4:\n",
    "        raise ValueError(\"clip should be a 4D tensor\")\n",
    "    return clip[..., i : i + h, j : j + w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f342bc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_crop_to_fill(clip, target_size):\n",
    "    if not _is_tensor_video_clip(clip):\n",
    "        raise ValueError(\"clip should be a 4D torch.tensor\")\n",
    "    h, w = clip.size(-2), clip.size(-1)\n",
    "    th, tw = target_size[0], target_size[1]\n",
    "    rh, rw = th / h, tw / w\n",
    "    if rh > rw:\n",
    "        sh, sw = th, round(w * rh)\n",
    "        clip = resize(clip, (sh, sw), \"bilinear\")\n",
    "        i = 0\n",
    "        j = int(round(sw - tw) / 2.0)\n",
    "    else:\n",
    "        sh, sw = round(h * rw), tw\n",
    "        clip = resize(clip, (sh, sw), \"bilinear\")\n",
    "        i = int(round(sh - th) / 2.0)\n",
    "        j = 0\n",
    "    assert i + th <= clip.size(-2) and j + tw <= clip.size(-1)\n",
    "    return crop(clip, i, j, th, tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65868e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeCrop:\n",
    "    def __init__(self, size):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        clip = resize_crop_to_fill(clip, self.size)\n",
    "        return clip\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}(size={self.size})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac14138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms_video(name=\"center\", image_size=(256, 256)):\n",
    "    if name is None:\n",
    "        return None\n",
    "    elif name == \"center\":\n",
    "        assert image_size[0] == image_size[1], \"image_size must be square for center crop\"\n",
    "        transform_video = transforms.Compose(\n",
    "            [\n",
    "                # video_transforms.ToTensorVideo(),  # TCHW\n",
    "                ToTensorVideo(),  # TCHW\n",
    "                # video_transforms.RandomHorizontalFlipVideo(),\n",
    "                UCFCenterCropVideo(image_size[0]),\n",
    "                # video_transforms.UCFCenterCropVideo(image_size[0]),\n",
    "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], inplace=True),\n",
    "            ]\n",
    "        )\n",
    "    elif name == \"resize_crop\":\n",
    "        transform_video = transforms.Compose(\n",
    "            [\n",
    "                # video_transforms.ToTensorVideo(),  # TCHW\n",
    "                ToTensorVideo(),  # TCHW\n",
    "                # video_transforms.ResizeCrop(image_size),\n",
    "                ResizeCrop(image_size),\n",
    "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], inplace=True),\n",
    "            ]\n",
    "        )\n",
    "    elif name == \"rand_size_crop\":\n",
    "        transform_video = transforms.Compose(\n",
    "            [\n",
    "                # video_transforms.ToTensorVideo(),  # TCHW\n",
    "                ToTensorVideo(),  # TCHW\n",
    "                video_transforms.RandomSizedCrop(image_size),\n",
    "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], inplace=True),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Transform {name} not implemented\")\n",
    "    return transform_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ca40e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @DATASETS.register_module(\"video_text\")\n",
    "class VideoTextDataset(TextDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        transform_name: str = None,\n",
    "        bucket_class: str = \"Bucket\",\n",
    "        rand_sample_interval: int = None,  # random sample_interval value from [1, min(rand_sample_interval, video_allowed_max)]\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.transform_name = transform_name\n",
    "        self.bucket_class = bucket_class\n",
    "        self.rand_sample_interval = rand_sample_interval\n",
    "\n",
    "    def get_image(self, index: int, height: int, width: int) -> dict:\n",
    "        sample = self.data.iloc[index]\n",
    "        path = sample[\"path\"]\n",
    "        # loading\n",
    "        image = pil_loader(path)\n",
    "\n",
    "        # transform\n",
    "        transform = get_transforms_image(self.transform_name, (height, width))\n",
    "        image = transform(image)\n",
    "\n",
    "        # CHW -> CTHW\n",
    "        video = image.unsqueeze(1)\n",
    "\n",
    "        return {\"video\": video}\n",
    "\n",
    "    def get_video(self, index: int, num_frames: int, height: int, width: int, sampling_interval: int) -> dict:\n",
    "        sample = self.data.iloc[index]\n",
    "        path = sample[\"path\"]\n",
    "\n",
    "        # loading\n",
    "        vframes, vinfo = read_video(path, backend=\"av\")\n",
    "\n",
    "        if self.rand_sample_interval is not None:\n",
    "            # randomly sample from 1 - self.rand_sample_interval\n",
    "            video_allowed_max = min(len(vframes) // num_frames, self.rand_sample_interval)\n",
    "            sampling_interval = random.randint(1, video_allowed_max)\n",
    "\n",
    "        # Sampling video frames\n",
    "        video = temporal_random_crop(vframes, num_frames, sampling_interval)\n",
    "\n",
    "        video = video.clone()\n",
    "        del vframes\n",
    "\n",
    "        # transform\n",
    "        transform = get_transforms_video(self.transform_name, (height, width))\n",
    "        video = transform(video)  # T C H W\n",
    "        video = video.permute(1, 0, 2, 3)\n",
    "\n",
    "        ret = {\"video\": video}\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def get_image_or_video(self, index: int, num_frames: int, height: int, width: int, sampling_interval: int) -> dict:\n",
    "        sample = self.data.iloc[index]\n",
    "        path = sample[\"path\"]\n",
    "\n",
    "        if is_img(path):\n",
    "            return self.get_image(index, height, width)\n",
    "        return self.get_video(index, num_frames, height, width, sampling_interval)\n",
    "\n",
    "    def getitem(self, index: str) -> dict:\n",
    "        # a hack to pass in the (time, height, width) info from sampler\n",
    "        index, num_frames, height, width = [int(val) for val in index.split(\"-\")]\n",
    "        ret = dict()\n",
    "        ret.update(super().getitem(index))\n",
    "        try:\n",
    "            ret.update(self.get_image_or_video(index, num_frames, height, width, ret[\"sampling_interval\"]))\n",
    "        except Exception as e:\n",
    "            path = self.data.iloc[index][\"path\"]\n",
    "            print(f\"video {path}: {e}\")\n",
    "            return None\n",
    "        return ret\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.getitem(index)\n",
    "\n",
    "# cfg.dataset =\n",
    "# {'type': 'video_text',\n",
    "#  'transform_name': 'resize_crop',\n",
    "#  'data_path': '/workspaces/open-sora/Open-Sora/datasets/pexels_5',\n",
    "#  'fps_max': 24}\n",
    "\n",
    "dataset = VideoTextDataset(\n",
    "    type = \"video_text\",\n",
    "    transform_name = \"resize_crop\",\n",
    "    data_path = PEXELS_CSV_PATH,\n",
    "    fps_max = 24,\n",
    ")\n",
    "logger.info(\"Dataset contains %s samples.\", len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea80876b",
   "metadata": {},
   "source": [
    "### ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ã®æ§‹ç¯‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815cd943",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_pin_memory = pin_memory_cache_pre_alloc_numels is not None\n",
    "cache_pin_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a994fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_parallel_group(get_mixed_dp_pg : bool = False):\n",
    "    if get_mixed_dp_pg and \"mixed_dp_group\" in _GLOBAL_PARALLEL_GROUPS:\n",
    "        return _GLOBAL_PARALLEL_GROUPS[\"mixed_dp_group\"]\n",
    "    return _GLOBAL_PARALLEL_GROUPS.get(\"data\", dist.group.WORLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a393b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"num_workers\"] = 0\n",
    "cfg[\"prefetch_factor\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dbeebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_args = dict(\n",
    "    dataset=dataset,\n",
    "    batch_size=cfg.get(\"batch_size\", None),\n",
    "    num_workers=cfg.get(\"num_workers\", 4),\n",
    "    seed=cfg.get(\"seed\", 1024),\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    process_group=get_data_parallel_group(),\n",
    "    prefetch_factor=cfg.get(\"prefetch_factor\", None),\n",
    "    cache_pin_memory=cache_pin_memory,\n",
    ")\n",
    "\n",
    "dataloader_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f012582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import functools\n",
    "import os\n",
    "import queue\n",
    "import random\n",
    "import threading\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.multiprocessing as multiprocessing\n",
    "from torch._utils import ExceptionWrapper\n",
    "from torch.distributed import ProcessGroup\n",
    "from torch.utils.data import DataLoader, _utils\n",
    "from torch.utils.data._utils import MP_STATUS_CHECK_INTERVAL\n",
    "from torch.utils.data.dataloader import (\n",
    "    IterDataPipe,\n",
    "    MapDataPipe,\n",
    "    _BaseDataLoaderIter,\n",
    "    _MultiProcessingDataLoaderIter,\n",
    "    _sharding_worker_init_fn,\n",
    "    _SingleProcessDataLoaderIter,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47bd86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict, defaultdict\n",
    "from typing import Iterator\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import Dataset, DistributedSampler\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203455c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ASPECT_RATIO_LD_LIST = [  # width:height\n",
    "    \"2.39:1\",  # cinemascope, 2.39\n",
    "    \"2:1\",  # rare, 2\n",
    "    \"16:9\",  # rare, 1.89\n",
    "    \"1.85:1\",  # american widescreen, 1.85\n",
    "    \"9:16\",  # popular, 1.78\n",
    "    \"5:8\",  # rare, 1.6\n",
    "    \"3:2\",  # rare, 1.5\n",
    "    \"4:3\",  # classic, 1.33\n",
    "    \"1:1\",  # square\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5df9739",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_aspect_ratios_dict(\n",
    "    total_pixels: int = 256 * 256, training: bool = True\n",
    ") -> dict[str, tuple[int, int]]:\n",
    "    D = int(os.environ.get(\"AE_SPATIAL_COMPRESSION\", 16))\n",
    "    aspect_ratios_dict = {}\n",
    "    aspect_ratios_vertical_dict = {}\n",
    "    for ratio in ASPECT_RATIO_LD_LIST:\n",
    "        width_ratio, height_ratio = map(float, ratio.split(\":\"))\n",
    "        width = int(math.sqrt(total_pixels * (width_ratio / height_ratio)) // D) * D\n",
    "        height = int((total_pixels / width) // D) * D\n",
    "\n",
    "        if training:\n",
    "            # adjust aspect ratio to match total pixels\n",
    "            diff = abs(height * width - total_pixels)\n",
    "            candidate = [\n",
    "                (height - D, width),\n",
    "                (height + D, width),\n",
    "                (height, width - D),\n",
    "                (height, width + D),\n",
    "            ]\n",
    "            for h, w in candidate:\n",
    "                if abs(h * w - total_pixels) < diff:\n",
    "                    height, width = h, w\n",
    "                    diff = abs(h * w - total_pixels)\n",
    "\n",
    "        # remove duplicated aspect ratio\n",
    "        if (height, width) not in aspect_ratios_dict.values() or not training:\n",
    "            aspect_ratios_dict[ratio] = (height, width)\n",
    "            vertial_ratios = \":\".join(ratio.split(\":\")[::-1])\n",
    "            aspect_ratios_vertical_dict[vertial_ratios] = (width, height)\n",
    "\n",
    "    aspect_ratios_dict.update(aspect_ratios_vertical_dict)\n",
    "\n",
    "    return aspect_ratios_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba124ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_pexels_from_name(resolution: str) -> int:\n",
    "    resolution = resolution.split(\"_\")[0]\n",
    "    if resolution.endswith(\"px\"):\n",
    "        size = int(resolution[:-2])\n",
    "        num_pexels = size * size\n",
    "    elif resolution.endswith(\"p\"):\n",
    "        size = int(resolution[:-1])\n",
    "        num_pexels = int(size * size / 9 * 16)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid resolution {resolution}\")\n",
    "    return num_pexels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7ff455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resolution_with_aspect_ratio(\n",
    "    resolution: str,\n",
    ") -> tuple[int, dict[str, tuple[int, int]]]:\n",
    "    \"\"\"Get resolution with aspect ratio\n",
    "\n",
    "    Args:\n",
    "        resolution (str): resolution name. The format is name only or \"{name}_{setting}\".\n",
    "            name supports \"256px\" or \"360p\". setting supports \"ar1:1\" or \"max\".\n",
    "\n",
    "    Returns:\n",
    "        tuple[int, dict[str, tuple[int, int]]]: resolution with aspect ratio\n",
    "    \"\"\"\n",
    "    keys = resolution.split(\"_\")\n",
    "    if len(keys) == 1:\n",
    "        resolution = keys[0]\n",
    "        setting = \"\"\n",
    "    else:\n",
    "        resolution, setting = keys\n",
    "        assert setting == \"max\" or setting.startswith(\n",
    "            \"ar\"\n",
    "        ), f\"Invalid setting {setting}\"\n",
    "\n",
    "    # get resolution\n",
    "    num_pexels = get_num_pexels_from_name(resolution)\n",
    "\n",
    "    # get aspect ratio\n",
    "    aspect_ratio_dict = get_aspect_ratios_dict(num_pexels)\n",
    "\n",
    "    # handle setting\n",
    "    if setting == \"max\":\n",
    "        aspect_ratio = max(\n",
    "            aspect_ratio_dict,\n",
    "            key=lambda x: aspect_ratio_dict[x][0] * aspect_ratio_dict[x][1],\n",
    "        )\n",
    "        aspect_ratio_dict = {aspect_ratio: aspect_ratio_dict[aspect_ratio]}\n",
    "    elif setting.startswith(\"ar\"):\n",
    "        aspect_ratio = setting[2:]\n",
    "        assert (\n",
    "            aspect_ratio in aspect_ratio_dict\n",
    "        ), f\"Aspect ratio {aspect_ratio} not found\"\n",
    "        aspect_ratio_dict = {aspect_ratio: aspect_ratio_dict[aspect_ratio]}\n",
    "\n",
    "    return num_pexels, aspect_ratio_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb2f6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_target_fps(\n",
    "    fps: float,\n",
    "    max_fps: float,\n",
    ") -> tuple[float, int]:\n",
    "    \"\"\"\n",
    "    Map fps to a new fps that is less than max_fps.\n",
    "\n",
    "    Args:\n",
    "        fps (float): Original fps.\n",
    "        max_fps (float): Maximum fps.\n",
    "\n",
    "    Returns:\n",
    "        tuple[float, int]: New fps and sampling interval.\n",
    "    \"\"\"\n",
    "    if math.isnan(fps):\n",
    "        return 0, 1\n",
    "    if fps < max_fps:\n",
    "        return fps, 1\n",
    "    sampling_interval = math.ceil(fps / max_fps)\n",
    "    new_fps = math.floor(fps / sampling_interval)\n",
    "    return new_fps, sampling_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538c19c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_ratio(height: float, width: float, ratios: dict) -> str:\n",
    "    aspect_ratio = height / width\n",
    "    closest_ratio = min(\n",
    "        ratios.keys(), key=lambda ratio: abs(aspect_ratio - get_ratio(ratio))\n",
    "    )\n",
    "    return closest_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575d7ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratio(name: str) -> float:\n",
    "    width, height = map(float, name.split(\":\"))\n",
    "    return height / width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dc0865",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bucket:\n",
    "    def __init__(self, bucket_config: dict[str, dict[int, tuple[float, int] | tuple[tuple[float, float], int]]]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            bucket_config (dict): A dictionary containing the bucket configuration.\n",
    "                The dictionary should be in the following format:\n",
    "                {\n",
    "                    \"bucket_name\": {\n",
    "                        \"time\": (probability, batch_size),\n",
    "                        \"time\": (probability, batch_size),\n",
    "                        ...\n",
    "                    },\n",
    "                    ...\n",
    "                }\n",
    "\n",
    "                Or in the following format:\n",
    "                {\n",
    "                    \"bucket_name\": {\n",
    "                        \"time\": ((probability, next_probability), batch_size),\n",
    "                        \"time\": ((probability, next_probability), batch_size),\n",
    "                        ...\n",
    "                    },\n",
    "                    ...\n",
    "                }\n",
    "                The bucket_name should be the name of the bucket, and the time should be the number of frames in the video.\n",
    "                The probability should be a float between 0 and 1, and the batch_size should be an integer.\n",
    "                If the probability is a tuple, the second value should be the probability to skip to the next time.\n",
    "        \"\"\"\n",
    "\n",
    "        aspect_ratios = {key: get_resolution_with_aspect_ratio(key) for key in bucket_config.keys()}\n",
    "        bucket_probs = OrderedDict()\n",
    "        bucket_bs = OrderedDict()\n",
    "        bucket_names = sorted(bucket_config.keys(), key=lambda x: aspect_ratios[x][0], reverse=True)\n",
    "\n",
    "        for key in bucket_names:\n",
    "            bucket_time_names = sorted(bucket_config[key].keys(), key=lambda x: x, reverse=True)\n",
    "            bucket_probs[key] = OrderedDict({k: bucket_config[key][k][0] for k in bucket_time_names})\n",
    "            bucket_bs[key] = OrderedDict({k: bucket_config[key][k][1] for k in bucket_time_names})\n",
    "\n",
    "        self.hw_criteria = {k: aspect_ratios[k][0] for k in bucket_names}\n",
    "        self.t_criteria = {k1: {k2: k2 for k2 in bucket_config[k1].keys()} for k1 in bucket_names}\n",
    "        self.ar_criteria = {\n",
    "            k1: {k2: {k3: v3 for k3, v3 in aspect_ratios[k1][1].items()} for k2 in bucket_config[k1].keys()}\n",
    "            for k1 in bucket_names\n",
    "        }\n",
    "\n",
    "        bucket_id_cnt = num_bucket = 0\n",
    "        bucket_id = dict()\n",
    "        for k1, v1 in bucket_probs.items():\n",
    "            bucket_id[k1] = dict()\n",
    "            for k2, _ in v1.items():\n",
    "                bucket_id[k1][k2] = bucket_id_cnt\n",
    "                bucket_id_cnt += 1\n",
    "                num_bucket += len(aspect_ratios[k1][1])\n",
    "\n",
    "        self.bucket_probs = bucket_probs\n",
    "        self.bucket_bs = bucket_bs\n",
    "        self.bucket_id = bucket_id\n",
    "        self.num_bucket = num_bucket\n",
    "\n",
    "        log_message(\"Number of buckets: %s\", num_bucket)\n",
    "\n",
    "    def get_bucket_id(\n",
    "        self,\n",
    "        T: int,\n",
    "        H: int,\n",
    "        W: int,\n",
    "        fps: float,\n",
    "        path: str | None = None,\n",
    "        seed: int | None = None,\n",
    "        fps_max: int = 16,\n",
    "    ) -> tuple[str, int, int] | None:\n",
    "        approx = 0.8\n",
    "        _, sampling_interval = map_target_fps(fps, fps_max)\n",
    "        T = T // sampling_interval\n",
    "        resolution = H * W\n",
    "        rng = np.random.default_rng(seed)\n",
    "\n",
    "        # Reference to probabilities and criteria for faster access\n",
    "        bucket_probs = self.bucket_probs\n",
    "        hw_criteria = self.hw_criteria\n",
    "        ar_criteria = self.ar_criteria\n",
    "\n",
    "        # Start searching for the appropriate bucket\n",
    "        for hw_id, t_criteria in bucket_probs.items():\n",
    "            # if resolution is too low, skip\n",
    "            if resolution < hw_criteria[hw_id] * approx:\n",
    "                continue\n",
    "\n",
    "            # if sample is an image\n",
    "            if T == 1:\n",
    "                if 1 in t_criteria:\n",
    "                    if rng.random() < t_criteria[1]:\n",
    "                        return hw_id, 1, get_closest_ratio(H, W, ar_criteria[hw_id][1])\n",
    "                continue\n",
    "\n",
    "            # Look for suitable t_id for video\n",
    "            for t_id, prob in t_criteria.items():\n",
    "                if T >= t_id and t_id != 1:\n",
    "                    # if prob is a tuple, use the second value as the threshold to skip\n",
    "                    # to the next t_id\n",
    "                    if isinstance(prob, tuple):\n",
    "                        next_hw_prob, next_t_prob = prob\n",
    "                        if next_t_prob >= 1 or rng.random() <= next_t_prob:\n",
    "                            continue\n",
    "                    else:\n",
    "                        next_hw_prob = prob\n",
    "                    if next_hw_prob >= 1 or rng.random() <= next_hw_prob:\n",
    "                        ar_id = get_closest_ratio(H, W, ar_criteria[hw_id][t_id])\n",
    "                        return hw_id, t_id, ar_id\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "        return None\n",
    "\n",
    "    def get_thw(self, bucket_idx: tuple[str, int, int]) -> tuple[int, int, int]:\n",
    "        assert len(bucket_idx) == 3\n",
    "        T = self.t_criteria[bucket_idx[0]][bucket_idx[1]]\n",
    "        H, W = self.ar_criteria[bucket_idx[0]][bucket_idx[1]][bucket_idx[2]]\n",
    "        return T, H, W\n",
    "\n",
    "    def get_prob(self, bucket_idx: tuple[str, int]) -> float:\n",
    "        return self.bucket_probs[bucket_idx[0]][bucket_idx[1]]\n",
    "\n",
    "    def get_batch_size(self, bucket_idx: tuple[str, int]) -> int:\n",
    "        return self.bucket_bs[bucket_idx[0]][bucket_idx[1]]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.num_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdca754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_message(*args, level: str = \"info\"):\n",
    "    \"\"\"\n",
    "    Log a message to the logger.\n",
    "\n",
    "    Args:\n",
    "        *args: The message to log.\n",
    "        level (str): The logging level.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    if level == \"info\":\n",
    "        logger.info(*args)\n",
    "    elif level == \"warning\":\n",
    "        logger.warning(*args)\n",
    "    elif level == \"error\":\n",
    "        logger.error(*args)\n",
    "    elif level == \"print\":\n",
    "        print(*args)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid logging level: {level}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959337cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandarallel import pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78779c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pandarallel to accelerate bucket processing\n",
    "# NOTE: pandarallel should only access local variables\n",
    "def apply(data, method=None, seed=None, num_bucket=None, fps_max=16):\n",
    "    return method(\n",
    "        data[\"num_frames\"],\n",
    "        data[\"height\"],\n",
    "        data[\"width\"],\n",
    "        data[\"fps\"],\n",
    "        data[\"path\"],\n",
    "        seed + data[\"id\"] * num_bucket,\n",
    "        fps_max,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcc8310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync_object_across_devices(obj: Any, rank: int = 0):\n",
    "    \"\"\"\n",
    "    Synchronizes any picklable object across devices in a PyTorch distributed setting\n",
    "    using `broadcast_object_list` with CUDA support.\n",
    "\n",
    "    Parameters:\n",
    "    obj (Any): The object to synchronize. Can be any picklable object (e.g., list, dict, custom class).\n",
    "    rank (int): The rank of the device from which to broadcast the object state. Default is 0.\n",
    "\n",
    "    Note: Ensure torch.distributed is initialized before using this function and CUDA is available.\n",
    "    \"\"\"\n",
    "\n",
    "    # Move the object to a list for broadcasting\n",
    "    object_list = [obj]\n",
    "\n",
    "    # Broadcast the object list from the source rank to all other ranks\n",
    "    dist.broadcast_object_list(object_list, src=rank, device=\"cuda\")\n",
    "\n",
    "    # Retrieve the synchronized object\n",
    "    obj = object_list[0]\n",
    "\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c95e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_numel_str(numel: int) -> str:\n",
    "    \"\"\"\n",
    "    Format a number of elements to a human-readable string.\n",
    "\n",
    "    Args:\n",
    "        numel (int): The number of elements.\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted string.\n",
    "    \"\"\"\n",
    "    B = 1024**3\n",
    "    M = 1024**2\n",
    "    K = 1024\n",
    "    if numel >= B:\n",
    "        return f\"{numel / B:.2f} B\"\n",
    "    elif numel >= M:\n",
    "        return f\"{numel / M:.2f} M\"\n",
    "    elif numel >= K:\n",
    "        return f\"{numel / K:.2f} K\"\n",
    "    else:\n",
    "        return f\"{numel}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c85a3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariableVideoBatchSampler(DistributedSampler):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: VideoTextDataset,\n",
    "        bucket_config: dict,\n",
    "        num_replicas: int | None = None,\n",
    "        rank: int | None = None,\n",
    "        shuffle: bool = True,\n",
    "        seed: int = 0,\n",
    "        drop_last: bool = False,\n",
    "        verbose: bool = False,\n",
    "        num_bucket_build_workers: int = 1,\n",
    "        num_groups: int = 1,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            dataset=dataset, num_replicas=num_replicas, rank=rank, shuffle=shuffle, seed=seed, drop_last=drop_last\n",
    "        )\n",
    "        self.dataset = dataset\n",
    "        assert dataset.bucket_class == \"Bucket\", \"Only support Bucket class for now\"\n",
    "        self.bucket = Bucket(bucket_config)\n",
    "        self.verbose = verbose\n",
    "        self.last_micro_batch_access_index = 0\n",
    "        self.num_bucket_build_workers = num_bucket_build_workers\n",
    "        self._cached_bucket_sample_dict = None\n",
    "        self._cached_num_total_batch = None\n",
    "        self.num_groups = num_groups\n",
    "\n",
    "        if dist.get_rank() == 0:\n",
    "            pandarallel.initialize(\n",
    "                nb_workers=self.num_bucket_build_workers,\n",
    "                progress_bar=False,\n",
    "                verbose=0,\n",
    "                use_memory_fs=False,\n",
    "            )\n",
    "\n",
    "    def __iter__(self) -> Iterator[list[int]]:\n",
    "        bucket_sample_dict, _ = self.group_by_bucket()\n",
    "        self.clear_cache()\n",
    "\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(self.seed + self.epoch)\n",
    "        bucket_micro_batch_count = OrderedDict()\n",
    "        bucket_last_consumed = OrderedDict()\n",
    "\n",
    "        # process the samples\n",
    "        for bucket_id, data_list in bucket_sample_dict.items():\n",
    "            # handle droplast\n",
    "            bs_per_gpu = self.bucket.get_batch_size(bucket_id)\n",
    "            remainder = len(data_list) % bs_per_gpu\n",
    "\n",
    "            if remainder > 0:\n",
    "                if not self.drop_last:\n",
    "                    # if there is remainder, we pad to make it divisible\n",
    "                    data_list += data_list[: bs_per_gpu - remainder]\n",
    "                else:\n",
    "                    # we just drop the remainder to make it divisible\n",
    "                    data_list = data_list[:-remainder]\n",
    "            bucket_sample_dict[bucket_id] = data_list\n",
    "\n",
    "            # handle shuffle\n",
    "            if self.shuffle:\n",
    "                data_indices = torch.randperm(len(data_list), generator=g).tolist()\n",
    "                data_list = [data_list[i] for i in data_indices]\n",
    "                bucket_sample_dict[bucket_id] = data_list\n",
    "\n",
    "            # compute how many micro-batches each bucket has\n",
    "            num_micro_batches = len(data_list) // bs_per_gpu\n",
    "            bucket_micro_batch_count[bucket_id] = num_micro_batches\n",
    "\n",
    "        # compute the bucket access order\n",
    "        # each bucket may have more than one batch of data\n",
    "        # thus bucket_id may appear more than 1 time\n",
    "        bucket_id_access_order = []\n",
    "        for bucket_id, num_micro_batch in bucket_micro_batch_count.items():\n",
    "            bucket_id_access_order.extend([bucket_id] * num_micro_batch)\n",
    "\n",
    "        # randomize the access order\n",
    "        if self.shuffle:\n",
    "            bucket_id_access_order_indices = torch.randperm(len(bucket_id_access_order), generator=g).tolist()\n",
    "            bucket_id_access_order = [bucket_id_access_order[i] for i in bucket_id_access_order_indices]\n",
    "\n",
    "        # make the number of bucket accesses divisible by dp size\n",
    "        remainder = len(bucket_id_access_order) % self.num_replicas\n",
    "        if remainder > 0:\n",
    "            if self.drop_last:\n",
    "                bucket_id_access_order = bucket_id_access_order[: len(bucket_id_access_order) - remainder]\n",
    "            else:\n",
    "                bucket_id_access_order += bucket_id_access_order[: self.num_replicas - remainder]\n",
    "\n",
    "        # prepare each batch from its bucket\n",
    "        # according to the predefined bucket access order\n",
    "        num_iters = len(bucket_id_access_order) // self.num_replicas\n",
    "        start_iter_idx = self.last_micro_batch_access_index // self.num_replicas\n",
    "\n",
    "        # re-compute the micro-batch consumption\n",
    "        # this is useful when resuming from a state dict with a different number of GPUs\n",
    "        self.last_micro_batch_access_index = start_iter_idx * self.num_replicas\n",
    "        for i in range(self.last_micro_batch_access_index):\n",
    "            bucket_id = bucket_id_access_order[i]\n",
    "            bucket_bs = self.bucket.get_batch_size(bucket_id)\n",
    "            if bucket_id in bucket_last_consumed:\n",
    "                bucket_last_consumed[bucket_id] += bucket_bs\n",
    "            else:\n",
    "                bucket_last_consumed[bucket_id] = bucket_bs\n",
    "\n",
    "        for i in range(start_iter_idx, num_iters):\n",
    "            bucket_access_list = bucket_id_access_order[i * self.num_replicas : (i + 1) * self.num_replicas]\n",
    "            self.last_micro_batch_access_index += self.num_replicas\n",
    "\n",
    "            # compute the data samples consumed by each access\n",
    "            bucket_access_boundaries = []\n",
    "            for bucket_id in bucket_access_list:\n",
    "                bucket_bs = self.bucket.get_batch_size(bucket_id)\n",
    "                last_consumed_index = bucket_last_consumed.get(bucket_id, 0)\n",
    "                bucket_access_boundaries.append([last_consumed_index, last_consumed_index + bucket_bs])\n",
    "\n",
    "                # update consumption\n",
    "                if bucket_id in bucket_last_consumed:\n",
    "                    bucket_last_consumed[bucket_id] += bucket_bs\n",
    "                else:\n",
    "                    bucket_last_consumed[bucket_id] = bucket_bs\n",
    "\n",
    "            # compute the range of data accessed by each GPU\n",
    "            bucket_id = bucket_access_list[self.rank]\n",
    "            boundary = bucket_access_boundaries[self.rank]\n",
    "            cur_micro_batch = bucket_sample_dict[bucket_id][boundary[0] : boundary[1]]\n",
    "\n",
    "            # encode t, h, w into the sample index\n",
    "            real_t, real_h, real_w = self.bucket.get_thw(bucket_id)\n",
    "            cur_micro_batch = [f\"{idx}-{real_t}-{real_h}-{real_w}\" for idx in cur_micro_batch]\n",
    "            yield cur_micro_batch\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.get_num_batch() // self.num_groups\n",
    "\n",
    "    def get_num_batch(self) -> int:\n",
    "        _, num_total_batch = self.group_by_bucket()\n",
    "        return num_total_batch\n",
    "\n",
    "    def clear_cache(self):\n",
    "        self._cached_bucket_sample_dict = None\n",
    "        self._cached_num_total_batch = 0\n",
    "\n",
    "    def group_by_bucket(self) -> dict:\n",
    "        \"\"\"\n",
    "        Group the dataset samples into buckets.\n",
    "        This method will set `self._cached_bucket_sample_dict` to the bucket sample dict.\n",
    "\n",
    "        Returns:\n",
    "            dict: a dictionary with bucket id as key and a list of sample indices as value\n",
    "        \"\"\"\n",
    "        if self._cached_bucket_sample_dict is not None:\n",
    "            return self._cached_bucket_sample_dict, self._cached_num_total_batch\n",
    "\n",
    "        # use pandarallel to accelerate bucket processing\n",
    "        log_message(\"Building buckets using %d workers...\", self.num_bucket_build_workers)\n",
    "        bucket_ids = None\n",
    "        if dist.get_rank() == 0:\n",
    "            data = self.dataset.data.copy(deep=True)\n",
    "            data[\"id\"] = data.index\n",
    "            bucket_ids = data.parallel_apply(\n",
    "                apply,\n",
    "                axis=1,\n",
    "                method=self.bucket.get_bucket_id,\n",
    "                seed=self.seed + self.epoch,\n",
    "                num_bucket=self.bucket.num_bucket,\n",
    "                fps_max=self.dataset.fps_max,\n",
    "            )\n",
    "        dist.barrier()\n",
    "        bucket_ids = sync_object_across_devices(bucket_ids)\n",
    "        dist.barrier()\n",
    "\n",
    "        # group by bucket\n",
    "        # each data sample is put into a bucket with a similar image/video size\n",
    "        bucket_sample_dict = defaultdict(list)\n",
    "        bucket_ids_np = np.array(bucket_ids)\n",
    "        valid_indices = np.where(bucket_ids_np != None)[0]\n",
    "        for i in valid_indices:\n",
    "            bucket_sample_dict[bucket_ids_np[i]].append(i)\n",
    "\n",
    "        # cache the bucket sample dict\n",
    "        self._cached_bucket_sample_dict = bucket_sample_dict\n",
    "\n",
    "        # num total batch\n",
    "        num_total_batch = self.print_bucket_info(bucket_sample_dict)\n",
    "        self._cached_num_total_batch = num_total_batch\n",
    "\n",
    "        return bucket_sample_dict, num_total_batch\n",
    "\n",
    "    def print_bucket_info(self, bucket_sample_dict: dict) -> int:\n",
    "        # collect statistics\n",
    "        num_total_samples = num_total_batch = 0\n",
    "        num_total_img_samples = num_total_vid_samples = 0\n",
    "        num_total_img_batch = num_total_vid_batch = 0\n",
    "        num_total_vid_batch_256 = num_total_vid_batch_768 = 0\n",
    "        num_aspect_dict = defaultdict(lambda: [0, 0])\n",
    "        num_hwt_dict = defaultdict(lambda: [0, 0])\n",
    "        for k, v in bucket_sample_dict.items():\n",
    "            size = len(v)\n",
    "            num_batch = size // self.bucket.get_batch_size(k[:-1])\n",
    "\n",
    "            num_total_samples += size\n",
    "            num_total_batch += num_batch\n",
    "\n",
    "            if k[1] == 1:\n",
    "                num_total_img_samples += size\n",
    "                num_total_img_batch += num_batch\n",
    "            else:\n",
    "                if k[0] == \"256px\":\n",
    "                    num_total_vid_batch_256 += num_batch\n",
    "                elif k[0] == \"768px\":\n",
    "                    num_total_vid_batch_768 += num_batch\n",
    "                num_total_vid_samples += size\n",
    "                num_total_vid_batch += num_batch\n",
    "\n",
    "            num_aspect_dict[k[-1]][0] += size\n",
    "            num_aspect_dict[k[-1]][1] += num_batch\n",
    "            num_hwt_dict[k[:-1]][0] += size\n",
    "            num_hwt_dict[k[:-1]][1] += num_batch\n",
    "\n",
    "        # sort\n",
    "        num_aspect_dict = dict(sorted(num_aspect_dict.items(), key=lambda x: x[0]))\n",
    "        num_hwt_dict = dict(\n",
    "            sorted(num_hwt_dict.items(), key=lambda x: (get_num_pexels_from_name(x[0][0]), x[0][1]), reverse=True)\n",
    "        )\n",
    "        num_hwt_img_dict = {k: v for k, v in num_hwt_dict.items() if k[1] == 1}\n",
    "        num_hwt_vid_dict = {k: v for k, v in num_hwt_dict.items() if k[1] > 1}\n",
    "\n",
    "        # log\n",
    "        if dist.get_rank() == 0 and self.verbose:\n",
    "            log_message(\"Bucket Info:\")\n",
    "            log_message(\"Bucket [#sample, #batch] by aspect ratio:\")\n",
    "            for k, v in num_aspect_dict.items():\n",
    "                log_message(\"(%s): #sample: %s, #batch: %s\", k, format_numel_str(v[0]), format_numel_str(v[1]))\n",
    "            log_message(\"===== Image Info =====\")\n",
    "            log_message(\"Image Bucket by HxWxT:\")\n",
    "            for k, v in num_hwt_img_dict.items():\n",
    "                log_message(\"%s: #sample: %s, #batch: %s\", k, format_numel_str(v[0]), format_numel_str(v[1]))\n",
    "            log_message(\"--------------------------------\")\n",
    "            log_message(\n",
    "                \"#image sample: %s, #image batch: %s\",\n",
    "                format_numel_str(num_total_img_samples),\n",
    "                format_numel_str(num_total_img_batch),\n",
    "            )\n",
    "            log_message(\"===== Video Info =====\")\n",
    "            log_message(\"Video Bucket by HxWxT:\")\n",
    "            for k, v in num_hwt_vid_dict.items():\n",
    "                log_message(\"%s: #sample: %s, #batch: %s\", k, format_numel_str(v[0]), format_numel_str(v[1]))\n",
    "            log_message(\"--------------------------------\")\n",
    "            log_message(\n",
    "                \"#video sample: %s, #video batch: %s\",\n",
    "                format_numel_str(num_total_vid_samples),\n",
    "                format_numel_str(num_total_vid_batch),\n",
    "            )\n",
    "            log_message(\"===== Summary =====\")\n",
    "            log_message(\"#non-empty buckets: %s\", len(bucket_sample_dict))\n",
    "            log_message(\n",
    "                \"Img/Vid sample ratio: %.2f\",\n",
    "                num_total_img_samples / num_total_vid_samples if num_total_vid_samples > 0 else 0,\n",
    "            )\n",
    "            log_message(\n",
    "                \"Img/Vid batch ratio: %.2f\", num_total_img_batch / num_total_vid_batch if num_total_vid_batch > 0 else 0\n",
    "            )\n",
    "            log_message(\n",
    "                \"vid batch 256: %s, vid batch 768: %s\", format_numel_str(num_total_vid_batch_256), format_numel_str(num_total_vid_batch_768)\n",
    "            )\n",
    "            log_message(\n",
    "                \"Vid batch ratio (256px/768px): %.2f\", num_total_vid_batch_256 / num_total_vid_batch_768 if num_total_vid_batch_768 > 0 else 0\n",
    "            )\n",
    "            log_message(\n",
    "                \"#training sample: %s, #training batch: %s\",\n",
    "                format_numel_str(num_total_samples),\n",
    "                format_numel_str(num_total_batch),\n",
    "            )\n",
    "        return num_total_batch\n",
    "\n",
    "    def reset(self):\n",
    "        self.last_micro_batch_access_index = 0\n",
    "\n",
    "    def set_step(self, start_step: int):\n",
    "        self.last_micro_batch_access_index = start_step * self.num_replicas\n",
    "\n",
    "    def state_dict(self, num_steps: int) -> dict:\n",
    "        # the last_micro_batch_access_index in the __iter__ is often\n",
    "        # not accurate during multi-workers and data prefetching\n",
    "        # thus, we need the user to pass the actual steps which have been executed\n",
    "        # to calculate the correct last_micro_batch_access_index\n",
    "        return {\"seed\": self.seed, \"epoch\": self.epoch, \"last_micro_batch_access_index\": num_steps * self.num_replicas}\n",
    "\n",
    "    def load_state_dict(self, state_dict: dict) -> None:\n",
    "        self.__dict__.update(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4ae84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_default(batch):\n",
    "    # filter out None\n",
    "    batch = [x for x in batch if x is not None]\n",
    "    assert len(batch) > 0, \"batch is empty\"\n",
    "\n",
    "    # HACK: for loading text features\n",
    "    use_mask = False\n",
    "    if \"mask\" in batch[0] and isinstance(batch[0][\"mask\"], int):\n",
    "        masks = [x.pop(\"mask\") for x in batch]\n",
    "\n",
    "        texts = [x.pop(\"text\") for x in batch]\n",
    "        texts = torch.cat(texts, dim=1)\n",
    "        use_mask = True\n",
    "\n",
    "    ret = torch.utils.data.default_collate(batch)\n",
    "\n",
    "    if use_mask:\n",
    "        ret[\"mask\"] = masks\n",
    "        ret[\"text\"] = texts\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a3e959",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Deterministic dataloader\n",
    "def get_seed_worker(seed):\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = seed\n",
    "        if seed is not None:\n",
    "            np.random.seed(worker_seed)\n",
    "            torch.manual_seed(worker_seed)\n",
    "            random.seed(worker_seed)\n",
    "\n",
    "    return seed_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db72e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import functools\n",
    "import os\n",
    "import queue\n",
    "import random\n",
    "import threading\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.multiprocessing as multiprocessing\n",
    "from torch._utils import ExceptionWrapper\n",
    "from torch.distributed import ProcessGroup\n",
    "from torch.utils.data import DataLoader, _utils\n",
    "from torch.utils.data._utils import MP_STATUS_CHECK_INTERVAL\n",
    "from torch.utils.data.dataloader import (\n",
    "    IterDataPipe,\n",
    "    MapDataPipe,\n",
    "    _BaseDataLoaderIter,\n",
    "    _MultiProcessingDataLoaderIter,\n",
    "    _sharding_worker_init_fn,\n",
    "    _SingleProcessDataLoaderIter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413389a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pin_memory_loop(\n",
    "    in_queue, out_queue, device_id, done_event, device, pin_memory_cache: PinMemoryCache, pin_memory_key: str\n",
    "):\n",
    "    # This setting is thread local, and prevents the copy in pin_memory from\n",
    "    # consuming all CPU cores.\n",
    "    torch.set_num_threads(1)\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.set_device(device_id)\n",
    "    elif device == \"xpu\":\n",
    "        torch.xpu.set_device(device_id)  # type: ignore[attr-defined]\n",
    "    elif device == torch._C._get_privateuse1_backend_name():\n",
    "        custom_device_mod = getattr(torch, torch._C._get_privateuse1_backend_name())\n",
    "        custom_device_mod.set_device(device_id)\n",
    "\n",
    "    def do_one_step():\n",
    "        try:\n",
    "            r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
    "        except queue.Empty:\n",
    "            return\n",
    "        idx, data = r\n",
    "        if not done_event.is_set() and not isinstance(data, ExceptionWrapper):\n",
    "            try:\n",
    "                assert isinstance(data, dict)\n",
    "                if pin_memory_key in data:\n",
    "                    val = data[pin_memory_key]\n",
    "                    pin_memory_value = pin_memory_cache.get(val)\n",
    "                    pin_memory_value.copy_(val)\n",
    "                    data[pin_memory_key] = pin_memory_value\n",
    "            except Exception:\n",
    "                data = ExceptionWrapper(where=f\"in pin memory thread for device {device_id}\")\n",
    "            r = (idx, data)\n",
    "        while not done_event.is_set():\n",
    "            try:\n",
    "                out_queue.put(r, timeout=MP_STATUS_CHECK_INTERVAL)\n",
    "                break\n",
    "            except queue.Full:\n",
    "                continue\n",
    "\n",
    "    # See NOTE [ Data Loader Multiprocessing Shutdown Logic ] for details on the\n",
    "    # logic of this function.\n",
    "    while not done_event.is_set():\n",
    "        # Make sure that we don't preserve any object from one iteration\n",
    "        # to the next\n",
    "        do_one_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e399ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _MultiProcessingDataLoaderIterForVideo(_MultiProcessingDataLoaderIter):\n",
    "    pin_memory_key: str = \"video\"\n",
    "\n",
    "    def __init__(self, loader):\n",
    "        _BaseDataLoaderIter.__init__(self, loader)\n",
    "        self.pin_memory_cache = PinMemoryCache()\n",
    "\n",
    "        self._prefetch_factor = loader.prefetch_factor\n",
    "\n",
    "        assert self._num_workers > 0\n",
    "        assert self._prefetch_factor > 0\n",
    "\n",
    "        if loader.multiprocessing_context is None:\n",
    "            multiprocessing_context = multiprocessing\n",
    "        else:\n",
    "            multiprocessing_context = loader.multiprocessing_context\n",
    "\n",
    "        self._worker_init_fn = loader.worker_init_fn\n",
    "\n",
    "        # Adds forward compatibilities so classic DataLoader can work with DataPipes:\n",
    "        #   Additional worker init function will take care of sharding in MP and Distributed\n",
    "        if isinstance(self._dataset, (IterDataPipe, MapDataPipe)):\n",
    "            self._worker_init_fn = functools.partial(\n",
    "                _sharding_worker_init_fn, self._worker_init_fn, self._world_size, self._rank\n",
    "            )\n",
    "\n",
    "        # No certainty which module multiprocessing_context is\n",
    "        self._worker_result_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]\n",
    "        self._worker_pids_set = False\n",
    "        self._shutdown = False\n",
    "        self._workers_done_event = multiprocessing_context.Event()\n",
    "\n",
    "        self._index_queues = []\n",
    "        self._workers = []\n",
    "        for i in range(self._num_workers):\n",
    "            # No certainty which module multiprocessing_context is\n",
    "            index_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]\n",
    "            # Need to `cancel_join_thread` here!\n",
    "            # See sections (2) and (3b) above.\n",
    "            index_queue.cancel_join_thread()\n",
    "            w = multiprocessing_context.Process(\n",
    "                target=_utils.worker._worker_loop,\n",
    "                args=(\n",
    "                    self._dataset_kind,\n",
    "                    self._dataset,\n",
    "                    index_queue,\n",
    "                    self._worker_result_queue,\n",
    "                    self._workers_done_event,\n",
    "                    self._auto_collation,\n",
    "                    self._collate_fn,\n",
    "                    self._drop_last,\n",
    "                    self._base_seed,\n",
    "                    self._worker_init_fn,\n",
    "                    i,\n",
    "                    self._num_workers,\n",
    "                    self._persistent_workers,\n",
    "                    self._shared_seed,\n",
    "                ),\n",
    "            )\n",
    "            w.daemon = True\n",
    "            # NB: Process.start() actually take some time as it needs to\n",
    "            #     start a process and pass the arguments over via a pipe.\n",
    "            #     Therefore, we only add a worker to self._workers list after\n",
    "            #     it started, so that we do not call .join() if program dies\n",
    "            #     before it starts, and __del__ tries to join but will get:\n",
    "            #     AssertionError: can only join a started process.\n",
    "            w.start()\n",
    "            self._index_queues.append(index_queue)\n",
    "            self._workers.append(w)\n",
    "\n",
    "        if self._pin_memory:\n",
    "            self._pin_memory_thread_done_event = threading.Event()\n",
    "\n",
    "            # Queue is not type-annotated\n",
    "            self._data_queue = queue.Queue()  # type: ignore[var-annotated]\n",
    "            if self._pin_memory_device == \"xpu\":\n",
    "                current_device = torch.xpu.current_device()  # type: ignore[attr-defined]\n",
    "            elif self._pin_memory_device == torch._C._get_privateuse1_backend_name():\n",
    "                custom_device_mod = getattr(torch, torch._C._get_privateuse1_backend_name())\n",
    "                current_device = custom_device_mod.current_device()\n",
    "            else:\n",
    "                current_device = torch.cuda.current_device()  # choose cuda for default\n",
    "            pin_memory_thread = threading.Thread(\n",
    "                target=_pin_memory_loop,\n",
    "                args=(\n",
    "                    self._worker_result_queue,\n",
    "                    self._data_queue,\n",
    "                    current_device,\n",
    "                    self._pin_memory_thread_done_event,\n",
    "                    self._pin_memory_device,\n",
    "                    self.pin_memory_cache,\n",
    "                    self.pin_memory_key,\n",
    "                ),\n",
    "            )\n",
    "            pin_memory_thread.daemon = True\n",
    "            pin_memory_thread.start()\n",
    "            # Similar to workers (see comment above), we only register\n",
    "            # pin_memory_thread once it is started.\n",
    "            self._pin_memory_thread = pin_memory_thread\n",
    "        else:\n",
    "            self._data_queue = self._worker_result_queue  # type: ignore[assignment]\n",
    "\n",
    "        # In some rare cases, persistent workers (daemonic processes)\n",
    "        # would be terminated before `__del__` of iterator is invoked\n",
    "        # when main process exits\n",
    "        # It would cause failure when pin_memory_thread tries to read\n",
    "        # corrupted data from worker_result_queue\n",
    "        # atexit is used to shutdown thread and child processes in the\n",
    "        # right sequence before main process exits\n",
    "        if self._persistent_workers and self._pin_memory:\n",
    "            import atexit\n",
    "\n",
    "            for w in self._workers:\n",
    "                atexit.register(_MultiProcessingDataLoaderIter._clean_up_worker, w)\n",
    "\n",
    "        # .pid can be None only before process is spawned (not the case, so ignore)\n",
    "        _utils.signal_handling._set_worker_pids(id(self), tuple(w.pid for w in self._workers))  # type: ignore[misc]\n",
    "        _utils.signal_handling._set_SIGCHLD_handler()\n",
    "        self._worker_pids_set = True\n",
    "        self._reset(loader, first_iter=True)\n",
    "\n",
    "    def remove_cache(self, output_tensor: torch.Tensor):\n",
    "        self.pin_memory_cache.remove(output_tensor)\n",
    "\n",
    "    def get_cache_info(self) -> str:\n",
    "        return str(self.pin_memory_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efc5896",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataloaderForVideo(DataLoader):\n",
    "    def _get_iterator(self) -> \"_BaseDataLoaderIter\":\n",
    "        if self.num_workers == 0:\n",
    "            return _SingleProcessDataLoaderIter(self)\n",
    "        else:\n",
    "            self.check_worker_number_rationality()\n",
    "            return _MultiProcessingDataLoaderIterForVideo(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21b113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(\n",
    "    dataset,\n",
    "    batch_size=None,\n",
    "    shuffle=False,\n",
    "    seed=1024,\n",
    "    drop_last=False,\n",
    "    pin_memory=False,\n",
    "    num_workers=0,\n",
    "    process_group: ProcessGroup | None = None,\n",
    "    bucket_config=None,\n",
    "    num_bucket_build_workers=1,\n",
    "    prefetch_factor=None,\n",
    "    cache_pin_memory=False,\n",
    "    num_groups=1,\n",
    "    **kwargs,\n",
    "):\n",
    "    _kwargs = kwargs.copy()\n",
    "    if isinstance(dataset, VideoTextDataset):\n",
    "        batch_sampler = VariableVideoBatchSampler(\n",
    "            dataset,\n",
    "            bucket_config,\n",
    "            num_replicas=process_group.size(),\n",
    "            rank=process_group.rank(),\n",
    "            shuffle=shuffle,\n",
    "            seed=seed,\n",
    "            drop_last=drop_last,\n",
    "            verbose=True,\n",
    "            num_bucket_build_workers=num_bucket_build_workers,\n",
    "            num_groups=num_groups,\n",
    "        )\n",
    "        dl_cls = DataloaderForVideo if cache_pin_memory else DataLoader\n",
    "        return (\n",
    "            dl_cls(\n",
    "                dataset,\n",
    "                batch_sampler=batch_sampler,\n",
    "                worker_init_fn=get_seed_worker(seed),\n",
    "                pin_memory=pin_memory,\n",
    "                num_workers=num_workers,\n",
    "                collate_fn=collate_fn_default,\n",
    "                prefetch_factor=prefetch_factor,\n",
    "                **_kwargs,\n",
    "            ),\n",
    "            batch_sampler,\n",
    "        )\n",
    "    elif isinstance(dataset, TextDataset):\n",
    "        if process_group is None:\n",
    "            return (\n",
    "                DataLoader(\n",
    "                    dataset,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=shuffle,\n",
    "                    worker_init_fn=get_seed_worker(seed),\n",
    "                    drop_last=drop_last,\n",
    "                    pin_memory=pin_memory,\n",
    "                    num_workers=num_workers,\n",
    "                    prefetch_factor=prefetch_factor,\n",
    "                    **_kwargs,\n",
    "                ),\n",
    "                None,\n",
    "            )\n",
    "        else:\n",
    "            sampler = DistributedSampler(\n",
    "                dataset,\n",
    "                num_replicas=process_group.size(),\n",
    "                rank=process_group.rank(),\n",
    "                shuffle=shuffle,\n",
    "                seed=seed,\n",
    "                drop_last=drop_last,\n",
    "            )\n",
    "            return (\n",
    "                DataLoader(\n",
    "                    dataset,\n",
    "                    sampler=sampler,\n",
    "                    worker_init_fn=get_seed_worker(seed),\n",
    "                    pin_memory=pin_memory,\n",
    "                    num_workers=num_workers,\n",
    "                    collate_fn=collate_fn_default,\n",
    "                    prefetch_factor=prefetch_factor,\n",
    "                    **_kwargs,\n",
    "                ),\n",
    "                sampler,\n",
    "            )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset type: {type(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07538605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == build dataloader ==\n",
    "dataloader, sampler = prepare_dataloader(\n",
    "    bucket_config=cfg.get(\"bucket_config\", None),\n",
    "    num_bucket_build_workers=cfg.get(\"num_bucket_build_workers\", 1),\n",
    "    **dataloader_args,\n",
    ")\n",
    "dataloader, sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ee3d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torchvision.datasets.folder import IMG_EXTENSIONS, pil_loader\n",
    "from torchvision.io import write_video\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ebf7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_batch = next(iter(dataloader))\n",
    "# first_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a5af0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_batch = next(iter(dataloader))\n",
    "# first_batch[\"sampling_interval\"], first_batch[\"video\"].shape, first_batch[\"path\"], first_batch[\"index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471580c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_frame = first_batch[\"video\"][:, :, 0].to(device=device, dtype=torch.float32)\n",
    "# first_frame.shape\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.imshow(first_frame[0].permute(1, 2, 0).cpu().numpy())\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca3f11d",
   "metadata": {},
   "source": [
    "## ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5cbab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25bb3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional\n",
    "\n",
    "import diffusers\n",
    "import torch\n",
    "from huggingface_hub import PyTorchModelHubMixin\n",
    "from torch import nn\n",
    "\n",
    "# from opensora.registry import MODELS\n",
    "# from opensora.utils.ckpt import load_checkpoint\n",
    "\n",
    "from opensora.models.dc_ae.ae_model_zoo import DCAE_HF\n",
    "from opensora.models.dc_ae.models.dc_ae import DCAE, DCAEConfig, dc_ae_f32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b3a53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @MODELS.register_module(\"dc_ae\")\n",
    "def DC_AE(\n",
    "    model_name: str,\n",
    "    device_map: str | torch.device = \"cuda\",\n",
    "    torch_dtype: torch.dtype = torch.bfloat16,\n",
    "    from_scratch: bool = False,\n",
    "    from_pretrained: str | None = None,\n",
    "    is_training: bool = False,\n",
    "    use_spatial_tiling: bool = False,\n",
    "    use_temporal_tiling: bool = False,\n",
    "    spatial_tile_size: int = 256,\n",
    "    temporal_tile_size: int = 32,\n",
    "    tile_overlap_factor: float = 0.25,\n",
    "    scaling_factor: float = None,\n",
    "    disc_off_grad_ckpt: bool = False,\n",
    ") -> DCAE_HF:\n",
    "    if not from_scratch:\n",
    "        model = DCAE_HF.from_pretrained(model_name).to(device_map, torch_dtype)\n",
    "    else:\n",
    "        model = DCAE_HF(model_name).to(device_map, torch_dtype)\n",
    "\n",
    "    if from_pretrained is not None:\n",
    "        model = load_checkpoint(model, from_pretrained, device_map=device_map)\n",
    "        print(f\"loaded dc_ae from ckpt path: {from_pretrained}\")\n",
    "\n",
    "    model.cfg.is_training = is_training\n",
    "    model.use_spatial_tiling = use_spatial_tiling\n",
    "    model.use_temporal_tiling = use_temporal_tiling\n",
    "    model.spatial_tile_size = spatial_tile_size\n",
    "    model.temporal_tile_size = temporal_tile_size\n",
    "    model.tile_overlap_factor = tile_overlap_factor\n",
    "    if scaling_factor is not None:\n",
    "        model.scaling_factor = scaling_factor\n",
    "    model.decoder.disc_off_grad_ckpt = disc_off_grad_ckpt\n",
    "    return model\n",
    "\n",
    "# {'type': 'dc_ae',\n",
    "#  'model_name': 'dc-ae-f32t4c128',\n",
    "#  'from_scratch': True,\n",
    "#  'from_pretrained': None}\n",
    "\n",
    "model = DC_AE(\n",
    "    model_name=cfg.model.model_name,\n",
    "    from_scratch=cfg.model.get(\"from_scratch\", False),\n",
    "    from_pretrained=cfg.model.get(\"from_pretrained\", None),\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f0ad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model_numel(model: torch.nn.Module) -> tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Get the number of parameters in a model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model.\n",
    "\n",
    "    Returns:\n",
    "        tuple[int, int]: The total number of parameters and the number of trainable parameters.\n",
    "    \"\"\"\n",
    "    num_params = 0\n",
    "    num_params_trainable = 0\n",
    "    for p in model.parameters():\n",
    "        num_params += p.numel()\n",
    "        if p.requires_grad:\n",
    "            num_params_trainable += p.numel()\n",
    "    return num_params, num_params_trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca80224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_model_params(model: nn.Module):\n",
    "    \"\"\"\n",
    "    Log the number of parameters in a model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model.\n",
    "    \"\"\"\n",
    "    num_params, num_params_trainable = get_model_numel(model)\n",
    "    model_name = model.__class__.__name__\n",
    "    log_message(f\"[{model_name}] Number of parameters: {format_numel_str(num_params)}\")\n",
    "    log_message(f\"[{model_name}] Number of trainable parameters: {format_numel_str(num_params_trainable)}\")\n",
    "\n",
    "\n",
    "log_model_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0e7d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# False\n",
    "if cfg.get(\"grad_checkpoint\", False):\n",
    "    set_grad_checkpoint(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f8be46",
   "metadata": {},
   "source": [
    "### æå¤±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289a4de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e004e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class vgg16(torch.nn.Module):\n",
    "    def __init__(self, requires_grad=False, pretrained=True):\n",
    "        super(vgg16, self).__init__()\n",
    "        vgg_pretrained_features = models.vgg16(pretrained=pretrained).features\n",
    "        self.slice1 = torch.nn.Sequential()\n",
    "        self.slice2 = torch.nn.Sequential()\n",
    "        self.slice3 = torch.nn.Sequential()\n",
    "        self.slice4 = torch.nn.Sequential()\n",
    "        self.slice5 = torch.nn.Sequential()\n",
    "        self.N_slices = 5\n",
    "        for x in range(4):\n",
    "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(4, 9):\n",
    "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(9, 16):\n",
    "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(16, 23):\n",
    "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(23, 30):\n",
    "            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n",
    "        if not requires_grad:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, X):\n",
    "        h = self.slice1(X)\n",
    "        h_relu1_2 = h\n",
    "        h = self.slice2(h)\n",
    "        h_relu2_2 = h\n",
    "        h = self.slice3(h)\n",
    "        h_relu3_3 = h\n",
    "        h = self.slice4(h)\n",
    "        h_relu4_3 = h\n",
    "        h = self.slice5(h)\n",
    "        h_relu5_3 = h\n",
    "        vgg_outputs = namedtuple(\"VggOutputs\", [\"relu1_2\", \"relu2_2\", \"relu3_3\", \"relu4_3\", \"relu5_3\"])\n",
    "        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3, h_relu5_3)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13237555",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetLinLayer(nn.Module):\n",
    "    \"\"\"A single linear layer which does a 1x1 conv\"\"\"\n",
    "\n",
    "    def __init__(self, chn_in, chn_out=1, use_dropout=False):\n",
    "        super(NetLinLayer, self).__init__()\n",
    "        layers = (\n",
    "            [\n",
    "                nn.Dropout(),\n",
    "            ]\n",
    "            if (use_dropout)\n",
    "            else []\n",
    "        )\n",
    "        layers += [\n",
    "            nn.Conv2d(chn_in, chn_out, 1, stride=1, padding=0, bias=False),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70602186",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "URL_MAP = {\"vgg_lpips\": \"https://heibox.uni-heidelberg.de/f/607503859c864bc1b30b/?dl=1\"}\n",
    "\n",
    "CKPT_MAP = {\"vgg_lpips\": \"vgg.pth\"}\n",
    "\n",
    "MD5_MAP = {\"vgg_lpips\": \"d507d7349b931f0638a25a48a722f98a\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d5af38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ckpt_path(name, root=\".\", check=False):\n",
    "    assert name in URL_MAP\n",
    "    path = os.path.join(root, CKPT_MAP[name])\n",
    "    if not os.path.exists(path) or (check and not md5_hash(path) == MD5_MAP[name]):\n",
    "        print(\"Downloading {} model from {} to {}\".format(name, URL_MAP[name], path))\n",
    "        download(URL_MAP[name], path)\n",
    "        md5 = md5_hash(path)\n",
    "        assert md5 == MD5_MAP[name], md5\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f582a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from collections.abc import Iterable\n",
    "from typing import Callable, ContextManager, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from colossalai.utils import get_current_device\n",
    "from torch.utils.checkpoint import (\n",
    "    _DEFAULT_DETERMINISM_MODE,\n",
    "    CheckpointFunction,\n",
    "    _checkpoint_without_reentrant_generator,\n",
    "    checkpoint_sequential,\n",
    "    noop_context_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13056512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TorchDynamo does not step inside utils.checkpoint function.  The flow\n",
    "# looks likes this\n",
    "#  1) TorchDynamo tries to wrap utils.checkpoint in a HigherOrderOp by\n",
    "#     speculatively checking if the forward function is safe to trace.\n",
    "#  2) If yes, then Dynamo-generated Fx graph has the wrapped higher\n",
    "#     order op. As a result, TorchDynamo does not look inside utils.checkpoint.\n",
    "#  3) If not, then TorchDynamo falls back to eager by performing a graph\n",
    "#     break. And here, the following disable wrapper ensures that\n",
    "#     TorchDynamo does not trigger again on the frames created by\n",
    "#     utils.checkpoint innards.\n",
    "@torch._disable_dynamo\n",
    "def checkpoint(\n",
    "    function,\n",
    "    *args,\n",
    "    use_reentrant: Optional[bool] = None,\n",
    "    context_fn: Callable[[], Tuple[ContextManager, ContextManager]] = noop_context_fn,\n",
    "    determinism_check: str = _DEFAULT_DETERMINISM_MODE,\n",
    "    debug: bool = False,\n",
    "    **kwargs,\n",
    "):\n",
    "    r\"\"\"Checkpoint a model or part of the model.\n",
    "\n",
    "    Activation checkpointing is a technique that trades compute for memory.\n",
    "    Instead of keeping tensors needed for backward alive until they are used in\n",
    "    gradient computation during backward, forward computation in checkpointed\n",
    "    regions omits saving tensors for backward and recomputes them during the\n",
    "    backward pass. Activation checkpointing can be applied to any part of a\n",
    "    model.\n",
    "\n",
    "    There are currently two checkpointing implementations available, determined\n",
    "    by the :attr:`use_reentrant` parameter. It is recommended that you use\n",
    "    ``use_reentrant=False``. Please refer the note below for a discussion of\n",
    "    their differences.\n",
    "\n",
    "    .. warning::\n",
    "\n",
    "        If the :attr:`function` invocation during the backward pass differs\n",
    "        from the forward pass, e.g., due to a global variable, the checkpointed\n",
    "        version may not be equivalent, potentially causing an\n",
    "        error being raised or leading to silently incorrect gradients.\n",
    "\n",
    "    .. warning::\n",
    "\n",
    "        The ``use_reentrant`` parameter should be passed explicitly. In version\n",
    "        2.4 we will raise an exception if ``use_reentrant`` is not passed.\n",
    "        If you are using the ``use_reentrant=True`` variant, please refer to the\n",
    "        note below for important considerations and potential limitations.\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        The reentrant variant of checkpoint (``use_reentrant=True``) and\n",
    "        the non-reentrant variant of checkpoint (``use_reentrant=False``)\n",
    "        differ in the following ways:\n",
    "\n",
    "        * Non-reentrant checkpoint stops recomputation as soon as all needed\n",
    "          intermediate activations have been recomputed. This feature is enabled\n",
    "          by default, but can be disabled with :func:`set_checkpoint_early_stop`.\n",
    "          Reentrant checkpoint always recomputes :attr:`function` in its\n",
    "          entirety during the backward pass.\n",
    "\n",
    "        * The reentrant variant does not record the autograd graph during the\n",
    "          forward pass, as it runs with the forward pass under\n",
    "          :func:`torch.no_grad`. The non-reentrant version does record the\n",
    "          autograd graph, allowing one to perform backward on the graph within\n",
    "          checkpointed regions.\n",
    "\n",
    "        * The reentrant checkpoint only supports the\n",
    "          :func:`torch.autograd.backward` API for the backward pass without its\n",
    "          `inputs` argument, while the non-reentrant version supports all ways\n",
    "          of performing the backward pass.\n",
    "\n",
    "        * At least one input and output must have ``requires_grad=True`` for the\n",
    "          reentrant variant. If this condition is unmet, the checkpointed part\n",
    "          of the model will not have gradients. The non-reentrant version does\n",
    "          not have this requirement.\n",
    "\n",
    "        * The reentrant version does not consider tensors in nested structures\n",
    "          (e.g., custom objects, lists, dicts, etc) as participating in\n",
    "          autograd, while the non-reentrant version does.\n",
    "\n",
    "        * The reentrant checkpoint does not support checkpointed regions with\n",
    "          detached tensors from the computational graph, whereas the\n",
    "          non-reentrant version does. For the reentrant variant, if the\n",
    "          checkpointed segment contains tensors detached using ``detach()`` or\n",
    "          with :func:`torch.no_grad`, the backward pass will raise an error.\n",
    "          This is because ``checkpoint`` makes all the outputs require gradients\n",
    "          and this causes issues when a tensor is defined to have no gradient in\n",
    "          the model. To avoid this, detach the tensors outside of the\n",
    "          ``checkpoint`` function.\n",
    "\n",
    "    Args:\n",
    "        function: describes what to run in the forward pass of the model or\n",
    "            part of the model. It should also know how to handle the inputs\n",
    "            passed as the tuple. For example, in LSTM, if user passes\n",
    "            ``(activation, hidden)``, :attr:`function` should correctly use the\n",
    "            first input as ``activation`` and the second input as ``hidden``\n",
    "        preserve_rng_state(bool, optional):  Omit stashing and restoring\n",
    "            the RNG state during each checkpoint. Note that under torch.compile,\n",
    "            this flag doesn't take effect and we always preserve RNG state.\n",
    "            Default: ``True``\n",
    "        use_reentrant(bool):\n",
    "            specify whether to use the activation checkpoint variant that\n",
    "            requires reentrant autograd. This parameter should be passed\n",
    "            explicitly. In version 2.4 we will raise an exception if\n",
    "            ``use_reentrant`` is not passed. If ``use_reentrant=False``,\n",
    "            ``checkpoint`` will use an implementation that does not require\n",
    "            reentrant autograd. This allows ``checkpoint`` to support additional\n",
    "            functionality, such as working as expected with\n",
    "            ``torch.autograd.grad`` and support for keyword arguments input into\n",
    "            the checkpointed function.\n",
    "        context_fn(Callable, optional): A callable returning a tuple of two\n",
    "            context managers. The function and its recomputation will be run\n",
    "            under the first and second context managers respectively.\n",
    "            This argument is only supported if ``use_reentrant=False``.\n",
    "        determinism_check(str, optional): A string specifying the determinism\n",
    "            check to perform. By default it is set to ``\"default\"`` which\n",
    "            compares the shapes, dtypes, and devices of the recomputed tensors\n",
    "            against those the saved tensors. To turn off this check, specify\n",
    "            ``\"none\"``. Currently these are the only two supported values.\n",
    "            Please open an issue if you would like to see more determinism\n",
    "            checks. This argument is only supported if ``use_reentrant=False``,\n",
    "            if ``use_reentrant=True``, the determinism check is always disabled.\n",
    "        debug(bool, optional): If ``True``, error messages will also include\n",
    "            a trace of the operators ran during the original forward computation\n",
    "            as well as the recomputation. This argument is only supported if\n",
    "            ``use_reentrant=False``.\n",
    "        args: tuple containing inputs to the :attr:`function`\n",
    "\n",
    "    Returns:\n",
    "        Output of running :attr:`function` on :attr:`*args`\n",
    "    \"\"\"\n",
    "    if use_reentrant is None:\n",
    "        warnings.warn(\n",
    "            \"torch.utils.checkpoint: the use_reentrant parameter should be \"\n",
    "            \"passed explicitly. In version 2.4 we will raise an exception \"\n",
    "            \"if use_reentrant is not passed. use_reentrant=False is \"\n",
    "            \"recommended, but if you need to preserve the current default \"\n",
    "            \"behavior, you can pass use_reentrant=True. Refer to docs for more \"\n",
    "            \"details on the differences between the two variants.\",\n",
    "            stacklevel=2,\n",
    "        )\n",
    "        use_reentrant = True\n",
    "\n",
    "    # Hack to mix *args with **kwargs in a python 2.7-compliant way\n",
    "    preserve = kwargs.pop(\"preserve_rng_state\", True)\n",
    "    if kwargs and use_reentrant:\n",
    "        raise ValueError(\"Unexpected keyword arguments: \" + \",\".join(arg for arg in kwargs))\n",
    "\n",
    "    if use_reentrant:\n",
    "        if context_fn is not noop_context_fn or debug is not False:\n",
    "            raise ValueError(\"Passing `context_fn` or `debug` is only supported when \" \"use_reentrant=False.\")\n",
    "        return CheckpointFunctionWithOffload.apply(function, preserve, *args)\n",
    "    else:\n",
    "        gen = _checkpoint_without_reentrant_generator(\n",
    "            function, preserve, context_fn, determinism_check, debug, *args, **kwargs\n",
    "        )\n",
    "        # Runs pre-forward logic\n",
    "        next(gen)\n",
    "        ret = function(*args, **kwargs)\n",
    "        # Runs post-forward logic\n",
    "        try:\n",
    "            next(gen)\n",
    "        except StopIteration:\n",
    "            return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35765ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tensor(x, eps=1e-10):\n",
    "    norm_factor = torch.sqrt(torch.sum(x**2, dim=1, keepdim=True))\n",
    "    return x / (norm_factor + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da396c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_average(x, keepdim=True):\n",
    "    return x.mean([2, 3], keepdim=keepdim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c95a008",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LPIPS(nn.Module):\n",
    "    # Learned perceptual metric\n",
    "    def __init__(self, use_dropout=True):\n",
    "        super().__init__()\n",
    "        self.scaling_layer = ScalingLayer()\n",
    "        self.chns = [64, 128, 256, 512, 512]  # vg16 features\n",
    "        self.net = vgg16(pretrained=True, requires_grad=False)\n",
    "        self.lin0 = NetLinLayer(self.chns[0], use_dropout=use_dropout)\n",
    "        self.lin1 = NetLinLayer(self.chns[1], use_dropout=use_dropout)\n",
    "        self.lin2 = NetLinLayer(self.chns[2], use_dropout=use_dropout)\n",
    "        self.lin3 = NetLinLayer(self.chns[3], use_dropout=use_dropout)\n",
    "        self.lin4 = NetLinLayer(self.chns[4], use_dropout=use_dropout)\n",
    "        self.lins = [self.lin0, self.lin1, self.lin2, self.lin3, self.lin4]\n",
    "        self.load_from_pretrained()\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def load_from_pretrained(self, name=\"vgg_lpips\"):\n",
    "        path = os.path.expanduser(\"~/.cache/opensora/taming/modules/autoencoder/lpips\")\n",
    "        ckpt = get_ckpt_path(name, path)\n",
    "        self.load_state_dict(torch.load(ckpt, map_location=torch.device(\"cpu\")), strict=False)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, name=\"vgg_lpips\"):\n",
    "        if name != \"vgg_lpips\":\n",
    "            raise NotImplementedError\n",
    "        model = cls()\n",
    "        ckpt = get_ckpt_path(name)\n",
    "        model.load_state_dict(torch.load(ckpt, map_location=torch.device(\"cpu\")), strict=False)\n",
    "        return model\n",
    "\n",
    "    def forward_old(self, input, target):\n",
    "        in0_input, in1_input = (self.scaling_layer(input), self.scaling_layer(target))\n",
    "        outs0, outs1 = self.net(in0_input), self.net(in1_input)\n",
    "        feats0, feats1, diffs = {}, {}, {}\n",
    "        lins = [self.lin0, self.lin1, self.lin2, self.lin3, self.lin4]\n",
    "        for kk in range(len(self.chns)):\n",
    "            feats0[kk], feats1[kk] = normalize_tensor(outs0[kk]), normalize_tensor(outs1[kk])\n",
    "            diffs[kk] = (feats0[kk] - feats1[kk]) ** 2\n",
    "\n",
    "        res = [spatial_average(lins[kk].model(diffs[kk]), keepdim=True) for kk in range(len(self.chns))]\n",
    "        val = res[0]\n",
    "        for l in range(1, len(self.chns)):\n",
    "            val += res[l]\n",
    "        return val\n",
    "\n",
    "    def get_layer_loss(self, input, target, i):\n",
    "        input, target = getattr(self.net, f\"slice{i+1}\")(input), getattr(self.net, f\"slice{i+1}\")(target)\n",
    "        feats0, feats1 = normalize_tensor(input), normalize_tensor(target)\n",
    "        diff = (feats0 - feats1) ** 2\n",
    "        avg = spatial_average(self.lins[i].model(diff), keepdim=True)\n",
    "        return avg, input, target\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        input, target = (self.scaling_layer(input), self.scaling_layer(target))\n",
    "\n",
    "        val = None\n",
    "        for i in range(len(self.chns)):\n",
    "            avg, input, target = checkpoint(self.get_layer_loss, input, target, i, use_reentrant=False)\n",
    "            val = avg if val is None else val + avg\n",
    "        return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d3b026",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScalingLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScalingLayer, self).__init__()\n",
    "        self.register_buffer(\"shift\", torch.Tensor([-0.030, -0.088, -0.188])[None, :, None, None])\n",
    "        self.register_buffer(\"scale\", torch.Tensor([0.458, 0.448, 0.450])[None, :, None, None])\n",
    "\n",
    "    def forward(self, inp):\n",
    "        return (inp - self.shift) / self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f78c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1(x, y):\n",
    "    return torch.abs(x - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7797a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_mean(x):\n",
    "    return torch.sum(x) / x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19bad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAELoss(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        logvar_init=0.0,\n",
    "        perceptual_loss_weight=1.0,\n",
    "        kl_loss_weight=5e-4,\n",
    "        device=\"cpu\",\n",
    "        dtype=\"bf16\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if type(dtype) == str:\n",
    "            if dtype == \"bf16\":\n",
    "                dtype = torch.bfloat16\n",
    "            elif dtype == \"fp16\":\n",
    "                dtype = torch.float16\n",
    "            elif dtype == \"fp32\":\n",
    "                dtype = torch.float32\n",
    "            else:\n",
    "                raise NotImplementedError(f\"dtype: {dtype}\")\n",
    "\n",
    "        # KL Loss\n",
    "        self.kl_weight = kl_loss_weight\n",
    "        # Perceptual Loss\n",
    "        self.perceptual_loss_fn = LPIPS().eval().to(device, dtype)\n",
    "        self.perceptual_loss_fn.requires_grad_(False)\n",
    "        self.perceptual_loss_weight = perceptual_loss_weight\n",
    "        self.logvar = nn.Parameter(torch.ones(size=()) * logvar_init)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        video,\n",
    "        recon_video,\n",
    "        posterior,\n",
    "    ) -> dict:\n",
    "        video.size(0)\n",
    "        video = rearrange(video, \"b c t h w -> (b t) c h w\").contiguous()\n",
    "        recon_video = rearrange(recon_video, \"b c t h w -> (b t) c h w\").contiguous()\n",
    "\n",
    "        # reconstruction loss\n",
    "        recon_loss = l1(video, recon_video)\n",
    "\n",
    "        # perceptual loss\n",
    "        perceptual_loss = self.perceptual_loss_fn(video, recon_video)\n",
    "        # nll loss (from reconstruction loss and perceptual loss)\n",
    "        nll_loss = recon_loss + perceptual_loss * self.perceptual_loss_weight\n",
    "        nll_loss = nll_loss / torch.exp(self.logvar) + self.logvar\n",
    "\n",
    "        # Batch Mean\n",
    "        nll_loss = batch_mean(nll_loss)\n",
    "        recon_loss = batch_mean(recon_loss)\n",
    "        numel_elements = video.numel() // video.size(0)\n",
    "        perceptual_loss = batch_mean(perceptual_loss) * numel_elements\n",
    "\n",
    "        # KL Loss\n",
    "        if posterior is None:\n",
    "            kl_loss = torch.tensor(0.0).to(video.device, video.dtype)\n",
    "        else:\n",
    "            kl_loss = posterior.kl()\n",
    "            kl_loss = batch_mean(kl_loss)\n",
    "        weighted_kl_loss = kl_loss * self.kl_weight\n",
    "\n",
    "        return {\n",
    "            \"nll_loss\": nll_loss,\n",
    "            \"kl_loss\": weighted_kl_loss,\n",
    "            \"recon_loss\": recon_loss,\n",
    "            \"perceptual_loss\": perceptual_loss,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e257a57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_loss_fn = VAELoss(**cfg.vae_loss_config, device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9edd96b",
   "metadata": {},
   "source": [
    "### EMAãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0dbb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adafc774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_model_param_shape(model: torch.nn.Module) -> dict:\n",
    "    \"\"\"\n",
    "    Record the shape of the model parameters.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to record the parameter shape of.\n",
    "\n",
    "    Returns:\n",
    "        dict: The shape of the model parameters.\n",
    "    \"\"\"\n",
    "    param_shape = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        param_shape[name] = param.shape\n",
    "    return param_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc340ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == build EMA model ==\n",
    "if cfg.get(\"ema_decay\", None) is not None:\n",
    "    ema = deepcopy(model).cpu().eval().requires_grad_(False)\n",
    "    ema_shape_dict = record_model_param_shape(ema)\n",
    "    logger.info(\"EMA model created.\")\n",
    "else:\n",
    "    ema = ema_shape_dict = None\n",
    "    logger.info(\"No EMA model created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d1e10b",
   "metadata": {},
   "source": [
    "### ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒŸãƒãƒ¼ã‚¿ãƒ¼ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a32a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == build discriminator model ==\n",
    "use_discriminator = cfg.get(\"discriminator\", None) is not None\n",
    "if use_discriminator:\n",
    "    discriminator = build_module(cfg.discriminator, MODELS).to(device, dtype).train()\n",
    "    log_model_params(discriminator)\n",
    "    generator_loss_fn = GeneratorLoss(**cfg.gen_loss_config)\n",
    "    discriminator_loss_fn = DiscriminatorLoss(**cfg.disc_loss_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfe8087",
   "metadata": {},
   "source": [
    "### ã‚ªãƒ—ãƒ†ã‚£ãƒžã‚¤ã‚¶ã®ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe92ef0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensora.utils.optimizer import create_lr_scheduler, create_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b7f953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == setup optimizer ==\n",
    "optimizer = create_optimizer(model, cfg.optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c532a5",
   "metadata": {},
   "source": [
    "### è¨“ç·´ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã®ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef3a04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps_per_epoch = len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26438bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == setup lr scheduler ==\n",
    "lr_scheduler = create_lr_scheduler(\n",
    "    optimizer=optimizer, num_steps_per_epoch=num_steps_per_epoch, epochs=cfg.get(\"epochs\", 1000), **cfg.lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f2a7a1",
   "metadata": {},
   "source": [
    "### ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒŸãƒãƒ¼ã‚¿ãƒ¼ã‚ªãƒ—ãƒ†ã‚£ãƒžã‚¤ã‚¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1f0e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == setup discriminator optimizer ==\n",
    "if use_discriminator:\n",
    "    disc_optimizer = create_optimizer(discriminator, cfg.optim_discriminator)\n",
    "    disc_lr_scheduler = create_lr_scheduler(\n",
    "        optimizer=disc_optimizer,\n",
    "        num_steps_per_epoch=num_steps_per_epoch,\n",
    "        epochs=cfg.get(\"epochs\", 1000),\n",
    "        **cfg.disc_lr_scheduler,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb642e84",
   "metadata": {},
   "source": [
    "### è¨“ç·´ã®æº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319e1931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# 4. distributed training preparation with colossalai\n",
    "# =======================================================\n",
    "logger.info(\"Preparing for distributed training...\")\n",
    "# == boosting ==\n",
    "torch.set_default_dtype(dtype)\n",
    "model, optimizer, _, dataloader, lr_scheduler = booster.boost(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    dataloader=dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73f47b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# False\n",
    "\n",
    "if use_discriminator:\n",
    "    discriminator, disc_optimizer, _, _, disc_lr_scheduler = booster.boost(\n",
    "        model=discriminator,\n",
    "        optimizer=disc_optimizer,\n",
    "        lr_scheduler=disc_lr_scheduler,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2a9344",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float)\n",
    "logger.info(\"Boosted model for distributed training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba8a3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_reduce_sum(tensor: torch.Tensor) -> torch.Tensor:\n",
    "    dist.all_reduce(tensor=tensor, group=get_data_parallel_group())\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d6fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == global variables ==\n",
    "cfg_epochs = cfg.get(\"epochs\", 1000)\n",
    "mixed_strategy = cfg.get(\"mixed_strategy\", None)\n",
    "mixed_image_ratio = cfg.get(\"mixed_image_ratio\", 0.0)\n",
    "# modulate mixed image ratio since we force rank 0 to be video\n",
    "num_ranks = dist.get_world_size()\n",
    "modulated_mixed_image_ratio = (\n",
    "    num_ranks * mixed_image_ratio / (num_ranks - 1) if num_ranks > 1 else mixed_image_ratio\n",
    ")\n",
    "if is_log_process(plugin_type, plugin_config):\n",
    "    print(\"modulated mixed image ratio:\", modulated_mixed_image_ratio)\n",
    "\n",
    "start_epoch = start_step = log_step = acc_step = 0\n",
    "running_loss = dict(  # loss accumulated over config.log_every steps\n",
    "    all=0.0,\n",
    "    nll=0.0,\n",
    "    nll_rec=0.0,\n",
    "    nll_per=0.0,\n",
    "    kl=0.0,\n",
    "    gen=0.0,\n",
    "    gen_w=0.0,\n",
    "    disc=0.0,\n",
    "    debug=0.0,\n",
    ")\n",
    "\n",
    "def log_loss(name, loss, loss_dict, use_video):\n",
    "    # only calculate loss for video\n",
    "    if use_video == 0:\n",
    "        loss.data = torch.tensor(0.0, device=device, dtype=dtype)\n",
    "    all_reduce_sum(loss.data)\n",
    "    num_video = torch.tensor(use_video, device=device, dtype=dtype)\n",
    "    all_reduce_sum(num_video)\n",
    "    loss_item = loss.item() / num_video.item()\n",
    "    loss_dict[name] = loss_item\n",
    "    running_loss[name] += loss_item\n",
    "\n",
    "logger.info(\"Training for %s epochs with %s steps per epoch\", cfg_epochs, num_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90f234e",
   "metadata": {},
   "source": [
    "### EMAãƒ¢ãƒ‡ãƒ«ã‚’ã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602f0962",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_sharding(model: torch.nn.Module, device: torch.device = None):\n",
    "    \"\"\"\n",
    "    Sharding the model parameters across multiple GPUs.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to shard.\n",
    "        device (torch.device): The device to shard the model to.\n",
    "    \"\"\"\n",
    "    global_rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "    for _, param in model.named_parameters():\n",
    "        if device is None:\n",
    "            device = param.device\n",
    "        padding_size = (world_size - param.numel() % world_size) % world_size\n",
    "        if padding_size > 0:\n",
    "            padding_param = torch.nn.functional.pad(param.data.view(-1), [0, padding_size])\n",
    "        else:\n",
    "            padding_param = param.data.view(-1)\n",
    "        splited_params = padding_param.split(padding_param.numel() // world_size)\n",
    "        splited_params = splited_params[global_rank]\n",
    "        param.data = splited_params.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26bb06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == sharding EMA model ==\n",
    "if ema is not None:\n",
    "    model_sharding(ema)\n",
    "    ema = ema.to(device)\n",
    "\n",
    "if cfg.get(\"freeze_layers\", None) == \"all\":\n",
    "    for param in model.module.parameters():\n",
    "        param.requires_grad = False\n",
    "    print(\"all layers frozen\")\n",
    "\n",
    "# model.module.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528c0f68",
   "metadata": {},
   "source": [
    "### å†é–‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73b9521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == resume ==\n",
    "if cfg.get(\"load\", None) is not None:\n",
    "    logger.info(\"Loading checkpoint from %s\", cfg.load)\n",
    "    start_epoch = cfg.get(\"start_epoch\", None)\n",
    "    start_step = cfg.get(\"start_step\", None)\n",
    "    ret = checkpoint_io.load(\n",
    "        booster,\n",
    "        cfg.load,\n",
    "        model=model,\n",
    "        ema=ema,\n",
    "        optimizer=optimizer,\n",
    "        lr_scheduler=lr_scheduler,\n",
    "        sampler=(\n",
    "            None if start_step is not None else sampler\n",
    "        ),  # if specify start step, set last_micro_batch_access_index of a new sampler instead\n",
    "    )\n",
    "    if start_step is not None:\n",
    "        # if start step exceeds data length, go to next epoch\n",
    "        if start_step > num_steps_per_epoch:\n",
    "            start_epoch = (\n",
    "                start_epoch + start_step // num_steps_per_epoch\n",
    "                if start_epoch is not None\n",
    "                else start_step // num_steps_per_epoch\n",
    "            )\n",
    "            start_step = start_step % num_steps_per_epoch\n",
    "        sampler.set_step(start_step)\n",
    "\n",
    "    start_epoch = start_epoch if start_epoch is not None else ret[0]\n",
    "    start_step = start_step if start_step is not None else ret[1]\n",
    "\n",
    "    if (\n",
    "        use_discriminator\n",
    "        and os.path.exists(os.path.join(cfg.load, \"discriminator\"))\n",
    "        and not cfg.get(\"restart_disc\", False)\n",
    "    ):\n",
    "        booster.load_model(discriminator, os.path.join(cfg.load, \"discriminator\"))\n",
    "        if cfg.get(\"load_optimizer\", True):\n",
    "            booster.load_optimizer(disc_optimizer, os.path.join(cfg.load, \"disc_optimizer\"))\n",
    "            if disc_lr_scheduler is not None:\n",
    "                booster.load_lr_scheduler(disc_lr_scheduler, os.path.join(cfg.load, \"disc_lr_scheduler\"))\n",
    "            if cfg.get(\"disc_lr\", None) is not None:\n",
    "                set_lr(disc_optimizer, disc_lr_scheduler, cfg.disc_lr)\n",
    "\n",
    "    logger.info(\"Loaded checkpoint %s at epoch %s step %s\", cfg.load, start_epoch, start_step)\n",
    "\n",
    "    if cfg.get(\"lr\", None) is not None:\n",
    "        set_lr(optimizer, lr_scheduler, cfg.lr, cfg.get(\"initial_lr\", None))\n",
    "\n",
    "    if cfg.get(\"update_warmup_steps\", False):\n",
    "        assert (\n",
    "            cfg.lr_scheduler.get(\"warmup_steps\", None) is not None\n",
    "        ), \"you need to set lr_scheduler.warmup_steps in order to pass --update-warmup-steps True\"\n",
    "        set_warmup_steps(lr_scheduler, cfg.lr_scheduler.warmup_steps)\n",
    "        if use_discriminator:\n",
    "            assert (\n",
    "                cfg.disc_lr_scheduler.get(\"warmup_steps\", None) is not None\n",
    "            ), \"you need to set disc_lr_scheduler.warmup_steps in order to pass --update-warmup-steps True\"\n",
    "            set_warmup_steps(disc_lr_scheduler, cfg.disc_lr_scheduler.warmup_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3d82a8",
   "metadata": {},
   "source": [
    "## è¨“ç·´ãƒ«ãƒ¼ãƒ—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108c67b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import random\n",
    "import subprocess\n",
    "import warnings\n",
    "from contextlib import nullcontext\n",
    "from copy import deepcopy\n",
    "from pprint import pformat\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "gc.disable()\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from colossalai.booster import Booster\n",
    "from colossalai.utils import set_seed\n",
    "from torch.profiler import ProfilerActivity, profile, schedule\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e12596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# 5. training loop\n",
    "# =======================================================\n",
    "dist.barrier()\n",
    "\n",
    "cfg[\"accumulation_steps\"] = 128\n",
    "accumulation_steps = int(cfg.get(\"accumulation_steps\", 1))\n",
    "logger.info(\"Using gradient accumulation steps: %s\", accumulation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7127104",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, cfg_epochs):\n",
    "    # == set dataloader to new epoch ==\n",
    "    sampler.set_epoch(epoch)\n",
    "    dataiter = iter(dataloader)\n",
    "    logger.info(\"Beginning epoch %s...\", epoch)\n",
    "    random.seed(1024 + dist.get_rank())  # load vid/img for each rank\n",
    "\n",
    "    # == training loop in an epoch ==\n",
    "    with tqdm(\n",
    "        enumerate(dataiter, start=start_step),\n",
    "        desc=f\"Epoch {epoch}\",\n",
    "        disable=not coordinator.is_master(),\n",
    "        total=num_steps_per_epoch,\n",
    "        initial=start_step,\n",
    "    ) as pbar:\n",
    "        pbar_iter = iter(pbar)\n",
    "\n",
    "        def fetch_data():\n",
    "            step, batch = next(pbar_iter)\n",
    "            pinned_video = batch[\"video\"]\n",
    "            batch[\"video\"] = pinned_video.to(device, dtype, non_blocking=True)\n",
    "            return batch, step, pinned_video\n",
    "\n",
    "        batch_, step_, pinned_video_ = fetch_data()\n",
    "\n",
    "        profiler_ctxt = (\n",
    "            profile(\n",
    "                activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "                schedule=my_schedule,\n",
    "                on_trace_ready=torch.profiler.tensorboard_trace_handler(\"./log/profile\"),\n",
    "                record_shapes=True,\n",
    "                profile_memory=True,\n",
    "                with_stack=True,\n",
    "            )\n",
    "            if cfg.get(\"profile\", False)\n",
    "            else nullcontext()\n",
    "        )\n",
    "\n",
    "        with profiler_ctxt:\n",
    "            for _ in range(start_step, num_steps_per_epoch):\n",
    "                if cfg.get(\"profile\", False) and _ == WARMUP + ACTIVE + WAIT + 3:\n",
    "                    break\n",
    "\n",
    "                # == load data ===\n",
    "                batch, step, pinned_video = batch_, step_, pinned_video_\n",
    "                if step + 1 < num_steps_per_epoch:\n",
    "                    batch_, step_, pinned_video_ = fetch_data()\n",
    "\n",
    "                # == log config ==\n",
    "                global_step = epoch * num_steps_per_epoch + step\n",
    "                actual_update_step = (global_step + 1) // accumulation_steps\n",
    "                log_step += 1\n",
    "                acc_step += 1\n",
    "\n",
    "                # == mixed strategy ==\n",
    "                x = batch[\"video\"]\n",
    "                t_length = x.size(2)\n",
    "                use_video = 1\n",
    "                if mixed_strategy == \"mixed_video_image\":\n",
    "                    if random.random() < modulated_mixed_image_ratio and dist.get_rank() != 0:\n",
    "                        # NOTE: enable the first rank to use video\n",
    "                        t_length = 1\n",
    "                        use_video = 0\n",
    "                elif mixed_strategy == \"mixed_video_random\":\n",
    "                    t_length = random.randint(1, x.size(2))\n",
    "                x = x[:, :, :t_length, :, :]\n",
    "\n",
    "                with Timer(\"model\", log=True) if cfg.get(\"profile\", False) else nullcontext():\n",
    "                    # == forward pass ==\n",
    "                    x_rec, posterior, z = model(x)\n",
    "\n",
    "                    if cfg.get(\"profile\", False):\n",
    "                        profiler_ctxt.step()\n",
    "\n",
    "                    if cache_pin_memory:\n",
    "                        dataiter.remove_cache(pinned_video)\n",
    "\n",
    "                    # == loss initialization ==\n",
    "                    vae_loss = torch.tensor(0.0, device=device, dtype=dtype)\n",
    "                    loss_dict = {}  # loss at every step\n",
    "\n",
    "                    # == reconstruction loss ==\n",
    "                    ret = vae_loss_fn(x, x_rec, posterior)\n",
    "                    nll_loss = ret[\"nll_loss\"]\n",
    "                    kl_loss = ret[\"kl_loss\"]\n",
    "                    recon_loss = ret[\"recon_loss\"]\n",
    "                    perceptual_loss = ret[\"perceptual_loss\"]\n",
    "                    vae_loss += nll_loss + kl_loss\n",
    "\n",
    "                    # == generator loss ==\n",
    "                    if use_discriminator:\n",
    "                        # turn off grad update for disc\n",
    "                        discriminator.requires_grad_(False)\n",
    "                        fake_logits = discriminator(x_rec.contiguous())\n",
    "\n",
    "                        generator_loss, g_loss = generator_loss_fn(\n",
    "                            fake_logits,\n",
    "                            nll_loss,\n",
    "                            model.module.get_last_layer(),\n",
    "                            actual_update_step,\n",
    "                            is_training=model.training,\n",
    "                        )\n",
    "                        # print(f\"generator_loss: {generator_loss}, recon_loss: {recon_loss}, perceptual_loss: {perceptual_loss}\")\n",
    "\n",
    "                        vae_loss += generator_loss\n",
    "                        # turn on disc training\n",
    "                        discriminator.requires_grad_(True)\n",
    "\n",
    "                    # == generator backward & update ==\n",
    "                    ctx = (\n",
    "                        booster.no_sync(model, optimizer)\n",
    "                        if cfg.get(\"plugin\", \"zero2\") in (\"zero1\", \"zero1-seq\")\n",
    "                        and (step + 1) % accumulation_steps != 0\n",
    "                        else nullcontext()\n",
    "                    )\n",
    "                    with Timer(\"backward\", log=True) if cfg.get(\"profile\", False) else nullcontext():\n",
    "                        with ctx:\n",
    "                            booster.backward(loss=vae_loss / accumulation_steps, optimizer=optimizer)\n",
    "\n",
    "                    with Timer(\"optimizer\", log=True) if cfg.get(\"profile\", False) else nullcontext():\n",
    "                        if (step + 1) % accumulation_steps == 0:\n",
    "                            optimizer.step()\n",
    "                            optimizer.zero_grad()\n",
    "                            if lr_scheduler is not None:\n",
    "                                lr_scheduler.step(\n",
    "                                    actual_update_step,\n",
    "                                )\n",
    "                            # == update EMA ==\n",
    "                            if ema is not None:\n",
    "                                update_ema(\n",
    "                                    ema,\n",
    "                                    model.unwrap(),\n",
    "                                    optimizer=optimizer,\n",
    "                                    decay=cfg.get(\"ema_decay\", 0.9999),\n",
    "                                )\n",
    "\n",
    "                # == logging ==\n",
    "                log_loss(\"all\", vae_loss, loss_dict, use_video)\n",
    "                log_loss(\"nll\", nll_loss, loss_dict, use_video)\n",
    "                log_loss(\"nll_rec\", recon_loss, loss_dict, use_video)\n",
    "                log_loss(\"nll_per\", perceptual_loss, loss_dict, use_video)\n",
    "                log_loss(\"kl\", kl_loss, loss_dict, use_video)\n",
    "                if use_discriminator:\n",
    "                    log_loss(\"gen_w\", generator_loss, loss_dict, use_video)\n",
    "                    log_loss(\"gen\", g_loss, loss_dict, use_video)\n",
    "\n",
    "                # == loss: discriminator adversarial ==\n",
    "                if use_discriminator:\n",
    "                    real_logits = discriminator(x.detach().contiguous())\n",
    "                    fake_logits = discriminator(x_rec.detach().contiguous())\n",
    "                    disc_loss = discriminator_loss_fn(\n",
    "                        real_logits,\n",
    "                        fake_logits,\n",
    "                        actual_update_step,\n",
    "                    )\n",
    "\n",
    "                    # == discriminator backward & update ==\n",
    "                    ctx = (\n",
    "                        booster.no_sync(discriminator, disc_optimizer)\n",
    "                        if cfg.get(\"plugin\", \"zero2\") in (\"zero1\", \"zero1-seq\")\n",
    "                        and (step + 1) % accumulation_steps != 0\n",
    "                        else nullcontext()\n",
    "                    )\n",
    "                    with ctx:\n",
    "                        booster.backward(loss=disc_loss / accumulation_steps, optimizer=disc_optimizer)\n",
    "                    if (step + 1) % accumulation_steps == 0:\n",
    "                        disc_optimizer.step()\n",
    "                        disc_optimizer.zero_grad()\n",
    "                        if disc_lr_scheduler is not None:\n",
    "                            disc_lr_scheduler.step(actual_update_step)\n",
    "\n",
    "                    # log\n",
    "                    log_loss(\"disc\", disc_loss, loss_dict, use_video)\n",
    "\n",
    "                # == logging ==\n",
    "                if (global_step + 1) % accumulation_steps == 0:\n",
    "                    if coordinator.is_master() and actual_update_step % cfg.get(\"log_every\", 1) == 0:\n",
    "                        avg_loss = {k: v / log_step for k, v in running_loss.items()}\n",
    "                        # progress bar\n",
    "                        pbar.set_postfix(\n",
    "                            {\n",
    "                                # \"step\": step,\n",
    "                                # \"global_step\": global_step,\n",
    "                                # \"actual_update_step\": actual_update_step,\n",
    "                                # \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "                                **{k: f\"{v:.2f}\" for k, v in avg_loss.items()},\n",
    "                            }\n",
    "                        )\n",
    "                        # tensorboard\n",
    "                        tb_writer.add_scalar(\"loss\", vae_loss.item(), actual_update_step)\n",
    "                        # wandb\n",
    "                        if cfg.get(\"wandb\", False):\n",
    "                            wandb.log(\n",
    "                                {\n",
    "                                    \"iter\": global_step,\n",
    "                                    \"epoch\": epoch,\n",
    "                                    \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "                                    \"avg_loss_\": avg_loss,\n",
    "                                    \"avg_loss\": avg_loss[\"all\"],\n",
    "                                    \"loss_\": loss_dict,\n",
    "                                    \"loss\": vae_loss.item(),\n",
    "                                    \"global_grad_norm\": optimizer.get_grad_norm(),\n",
    "                                },\n",
    "                                step=actual_update_step,\n",
    "                            )\n",
    "\n",
    "                        running_loss = {k: 0.0 for k in running_loss}\n",
    "                        log_step = 0\n",
    "\n",
    "                    # == checkpoint saving ==\n",
    "                    ckpt_every = cfg.get(\"ckpt_every\", 0)\n",
    "                    if ckpt_every > 0 and actual_update_step % ckpt_every == 0 and coordinator.is_master():\n",
    "                        subprocess.run(\"sudo drop_cache\", shell=True)\n",
    "\n",
    "                    if ckpt_every > 0 and actual_update_step % ckpt_every == 0:\n",
    "                        # mannually garbage collection\n",
    "                        gc.collect()\n",
    "\n",
    "                        save_dir = checkpoint_io.save(\n",
    "                            booster,\n",
    "                            exp_dir,\n",
    "                            model=model,\n",
    "                            ema=ema,\n",
    "                            optimizer=optimizer,\n",
    "                            lr_scheduler=lr_scheduler,\n",
    "                            sampler=sampler,\n",
    "                            epoch=epoch,\n",
    "                            step=step + 1,\n",
    "                            global_step=global_step + 1,\n",
    "                            batch_size=cfg.get(\"batch_size\", None),\n",
    "                            actual_update_step=actual_update_step,\n",
    "                            ema_shape_dict=ema_shape_dict,\n",
    "                            async_io=True,\n",
    "                        )\n",
    "\n",
    "                        if is_log_process(plugin_type, plugin_config):\n",
    "                            os.system(f\"chgrp -R share {save_dir}\")\n",
    "\n",
    "                        if use_discriminator:\n",
    "                            booster.save_model(discriminator, os.path.join(save_dir, \"discriminator\"), shard=True)\n",
    "                            booster.save_optimizer(\n",
    "                                disc_optimizer,\n",
    "                                os.path.join(save_dir, \"disc_optimizer\"),\n",
    "                                shard=True,\n",
    "                                size_per_shard=4096,\n",
    "                            )\n",
    "                            if disc_lr_scheduler is not None:\n",
    "                                booster.save_lr_scheduler(\n",
    "                                    disc_lr_scheduler, os.path.join(save_dir, \"disc_lr_scheduler\")\n",
    "                                )\n",
    "                        dist.barrier()\n",
    "\n",
    "                        logger.info(\n",
    "                            \"Saved checkpoint at epoch %s, step %s, global_step %s to %s\",\n",
    "                            epoch,\n",
    "                            step + 1,\n",
    "                            actual_update_step,\n",
    "                            save_dir,\n",
    "                        )\n",
    "\n",
    "                        # remove old checkpoints\n",
    "                        rm_checkpoints(exp_dir, keep_n_latest=cfg.get(\"keep_n_latest\", -1))\n",
    "                        logger.info(\n",
    "                            \"Removed old checkpoints and kept %s latest ones.\", cfg.get(\"keep_n_latest\", -1)\n",
    "                        )\n",
    "\n",
    "        if cfg.get(\"profile\", False):\n",
    "            profiler_ctxt.export_chrome_trace(\"./log/profile/trace.json\")\n",
    "\n",
    "    sampler.reset()\n",
    "    start_step = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
